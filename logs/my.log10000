nohup: ignoring input


starting seed  10000 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-342.64 +/- 201.23
Episode length: 343.02 +/- 200.74
New best mean reward!
Eval num_timesteps=10000, episode_reward=-337.51 +/- 199.48
Episode length: 337.91 +/- 198.99
New best mean reward!
Eval num_timesteps=10500, episode_reward=-264.89 +/- 204.65
Episode length: 265.46 +/- 204.15
New best mean reward!
Eval num_timesteps=11000, episode_reward=-197.13 +/- 184.84
Episode length: 197.86 +/- 184.40
New best mean reward!
Eval num_timesteps=11500, episode_reward=-143.30 +/- 145.51
Episode length: 144.16 +/- 145.16
New best mean reward!
Eval num_timesteps=12000, episode_reward=-143.53 +/- 145.94
Episode length: 144.39 +/- 145.60
Eval num_timesteps=12500, episode_reward=-217.74 +/- 194.13
Episode length: 218.42 +/- 193.66
Eval num_timesteps=13000, episode_reward=-189.89 +/- 179.88
Episode length: 190.64 +/- 179.45
Eval num_timesteps=13500, episode_reward=-121.38 +/- 114.07
Episode length: 122.30 +/- 113.81
New best mean reward!
Eval num_timesteps=14000, episode_reward=-96.41 +/- 73.60
Episode length: 97.38 +/- 73.44
New best mean reward!
FINISHED IN 158.68731860799016 s


starting seed  10001 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-492.34 +/- 53.62
Episode length: 492.36 +/- 53.48
New best mean reward!
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-467.43 +/- 106.73
Episode length: 467.52 +/- 106.45
New best mean reward!
Eval num_timesteps=15500, episode_reward=-411.85 +/- 163.90
Episode length: 412.08 +/- 163.49
New best mean reward!
Eval num_timesteps=16000, episode_reward=-434.06 +/- 146.12
Episode length: 434.23 +/- 145.74
Eval num_timesteps=16500, episode_reward=-394.90 +/- 174.72
Episode length: 395.17 +/- 174.28
New best mean reward!
Eval num_timesteps=17000, episode_reward=-355.54 +/- 191.17
Episode length: 355.91 +/- 190.69
New best mean reward!
Eval num_timesteps=17500, episode_reward=-168.37 +/- 162.65
Episode length: 169.18 +/- 162.26
New best mean reward!
Eval num_timesteps=18000, episode_reward=-122.20 +/- 115.19
Episode length: 123.12 +/- 114.92
New best mean reward!
Eval num_timesteps=18500, episode_reward=-108.64 +/- 87.84
Episode length: 109.60 +/- 87.66
New best mean reward!
Eval num_timesteps=19000, episode_reward=-112.24 +/- 103.11
Episode length: 113.18 +/- 102.88
Eval num_timesteps=19500, episode_reward=-122.05 +/- 110.97
Episode length: 122.98 +/- 110.73
Eval num_timesteps=20000, episode_reward=-109.89 +/- 92.68
Episode length: 110.84 +/- 92.47
Eval num_timesteps=20500, episode_reward=-123.63 +/- 106.97
Episode length: 124.58 +/- 106.80
Eval num_timesteps=21000, episode_reward=-125.21 +/- 114.25
Episode length: 126.14 +/- 114.02
Eval num_timesteps=21500, episode_reward=-146.59 +/- 136.74
Episode length: 147.47 +/- 136.43
Eval num_timesteps=22000, episode_reward=-128.59 +/- 120.52
Episode length: 129.50 +/- 120.25
Eval num_timesteps=22500, episode_reward=-110.75 +/- 85.64
Episode length: 111.71 +/- 85.46
Eval num_timesteps=23000, episode_reward=-121.10 +/- 108.47
Episode length: 122.03 +/- 108.23
Eval num_timesteps=23500, episode_reward=-124.16 +/- 116.28
Episode length: 125.09 +/- 116.06
Eval num_timesteps=24000, episode_reward=-116.57 +/- 104.25
Episode length: 117.51 +/- 104.03
Eval num_timesteps=24500, episode_reward=-124.00 +/- 109.75
Episode length: 124.94 +/- 109.54
Eval num_timesteps=25000, episode_reward=-94.79 +/- 53.18
Episode length: 95.78 +/- 53.11
New best mean reward!
FINISHED IN 467.4739648430259 s


starting seed  10002 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-492.89 +/- 50.82
Episode length: 492.91 +/- 50.68
New best mean reward!
Eval num_timesteps=8000, episode_reward=-292.42 +/- 198.73
Episode length: 292.95 +/- 198.24
New best mean reward!
Eval num_timesteps=8500, episode_reward=-308.24 +/- 198.01
Episode length: 308.73 +/- 197.52
Eval num_timesteps=9000, episode_reward=-256.04 +/- 196.75
Episode length: 256.66 +/- 196.28
New best mean reward!
Eval num_timesteps=9500, episode_reward=-276.25 +/- 199.08
Episode length: 276.82 +/- 198.60
Eval num_timesteps=10000, episode_reward=-174.00 +/- 158.70
Episode length: 174.82 +/- 158.33
New best mean reward!
Eval num_timesteps=10500, episode_reward=-148.28 +/- 139.68
Episode length: 149.16 +/- 139.38
New best mean reward!
Eval num_timesteps=11000, episode_reward=-137.89 +/- 123.06
Episode length: 138.80 +/- 122.80
New best mean reward!
Eval num_timesteps=11500, episode_reward=-141.29 +/- 127.38
Episode length: 142.20 +/- 127.13
Eval num_timesteps=12000, episode_reward=-162.66 +/- 143.27
Episode length: 163.52 +/- 142.94
Eval num_timesteps=12500, episode_reward=-178.76 +/- 168.11
Episode length: 179.55 +/- 167.71
Eval num_timesteps=13000, episode_reward=-174.03 +/- 162.78
Episode length: 174.85 +/- 162.42
Eval num_timesteps=13500, episode_reward=-126.32 +/- 114.57
Episode length: 127.24 +/- 114.31
New best mean reward!
Eval num_timesteps=14000, episode_reward=-106.15 +/- 55.58
Episode length: 107.14 +/- 55.51
New best mean reward!
Eval num_timesteps=14500, episode_reward=-115.49 +/- 93.52
Episode length: 116.45 +/- 93.36
Eval num_timesteps=15000, episode_reward=-107.19 +/- 56.72
Episode length: 108.19 +/- 56.72
Eval num_timesteps=15500, episode_reward=-107.18 +/- 71.31
Episode length: 108.16 +/- 71.20
Eval num_timesteps=16000, episode_reward=-97.54 +/- 47.84
Episode length: 98.53 +/- 47.76
New best mean reward!
FINISHED IN 241.38692555302987 s


starting seed  10003 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-439.43 +/- 144.32
Episode length: 439.58 +/- 143.96
New best mean reward!
Eval num_timesteps=7500, episode_reward=-178.51 +/- 165.28
Episode length: 179.31 +/- 164.90
New best mean reward!
Eval num_timesteps=8000, episode_reward=-280.53 +/- 204.12
Episode length: 281.07 +/- 203.63
Eval num_timesteps=8500, episode_reward=-225.20 +/- 189.76
Episode length: 225.88 +/- 189.29
Eval num_timesteps=9000, episode_reward=-230.84 +/- 193.80
Episode length: 231.51 +/- 193.34
Eval num_timesteps=9500, episode_reward=-274.99 +/- 204.74
Episode length: 275.54 +/- 204.25
Eval num_timesteps=10000, episode_reward=-311.72 +/- 204.46
Episode length: 312.18 +/- 203.97
Eval num_timesteps=10500, episode_reward=-110.68 +/- 91.68
Episode length: 111.63 +/- 91.47
New best mean reward!
Eval num_timesteps=11000, episode_reward=-85.72 +/- 18.26
Episode length: 86.72 +/- 18.26
New best mean reward!
FINISHED IN 221.31362366501708 s


starting seed  10004 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-431.50 +/- 151.51
Episode length: 431.67 +/- 151.14
New best mean reward!
Eval num_timesteps=7500, episode_reward=-174.99 +/- 155.19
Episode length: 175.81 +/- 154.81
New best mean reward!
Eval num_timesteps=8000, episode_reward=-97.08 +/- 38.47
Episode length: 98.08 +/- 38.47
New best mean reward!
FINISHED IN 198.7507241150015 s


starting seed  10005 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-184.22 +/- 51.58
Episode length: 185.21 +/- 51.52
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-488.25 +/- 57.99
Episode length: 488.29 +/- 57.80
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-479.60 +/- 75.51
Episode length: 479.67 +/- 75.25
Eval num_timesteps=13000, episode_reward=-494.84 +/- 26.89
Episode length: 494.89 +/- 26.71
Eval num_timesteps=13500, episode_reward=-407.03 +/- 152.54
Episode length: 407.31 +/- 152.10
Eval num_timesteps=14000, episode_reward=-412.28 +/- 157.02
Episode length: 412.52 +/- 156.60
Eval num_timesteps=14500, episode_reward=-356.98 +/- 191.38
Episode length: 357.34 +/- 190.90
Eval num_timesteps=15000, episode_reward=-359.62 +/- 191.80
Episode length: 359.97 +/- 191.32
Eval num_timesteps=15500, episode_reward=-368.52 +/- 172.52
Episode length: 368.89 +/- 172.04
Eval num_timesteps=16000, episode_reward=-271.55 +/- 195.79
Episode length: 272.13 +/- 195.30
Eval num_timesteps=16500, episode_reward=-204.19 +/- 181.30
Episode length: 204.92 +/- 180.86
Eval num_timesteps=17000, episode_reward=-130.37 +/- 113.94
Episode length: 131.29 +/- 113.68
New best mean reward!
Eval num_timesteps=17500, episode_reward=-112.11 +/- 91.49
Episode length: 113.06 +/- 91.28
New best mean reward!
Eval num_timesteps=18000, episode_reward=-110.68 +/- 76.42
Episode length: 111.66 +/- 76.32
New best mean reward!
Eval num_timesteps=18500, episode_reward=-133.68 +/- 124.47
Episode length: 134.58 +/- 124.18
Eval num_timesteps=19000, episode_reward=-140.03 +/- 129.90
Episode length: 140.92 +/- 129.60
Eval num_timesteps=19500, episode_reward=-92.86 +/- 47.54
Episode length: 93.85 +/- 47.46
New best mean reward!
FINISHED IN 444.01502203498967 s


starting seed  10006 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-496.40 +/- 35.82
Episode length: 496.41 +/- 35.72
New best mean reward!
Eval num_timesteps=3000, episode_reward=-457.12 +/- 112.21
Episode length: 457.25 +/- 111.88
New best mean reward!
Eval num_timesteps=3500, episode_reward=-169.62 +/- 64.23
Episode length: 170.60 +/- 64.13
New best mean reward!
Eval num_timesteps=4000, episode_reward=-185.77 +/- 66.33
Episode length: 186.75 +/- 66.24
Eval num_timesteps=4500, episode_reward=-162.23 +/- 33.69
Episode length: 163.23 +/- 33.69
New best mean reward!
Eval num_timesteps=5000, episode_reward=-435.01 +/- 127.94
Episode length: 435.22 +/- 127.54
Eval num_timesteps=5500, episode_reward=-377.61 +/- 158.77
Episode length: 377.99 +/- 158.29
Eval num_timesteps=6000, episode_reward=-360.92 +/- 163.92
Episode length: 361.35 +/- 163.44
Eval num_timesteps=6500, episode_reward=-282.41 +/- 168.40
Episode length: 283.04 +/- 167.92
Eval num_timesteps=7000, episode_reward=-151.17 +/- 45.25
Episode length: 152.17 +/- 45.25
New best mean reward!
Eval num_timesteps=7500, episode_reward=-145.85 +/- 44.16
Episode length: 146.84 +/- 44.08
New best mean reward!
Eval num_timesteps=8000, episode_reward=-120.95 +/- 25.86
Episode length: 121.95 +/- 25.86
New best mean reward!
Eval num_timesteps=8500, episode_reward=-158.88 +/- 78.63
Episode length: 159.84 +/- 78.46
Eval num_timesteps=9000, episode_reward=-134.62 +/- 30.82
Episode length: 135.62 +/- 30.82
Eval num_timesteps=9500, episode_reward=-148.31 +/- 89.37
Episode length: 149.26 +/- 89.17
Eval num_timesteps=10000, episode_reward=-122.40 +/- 26.31
Episode length: 123.40 +/- 26.31
Eval num_timesteps=10500, episode_reward=-113.85 +/- 20.35
Episode length: 114.85 +/- 20.35
New best mean reward!
Eval num_timesteps=11000, episode_reward=-118.02 +/- 44.12
Episode length: 119.01 +/- 44.04
Eval num_timesteps=11500, episode_reward=-120.93 +/- 43.71
Episode length: 121.93 +/- 43.71
Eval num_timesteps=12000, episode_reward=-124.39 +/- 24.05
Episode length: 125.39 +/- 24.05
Eval num_timesteps=12500, episode_reward=-126.74 +/- 22.38
Episode length: 127.74 +/- 22.38
Eval num_timesteps=13000, episode_reward=-122.83 +/- 22.49
Episode length: 123.83 +/- 22.49
Eval num_timesteps=13500, episode_reward=-138.42 +/- 46.42
Episode length: 139.41 +/- 46.34
Eval num_timesteps=14000, episode_reward=-154.78 +/- 88.75
Episode length: 155.73 +/- 88.55
Eval num_timesteps=14500, episode_reward=-323.90 +/- 181.13
Episode length: 324.40 +/- 180.65
Eval num_timesteps=15000, episode_reward=-166.42 +/- 118.97
Episode length: 167.31 +/- 118.66
Eval num_timesteps=15500, episode_reward=-140.54 +/- 86.66
Episode length: 141.49 +/- 86.45
Eval num_timesteps=16000, episode_reward=-128.16 +/- 57.73
Episode length: 129.14 +/- 57.60
Eval num_timesteps=16500, episode_reward=-129.44 +/- 40.79
Episode length: 130.43 +/- 40.70
Eval num_timesteps=17000, episode_reward=-120.90 +/- 34.69
Episode length: 121.90 +/- 34.69
Eval num_timesteps=17500, episode_reward=-116.22 +/- 38.41
Episode length: 117.22 +/- 38.41
Eval num_timesteps=18000, episode_reward=-121.50 +/- 32.88
Episode length: 122.50 +/- 32.88
Eval num_timesteps=18500, episode_reward=-116.43 +/- 33.65
Episode length: 117.43 +/- 33.65
Eval num_timesteps=19000, episode_reward=-111.42 +/- 38.46
Episode length: 112.42 +/- 38.46
New best mean reward!
Eval num_timesteps=19500, episode_reward=-109.01 +/- 37.75
Episode length: 110.01 +/- 37.75
New best mean reward!
Eval num_timesteps=20000, episode_reward=-119.55 +/- 57.55
Episode length: 120.54 +/- 57.48
Eval num_timesteps=20500, episode_reward=-155.77 +/- 119.76
Episode length: 156.67 +/- 119.48
Eval num_timesteps=21000, episode_reward=-235.32 +/- 179.89
Episode length: 236.01 +/- 179.43
Eval num_timesteps=21500, episode_reward=-273.38 +/- 186.96
Episode length: 273.99 +/- 186.49
Eval num_timesteps=22000, episode_reward=-225.25 +/- 177.14
Episode length: 225.96 +/- 176.69
Eval num_timesteps=22500, episode_reward=-155.86 +/- 121.80
Episode length: 156.76 +/- 121.52
Eval num_timesteps=23000, episode_reward=-160.50 +/- 127.43
Episode length: 161.38 +/- 127.11
Eval num_timesteps=23500, episode_reward=-142.08 +/- 109.33
Episode length: 143.00 +/- 109.07
Eval num_timesteps=24000, episode_reward=-128.53 +/- 90.61
Episode length: 129.48 +/- 90.40
Eval num_timesteps=24500, episode_reward=-102.24 +/- 42.03
Episode length: 103.23 +/- 41.94
New best mean reward!
Eval num_timesteps=25000, episode_reward=-109.32 +/- 38.71
Episode length: 110.32 +/- 38.71
Eval num_timesteps=25500, episode_reward=-112.07 +/- 59.47
Episode length: 113.06 +/- 59.41
Eval num_timesteps=26000, episode_reward=-106.19 +/- 25.95
Episode length: 107.19 +/- 25.95
Eval num_timesteps=26500, episode_reward=-118.73 +/- 60.16
Episode length: 119.72 +/- 60.10
Eval num_timesteps=27000, episode_reward=-108.45 +/- 31.77
Episode length: 109.45 +/- 31.77
Eval num_timesteps=27500, episode_reward=-108.17 +/- 33.59
Episode length: 109.17 +/- 33.59
Eval num_timesteps=28000, episode_reward=-100.44 +/- 14.80
Episode length: 101.44 +/- 14.80
New best mean reward!
Eval num_timesteps=28500, episode_reward=-109.04 +/- 47.62
Episode length: 110.03 +/- 47.54
Eval num_timesteps=29000, episode_reward=-110.78 +/- 47.58
Episode length: 111.77 +/- 47.49
Eval num_timesteps=29500, episode_reward=-110.35 +/- 38.72
Episode length: 111.35 +/- 38.72
Eval num_timesteps=30000, episode_reward=-108.03 +/- 30.62
Episode length: 109.03 +/- 30.62
Eval num_timesteps=30500, episode_reward=-107.41 +/- 28.11
Episode length: 108.41 +/- 28.11
Eval num_timesteps=31000, episode_reward=-107.90 +/- 45.94
Episode length: 108.89 +/- 45.86
Eval num_timesteps=31500, episode_reward=-108.22 +/- 28.08
Episode length: 109.22 +/- 28.08
Eval num_timesteps=32000, episode_reward=-101.57 +/- 20.70
Episode length: 102.57 +/- 20.70
Eval num_timesteps=32500, episode_reward=-107.43 +/- 36.97
Episode length: 108.43 +/- 36.97
Eval num_timesteps=33000, episode_reward=-117.66 +/- 61.69
Episode length: 118.64 +/- 61.56
Eval num_timesteps=33500, episode_reward=-116.34 +/- 60.40
Episode length: 117.32 +/- 60.27
Eval num_timesteps=34000, episode_reward=-128.08 +/- 84.02
Episode length: 129.04 +/- 83.84
Eval num_timesteps=34500, episode_reward=-157.76 +/- 126.00
Episode length: 158.66 +/- 125.73
Eval num_timesteps=35000, episode_reward=-176.57 +/- 141.71
Episode length: 177.42 +/- 141.36
Eval num_timesteps=35500, episode_reward=-184.22 +/- 152.70
Episode length: 185.04 +/- 152.33
Eval num_timesteps=36000, episode_reward=-187.65 +/- 159.02
Episode length: 188.46 +/- 158.65
Eval num_timesteps=36500, episode_reward=-189.47 +/- 159.01
Episode length: 190.28 +/- 158.64
Eval num_timesteps=37000, episode_reward=-237.80 +/- 183.96
Episode length: 238.48 +/- 183.51
Eval num_timesteps=37500, episode_reward=-227.30 +/- 180.88
Episode length: 228.00 +/- 180.42
Eval num_timesteps=38000, episode_reward=-220.85 +/- 177.15
Episode length: 221.58 +/- 176.72
Eval num_timesteps=38500, episode_reward=-250.47 +/- 187.38
Episode length: 251.12 +/- 186.91
Eval num_timesteps=39000, episode_reward=-208.93 +/- 169.34
Episode length: 209.68 +/- 168.91
Eval num_timesteps=39500, episode_reward=-199.87 +/- 167.99
Episode length: 200.64 +/- 167.58
Eval num_timesteps=40000, episode_reward=-214.22 +/- 175.21
Episode length: 214.95 +/- 174.77
Eval num_timesteps=40500, episode_reward=-178.79 +/- 153.13
Episode length: 179.61 +/- 152.76
Eval num_timesteps=41000, episode_reward=-199.20 +/- 164.67
Episode length: 199.99 +/- 164.29
Eval num_timesteps=41500, episode_reward=-183.63 +/- 156.51
Episode length: 184.44 +/- 156.13
Eval num_timesteps=42000, episode_reward=-192.61 +/- 160.42
Episode length: 193.41 +/- 160.03
Eval num_timesteps=42500, episode_reward=-178.24 +/- 150.01
Episode length: 179.07 +/- 149.65
Eval num_timesteps=43000, episode_reward=-215.80 +/- 177.44
Episode length: 216.53 +/- 177.01
Eval num_timesteps=43500, episode_reward=-156.09 +/- 131.83
Episode length: 156.97 +/- 131.51
Eval num_timesteps=44000, episode_reward=-190.27 +/- 162.86
Episode length: 191.07 +/- 162.48
Eval num_timesteps=44500, episode_reward=-195.39 +/- 168.56
Episode length: 196.18 +/- 168.18
Eval num_timesteps=45000, episode_reward=-160.49 +/- 130.11
Episode length: 161.38 +/- 129.82
Eval num_timesteps=45500, episode_reward=-186.40 +/- 156.53
Episode length: 187.21 +/- 156.15
Eval num_timesteps=46000, episode_reward=-213.79 +/- 176.06
Episode length: 214.52 +/- 175.62
Eval num_timesteps=46500, episode_reward=-202.28 +/- 167.49
Episode length: 203.06 +/- 167.10
Eval num_timesteps=47000, episode_reward=-208.96 +/- 167.96
Episode length: 209.73 +/- 167.56
Eval num_timesteps=47500, episode_reward=-217.69 +/- 170.10
Episode length: 218.44 +/- 169.68
Eval num_timesteps=48000, episode_reward=-213.25 +/- 176.05
Episode length: 213.98 +/- 175.61
Eval num_timesteps=48500, episode_reward=-208.39 +/- 171.21
Episode length: 209.15 +/- 170.80
Eval num_timesteps=49000, episode_reward=-194.41 +/- 165.53
Episode length: 195.19 +/- 165.12
Eval num_timesteps=49500, episode_reward=-213.22 +/- 178.31
Episode length: 213.96 +/- 177.90
Eval num_timesteps=50000, episode_reward=-203.38 +/- 173.08
Episode length: 204.13 +/- 172.65
FINISHED IN 699.8892434890149 s


starting seed  10007 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-95.85 +/- 66.78
Episode length: 96.83 +/- 66.66
New best mean reward!
FINISHED IN 254.16581693198532 s


starting seed  10008 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-249.26 +/- 201.10
Episode length: 249.87 +/- 200.62
New best mean reward!
Eval num_timesteps=9500, episode_reward=-145.08 +/- 148.91
Episode length: 145.94 +/- 148.57
New best mean reward!
Eval num_timesteps=10000, episode_reward=-84.66 +/- 26.06
Episode length: 85.66 +/- 26.06
New best mean reward!
FINISHED IN 209.064683383971 s


starting seed  10009 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-100.88 +/- 28.84
Episode length: 101.88 +/- 28.84
New best mean reward!
Eval num_timesteps=10500, episode_reward=-104.50 +/- 38.13
Episode length: 105.50 +/- 38.13
Eval num_timesteps=11000, episode_reward=-101.90 +/- 42.15
Episode length: 102.90 +/- 42.15
Eval num_timesteps=11500, episode_reward=-110.70 +/- 60.51
Episode length: 111.69 +/- 60.44
Eval num_timesteps=12000, episode_reward=-96.80 +/- 25.94
Episode length: 97.80 +/- 25.94
New best mean reward!
FINISHED IN 298.66991014097584 s


starting seed  10010 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-114.27 +/- 74.43
Episode length: 115.24 +/- 74.27
New best mean reward!
Eval num_timesteps=1000, episode_reward=-247.82 +/- 99.64
Episode length: 248.73 +/- 99.42
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-397.50 +/- 174.04
Episode length: 397.76 +/- 173.61
Eval num_timesteps=10500, episode_reward=-120.15 +/- 85.09
Episode length: 121.11 +/- 84.91
Eval num_timesteps=11000, episode_reward=-156.05 +/- 141.50
Episode length: 156.91 +/- 141.16
Eval num_timesteps=11500, episode_reward=-122.34 +/- 96.24
Episode length: 123.29 +/- 96.04
Eval num_timesteps=12000, episode_reward=-178.83 +/- 161.27
Episode length: 179.64 +/- 160.89
Eval num_timesteps=12500, episode_reward=-167.20 +/- 149.59
Episode length: 168.05 +/- 149.25
Eval num_timesteps=13000, episode_reward=-231.21 +/- 178.30
Episode length: 231.92 +/- 177.87
Eval num_timesteps=13500, episode_reward=-250.91 +/- 184.16
Episode length: 251.57 +/- 183.70
Eval num_timesteps=14000, episode_reward=-234.86 +/- 180.86
Episode length: 235.55 +/- 180.41
Eval num_timesteps=14500, episode_reward=-119.06 +/- 81.14
Episode length: 120.04 +/- 81.05
Eval num_timesteps=15000, episode_reward=-101.05 +/- 38.24
Episode length: 102.05 +/- 38.24
New best mean reward!
Eval num_timesteps=15500, episode_reward=-117.44 +/- 86.54
Episode length: 118.40 +/- 86.36
Eval num_timesteps=16000, episode_reward=-133.19 +/- 108.23
Episode length: 134.12 +/- 107.99
Eval num_timesteps=16500, episode_reward=-116.51 +/- 75.46
Episode length: 117.48 +/- 75.31
Eval num_timesteps=17000, episode_reward=-115.79 +/- 78.44
Episode length: 116.76 +/- 78.29
Eval num_timesteps=17500, episode_reward=-149.38 +/- 126.32
Episode length: 150.27 +/- 126.02
Eval num_timesteps=18000, episode_reward=-124.38 +/- 94.81
Episode length: 125.33 +/- 94.61
Eval num_timesteps=18500, episode_reward=-108.43 +/- 53.29
Episode length: 109.42 +/- 53.22
Eval num_timesteps=19000, episode_reward=-115.92 +/- 79.99
Episode length: 116.89 +/- 79.85
Eval num_timesteps=19500, episode_reward=-109.90 +/- 72.60
Episode length: 110.88 +/- 72.49
Eval num_timesteps=20000, episode_reward=-113.82 +/- 60.90
Episode length: 114.81 +/- 60.84
Eval num_timesteps=20500, episode_reward=-128.27 +/- 96.63
Episode length: 129.22 +/- 96.44
Eval num_timesteps=21000, episode_reward=-115.19 +/- 92.32
Episode length: 116.15 +/- 92.15
Eval num_timesteps=21500, episode_reward=-103.75 +/- 42.20
Episode length: 104.75 +/- 42.20
Eval num_timesteps=22000, episode_reward=-97.16 +/- 52.16
Episode length: 98.15 +/- 52.08
New best mean reward!
FINISHED IN 468.7710085900035 s


starting seed  10011 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-298.22 +/- 164.39
Episode length: 298.83 +/- 163.91
New best mean reward!
Eval num_timesteps=5500, episode_reward=-294.82 +/- 166.81
Episode length: 295.43 +/- 166.33
New best mean reward!
Eval num_timesteps=6000, episode_reward=-189.31 +/- 103.17
Episode length: 190.22 +/- 102.90
New best mean reward!
Eval num_timesteps=6500, episode_reward=-262.41 +/- 71.16
Episode length: 263.37 +/- 71.03
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-265.28 +/- 167.51
Episode length: 265.97 +/- 167.08
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-471.97 +/- 89.75
Episode length: 472.06 +/- 89.47
Eval num_timesteps=10000, episode_reward=-241.16 +/- 175.90
Episode length: 241.85 +/- 175.44
Eval num_timesteps=10500, episode_reward=-215.86 +/- 170.40
Episode length: 216.61 +/- 169.98
Eval num_timesteps=11000, episode_reward=-166.63 +/- 138.75
Episode length: 167.49 +/- 138.41
New best mean reward!
Eval num_timesteps=11500, episode_reward=-180.84 +/- 142.65
Episode length: 181.69 +/- 142.32
Eval num_timesteps=12000, episode_reward=-155.92 +/- 128.94
Episode length: 156.81 +/- 128.64
New best mean reward!
Eval num_timesteps=12500, episode_reward=-118.70 +/- 85.83
Episode length: 119.66 +/- 85.65
New best mean reward!
Eval num_timesteps=13000, episode_reward=-149.42 +/- 107.17
Episode length: 150.35 +/- 106.94
Eval num_timesteps=13500, episode_reward=-139.21 +/- 97.05
Episode length: 140.15 +/- 96.83
Eval num_timesteps=14000, episode_reward=-142.22 +/- 93.71
Episode length: 143.17 +/- 93.51
Eval num_timesteps=14500, episode_reward=-93.52 +/- 51.09
Episode length: 94.51 +/- 51.01
New best mean reward!
FINISHED IN 281.1810861660051 s


starting seed  10012 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-87.95 +/- 23.98
Episode length: 88.95 +/- 23.98
New best mean reward!
FINISHED IN 174.90530228900025 s


starting seed  10013 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=17000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=17500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=18000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=18500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=19000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=19500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=20000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=20500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=21000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=21500, episode_reward=-449.09 +/- 131.82
Episode length: 449.22 +/- 131.48
New best mean reward!
Eval num_timesteps=22000, episode_reward=-421.47 +/- 157.33
Episode length: 421.67 +/- 156.93
New best mean reward!
Eval num_timesteps=22500, episode_reward=-405.97 +/- 172.21
Episode length: 406.20 +/- 171.79
New best mean reward!
Eval num_timesteps=23000, episode_reward=-371.54 +/- 186.68
Episode length: 371.87 +/- 186.22
New best mean reward!
Eval num_timesteps=23500, episode_reward=-326.16 +/- 202.47
Episode length: 326.59 +/- 201.98
New best mean reward!
Eval num_timesteps=24000, episode_reward=-358.69 +/- 192.90
Episode length: 359.04 +/- 192.42
Eval num_timesteps=24500, episode_reward=-348.15 +/- 198.02
Episode length: 348.53 +/- 197.55
Eval num_timesteps=25000, episode_reward=-319.21 +/- 201.45
Episode length: 319.66 +/- 200.96
New best mean reward!
Eval num_timesteps=25500, episode_reward=-310.79 +/- 202.32
Episode length: 311.26 +/- 201.82
New best mean reward!
Eval num_timesteps=26000, episode_reward=-340.74 +/- 193.75
Episode length: 341.16 +/- 193.28
Eval num_timesteps=26500, episode_reward=-308.36 +/- 203.10
Episode length: 308.85 +/- 202.62
New best mean reward!
Eval num_timesteps=27000, episode_reward=-244.33 +/- 198.79
Episode length: 244.96 +/- 198.32
New best mean reward!
Eval num_timesteps=27500, episode_reward=-229.26 +/- 193.12
Episode length: 229.93 +/- 192.65
New best mean reward!
Eval num_timesteps=28000, episode_reward=-210.76 +/- 184.03
Episode length: 211.49 +/- 183.60
New best mean reward!
Eval num_timesteps=28500, episode_reward=-168.38 +/- 159.00
Episode length: 169.20 +/- 158.62
New best mean reward!
Eval num_timesteps=29000, episode_reward=-151.84 +/- 144.29
Episode length: 152.70 +/- 143.95
New best mean reward!
Eval num_timesteps=29500, episode_reward=-113.80 +/- 99.92
Episode length: 114.75 +/- 99.72
New best mean reward!
Eval num_timesteps=30000, episode_reward=-122.55 +/- 112.34
Episode length: 123.48 +/- 112.11
Eval num_timesteps=30500, episode_reward=-106.96 +/- 87.83
Episode length: 107.92 +/- 87.65
New best mean reward!
Eval num_timesteps=31000, episode_reward=-125.05 +/- 115.00
Episode length: 125.97 +/- 114.73
Eval num_timesteps=31500, episode_reward=-89.79 +/- 44.91
Episode length: 90.78 +/- 44.82
New best mean reward!
FINISHED IN 733.0623888419941 s


starting seed  10014 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-470.12 +/- 65.70
Episode length: 470.32 +/- 65.33
New best mean reward!
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-388.03 +/- 158.03
Episode length: 388.37 +/- 157.56
New best mean reward!
Eval num_timesteps=4000, episode_reward=-261.78 +/- 157.12
Episode length: 262.49 +/- 156.68
New best mean reward!
Eval num_timesteps=4500, episode_reward=-199.98 +/- 78.33
Episode length: 200.93 +/- 78.14
New best mean reward!
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-326.33 +/- 118.59
Episode length: 327.03 +/- 118.15
Eval num_timesteps=6000, episode_reward=-350.61 +/- 60.92
Episode length: 351.56 +/- 60.80
Eval num_timesteps=6500, episode_reward=-256.27 +/- 71.97
Episode length: 257.20 +/- 71.73
Eval num_timesteps=7000, episode_reward=-460.09 +/- 79.24
Episode length: 460.32 +/- 78.85
Eval num_timesteps=7500, episode_reward=-474.55 +/- 48.39
Episode length: 474.86 +/- 48.03
Eval num_timesteps=8000, episode_reward=-452.80 +/- 80.23
Episode length: 453.14 +/- 79.84
Eval num_timesteps=8500, episode_reward=-448.34 +/- 87.97
Episode length: 448.68 +/- 87.59
Eval num_timesteps=9000, episode_reward=-471.73 +/- 62.05
Episode length: 471.95 +/- 61.70
Eval num_timesteps=9500, episode_reward=-425.88 +/- 105.28
Episode length: 426.26 +/- 104.84
Eval num_timesteps=10000, episode_reward=-408.11 +/- 105.45
Episode length: 408.63 +/- 105.04
Eval num_timesteps=10500, episode_reward=-394.43 +/- 115.58
Episode length: 394.94 +/- 115.13
Eval num_timesteps=11000, episode_reward=-364.32 +/- 118.36
Episode length: 364.96 +/- 117.95
Eval num_timesteps=11500, episode_reward=-416.05 +/- 133.70
Episode length: 416.35 +/- 133.26
Eval num_timesteps=12000, episode_reward=-374.40 +/- 160.07
Episode length: 374.80 +/- 159.60
Eval num_timesteps=12500, episode_reward=-306.87 +/- 161.90
Episode length: 307.47 +/- 161.42
Eval num_timesteps=13000, episode_reward=-227.36 +/- 114.60
Episode length: 228.24 +/- 114.31
Eval num_timesteps=13500, episode_reward=-170.53 +/- 62.71
Episode length: 171.53 +/- 62.71
New best mean reward!
Eval num_timesteps=14000, episode_reward=-174.50 +/- 58.70
Episode length: 175.49 +/- 58.64
Eval num_timesteps=14500, episode_reward=-179.08 +/- 58.03
Episode length: 180.07 +/- 57.97
Eval num_timesteps=15000, episode_reward=-182.78 +/- 75.33
Episode length: 183.76 +/- 75.25
Eval num_timesteps=15500, episode_reward=-167.55 +/- 49.71
Episode length: 168.55 +/- 49.71
New best mean reward!
Eval num_timesteps=16000, episode_reward=-178.96 +/- 68.61
Episode length: 179.94 +/- 68.52
Eval num_timesteps=16500, episode_reward=-173.47 +/- 60.62
Episode length: 174.46 +/- 60.57
Eval num_timesteps=17000, episode_reward=-146.72 +/- 27.59
Episode length: 147.72 +/- 27.59
New best mean reward!
Eval num_timesteps=17500, episode_reward=-135.29 +/- 31.82
Episode length: 136.29 +/- 31.82
New best mean reward!
Eval num_timesteps=18000, episode_reward=-152.06 +/- 44.62
Episode length: 153.06 +/- 44.62
Eval num_timesteps=18500, episode_reward=-138.06 +/- 37.78
Episode length: 139.06 +/- 37.78
Eval num_timesteps=19000, episode_reward=-147.68 +/- 61.59
Episode length: 148.67 +/- 61.53
Eval num_timesteps=19500, episode_reward=-140.82 +/- 45.34
Episode length: 141.81 +/- 45.26
Eval num_timesteps=20000, episode_reward=-118.99 +/- 45.30
Episode length: 119.98 +/- 45.22
New best mean reward!
Eval num_timesteps=20500, episode_reward=-123.80 +/- 31.74
Episode length: 124.80 +/- 31.74
Eval num_timesteps=21000, episode_reward=-117.46 +/- 21.94
Episode length: 118.46 +/- 21.94
New best mean reward!
Eval num_timesteps=21500, episode_reward=-126.44 +/- 52.39
Episode length: 127.43 +/- 52.32
Eval num_timesteps=22000, episode_reward=-127.57 +/- 50.80
Episode length: 128.56 +/- 50.73
Eval num_timesteps=22500, episode_reward=-114.55 +/- 27.04
Episode length: 115.55 +/- 27.04
New best mean reward!
Eval num_timesteps=23000, episode_reward=-119.40 +/- 35.13
Episode length: 120.40 +/- 35.13
Eval num_timesteps=23500, episode_reward=-117.72 +/- 45.85
Episode length: 118.71 +/- 45.76
Eval num_timesteps=24000, episode_reward=-107.02 +/- 35.04
Episode length: 108.02 +/- 35.04
New best mean reward!
Eval num_timesteps=24500, episode_reward=-98.93 +/- 30.56
Episode length: 99.93 +/- 30.56
New best mean reward!
FINISHED IN 404.29229976201896 s


starting seed  10015 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-179.49 +/- 55.57
Episode length: 180.48 +/- 55.51
New best mean reward!
Eval num_timesteps=7000, episode_reward=-187.40 +/- 78.92
Episode length: 188.36 +/- 78.76
Eval num_timesteps=7500, episode_reward=-324.01 +/- 163.41
Episode length: 324.56 +/- 162.92
Eval num_timesteps=8000, episode_reward=-398.80 +/- 148.11
Episode length: 399.13 +/- 147.65
Eval num_timesteps=8500, episode_reward=-176.90 +/- 66.48
Episode length: 177.87 +/- 66.33
New best mean reward!
Eval num_timesteps=9000, episode_reward=-187.72 +/- 81.45
Episode length: 188.67 +/- 81.26
Eval num_timesteps=9500, episode_reward=-177.18 +/- 60.28
Episode length: 178.17 +/- 60.23
Eval num_timesteps=10000, episode_reward=-176.22 +/- 46.15
Episode length: 177.22 +/- 46.15
New best mean reward!
Eval num_timesteps=10500, episode_reward=-176.46 +/- 60.01
Episode length: 177.44 +/- 59.90
Eval num_timesteps=11000, episode_reward=-172.54 +/- 39.02
Episode length: 173.54 +/- 39.02
New best mean reward!
Eval num_timesteps=11500, episode_reward=-194.91 +/- 86.27
Episode length: 195.85 +/- 86.06
Eval num_timesteps=12000, episode_reward=-197.88 +/- 89.79
Episode length: 198.81 +/- 89.55
Eval num_timesteps=12500, episode_reward=-210.26 +/- 110.51
Episode length: 211.15 +/- 110.22
Eval num_timesteps=13000, episode_reward=-198.23 +/- 91.69
Episode length: 199.16 +/- 91.46
Eval num_timesteps=13500, episode_reward=-211.37 +/- 103.25
Episode length: 212.28 +/- 103.00
Eval num_timesteps=14000, episode_reward=-203.07 +/- 105.63
Episode length: 203.97 +/- 105.35
Eval num_timesteps=14500, episode_reward=-202.45 +/- 96.32
Episode length: 203.37 +/- 96.07
Eval num_timesteps=15000, episode_reward=-249.83 +/- 142.66
Episode length: 250.59 +/- 142.24
Eval num_timesteps=15500, episode_reward=-264.64 +/- 150.23
Episode length: 265.36 +/- 149.79
Eval num_timesteps=16000, episode_reward=-222.34 +/- 117.34
Episode length: 223.21 +/- 117.03
Eval num_timesteps=16500, episode_reward=-196.00 +/- 85.07
Episode length: 196.95 +/- 84.89
Eval num_timesteps=17000, episode_reward=-195.93 +/- 80.09
Episode length: 196.88 +/- 79.90
Eval num_timesteps=17500, episode_reward=-230.42 +/- 129.49
Episode length: 231.24 +/- 129.12
Eval num_timesteps=18000, episode_reward=-266.73 +/- 151.61
Episode length: 267.44 +/- 151.17
Eval num_timesteps=18500, episode_reward=-244.33 +/- 139.29
Episode length: 245.11 +/- 138.88
Eval num_timesteps=19000, episode_reward=-237.46 +/- 135.54
Episode length: 238.26 +/- 135.15
Eval num_timesteps=19500, episode_reward=-264.29 +/- 149.87
Episode length: 265.01 +/- 149.43
Eval num_timesteps=20000, episode_reward=-236.97 +/- 131.45
Episode length: 237.78 +/- 131.07
Eval num_timesteps=20500, episode_reward=-248.17 +/- 131.19
Episode length: 248.98 +/- 130.83
Eval num_timesteps=21000, episode_reward=-204.81 +/- 101.52
Episode length: 205.72 +/- 101.26
Eval num_timesteps=21500, episode_reward=-228.43 +/- 125.74
Episode length: 229.27 +/- 125.39
Eval num_timesteps=22000, episode_reward=-224.72 +/- 123.72
Episode length: 225.57 +/- 123.38
Eval num_timesteps=22500, episode_reward=-223.61 +/- 122.09
Episode length: 224.46 +/- 121.75
Eval num_timesteps=23000, episode_reward=-224.74 +/- 119.48
Episode length: 225.60 +/- 119.16
Eval num_timesteps=23500, episode_reward=-201.97 +/- 101.30
Episode length: 202.88 +/- 101.04
Eval num_timesteps=24000, episode_reward=-217.79 +/- 105.18
Episode length: 218.69 +/- 104.91
Eval num_timesteps=24500, episode_reward=-231.56 +/- 123.07
Episode length: 232.40 +/- 122.72
Eval num_timesteps=25000, episode_reward=-204.65 +/- 99.54
Episode length: 205.57 +/- 99.30
Eval num_timesteps=25500, episode_reward=-218.76 +/- 116.72
Episode length: 219.63 +/- 116.41
Eval num_timesteps=26000, episode_reward=-212.48 +/- 107.84
Episode length: 213.37 +/- 107.54
Eval num_timesteps=26500, episode_reward=-204.35 +/- 100.14
Episode length: 205.26 +/- 99.87
Eval num_timesteps=27000, episode_reward=-214.33 +/- 113.30
Episode length: 215.21 +/- 113.00
Eval num_timesteps=27500, episode_reward=-179.56 +/- 99.70
Episode length: 180.48 +/- 99.44
Eval num_timesteps=28000, episode_reward=-186.38 +/- 105.01
Episode length: 187.29 +/- 104.75
Eval num_timesteps=28500, episode_reward=-179.31 +/- 129.29
Episode length: 180.19 +/- 128.99
Eval num_timesteps=29000, episode_reward=-141.80 +/- 108.71
Episode length: 142.72 +/- 108.44
New best mean reward!
Eval num_timesteps=29500, episode_reward=-137.70 +/- 126.42
Episode length: 138.60 +/- 126.13
New best mean reward!
Eval num_timesteps=30000, episode_reward=-130.45 +/- 122.03
Episode length: 131.37 +/- 121.79
New best mean reward!
Eval num_timesteps=30500, episode_reward=-121.95 +/- 113.60
Episode length: 122.87 +/- 113.34
New best mean reward!
Eval num_timesteps=31000, episode_reward=-105.21 +/- 66.79
Episode length: 106.19 +/- 66.68
New best mean reward!
Eval num_timesteps=31500, episode_reward=-127.60 +/- 119.29
Episode length: 128.51 +/- 119.01
Eval num_timesteps=32000, episode_reward=-108.20 +/- 80.15
Episode length: 109.17 +/- 80.00
Eval num_timesteps=32500, episode_reward=-108.07 +/- 80.66
Episode length: 109.04 +/- 80.52
Eval num_timesteps=33000, episode_reward=-110.27 +/- 92.98
Episode length: 111.22 +/- 92.77
Eval num_timesteps=33500, episode_reward=-113.02 +/- 91.79
Episode length: 113.97 +/- 91.58
Eval num_timesteps=34000, episode_reward=-101.30 +/- 72.79
Episode length: 102.29 +/- 72.74
New best mean reward!
Eval num_timesteps=34500, episode_reward=-125.71 +/- 113.71
Episode length: 126.63 +/- 113.45
Eval num_timesteps=35000, episode_reward=-130.61 +/- 120.84
Episode length: 131.52 +/- 120.56
Eval num_timesteps=35500, episode_reward=-116.84 +/- 99.98
Episode length: 117.78 +/- 99.75
Eval num_timesteps=36000, episode_reward=-117.57 +/- 110.05
Episode length: 118.50 +/- 109.80
Eval num_timesteps=36500, episode_reward=-99.92 +/- 67.57
Episode length: 100.90 +/- 67.45
New best mean reward!
FINISHED IN 706.9218720750068 s


starting seed  10016 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-485.26 +/- 72.23
Episode length: 485.30 +/- 72.03
New best mean reward!
Eval num_timesteps=11500, episode_reward=-208.78 +/- 128.62
Episode length: 209.62 +/- 128.26
New best mean reward!
Eval num_timesteps=12000, episode_reward=-177.45 +/- 126.89
Episode length: 178.32 +/- 126.56
New best mean reward!
Eval num_timesteps=12500, episode_reward=-144.68 +/- 100.72
Episode length: 145.61 +/- 100.47
New best mean reward!
Eval num_timesteps=13000, episode_reward=-123.18 +/- 62.33
Episode length: 124.16 +/- 62.21
New best mean reward!
Eval num_timesteps=13500, episode_reward=-128.97 +/- 61.34
Episode length: 129.95 +/- 61.22
Eval num_timesteps=14000, episode_reward=-132.58 +/- 43.40
Episode length: 133.57 +/- 43.31
Eval num_timesteps=14500, episode_reward=-145.34 +/- 45.97
Episode length: 146.34 +/- 45.97
Eval num_timesteps=15000, episode_reward=-138.58 +/- 27.13
Episode length: 139.58 +/- 27.13
Eval num_timesteps=15500, episode_reward=-104.27 +/- 24.91
Episode length: 105.27 +/- 24.91
New best mean reward!
Eval num_timesteps=16000, episode_reward=-113.43 +/- 27.34
Episode length: 114.43 +/- 27.34
Eval num_timesteps=16500, episode_reward=-121.94 +/- 44.50
Episode length: 122.93 +/- 44.41
Eval num_timesteps=17000, episode_reward=-112.91 +/- 45.79
Episode length: 113.90 +/- 45.71
Eval num_timesteps=17500, episode_reward=-117.46 +/- 28.51
Episode length: 118.46 +/- 28.51
Eval num_timesteps=18000, episode_reward=-117.30 +/- 27.91
Episode length: 118.30 +/- 27.91
Eval num_timesteps=18500, episode_reward=-116.84 +/- 25.49
Episode length: 117.84 +/- 25.49
Eval num_timesteps=19000, episode_reward=-169.08 +/- 65.77
Episode length: 170.06 +/- 65.67
Eval num_timesteps=19500, episode_reward=-133.85 +/- 24.12
Episode length: 134.85 +/- 24.12
Eval num_timesteps=20000, episode_reward=-138.97 +/- 29.22
Episode length: 139.97 +/- 29.22
Eval num_timesteps=20500, episode_reward=-130.13 +/- 52.03
Episode length: 131.12 +/- 51.96
Eval num_timesteps=21000, episode_reward=-125.28 +/- 36.31
Episode length: 126.28 +/- 36.31
Eval num_timesteps=21500, episode_reward=-106.23 +/- 32.35
Episode length: 107.23 +/- 32.35
Eval num_timesteps=22000, episode_reward=-118.08 +/- 60.59
Episode length: 119.06 +/- 60.47
Eval num_timesteps=22500, episode_reward=-113.06 +/- 25.81
Episode length: 114.06 +/- 25.81
Eval num_timesteps=23000, episode_reward=-107.40 +/- 24.52
Episode length: 108.40 +/- 24.52
Eval num_timesteps=23500, episode_reward=-113.36 +/- 73.56
Episode length: 114.33 +/- 73.40
Eval num_timesteps=24000, episode_reward=-91.92 +/- 20.33
Episode length: 92.92 +/- 20.33
New best mean reward!
FINISHED IN 473.5447143279598 s


starting seed  10017 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-205.99 +/- 181.81
Episode length: 206.72 +/- 181.38
New best mean reward!
Eval num_timesteps=14500, episode_reward=-189.03 +/- 171.01
Episode length: 189.81 +/- 170.61
New best mean reward!
Eval num_timesteps=15000, episode_reward=-154.18 +/- 143.80
Episode length: 155.04 +/- 143.46
New best mean reward!
Eval num_timesteps=15500, episode_reward=-192.09 +/- 166.71
Episode length: 192.87 +/- 166.30
Eval num_timesteps=16000, episode_reward=-106.80 +/- 71.27
Episode length: 107.78 +/- 71.16
New best mean reward!
Eval num_timesteps=16500, episode_reward=-101.86 +/- 64.32
Episode length: 102.84 +/- 64.20
New best mean reward!
Eval num_timesteps=17000, episode_reward=-107.05 +/- 69.70
Episode length: 108.04 +/- 69.65
Eval num_timesteps=17500, episode_reward=-95.26 +/- 40.32
Episode length: 96.26 +/- 40.32
New best mean reward!
FINISHED IN 497.50732105603674 s


starting seed  10018 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-202.15 +/- 122.22
Episode length: 203.02 +/- 121.91
New best mean reward!
Eval num_timesteps=7500, episode_reward=-120.39 +/- 65.77
Episode length: 121.38 +/- 65.71
New best mean reward!
Eval num_timesteps=8000, episode_reward=-125.71 +/- 107.82
Episode length: 126.64 +/- 107.58
Eval num_timesteps=8500, episode_reward=-90.52 +/- 31.99
Episode length: 91.52 +/- 31.99
New best mean reward!
FINISHED IN 201.65832699998282 s


starting seed  10019 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-478.61 +/- 64.93
Episode length: 478.71 +/- 64.64
New best mean reward!
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-443.58 +/- 134.44
Episode length: 443.73 +/- 134.08
New best mean reward!
Eval num_timesteps=8500, episode_reward=-210.95 +/- 181.00
Episode length: 211.68 +/- 180.57
New best mean reward!
Eval num_timesteps=9000, episode_reward=-101.12 +/- 57.25
Episode length: 102.11 +/- 57.18
New best mean reward!
Eval num_timesteps=9500, episode_reward=-102.59 +/- 48.62
Episode length: 103.58 +/- 48.54
Eval num_timesteps=10000, episode_reward=-104.62 +/- 54.73
Episode length: 105.61 +/- 54.66
Eval num_timesteps=10500, episode_reward=-186.66 +/- 160.19
Episode length: 187.47 +/- 159.82
Eval num_timesteps=11000, episode_reward=-314.66 +/- 201.84
Episode length: 315.12 +/- 201.34
Eval num_timesteps=11500, episode_reward=-136.61 +/- 135.82
Episode length: 137.49 +/- 135.50
Eval num_timesteps=12000, episode_reward=-85.83 +/- 21.67
Episode length: 86.83 +/- 21.67
New best mean reward!
FINISHED IN 305.79280145000666 s


starting seed  10020 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-492.05 +/- 45.22
Episode length: 492.08 +/- 45.05
New best mean reward!
Eval num_timesteps=2000, episode_reward=-459.92 +/- 92.60
Episode length: 460.08 +/- 92.24
New best mean reward!
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-380.65 +/- 148.56
Episode length: 381.05 +/- 148.08
New best mean reward!
Eval num_timesteps=3500, episode_reward=-499.92 +/- 0.80
Episode length: 499.93 +/- 0.70
Eval num_timesteps=4000, episode_reward=-496.60 +/- 14.43
Episode length: 496.69 +/- 14.21
Eval num_timesteps=4500, episode_reward=-479.88 +/- 44.41
Episode length: 480.10 +/- 44.05
Eval num_timesteps=5000, episode_reward=-458.82 +/- 86.00
Episode length: 459.02 +/- 85.61
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-498.89 +/- 6.89
Episode length: 498.92 +/- 6.73
Eval num_timesteps=8500, episode_reward=-494.47 +/- 28.84
Episode length: 494.51 +/- 28.65
Eval num_timesteps=9000, episode_reward=-497.09 +/- 11.42
Episode length: 497.17 +/- 11.18
Eval num_timesteps=9500, episode_reward=-451.48 +/- 101.55
Episode length: 451.67 +/- 101.17
Eval num_timesteps=10000, episode_reward=-460.53 +/- 91.70
Episode length: 460.69 +/- 91.33
Eval num_timesteps=10500, episode_reward=-455.00 +/- 105.59
Episode length: 455.16 +/- 105.23
Eval num_timesteps=11000, episode_reward=-430.55 +/- 127.75
Episode length: 430.78 +/- 127.34
Eval num_timesteps=11500, episode_reward=-444.19 +/- 117.17
Episode length: 444.38 +/- 116.79
Eval num_timesteps=12000, episode_reward=-446.82 +/- 118.37
Episode length: 446.99 +/- 118.00
Eval num_timesteps=12500, episode_reward=-382.39 +/- 158.32
Episode length: 382.75 +/- 157.85
Eval num_timesteps=13000, episode_reward=-392.27 +/- 152.63
Episode length: 392.61 +/- 152.17
Eval num_timesteps=13500, episode_reward=-430.19 +/- 132.66
Episode length: 430.41 +/- 132.25
Eval num_timesteps=14000, episode_reward=-350.11 +/- 162.17
Episode length: 350.58 +/- 161.68
New best mean reward!
Eval num_timesteps=14500, episode_reward=-335.69 +/- 167.23
Episode length: 336.19 +/- 166.74
New best mean reward!
Eval num_timesteps=15000, episode_reward=-266.88 +/- 152.87
Episode length: 267.59 +/- 152.42
New best mean reward!
Eval num_timesteps=15500, episode_reward=-291.64 +/- 162.18
Episode length: 292.28 +/- 161.72
Eval num_timesteps=16000, episode_reward=-263.25 +/- 152.29
Episode length: 263.97 +/- 151.85
New best mean reward!
Eval num_timesteps=16500, episode_reward=-224.22 +/- 126.78
Episode length: 225.06 +/- 126.43
New best mean reward!
Eval num_timesteps=17000, episode_reward=-212.12 +/- 115.16
Episode length: 213.00 +/- 114.86
New best mean reward!
Eval num_timesteps=17500, episode_reward=-193.70 +/- 93.08
Episode length: 194.63 +/- 92.85
New best mean reward!
Eval num_timesteps=18000, episode_reward=-237.62 +/- 132.97
Episode length: 238.43 +/- 132.60
Eval num_timesteps=18500, episode_reward=-193.74 +/- 104.18
Episode length: 194.65 +/- 103.92
Eval num_timesteps=19000, episode_reward=-181.33 +/- 141.66
Episode length: 182.18 +/- 141.32
New best mean reward!
Eval num_timesteps=19500, episode_reward=-161.06 +/- 133.66
Episode length: 161.93 +/- 133.33
New best mean reward!
Eval num_timesteps=20000, episode_reward=-229.66 +/- 175.65
Episode length: 230.37 +/- 175.21
Eval num_timesteps=20500, episode_reward=-200.48 +/- 162.29
Episode length: 201.27 +/- 161.91
Eval num_timesteps=21000, episode_reward=-189.32 +/- 168.24
Episode length: 190.10 +/- 167.84
Eval num_timesteps=21500, episode_reward=-150.29 +/- 144.41
Episode length: 151.15 +/- 144.07
New best mean reward!
Eval num_timesteps=22000, episode_reward=-140.86 +/- 134.98
Episode length: 141.75 +/- 134.69
New best mean reward!
Eval num_timesteps=22500, episode_reward=-181.01 +/- 172.34
Episode length: 181.79 +/- 171.94
Eval num_timesteps=23000, episode_reward=-152.75 +/- 149.43
Episode length: 153.60 +/- 149.09
Eval num_timesteps=23500, episode_reward=-126.15 +/- 123.40
Episode length: 127.06 +/- 123.13
New best mean reward!
Eval num_timesteps=24000, episode_reward=-100.45 +/- 83.67
Episode length: 101.41 +/- 83.48
New best mean reward!
Eval num_timesteps=24500, episode_reward=-109.22 +/- 90.42
Episode length: 110.18 +/- 90.24
Eval num_timesteps=25000, episode_reward=-104.79 +/- 79.57
Episode length: 105.76 +/- 79.42
Eval num_timesteps=25500, episode_reward=-109.92 +/- 79.22
Episode length: 110.90 +/- 79.12
Eval num_timesteps=26000, episode_reward=-94.09 +/- 54.95
Episode length: 95.08 +/- 54.88
New best mean reward!
FINISHED IN 619.735400620033 s


starting seed  10021 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-486.30 +/- 48.35
Episode length: 486.41 +/- 48.10
New best mean reward!
Eval num_timesteps=3500, episode_reward=-491.30 +/- 41.21
Episode length: 491.35 +/- 41.01
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-194.85 +/- 93.79
Episode length: 195.78 +/- 93.56
New best mean reward!
Eval num_timesteps=9500, episode_reward=-258.77 +/- 156.41
Episode length: 259.48 +/- 155.96
Eval num_timesteps=10000, episode_reward=-240.44 +/- 154.33
Episode length: 241.19 +/- 153.91
Eval num_timesteps=10500, episode_reward=-177.42 +/- 103.52
Episode length: 178.34 +/- 103.27
New best mean reward!
Eval num_timesteps=11000, episode_reward=-143.51 +/- 71.76
Episode length: 144.49 +/- 71.66
New best mean reward!
Eval num_timesteps=11500, episode_reward=-121.23 +/- 53.47
Episode length: 122.22 +/- 53.40
New best mean reward!
Eval num_timesteps=12000, episode_reward=-239.23 +/- 151.99
Episode length: 239.99 +/- 151.58
Eval num_timesteps=12500, episode_reward=-154.03 +/- 92.72
Episode length: 154.99 +/- 92.57
Eval num_timesteps=13000, episode_reward=-130.17 +/- 28.51
Episode length: 131.17 +/- 28.51
Eval num_timesteps=13500, episode_reward=-185.88 +/- 159.89
Episode length: 186.68 +/- 159.50
Eval num_timesteps=14000, episode_reward=-174.58 +/- 147.08
Episode length: 175.42 +/- 146.73
Eval num_timesteps=14500, episode_reward=-173.63 +/- 152.50
Episode length: 174.46 +/- 152.13
Eval num_timesteps=15000, episode_reward=-134.73 +/- 107.20
Episode length: 135.66 +/- 106.96
Eval num_timesteps=15500, episode_reward=-122.76 +/- 81.36
Episode length: 123.72 +/- 81.17
Eval num_timesteps=16000, episode_reward=-105.28 +/- 32.68
Episode length: 106.28 +/- 32.68
New best mean reward!
Eval num_timesteps=16500, episode_reward=-107.52 +/- 64.32
Episode length: 108.50 +/- 64.20
Eval num_timesteps=17000, episode_reward=-99.76 +/- 30.29
Episode length: 100.76 +/- 30.29
New best mean reward!
FINISHED IN 322.6299442760064 s


starting seed  10022 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-427.90 +/- 149.09
Episode length: 428.09 +/- 148.70
New best mean reward!
Eval num_timesteps=9000, episode_reward=-496.40 +/- 35.82
Episode length: 496.41 +/- 35.72
Eval num_timesteps=9500, episode_reward=-355.78 +/- 173.98
Episode length: 356.19 +/- 173.49
New best mean reward!
Eval num_timesteps=10000, episode_reward=-156.89 +/- 38.82
Episode length: 157.89 +/- 38.82
New best mean reward!
Eval num_timesteps=10500, episode_reward=-238.93 +/- 160.32
Episode length: 239.67 +/- 159.89
Eval num_timesteps=11000, episode_reward=-191.12 +/- 136.94
Episode length: 191.96 +/- 136.58
Eval num_timesteps=11500, episode_reward=-284.42 +/- 185.41
Episode length: 285.00 +/- 184.92
Eval num_timesteps=12000, episode_reward=-259.52 +/- 196.77
Episode length: 260.12 +/- 196.28
Eval num_timesteps=12500, episode_reward=-295.89 +/- 194.69
Episode length: 296.42 +/- 194.20
Eval num_timesteps=13000, episode_reward=-94.89 +/- 30.06
Episode length: 95.89 +/- 30.06
New best mean reward!
FINISHED IN 291.4646032669698 s


starting seed  10023 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-232.36 +/- 137.04
Episode length: 233.19 +/- 136.70
New best mean reward!
Eval num_timesteps=8500, episode_reward=-260.37 +/- 157.60
Episode length: 261.10 +/- 157.19
Eval num_timesteps=9000, episode_reward=-271.62 +/- 158.62
Episode length: 272.32 +/- 158.19
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-92.73 +/- 24.82
Episode length: 93.73 +/- 24.82
New best mean reward!
FINISHED IN 346.1526997649926 s


starting seed  10024 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-140.11 +/- 72.26
Episode length: 141.08 +/- 72.11
New best mean reward!
Eval num_timesteps=8500, episode_reward=-87.42 +/- 15.31
Episode length: 88.42 +/- 15.31
New best mean reward!
FINISHED IN 193.10999789997004 s


starting seed  10025 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-284.66 +/- 166.16
Episode length: 285.29 +/- 165.68
New best mean reward!
Eval num_timesteps=9500, episode_reward=-344.33 +/- 172.83
Episode length: 344.78 +/- 172.33
Eval num_timesteps=10000, episode_reward=-271.01 +/- 161.95
Episode length: 271.68 +/- 161.48
New best mean reward!
Eval num_timesteps=10500, episode_reward=-223.58 +/- 136.41
Episode length: 224.39 +/- 136.03
New best mean reward!
Eval num_timesteps=11000, episode_reward=-251.33 +/- 195.60
Episode length: 251.95 +/- 195.12
Eval num_timesteps=11500, episode_reward=-132.90 +/- 128.61
Episode length: 133.80 +/- 128.33
New best mean reward!
Eval num_timesteps=12000, episode_reward=-111.90 +/- 100.08
Episode length: 112.84 +/- 99.85
New best mean reward!
Eval num_timesteps=12500, episode_reward=-86.42 +/- 33.65
Episode length: 87.42 +/- 33.65
New best mean reward!
FINISHED IN 269.63215450698044 s


starting seed  10026 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-476.03 +/- 94.89
Episode length: 476.09 +/- 94.66
New best mean reward!
Eval num_timesteps=16000, episode_reward=-393.95 +/- 175.15
Episode length: 394.22 +/- 174.71
New best mean reward!
Eval num_timesteps=16500, episode_reward=-475.82 +/- 95.74
Episode length: 475.88 +/- 95.50
Eval num_timesteps=17000, episode_reward=-399.08 +/- 175.07
Episode length: 399.33 +/- 174.64
Eval num_timesteps=17500, episode_reward=-325.94 +/- 200.77
Episode length: 326.37 +/- 200.27
New best mean reward!
Eval num_timesteps=18000, episode_reward=-348.39 +/- 194.33
Episode length: 348.77 +/- 193.85
Eval num_timesteps=18500, episode_reward=-387.40 +/- 181.17
Episode length: 387.68 +/- 180.72
Eval num_timesteps=19000, episode_reward=-337.65 +/- 196.60
Episode length: 338.06 +/- 196.12
Eval num_timesteps=19500, episode_reward=-303.60 +/- 199.49
Episode length: 304.10 +/- 199.00
New best mean reward!
Eval num_timesteps=20000, episode_reward=-241.71 +/- 196.01
Episode length: 242.35 +/- 195.54
New best mean reward!
Eval num_timesteps=20500, episode_reward=-139.50 +/- 141.00
Episode length: 140.37 +/- 140.67
New best mean reward!
Eval num_timesteps=21000, episode_reward=-175.70 +/- 170.55
Episode length: 176.49 +/- 170.15
Eval num_timesteps=21500, episode_reward=-221.41 +/- 192.09
Episode length: 222.09 +/- 191.62
Eval num_timesteps=22000, episode_reward=-186.15 +/- 174.22
Episode length: 186.92 +/- 173.81
Eval num_timesteps=22500, episode_reward=-199.28 +/- 183.84
Episode length: 200.01 +/- 183.40
Eval num_timesteps=23000, episode_reward=-188.27 +/- 176.95
Episode length: 189.03 +/- 176.53
Eval num_timesteps=23500, episode_reward=-106.48 +/- 93.55
Episode length: 107.43 +/- 93.34
New best mean reward!
Eval num_timesteps=24000, episode_reward=-132.17 +/- 133.36
Episode length: 133.06 +/- 133.06
Eval num_timesteps=24500, episode_reward=-121.35 +/- 110.10
Episode length: 122.28 +/- 109.86
Eval num_timesteps=25000, episode_reward=-128.03 +/- 117.62
Episode length: 128.95 +/- 117.37
Eval num_timesteps=25500, episode_reward=-233.75 +/- 193.70
Episode length: 234.41 +/- 193.23
Eval num_timesteps=26000, episode_reward=-180.99 +/- 171.63
Episode length: 181.77 +/- 171.22
Eval num_timesteps=26500, episode_reward=-189.30 +/- 175.51
Episode length: 190.07 +/- 175.11
Eval num_timesteps=27000, episode_reward=-187.27 +/- 176.41
Episode length: 188.03 +/- 175.98
Eval num_timesteps=27500, episode_reward=-235.51 +/- 199.48
Episode length: 236.15 +/- 199.00
Eval num_timesteps=28000, episode_reward=-196.53 +/- 181.54
Episode length: 197.27 +/- 181.11
Eval num_timesteps=28500, episode_reward=-138.13 +/- 141.41
Episode length: 139.01 +/- 141.10
Eval num_timesteps=29000, episode_reward=-116.31 +/- 106.07
Episode length: 117.25 +/- 105.85
Eval num_timesteps=29500, episode_reward=-163.95 +/- 159.72
Episode length: 164.77 +/- 159.34
Eval num_timesteps=30000, episode_reward=-149.21 +/- 153.39
Episode length: 150.06 +/- 153.04
Eval num_timesteps=30500, episode_reward=-156.97 +/- 156.07
Episode length: 157.80 +/- 155.70
Eval num_timesteps=31000, episode_reward=-191.42 +/- 179.27
Episode length: 192.17 +/- 178.84
Eval num_timesteps=31500, episode_reward=-199.62 +/- 184.06
Episode length: 200.35 +/- 183.62
Eval num_timesteps=32000, episode_reward=-157.10 +/- 151.61
Episode length: 157.95 +/- 151.27
Eval num_timesteps=32500, episode_reward=-177.04 +/- 168.07
Episode length: 177.83 +/- 167.67
Eval num_timesteps=33000, episode_reward=-134.44 +/- 136.63
Episode length: 135.32 +/- 136.31
Eval num_timesteps=33500, episode_reward=-157.69 +/- 156.60
Episode length: 158.52 +/- 156.23
Eval num_timesteps=34000, episode_reward=-154.89 +/- 153.17
Episode length: 155.74 +/- 152.84
Eval num_timesteps=34500, episode_reward=-142.04 +/- 139.74
Episode length: 142.92 +/- 139.43
Eval num_timesteps=35000, episode_reward=-140.20 +/- 145.65
Episode length: 141.07 +/- 145.33
Eval num_timesteps=35500, episode_reward=-106.23 +/- 95.50
Episode length: 107.18 +/- 95.29
New best mean reward!
Eval num_timesteps=36000, episode_reward=-128.40 +/- 129.85
Episode length: 129.30 +/- 129.56
Eval num_timesteps=36500, episode_reward=-129.59 +/- 132.03
Episode length: 130.48 +/- 131.73
Eval num_timesteps=37000, episode_reward=-138.65 +/- 137.63
Episode length: 139.53 +/- 137.32
Eval num_timesteps=37500, episode_reward=-126.69 +/- 126.77
Episode length: 127.59 +/- 126.48
Eval num_timesteps=38000, episode_reward=-107.21 +/- 94.61
Episode length: 108.16 +/- 94.40
Eval num_timesteps=38500, episode_reward=-116.42 +/- 113.62
Episode length: 117.35 +/- 113.39
Eval num_timesteps=39000, episode_reward=-109.90 +/- 100.72
Episode length: 110.85 +/- 100.53
Eval num_timesteps=39500, episode_reward=-131.74 +/- 130.41
Episode length: 132.64 +/- 130.13
Eval num_timesteps=40000, episode_reward=-99.64 +/- 83.60
Episode length: 100.61 +/- 83.46
New best mean reward!
FINISHED IN 740.1046223130543 s


starting seed  10027 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-499.28 +/- 7.16
Episode length: 499.29 +/- 7.06
New best mean reward!
Eval num_timesteps=5000, episode_reward=-499.36 +/- 6.27
Episode length: 499.38 +/- 6.17
Eval num_timesteps=5500, episode_reward=-162.08 +/- 55.52
Episode length: 163.06 +/- 55.40
New best mean reward!
Eval num_timesteps=6000, episode_reward=-426.03 +/- 99.05
Episode length: 426.48 +/- 98.64
Eval num_timesteps=6500, episode_reward=-177.84 +/- 49.14
Episode length: 178.83 +/- 49.08
Eval num_timesteps=7000, episode_reward=-189.92 +/- 67.30
Episode length: 190.90 +/- 67.21
Eval num_timesteps=7500, episode_reward=-166.92 +/- 43.36
Episode length: 167.92 +/- 43.36
Eval num_timesteps=8000, episode_reward=-151.63 +/- 37.09
Episode length: 152.63 +/- 37.09
New best mean reward!
Eval num_timesteps=8500, episode_reward=-152.33 +/- 36.42
Episode length: 153.33 +/- 36.42
Eval num_timesteps=9000, episode_reward=-128.92 +/- 31.77
Episode length: 129.92 +/- 31.77
New best mean reward!
Eval num_timesteps=9500, episode_reward=-109.81 +/- 49.42
Episode length: 110.80 +/- 49.34
New best mean reward!
Eval num_timesteps=10000, episode_reward=-145.76 +/- 30.20
Episode length: 146.76 +/- 30.20
Eval num_timesteps=10500, episode_reward=-142.95 +/- 28.90
Episode length: 143.95 +/- 28.90
Eval num_timesteps=11000, episode_reward=-99.99 +/- 29.25
Episode length: 100.99 +/- 29.25
New best mean reward!
FINISHED IN 207.900694912998 s


starting seed  10028 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-462.04 +/- 85.62
Episode length: 462.24 +/- 85.27
New best mean reward!
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-389.97 +/- 181.21
Episode length: 390.24 +/- 180.76
New best mean reward!
Eval num_timesteps=10500, episode_reward=-266.31 +/- 199.66
Episode length: 266.89 +/- 199.17
New best mean reward!
Eval num_timesteps=11000, episode_reward=-179.46 +/- 171.07
Episode length: 180.24 +/- 170.66
New best mean reward!
Eval num_timesteps=11500, episode_reward=-119.55 +/- 98.14
Episode length: 120.51 +/- 97.98
New best mean reward!
Eval num_timesteps=12000, episode_reward=-103.21 +/- 65.71
Episode length: 104.19 +/- 65.59
New best mean reward!
Eval num_timesteps=12500, episode_reward=-90.39 +/- 29.61
Episode length: 91.39 +/- 29.61
New best mean reward!
FINISHED IN 282.67541026102845 s


starting seed  10029 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-173.64 +/- 68.99
Episode length: 174.61 +/- 68.85
New best mean reward!
Eval num_timesteps=1500, episode_reward=-302.33 +/- 103.70
Episode length: 303.14 +/- 103.33
Eval num_timesteps=2000, episode_reward=-182.50 +/- 92.06
Episode length: 183.43 +/- 91.81
Eval num_timesteps=2500, episode_reward=-157.61 +/- 39.89
Episode length: 158.60 +/- 39.80
New best mean reward!
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=17000, episode_reward=-494.48 +/- 38.72
Episode length: 494.50 +/- 38.58
Eval num_timesteps=17500, episode_reward=-461.06 +/- 106.07
Episode length: 461.18 +/- 105.74
Eval num_timesteps=18000, episode_reward=-466.90 +/- 105.61
Episode length: 466.99 +/- 105.33
Eval num_timesteps=18500, episode_reward=-326.78 +/- 172.05
Episode length: 327.29 +/- 171.55
Eval num_timesteps=19000, episode_reward=-320.35 +/- 191.72
Episode length: 320.82 +/- 191.22
Eval num_timesteps=19500, episode_reward=-346.06 +/- 185.38
Episode length: 346.49 +/- 184.91
Eval num_timesteps=20000, episode_reward=-234.67 +/- 172.69
Episode length: 235.39 +/- 172.26
Eval num_timesteps=20500, episode_reward=-193.28 +/- 160.82
Episode length: 194.08 +/- 160.44
Eval num_timesteps=21000, episode_reward=-226.12 +/- 187.23
Episode length: 226.81 +/- 186.77
Eval num_timesteps=21500, episode_reward=-214.72 +/- 173.76
Episode length: 215.47 +/- 173.35
Eval num_timesteps=22000, episode_reward=-228.23 +/- 187.64
Episode length: 228.91 +/- 187.17
Eval num_timesteps=22500, episode_reward=-231.19 +/- 190.48
Episode length: 231.86 +/- 190.02
Eval num_timesteps=23000, episode_reward=-176.81 +/- 158.62
Episode length: 177.62 +/- 158.23
Eval num_timesteps=23500, episode_reward=-158.35 +/- 143.20
Episode length: 159.21 +/- 142.87
Eval num_timesteps=24000, episode_reward=-165.80 +/- 148.49
Episode length: 166.65 +/- 148.16
Eval num_timesteps=24500, episode_reward=-226.50 +/- 186.31
Episode length: 227.20 +/- 185.87
Eval num_timesteps=25000, episode_reward=-230.70 +/- 190.99
Episode length: 231.37 +/- 190.52
Eval num_timesteps=25500, episode_reward=-192.68 +/- 170.62
Episode length: 193.45 +/- 170.20
Eval num_timesteps=26000, episode_reward=-215.30 +/- 185.06
Episode length: 216.02 +/- 184.63
Eval num_timesteps=26500, episode_reward=-192.49 +/- 175.27
Episode length: 193.25 +/- 174.85
Eval num_timesteps=27000, episode_reward=-178.09 +/- 167.45
Episode length: 178.89 +/- 167.06
Eval num_timesteps=27500, episode_reward=-173.32 +/- 159.02
Episode length: 174.14 +/- 158.65
Eval num_timesteps=28000, episode_reward=-168.98 +/- 156.86
Episode length: 169.81 +/- 156.50
Eval num_timesteps=28500, episode_reward=-127.71 +/- 116.27
Episode length: 128.63 +/- 116.01
New best mean reward!
Eval num_timesteps=29000, episode_reward=-134.94 +/- 121.61
Episode length: 135.85 +/- 121.34
Eval num_timesteps=29500, episode_reward=-134.72 +/- 122.24
Episode length: 135.63 +/- 121.97
Eval num_timesteps=30000, episode_reward=-125.51 +/- 120.87
Episode length: 126.42 +/- 120.59
New best mean reward!
Eval num_timesteps=30500, episode_reward=-121.13 +/- 98.03
Episode length: 122.08 +/- 97.84
New best mean reward!
Eval num_timesteps=31000, episode_reward=-123.83 +/- 108.73
Episode length: 124.76 +/- 108.48
Eval num_timesteps=31500, episode_reward=-128.01 +/- 108.20
Episode length: 128.95 +/- 107.99
Eval num_timesteps=32000, episode_reward=-112.12 +/- 90.71
Episode length: 113.08 +/- 90.54
New best mean reward!
Eval num_timesteps=32500, episode_reward=-120.79 +/- 106.89
Episode length: 121.73 +/- 106.68
Eval num_timesteps=33000, episode_reward=-133.54 +/- 128.68
Episode length: 134.44 +/- 128.39
Eval num_timesteps=33500, episode_reward=-119.02 +/- 108.27
Episode length: 119.95 +/- 108.02
Eval num_timesteps=34000, episode_reward=-119.53 +/- 102.06
Episode length: 120.47 +/- 101.83
Eval num_timesteps=34500, episode_reward=-122.36 +/- 105.74
Episode length: 123.30 +/- 105.53
Eval num_timesteps=35000, episode_reward=-116.97 +/- 108.68
Episode length: 117.91 +/- 108.47
Eval num_timesteps=35500, episode_reward=-118.10 +/- 105.58
Episode length: 119.04 +/- 105.37
Eval num_timesteps=36000, episode_reward=-123.39 +/- 110.10
Episode length: 124.32 +/- 109.86
Eval num_timesteps=36500, episode_reward=-112.92 +/- 94.75
Episode length: 113.88 +/- 94.59
Eval num_timesteps=37000, episode_reward=-100.91 +/- 73.86
Episode length: 101.88 +/- 73.70
New best mean reward!
Eval num_timesteps=37500, episode_reward=-101.11 +/- 70.23
Episode length: 102.10 +/- 70.17
Eval num_timesteps=38000, episode_reward=-101.21 +/- 80.81
Episode length: 102.18 +/- 80.66
Eval num_timesteps=38500, episode_reward=-90.32 +/- 27.59
Episode length: 91.32 +/- 27.59
New best mean reward!
FINISHED IN 666.0092949940008 s


starting seed  10030 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-387.98 +/- 150.82
Episode length: 388.34 +/- 150.35
New best mean reward!
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-169.23 +/- 42.75
Episode length: 170.23 +/- 42.75
New best mean reward!
Eval num_timesteps=7500, episode_reward=-164.60 +/- 45.28
Episode length: 165.60 +/- 45.28
New best mean reward!
Eval num_timesteps=8000, episode_reward=-171.15 +/- 45.78
Episode length: 172.15 +/- 45.78
Eval num_timesteps=8500, episode_reward=-160.28 +/- 33.05
Episode length: 161.28 +/- 33.05
New best mean reward!
Eval num_timesteps=9000, episode_reward=-157.31 +/- 32.55
Episode length: 158.31 +/- 32.55
New best mean reward!
Eval num_timesteps=9500, episode_reward=-159.12 +/- 38.45
Episode length: 160.12 +/- 38.45
Eval num_timesteps=10000, episode_reward=-153.28 +/- 32.43
Episode length: 154.28 +/- 32.43
New best mean reward!
Eval num_timesteps=10500, episode_reward=-145.92 +/- 29.83
Episode length: 146.92 +/- 29.83
New best mean reward!
Eval num_timesteps=11000, episode_reward=-150.62 +/- 33.64
Episode length: 151.62 +/- 33.64
Eval num_timesteps=11500, episode_reward=-146.81 +/- 25.16
Episode length: 147.81 +/- 25.16
Eval num_timesteps=12000, episode_reward=-150.00 +/- 36.14
Episode length: 151.00 +/- 36.14
Eval num_timesteps=12500, episode_reward=-146.04 +/- 16.53
Episode length: 147.04 +/- 16.53
Eval num_timesteps=13000, episode_reward=-158.03 +/- 26.37
Episode length: 159.03 +/- 26.37
Eval num_timesteps=13500, episode_reward=-409.66 +/- 145.97
Episode length: 409.94 +/- 145.52
Eval num_timesteps=14000, episode_reward=-155.95 +/- 28.79
Episode length: 156.95 +/- 28.79
Eval num_timesteps=14500, episode_reward=-154.90 +/- 24.50
Episode length: 155.90 +/- 24.50
Eval num_timesteps=15000, episode_reward=-152.91 +/- 24.84
Episode length: 153.91 +/- 24.84
Eval num_timesteps=15500, episode_reward=-151.33 +/- 20.54
Episode length: 152.33 +/- 20.54
Eval num_timesteps=16000, episode_reward=-174.96 +/- 68.37
Episode length: 175.93 +/- 68.23
Eval num_timesteps=16500, episode_reward=-494.39 +/- 39.28
Episode length: 494.41 +/- 39.14
Eval num_timesteps=17000, episode_reward=-234.64 +/- 134.85
Episode length: 235.46 +/- 134.50
Eval num_timesteps=17500, episode_reward=-154.89 +/- 25.19
Episode length: 155.89 +/- 25.19
Eval num_timesteps=18000, episode_reward=-465.53 +/- 98.29
Episode length: 465.64 +/- 97.98
Eval num_timesteps=18500, episode_reward=-486.56 +/- 54.41
Episode length: 486.63 +/- 54.18
Eval num_timesteps=19000, episode_reward=-342.72 +/- 171.30
Episode length: 343.18 +/- 170.80
Eval num_timesteps=19500, episode_reward=-150.27 +/- 25.06
Episode length: 151.27 +/- 25.06
Eval num_timesteps=20000, episode_reward=-155.04 +/- 43.97
Episode length: 156.03 +/- 43.89
Eval num_timesteps=20500, episode_reward=-140.13 +/- 41.41
Episode length: 141.12 +/- 41.32
New best mean reward!
Eval num_timesteps=21000, episode_reward=-138.16 +/- 17.92
Episode length: 139.16 +/- 17.92
New best mean reward!
Eval num_timesteps=21500, episode_reward=-104.37 +/- 20.80
Episode length: 105.37 +/- 20.80
New best mean reward!
Eval num_timesteps=22000, episode_reward=-115.63 +/- 45.14
Episode length: 116.62 +/- 45.06
Eval num_timesteps=22500, episode_reward=-348.11 +/- 175.90
Episode length: 348.54 +/- 175.41
Eval num_timesteps=23000, episode_reward=-297.35 +/- 168.58
Episode length: 297.95 +/- 168.10
Eval num_timesteps=23500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=24000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=24500, episode_reward=-286.99 +/- 182.68
Episode length: 287.57 +/- 182.19
Eval num_timesteps=25000, episode_reward=-146.10 +/- 107.48
Episode length: 147.02 +/- 107.22
Eval num_timesteps=25500, episode_reward=-116.24 +/- 64.89
Episode length: 117.22 +/- 64.77
Eval num_timesteps=26000, episode_reward=-127.99 +/- 80.43
Episode length: 128.95 +/- 80.24
Eval num_timesteps=26500, episode_reward=-119.04 +/- 49.54
Episode length: 120.03 +/- 49.47
Eval num_timesteps=27000, episode_reward=-108.88 +/- 13.22
Episode length: 109.88 +/- 13.22
Eval num_timesteps=27500, episode_reward=-117.05 +/- 28.15
Episode length: 118.05 +/- 28.15
Eval num_timesteps=28000, episode_reward=-111.82 +/- 43.78
Episode length: 112.81 +/- 43.69
Eval num_timesteps=28500, episode_reward=-102.99 +/- 45.32
Episode length: 103.98 +/- 45.23
New best mean reward!
Eval num_timesteps=29000, episode_reward=-107.02 +/- 17.16
Episode length: 108.02 +/- 17.16
Eval num_timesteps=29500, episode_reward=-113.09 +/- 46.44
Episode length: 114.08 +/- 46.36
Eval num_timesteps=30000, episode_reward=-98.95 +/- 16.50
Episode length: 99.95 +/- 16.50
New best mean reward!
FINISHED IN 534.0787349560414 s


starting seed  10031 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-417.18 +/- 146.55
Episode length: 417.43 +/- 146.13
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-285.76 +/- 180.68
Episode length: 286.35 +/- 180.20
New best mean reward!
Eval num_timesteps=2000, episode_reward=-367.15 +/- 113.79
Episode length: 367.80 +/- 113.38
Eval num_timesteps=2500, episode_reward=-191.85 +/- 37.84
Episode length: 192.85 +/- 37.84
New best mean reward!
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-433.27 +/- 82.37
Episode length: 433.75 +/- 81.95
Eval num_timesteps=5000, episode_reward=-280.80 +/- 146.93
Episode length: 281.50 +/- 146.48
Eval num_timesteps=5500, episode_reward=-476.76 +/- 84.78
Episode length: 476.83 +/- 84.53
Eval num_timesteps=6000, episode_reward=-447.49 +/- 116.83
Episode length: 447.66 +/- 116.46
Eval num_timesteps=6500, episode_reward=-474.79 +/- 83.92
Episode length: 474.88 +/- 83.64
Eval num_timesteps=7000, episode_reward=-463.54 +/- 99.86
Episode length: 463.66 +/- 99.54
Eval num_timesteps=7500, episode_reward=-490.81 +/- 52.71
Episode length: 490.84 +/- 52.54
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-381.76 +/- 185.91
Episode length: 382.05 +/- 185.46
Eval num_timesteps=10000, episode_reward=-280.38 +/- 198.19
Episode length: 280.94 +/- 197.70
Eval num_timesteps=10500, episode_reward=-171.66 +/- 156.93
Episode length: 172.48 +/- 156.55
New best mean reward!
Eval num_timesteps=11000, episode_reward=-152.67 +/- 144.27
Episode length: 153.53 +/- 143.93
New best mean reward!
Eval num_timesteps=11500, episode_reward=-139.08 +/- 124.20
Episode length: 139.98 +/- 123.91
New best mean reward!
Eval num_timesteps=12000, episode_reward=-105.73 +/- 67.58
Episode length: 106.71 +/- 67.46
New best mean reward!
Eval num_timesteps=12500, episode_reward=-102.17 +/- 31.29
Episode length: 103.17 +/- 31.29
New best mean reward!
Eval num_timesteps=13000, episode_reward=-99.69 +/- 25.70
Episode length: 100.69 +/- 25.70
New best mean reward!
FINISHED IN 244.12567296100315 s


starting seed  10032 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-178.76 +/- 49.25
Episode length: 179.76 +/- 49.25
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-118.96 +/- 32.26
Episode length: 119.96 +/- 32.26
New best mean reward!
Eval num_timesteps=12000, episode_reward=-337.21 +/- 172.06
Episode length: 337.70 +/- 171.58
Eval num_timesteps=12500, episode_reward=-284.84 +/- 173.18
Episode length: 285.46 +/- 172.71
Eval num_timesteps=13000, episode_reward=-91.32 +/- 25.06
Episode length: 92.32 +/- 25.06
New best mean reward!
FINISHED IN 278.05012506700587 s


starting seed  10033 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-491.81 +/- 57.34
Episode length: 491.83 +/- 57.20
New best mean reward!
Eval num_timesteps=13500, episode_reward=-433.18 +/- 142.81
Episode length: 433.36 +/- 142.42
New best mean reward!
Eval num_timesteps=14000, episode_reward=-188.52 +/- 144.40
Episode length: 189.36 +/- 144.06
New best mean reward!
Eval num_timesteps=14500, episode_reward=-140.63 +/- 88.38
Episode length: 141.58 +/- 88.18
New best mean reward!
Eval num_timesteps=15000, episode_reward=-141.76 +/- 109.45
Episode length: 142.68 +/- 109.19
Eval num_timesteps=15500, episode_reward=-130.04 +/- 117.85
Episode length: 130.95 +/- 117.57
New best mean reward!
Eval num_timesteps=16000, episode_reward=-283.29 +/- 201.44
Episode length: 283.83 +/- 200.94
Eval num_timesteps=16500, episode_reward=-180.61 +/- 129.53
Episode length: 181.48 +/- 129.21
Eval num_timesteps=17000, episode_reward=-227.83 +/- 167.22
Episode length: 228.56 +/- 166.78
Eval num_timesteps=17500, episode_reward=-166.58 +/- 145.16
Episode length: 167.43 +/- 144.81
Eval num_timesteps=18000, episode_reward=-144.74 +/- 89.34
Episode length: 145.69 +/- 89.14
Eval num_timesteps=18500, episode_reward=-129.79 +/- 107.23
Episode length: 130.72 +/- 106.98
New best mean reward!
Eval num_timesteps=19000, episode_reward=-129.14 +/- 68.50
Episode length: 130.12 +/- 68.39
New best mean reward!
Eval num_timesteps=19500, episode_reward=-137.99 +/- 73.56
Episode length: 138.97 +/- 73.46
Eval num_timesteps=20000, episode_reward=-126.72 +/- 54.86
Episode length: 127.71 +/- 54.79
New best mean reward!
Eval num_timesteps=20500, episode_reward=-149.49 +/- 124.89
Episode length: 150.39 +/- 124.61
Eval num_timesteps=21000, episode_reward=-146.92 +/- 112.38
Episode length: 147.84 +/- 112.13
Eval num_timesteps=21500, episode_reward=-140.34 +/- 114.18
Episode length: 141.27 +/- 113.96
Eval num_timesteps=22000, episode_reward=-145.56 +/- 109.00
Episode length: 146.48 +/- 108.74
Eval num_timesteps=22500, episode_reward=-185.51 +/- 109.17
Episode length: 186.42 +/- 108.91
Eval num_timesteps=23000, episode_reward=-143.50 +/- 99.22
Episode length: 144.44 +/- 99.01
Eval num_timesteps=23500, episode_reward=-120.06 +/- 81.47
Episode length: 121.02 +/- 81.29
New best mean reward!
Eval num_timesteps=24000, episode_reward=-105.84 +/- 62.05
Episode length: 106.82 +/- 61.92
New best mean reward!
Eval num_timesteps=24500, episode_reward=-137.05 +/- 110.12
Episode length: 137.98 +/- 109.89
Eval num_timesteps=25000, episode_reward=-128.09 +/- 105.81
Episode length: 129.02 +/- 105.56
Eval num_timesteps=25500, episode_reward=-126.21 +/- 112.25
Episode length: 127.14 +/- 112.02
Eval num_timesteps=26000, episode_reward=-119.33 +/- 103.15
Episode length: 120.27 +/- 102.93
Eval num_timesteps=26500, episode_reward=-130.48 +/- 124.93
Episode length: 131.38 +/- 124.64
Eval num_timesteps=27000, episode_reward=-108.76 +/- 91.80
Episode length: 109.71 +/- 91.59
Eval num_timesteps=27500, episode_reward=-110.16 +/- 84.91
Episode length: 111.12 +/- 84.73
Eval num_timesteps=28000, episode_reward=-120.03 +/- 103.70
Episode length: 120.97 +/- 103.48
Eval num_timesteps=28500, episode_reward=-122.68 +/- 112.83
Episode length: 123.60 +/- 112.57
Eval num_timesteps=29000, episode_reward=-99.39 +/- 63.87
Episode length: 100.37 +/- 63.75
New best mean reward!
FINISHED IN 503.1081234490266 s


starting seed  10034 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-268.80 +/- 193.32
Episode length: 269.39 +/- 192.83
New best mean reward!
Eval num_timesteps=10500, episode_reward=-133.09 +/- 95.78
Episode length: 134.04 +/- 95.59
New best mean reward!
Eval num_timesteps=11000, episode_reward=-467.42 +/- 110.51
Episode length: 467.50 +/- 110.24
Eval num_timesteps=11500, episode_reward=-88.35 +/- 19.46
Episode length: 89.35 +/- 19.46
New best mean reward!
FINISHED IN 277.65330987004563 s


starting seed  10035 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-432.86 +/- 148.41
Episode length: 433.03 +/- 148.03
New best mean reward!
Eval num_timesteps=9500, episode_reward=-173.87 +/- 169.07
Episode length: 174.66 +/- 168.67
New best mean reward!
Eval num_timesteps=10000, episode_reward=-124.89 +/- 119.47
Episode length: 125.80 +/- 119.19
New best mean reward!
Eval num_timesteps=10500, episode_reward=-135.24 +/- 124.77
Episode length: 136.14 +/- 124.47
Eval num_timesteps=11000, episode_reward=-134.18 +/- 128.91
Episode length: 135.08 +/- 128.63
Eval num_timesteps=11500, episode_reward=-199.02 +/- 179.46
Episode length: 199.76 +/- 179.03
Eval num_timesteps=12000, episode_reward=-109.45 +/- 92.42
Episode length: 110.40 +/- 92.21
New best mean reward!
Eval num_timesteps=12500, episode_reward=-98.65 +/- 70.68
Episode length: 99.63 +/- 70.57
New best mean reward!
FINISHED IN 350.8862037989893 s


starting seed  10036 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-133.19 +/- 28.57
Episode length: 134.19 +/- 28.57
New best mean reward!
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-183.70 +/- 75.55
Episode length: 184.70 +/- 75.55
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-386.23 +/- 163.37
Episode length: 386.57 +/- 162.91
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-487.92 +/- 68.69
Episode length: 487.95 +/- 68.52
Eval num_timesteps=8500, episode_reward=-458.88 +/- 123.39
Episode length: 458.98 +/- 123.09
Eval num_timesteps=9000, episode_reward=-416.14 +/- 162.90
Episode length: 416.35 +/- 162.50
Eval num_timesteps=9500, episode_reward=-357.37 +/- 190.96
Episode length: 357.73 +/- 190.48
Eval num_timesteps=10000, episode_reward=-403.65 +/- 171.77
Episode length: 403.89 +/- 171.34
Eval num_timesteps=10500, episode_reward=-351.28 +/- 189.15
Episode length: 351.67 +/- 188.67
Eval num_timesteps=11000, episode_reward=-268.98 +/- 195.77
Episode length: 269.57 +/- 195.29
Eval num_timesteps=11500, episode_reward=-212.23 +/- 185.58
Episode length: 212.94 +/- 185.13
Eval num_timesteps=12000, episode_reward=-262.69 +/- 201.35
Episode length: 263.28 +/- 200.87
Eval num_timesteps=12500, episode_reward=-213.45 +/- 186.88
Episode length: 214.16 +/- 186.44
Eval num_timesteps=13000, episode_reward=-274.92 +/- 200.83
Episode length: 275.48 +/- 200.33
Eval num_timesteps=13500, episode_reward=-260.02 +/- 200.70
Episode length: 260.62 +/- 200.22
Eval num_timesteps=14000, episode_reward=-229.95 +/- 188.44
Episode length: 230.64 +/- 188.00
Eval num_timesteps=14500, episode_reward=-224.11 +/- 191.13
Episode length: 224.80 +/- 190.68
Eval num_timesteps=15000, episode_reward=-186.33 +/- 176.74
Episode length: 187.10 +/- 176.33
Eval num_timesteps=15500, episode_reward=-174.10 +/- 165.27
Episode length: 174.90 +/- 164.87
Eval num_timesteps=16000, episode_reward=-137.42 +/- 130.53
Episode length: 138.32 +/- 130.25
Eval num_timesteps=16500, episode_reward=-159.46 +/- 151.98
Episode length: 160.30 +/- 151.63
Eval num_timesteps=17000, episode_reward=-143.05 +/- 134.11
Episode length: 143.95 +/- 133.84
Eval num_timesteps=17500, episode_reward=-120.30 +/- 110.90
Episode length: 121.23 +/- 110.66
New best mean reward!
Eval num_timesteps=18000, episode_reward=-108.52 +/- 92.99
Episode length: 109.47 +/- 92.78
New best mean reward!
Eval num_timesteps=18500, episode_reward=-119.41 +/- 104.53
Episode length: 120.35 +/- 104.31
Eval num_timesteps=19000, episode_reward=-130.64 +/- 126.45
Episode length: 131.55 +/- 126.19
Eval num_timesteps=19500, episode_reward=-121.79 +/- 109.39
Episode length: 122.72 +/- 109.14
Eval num_timesteps=20000, episode_reward=-99.07 +/- 80.58
Episode length: 100.04 +/- 80.43
New best mean reward!
FINISHED IN 395.37564187298995 s


starting seed  10037 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-476.09 +/- 94.67
Episode length: 476.15 +/- 94.43
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-424.28 +/- 157.04
Episode length: 424.47 +/- 156.65
New best mean reward!
Eval num_timesteps=11000, episode_reward=-412.04 +/- 166.29
Episode length: 412.26 +/- 165.88
New best mean reward!
Eval num_timesteps=11500, episode_reward=-313.62 +/- 202.97
Episode length: 314.08 +/- 202.47
New best mean reward!
Eval num_timesteps=12000, episode_reward=-355.19 +/- 195.13
Episode length: 355.55 +/- 194.65
Eval num_timesteps=12500, episode_reward=-285.45 +/- 200.88
Episode length: 285.99 +/- 200.38
New best mean reward!
Eval num_timesteps=13000, episode_reward=-295.98 +/- 204.76
Episode length: 296.48 +/- 204.27
Eval num_timesteps=13500, episode_reward=-246.88 +/- 200.84
Episode length: 247.50 +/- 200.36
New best mean reward!
Eval num_timesteps=14000, episode_reward=-192.36 +/- 177.28
Episode length: 193.12 +/- 176.86
New best mean reward!
Eval num_timesteps=14500, episode_reward=-182.01 +/- 169.48
Episode length: 182.80 +/- 169.09
New best mean reward!
Eval num_timesteps=15000, episode_reward=-181.47 +/- 172.31
Episode length: 182.25 +/- 171.91
New best mean reward!
Eval num_timesteps=15500, episode_reward=-147.79 +/- 144.32
Episode length: 148.65 +/- 143.98
New best mean reward!
Eval num_timesteps=16000, episode_reward=-144.87 +/- 142.02
Episode length: 145.75 +/- 141.72
New best mean reward!
Eval num_timesteps=16500, episode_reward=-145.37 +/- 140.70
Episode length: 146.24 +/- 140.37
Eval num_timesteps=17000, episode_reward=-145.96 +/- 145.35
Episode length: 146.82 +/- 145.01
Eval num_timesteps=17500, episode_reward=-169.00 +/- 167.31
Episode length: 169.80 +/- 166.92
Eval num_timesteps=18000, episode_reward=-114.51 +/- 101.66
Episode length: 115.45 +/- 101.43
New best mean reward!
Eval num_timesteps=18500, episode_reward=-98.63 +/- 64.22
Episode length: 99.61 +/- 64.10
New best mean reward!
FINISHED IN 385.76405232201796 s


starting seed  10038 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-408.47 +/- 142.08
Episode length: 408.77 +/- 141.63
New best mean reward!
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-161.19 +/- 39.00
Episode length: 162.19 +/- 39.00
New best mean reward!
Eval num_timesteps=7500, episode_reward=-135.22 +/- 25.38
Episode length: 136.22 +/- 25.38
New best mean reward!
Eval num_timesteps=8000, episode_reward=-121.06 +/- 23.03
Episode length: 122.06 +/- 23.03
New best mean reward!
Eval num_timesteps=8500, episode_reward=-136.00 +/- 46.33
Episode length: 136.99 +/- 46.25
Eval num_timesteps=9000, episode_reward=-136.29 +/- 39.53
Episode length: 137.29 +/- 39.53
Eval num_timesteps=9500, episode_reward=-94.45 +/- 25.49
Episode length: 95.45 +/- 25.49
New best mean reward!
FINISHED IN 188.19374660897302 s


starting seed  10039 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-157.89 +/- 40.57
Episode length: 158.88 +/- 40.49
New best mean reward!
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-169.86 +/- 89.17
Episode length: 170.80 +/- 88.95
Eval num_timesteps=8000, episode_reward=-481.54 +/- 80.60
Episode length: 481.59 +/- 80.38
Eval num_timesteps=8500, episode_reward=-118.49 +/- 63.38
Episode length: 119.47 +/- 63.26
New best mean reward!
Eval num_timesteps=9000, episode_reward=-107.66 +/- 22.79
Episode length: 108.66 +/- 22.79
New best mean reward!
Eval num_timesteps=9500, episode_reward=-454.81 +/- 123.37
Episode length: 454.93 +/- 123.05
Eval num_timesteps=10000, episode_reward=-285.41 +/- 194.75
Episode length: 285.96 +/- 194.25
Eval num_timesteps=10500, episode_reward=-254.54 +/- 192.74
Episode length: 255.16 +/- 192.26
Eval num_timesteps=11000, episode_reward=-86.08 +/- 18.21
Episode length: 87.08 +/- 18.21
New best mean reward!
FINISHED IN 226.14202572696377 s


starting seed  10040 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-176.30 +/- 88.27
Episode length: 177.26 +/- 88.13
New best mean reward!
Eval num_timesteps=6500, episode_reward=-126.67 +/- 57.66
Episode length: 127.66 +/- 57.59
New best mean reward!
Eval num_timesteps=7000, episode_reward=-183.24 +/- 82.69
Episode length: 184.20 +/- 82.54
Eval num_timesteps=7500, episode_reward=-188.06 +/- 110.84
Episode length: 188.98 +/- 110.62
Eval num_timesteps=8000, episode_reward=-206.77 +/- 111.19
Episode length: 207.74 +/- 111.11
Eval num_timesteps=8500, episode_reward=-188.69 +/- 118.17
Episode length: 189.62 +/- 117.99
Eval num_timesteps=9000, episode_reward=-202.65 +/- 138.42
Episode length: 203.49 +/- 138.07
Eval num_timesteps=9500, episode_reward=-264.48 +/- 171.60
Episode length: 265.15 +/- 171.14
Eval num_timesteps=10000, episode_reward=-275.28 +/- 173.71
Episode length: 275.92 +/- 173.24
Eval num_timesteps=10500, episode_reward=-214.15 +/- 118.06
Episode length: 215.05 +/- 117.81
Eval num_timesteps=11000, episode_reward=-151.64 +/- 92.89
Episode length: 152.59 +/- 92.70
Eval num_timesteps=11500, episode_reward=-159.04 +/- 95.00
Episode length: 160.00 +/- 94.86
Eval num_timesteps=12000, episode_reward=-167.18 +/- 108.50
Episode length: 168.10 +/- 108.26
Eval num_timesteps=12500, episode_reward=-183.75 +/- 116.56
Episode length: 184.65 +/- 116.29
Eval num_timesteps=13000, episode_reward=-163.11 +/- 74.23
Episode length: 164.09 +/- 74.14
Eval num_timesteps=13500, episode_reward=-164.45 +/- 60.85
Episode length: 165.45 +/- 60.85
Eval num_timesteps=14000, episode_reward=-195.05 +/- 89.80
Episode length: 196.03 +/- 89.73
Eval num_timesteps=14500, episode_reward=-205.48 +/- 95.67
Episode length: 206.43 +/- 95.52
Eval num_timesteps=15000, episode_reward=-249.44 +/- 101.26
Episode length: 250.37 +/- 101.08
Eval num_timesteps=15500, episode_reward=-227.61 +/- 113.58
Episode length: 228.53 +/- 113.39
Eval num_timesteps=16000, episode_reward=-262.91 +/- 97.29
Episode length: 263.86 +/- 97.17
Eval num_timesteps=16500, episode_reward=-186.40 +/- 66.05
Episode length: 187.39 +/- 66.00
Eval num_timesteps=17000, episode_reward=-298.81 +/- 103.56
Episode length: 299.72 +/- 103.39
Eval num_timesteps=17500, episode_reward=-250.70 +/- 100.27
Episode length: 251.65 +/- 100.15
Eval num_timesteps=18000, episode_reward=-219.39 +/- 117.12
Episode length: 220.29 +/- 116.88
Eval num_timesteps=18500, episode_reward=-210.72 +/- 116.64
Episode length: 211.64 +/- 116.44
Eval num_timesteps=19000, episode_reward=-194.97 +/- 83.54
Episode length: 195.93 +/- 83.40
Eval num_timesteps=19500, episode_reward=-196.03 +/- 70.60
Episode length: 197.02 +/- 70.56
Eval num_timesteps=20000, episode_reward=-196.72 +/- 96.91
Episode length: 197.68 +/- 96.79
Eval num_timesteps=20500, episode_reward=-177.13 +/- 97.67
Episode length: 178.10 +/- 97.57
Eval num_timesteps=21000, episode_reward=-198.19 +/- 98.67
Episode length: 199.16 +/- 98.57
Eval num_timesteps=21500, episode_reward=-228.99 +/- 112.93
Episode length: 229.91 +/- 112.74
Eval num_timesteps=22000, episode_reward=-251.43 +/- 125.61
Episode length: 252.30 +/- 125.35
Eval num_timesteps=22500, episode_reward=-233.10 +/- 118.81
Episode length: 234.00 +/- 118.59
Eval num_timesteps=23000, episode_reward=-363.83 +/- 142.05
Episode length: 364.39 +/- 141.63
Eval num_timesteps=23500, episode_reward=-356.72 +/- 154.79
Episode length: 357.22 +/- 154.33
Eval num_timesteps=24000, episode_reward=-317.83 +/- 147.72
Episode length: 318.46 +/- 147.27
Eval num_timesteps=24500, episode_reward=-270.95 +/- 152.43
Episode length: 271.67 +/- 152.01
Eval num_timesteps=25000, episode_reward=-219.98 +/- 161.46
Episode length: 220.74 +/- 161.05
Eval num_timesteps=25500, episode_reward=-197.53 +/- 163.82
Episode length: 198.31 +/- 163.41
Eval num_timesteps=26000, episode_reward=-165.15 +/- 135.80
Episode length: 166.03 +/- 135.51
Eval num_timesteps=26500, episode_reward=-180.83 +/- 151.52
Episode length: 181.69 +/- 151.22
Eval num_timesteps=27000, episode_reward=-188.55 +/- 144.30
Episode length: 189.38 +/- 143.93
Eval num_timesteps=27500, episode_reward=-198.11 +/- 159.90
Episode length: 198.91 +/- 159.52
Eval num_timesteps=28000, episode_reward=-226.68 +/- 187.55
Episode length: 227.37 +/- 187.09
Eval num_timesteps=28500, episode_reward=-274.28 +/- 198.61
Episode length: 274.86 +/- 198.14
Eval num_timesteps=29000, episode_reward=-302.74 +/- 201.65
Episode length: 303.24 +/- 201.16
Eval num_timesteps=29500, episode_reward=-193.87 +/- 178.77
Episode length: 194.62 +/- 178.35
Eval num_timesteps=30000, episode_reward=-94.60 +/- 49.40
Episode length: 95.59 +/- 49.32
New best mean reward!
FINISHED IN 519.8986491910182 s


starting seed  10041 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-181.00 +/- 103.40
Episode length: 181.92 +/- 103.16
New best mean reward!
Eval num_timesteps=6000, episode_reward=-118.51 +/- 36.39
Episode length: 119.51 +/- 36.39
New best mean reward!
Eval num_timesteps=6500, episode_reward=-108.54 +/- 92.98
Episode length: 109.49 +/- 92.77
New best mean reward!
Eval num_timesteps=7000, episode_reward=-179.59 +/- 166.80
Episode length: 180.39 +/- 166.41
Eval num_timesteps=7500, episode_reward=-249.79 +/- 197.46
Episode length: 250.41 +/- 196.98
Eval num_timesteps=8000, episode_reward=-396.38 +/- 175.29
Episode length: 396.64 +/- 174.85
Eval num_timesteps=8500, episode_reward=-321.81 +/- 201.86
Episode length: 322.25 +/- 201.36
Eval num_timesteps=9000, episode_reward=-166.38 +/- 158.21
Episode length: 167.20 +/- 157.83
Eval num_timesteps=9500, episode_reward=-284.35 +/- 203.78
Episode length: 284.88 +/- 203.28
Eval num_timesteps=10000, episode_reward=-332.84 +/- 201.00
Episode length: 333.25 +/- 200.51
Eval num_timesteps=10500, episode_reward=-327.85 +/- 199.14
Episode length: 328.28 +/- 198.65
Eval num_timesteps=11000, episode_reward=-266.44 +/- 196.60
Episode length: 267.03 +/- 196.12
Eval num_timesteps=11500, episode_reward=-189.23 +/- 176.05
Episode length: 189.99 +/- 175.62
Eval num_timesteps=12000, episode_reward=-194.98 +/- 178.68
Episode length: 195.73 +/- 178.25
Eval num_timesteps=12500, episode_reward=-254.73 +/- 197.23
Episode length: 255.34 +/- 196.75
Eval num_timesteps=13000, episode_reward=-153.98 +/- 147.65
Episode length: 154.83 +/- 147.30
Eval num_timesteps=13500, episode_reward=-156.14 +/- 152.57
Episode length: 156.98 +/- 152.21
Eval num_timesteps=14000, episode_reward=-116.51 +/- 107.62
Episode length: 117.44 +/- 107.38
Eval num_timesteps=14500, episode_reward=-116.63 +/- 107.55
Episode length: 117.57 +/- 107.34
Eval num_timesteps=15000, episode_reward=-144.50 +/- 145.74
Episode length: 145.36 +/- 145.40
Eval num_timesteps=15500, episode_reward=-118.43 +/- 101.25
Episode length: 119.37 +/- 101.02
Eval num_timesteps=16000, episode_reward=-103.40 +/- 75.35
Episode length: 104.37 +/- 75.19
New best mean reward!
Eval num_timesteps=16500, episode_reward=-85.95 +/- 44.53
Episode length: 86.94 +/- 44.44
New best mean reward!
FINISHED IN 348.5911876820028 s


starting seed  10042 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-470.40 +/- 78.86
Episode length: 470.55 +/- 78.54
New best mean reward!
Eval num_timesteps=1500, episode_reward=-446.85 +/- 90.11
Episode length: 447.19 +/- 89.72
New best mean reward!
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-454.54 +/- 123.49
Episode length: 454.66 +/- 123.17
Eval num_timesteps=9500, episode_reward=-294.53 +/- 198.62
Episode length: 295.05 +/- 198.12
New best mean reward!
Eval num_timesteps=10000, episode_reward=-239.30 +/- 189.74
Episode length: 239.96 +/- 189.27
New best mean reward!
Eval num_timesteps=10500, episode_reward=-183.11 +/- 161.06
Episode length: 183.91 +/- 160.67
New best mean reward!
Eval num_timesteps=11000, episode_reward=-145.48 +/- 133.44
Episode length: 146.36 +/- 133.12
New best mean reward!
Eval num_timesteps=11500, episode_reward=-143.74 +/- 135.81
Episode length: 144.62 +/- 135.50
New best mean reward!
Eval num_timesteps=12000, episode_reward=-95.57 +/- 37.45
Episode length: 96.57 +/- 37.45
New best mean reward!
FINISHED IN 292.9653576610144 s


starting seed  10043 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-183.15 +/- 63.98
Episode length: 184.13 +/- 63.88
New best mean reward!
Eval num_timesteps=9500, episode_reward=-162.25 +/- 42.86
Episode length: 163.25 +/- 42.86
New best mean reward!
Eval num_timesteps=10000, episode_reward=-184.14 +/- 54.07
Episode length: 185.14 +/- 54.07
Eval num_timesteps=10500, episode_reward=-189.88 +/- 91.98
Episode length: 190.81 +/- 91.74
Eval num_timesteps=11000, episode_reward=-157.34 +/- 40.78
Episode length: 158.34 +/- 40.78
New best mean reward!
Eval num_timesteps=11500, episode_reward=-183.35 +/- 75.04
Episode length: 184.33 +/- 74.96
Eval num_timesteps=12000, episode_reward=-174.80 +/- 82.42
Episode length: 175.75 +/- 82.22
Eval num_timesteps=12500, episode_reward=-157.62 +/- 41.46
Episode length: 158.62 +/- 41.46
Eval num_timesteps=13000, episode_reward=-152.23 +/- 33.30
Episode length: 153.23 +/- 33.30
New best mean reward!
Eval num_timesteps=13500, episode_reward=-155.66 +/- 45.84
Episode length: 156.65 +/- 45.77
Eval num_timesteps=14000, episode_reward=-148.38 +/- 25.69
Episode length: 149.38 +/- 25.69
New best mean reward!
Eval num_timesteps=14500, episode_reward=-158.55 +/- 37.87
Episode length: 159.55 +/- 37.87
Eval num_timesteps=15000, episode_reward=-156.87 +/- 32.57
Episode length: 157.87 +/- 32.57
Eval num_timesteps=15500, episode_reward=-166.33 +/- 51.79
Episode length: 167.32 +/- 51.73
Eval num_timesteps=16000, episode_reward=-155.93 +/- 33.36
Episode length: 156.93 +/- 33.36
Eval num_timesteps=16500, episode_reward=-157.54 +/- 32.65
Episode length: 158.54 +/- 32.65
Eval num_timesteps=17000, episode_reward=-149.34 +/- 30.15
Episode length: 150.34 +/- 30.15
Eval num_timesteps=17500, episode_reward=-151.95 +/- 34.06
Episode length: 152.95 +/- 34.06
Eval num_timesteps=18000, episode_reward=-162.86 +/- 52.80
Episode length: 163.85 +/- 52.74
Eval num_timesteps=18500, episode_reward=-148.09 +/- 25.59
Episode length: 149.09 +/- 25.59
New best mean reward!
Eval num_timesteps=19000, episode_reward=-157.62 +/- 53.44
Episode length: 158.61 +/- 53.38
Eval num_timesteps=19500, episode_reward=-154.10 +/- 32.94
Episode length: 155.10 +/- 32.94
Eval num_timesteps=20000, episode_reward=-155.42 +/- 49.81
Episode length: 156.41 +/- 49.74
Eval num_timesteps=20500, episode_reward=-155.75 +/- 35.93
Episode length: 156.75 +/- 35.93
Eval num_timesteps=21000, episode_reward=-160.80 +/- 57.63
Episode length: 161.78 +/- 57.51
Eval num_timesteps=21500, episode_reward=-153.79 +/- 42.06
Episode length: 154.78 +/- 41.98
Eval num_timesteps=22000, episode_reward=-132.74 +/- 22.93
Episode length: 133.74 +/- 22.93
New best mean reward!
Eval num_timesteps=22500, episode_reward=-126.96 +/- 21.45
Episode length: 127.96 +/- 21.45
New best mean reward!
Eval num_timesteps=23000, episode_reward=-120.00 +/- 34.19
Episode length: 121.00 +/- 34.19
New best mean reward!
Eval num_timesteps=23500, episode_reward=-124.50 +/- 62.73
Episode length: 125.48 +/- 62.61
Eval num_timesteps=24000, episode_reward=-89.71 +/- 21.12
Episode length: 90.71 +/- 21.12
New best mean reward!
FINISHED IN 393.3195854279911 s


starting seed  10044 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-439.31 +/- 126.72
Episode length: 439.50 +/- 126.33
New best mean reward!
Eval num_timesteps=9500, episode_reward=-85.50 +/- 26.02
Episode length: 86.50 +/- 26.02
New best mean reward!
FINISHED IN 296.2185930289561 s


starting seed  10045 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-172.54 +/- 69.26
Episode length: 173.51 +/- 69.11
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-313.27 +/- 194.57
Episode length: 313.76 +/- 194.08
Eval num_timesteps=8000, episode_reward=-180.52 +/- 159.12
Episode length: 181.34 +/- 158.76
Eval num_timesteps=8500, episode_reward=-188.38 +/- 166.09
Episode length: 189.17 +/- 165.69
Eval num_timesteps=9000, episode_reward=-159.62 +/- 139.71
Episode length: 160.50 +/- 139.42
New best mean reward!
Eval num_timesteps=9500, episode_reward=-118.42 +/- 92.96
Episode length: 119.38 +/- 92.80
New best mean reward!
Eval num_timesteps=10000, episode_reward=-158.47 +/- 138.14
Episode length: 159.35 +/- 137.84
Eval num_timesteps=10500, episode_reward=-146.12 +/- 133.02
Episode length: 147.01 +/- 132.72
Eval num_timesteps=11000, episode_reward=-153.32 +/- 137.35
Episode length: 154.20 +/- 137.05
Eval num_timesteps=11500, episode_reward=-124.68 +/- 101.69
Episode length: 125.62 +/- 101.47
Eval num_timesteps=12000, episode_reward=-152.38 +/- 130.86
Episode length: 153.28 +/- 130.60
Eval num_timesteps=12500, episode_reward=-148.26 +/- 132.01
Episode length: 149.15 +/- 131.72
Eval num_timesteps=13000, episode_reward=-150.56 +/- 138.43
Episode length: 151.43 +/- 138.10
Eval num_timesteps=13500, episode_reward=-153.47 +/- 144.57
Episode length: 154.33 +/- 144.24
Eval num_timesteps=14000, episode_reward=-141.87 +/- 126.41
Episode length: 142.77 +/- 126.13
Eval num_timesteps=14500, episode_reward=-153.53 +/- 144.91
Episode length: 154.39 +/- 144.57
Eval num_timesteps=15000, episode_reward=-112.54 +/- 82.82
Episode length: 113.51 +/- 82.68
New best mean reward!
Eval num_timesteps=15500, episode_reward=-111.23 +/- 78.85
Episode length: 112.20 +/- 78.70
New best mean reward!
Eval num_timesteps=16000, episode_reward=-109.18 +/- 87.92
Episode length: 110.14 +/- 87.74
New best mean reward!
Eval num_timesteps=16500, episode_reward=-104.45 +/- 77.95
Episode length: 105.42 +/- 77.79
New best mean reward!
Eval num_timesteps=17000, episode_reward=-106.38 +/- 76.84
Episode length: 107.36 +/- 76.73
Eval num_timesteps=17500, episode_reward=-131.86 +/- 112.17
Episode length: 132.79 +/- 111.94
Eval num_timesteps=18000, episode_reward=-100.36 +/- 63.87
Episode length: 101.34 +/- 63.74
New best mean reward!
Eval num_timesteps=18500, episode_reward=-106.10 +/- 62.79
Episode length: 107.08 +/- 62.67
Eval num_timesteps=19000, episode_reward=-142.16 +/- 119.39
Episode length: 143.08 +/- 119.15
Eval num_timesteps=19500, episode_reward=-109.91 +/- 72.28
Episode length: 110.89 +/- 72.17
Eval num_timesteps=20000, episode_reward=-99.32 +/- 62.38
Episode length: 100.30 +/- 62.25
New best mean reward!
FINISHED IN 315.75128425698495 s


starting seed  10046 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-414.90 +/- 160.50
Episode length: 415.12 +/- 160.08
New best mean reward!
Eval num_timesteps=7500, episode_reward=-103.44 +/- 29.07
Episode length: 104.44 +/- 29.07
New best mean reward!
Eval num_timesteps=8000, episode_reward=-269.49 +/- 154.47
Episode length: 270.19 +/- 154.02
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-451.79 +/- 125.63
Episode length: 451.92 +/- 125.30
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-115.63 +/- 48.28
Episode length: 116.62 +/- 48.20
Eval num_timesteps=13000, episode_reward=-172.80 +/- 66.69
Episode length: 173.77 +/- 66.54
Eval num_timesteps=13500, episode_reward=-182.30 +/- 96.40
Episode length: 183.23 +/- 96.17
Eval num_timesteps=14000, episode_reward=-180.51 +/- 91.93
Episode length: 181.44 +/- 91.68
Eval num_timesteps=14500, episode_reward=-172.03 +/- 110.54
Episode length: 172.94 +/- 110.27
Eval num_timesteps=15000, episode_reward=-213.54 +/- 144.71
Episode length: 214.35 +/- 144.33
Eval num_timesteps=15500, episode_reward=-177.76 +/- 95.23
Episode length: 178.69 +/- 95.00
Eval num_timesteps=16000, episode_reward=-356.25 +/- 155.46
Episode length: 356.72 +/- 154.96
Eval num_timesteps=16500, episode_reward=-219.04 +/- 144.54
Episode length: 219.84 +/- 144.15
Eval num_timesteps=17000, episode_reward=-309.87 +/- 154.92
Episode length: 310.48 +/- 154.44
Eval num_timesteps=17500, episode_reward=-341.93 +/- 156.92
Episode length: 342.44 +/- 156.42
Eval num_timesteps=18000, episode_reward=-272.72 +/- 143.82
Episode length: 273.45 +/- 143.39
Eval num_timesteps=18500, episode_reward=-445.51 +/- 117.43
Episode length: 445.69 +/- 117.05
Eval num_timesteps=19000, episode_reward=-245.50 +/- 173.46
Episode length: 246.19 +/- 173.00
Eval num_timesteps=19500, episode_reward=-211.26 +/- 106.98
Episode length: 212.15 +/- 106.69
Eval num_timesteps=20000, episode_reward=-236.06 +/- 126.01
Episode length: 236.91 +/- 125.70
Eval num_timesteps=20500, episode_reward=-302.49 +/- 156.47
Episode length: 303.12 +/- 156.00
Eval num_timesteps=21000, episode_reward=-270.53 +/- 136.78
Episode length: 271.31 +/- 136.41
Eval num_timesteps=21500, episode_reward=-189.93 +/- 81.55
Episode length: 190.89 +/- 81.40
Eval num_timesteps=22000, episode_reward=-192.27 +/- 81.56
Episode length: 193.22 +/- 81.37
Eval num_timesteps=22500, episode_reward=-208.48 +/- 107.45
Episode length: 209.38 +/- 107.18
Eval num_timesteps=23000, episode_reward=-187.46 +/- 73.89
Episode length: 188.42 +/- 73.72
Eval num_timesteps=23500, episode_reward=-164.23 +/- 72.77
Episode length: 165.20 +/- 72.63
Eval num_timesteps=24000, episode_reward=-143.55 +/- 96.06
Episode length: 144.49 +/- 95.84
Eval num_timesteps=24500, episode_reward=-125.66 +/- 88.43
Episode length: 126.62 +/- 88.26
Eval num_timesteps=25000, episode_reward=-125.36 +/- 97.72
Episode length: 126.31 +/- 97.53
Eval num_timesteps=25500, episode_reward=-111.14 +/- 83.91
Episode length: 112.10 +/- 83.72
Eval num_timesteps=26000, episode_reward=-135.51 +/- 118.04
Episode length: 136.43 +/- 117.79
Eval num_timesteps=26500, episode_reward=-128.24 +/- 111.27
Episode length: 129.16 +/- 111.00
Eval num_timesteps=27000, episode_reward=-102.76 +/- 72.56
Episode length: 103.73 +/- 72.40
New best mean reward!
Eval num_timesteps=27500, episode_reward=-94.09 +/- 45.69
Episode length: 95.08 +/- 45.60
New best mean reward!
FINISHED IN 520.6598945760052 s


starting seed  10047 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-142.13 +/- 33.72
Episode length: 143.13 +/- 33.72
New best mean reward!
Eval num_timesteps=10000, episode_reward=-458.53 +/- 41.89
Episode length: 459.26 +/- 41.62
Eval num_timesteps=10500, episode_reward=-494.77 +/- 14.26
Episode length: 494.92 +/- 13.95
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-176.78 +/- 62.62
Episode length: 177.77 +/- 62.57
Eval num_timesteps=12000, episode_reward=-120.77 +/- 48.52
Episode length: 121.76 +/- 48.44
New best mean reward!
Eval num_timesteps=12500, episode_reward=-93.15 +/- 20.94
Episode length: 94.15 +/- 20.94
New best mean reward!
FINISHED IN 280.2050363770104 s


starting seed  10048 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=17000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=17500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=18000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=18500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=19000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=19500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=20000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=20500, episode_reward=-441.98 +/- 138.60
Episode length: 442.13 +/- 138.24
New best mean reward!
Eval num_timesteps=21000, episode_reward=-416.36 +/- 162.48
Episode length: 416.57 +/- 162.08
New best mean reward!
Eval num_timesteps=21500, episode_reward=-354.13 +/- 194.86
Episode length: 354.49 +/- 194.38
New best mean reward!
Eval num_timesteps=22000, episode_reward=-389.93 +/- 178.64
Episode length: 390.21 +/- 178.19
Eval num_timesteps=22500, episode_reward=-345.41 +/- 198.23
Episode length: 345.79 +/- 197.75
New best mean reward!
Eval num_timesteps=23000, episode_reward=-346.12 +/- 194.58
Episode length: 346.51 +/- 194.10
Eval num_timesteps=23500, episode_reward=-330.13 +/- 200.22
Episode length: 330.55 +/- 199.73
New best mean reward!
Eval num_timesteps=24000, episode_reward=-260.63 +/- 199.02
Episode length: 261.24 +/- 198.55
New best mean reward!
Eval num_timesteps=24500, episode_reward=-293.14 +/- 200.88
Episode length: 293.66 +/- 200.38
Eval num_timesteps=25000, episode_reward=-248.97 +/- 197.18
Episode length: 249.59 +/- 196.70
New best mean reward!
Eval num_timesteps=25500, episode_reward=-215.39 +/- 187.10
Episode length: 216.10 +/- 186.66
New best mean reward!
Eval num_timesteps=26000, episode_reward=-239.27 +/- 196.14
Episode length: 239.92 +/- 195.67
Eval num_timesteps=26500, episode_reward=-217.93 +/- 188.04
Episode length: 218.63 +/- 187.59
Eval num_timesteps=27000, episode_reward=-211.09 +/- 185.21
Episode length: 211.81 +/- 184.78
New best mean reward!
Eval num_timesteps=27500, episode_reward=-145.58 +/- 139.02
Episode length: 146.45 +/- 138.69
New best mean reward!
Eval num_timesteps=28000, episode_reward=-172.71 +/- 160.50
Episode length: 173.52 +/- 160.11
Eval num_timesteps=28500, episode_reward=-169.50 +/- 158.89
Episode length: 170.32 +/- 158.52
Eval num_timesteps=29000, episode_reward=-146.16 +/- 135.15
Episode length: 147.05 +/- 134.87
Eval num_timesteps=29500, episode_reward=-194.29 +/- 173.65
Episode length: 195.06 +/- 173.24
Eval num_timesteps=30000, episode_reward=-164.82 +/- 158.84
Episode length: 165.64 +/- 158.46
Eval num_timesteps=30500, episode_reward=-187.67 +/- 170.65
Episode length: 188.45 +/- 170.25
Eval num_timesteps=31000, episode_reward=-162.35 +/- 154.86
Episode length: 163.19 +/- 154.51
Eval num_timesteps=31500, episode_reward=-164.71 +/- 158.22
Episode length: 165.53 +/- 157.84
Eval num_timesteps=32000, episode_reward=-180.67 +/- 168.29
Episode length: 181.46 +/- 167.89
Eval num_timesteps=32500, episode_reward=-194.61 +/- 175.29
Episode length: 195.37 +/- 174.87
Eval num_timesteps=33000, episode_reward=-169.67 +/- 165.08
Episode length: 170.49 +/- 164.72
Eval num_timesteps=33500, episode_reward=-104.26 +/- 80.51
Episode length: 105.23 +/- 80.36
New best mean reward!
Eval num_timesteps=34000, episode_reward=-140.89 +/- 138.37
Episode length: 141.77 +/- 138.06
Eval num_timesteps=34500, episode_reward=-144.24 +/- 143.25
Episode length: 145.11 +/- 142.93
Eval num_timesteps=35000, episode_reward=-160.65 +/- 153.91
Episode length: 161.49 +/- 153.55
Eval num_timesteps=35500, episode_reward=-152.68 +/- 147.02
Episode length: 153.54 +/- 146.69
Eval num_timesteps=36000, episode_reward=-147.20 +/- 142.29
Episode length: 148.07 +/- 141.97
Eval num_timesteps=36500, episode_reward=-131.86 +/- 118.02
Episode length: 132.78 +/- 117.77
Eval num_timesteps=37000, episode_reward=-129.60 +/- 122.63
Episode length: 130.51 +/- 122.36
Eval num_timesteps=37500, episode_reward=-124.63 +/- 113.68
Episode length: 125.56 +/- 113.44
Eval num_timesteps=38000, episode_reward=-128.35 +/- 120.23
Episode length: 129.26 +/- 119.95
Eval num_timesteps=38500, episode_reward=-120.86 +/- 107.17
Episode length: 121.79 +/- 106.92
Eval num_timesteps=39000, episode_reward=-133.81 +/- 122.27
Episode length: 134.72 +/- 122.00
Eval num_timesteps=39500, episode_reward=-137.97 +/- 133.72
Episode length: 138.86 +/- 133.42
Eval num_timesteps=40000, episode_reward=-114.93 +/- 102.14
Episode length: 115.87 +/- 101.91
Eval num_timesteps=40500, episode_reward=-110.94 +/- 101.14
Episode length: 111.88 +/- 100.91
Eval num_timesteps=41000, episode_reward=-118.59 +/- 97.94
Episode length: 119.54 +/- 97.74
Eval num_timesteps=41500, episode_reward=-125.50 +/- 113.16
Episode length: 126.43 +/- 112.92
Eval num_timesteps=42000, episode_reward=-116.50 +/- 96.16
Episode length: 117.45 +/- 95.96
Eval num_timesteps=42500, episode_reward=-122.98 +/- 112.08
Episode length: 123.91 +/- 111.85
Eval num_timesteps=43000, episode_reward=-107.68 +/- 83.07
Episode length: 108.64 +/- 82.88
Eval num_timesteps=43500, episode_reward=-104.96 +/- 76.79
Episode length: 105.93 +/- 76.64
Eval num_timesteps=44000, episode_reward=-113.90 +/- 95.08
Episode length: 114.85 +/- 94.88
Eval num_timesteps=44500, episode_reward=-126.03 +/- 113.32
Episode length: 126.96 +/- 113.09
Eval num_timesteps=45000, episode_reward=-127.75 +/- 111.78
Episode length: 128.68 +/- 111.55
Eval num_timesteps=45500, episode_reward=-110.03 +/- 94.50
Episode length: 110.98 +/- 94.29
Eval num_timesteps=46000, episode_reward=-135.76 +/- 126.90
Episode length: 136.66 +/- 126.61
Eval num_timesteps=46500, episode_reward=-130.70 +/- 115.82
Episode length: 131.63 +/- 115.60
Eval num_timesteps=47000, episode_reward=-106.83 +/- 86.16
Episode length: 107.79 +/- 85.98
Eval num_timesteps=47500, episode_reward=-128.08 +/- 113.53
Episode length: 129.00 +/- 113.27
Eval num_timesteps=48000, episode_reward=-122.46 +/- 107.62
Episode length: 123.39 +/- 107.38
Eval num_timesteps=48500, episode_reward=-135.08 +/- 124.74
Episode length: 135.98 +/- 124.45
Eval num_timesteps=49000, episode_reward=-118.38 +/- 108.66
Episode length: 119.31 +/- 108.41
Eval num_timesteps=49500, episode_reward=-144.41 +/- 135.98
Episode length: 145.30 +/- 135.69
Eval num_timesteps=50000, episode_reward=-130.22 +/- 120.61
Episode length: 131.14 +/- 120.36
FINISHED IN 920.642568885989 s


starting seed  10049 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-410.60 +/- 167.13
Episode length: 410.83 +/- 166.72
New best mean reward!
Eval num_timesteps=12500, episode_reward=-343.13 +/- 198.50
Episode length: 343.52 +/- 198.02
New best mean reward!
Eval num_timesteps=13000, episode_reward=-247.34 +/- 199.09
Episode length: 247.96 +/- 198.61
New best mean reward!
Eval num_timesteps=13500, episode_reward=-209.63 +/- 186.86
Episode length: 210.35 +/- 186.42
New best mean reward!
Eval num_timesteps=14000, episode_reward=-307.75 +/- 203.41
Episode length: 308.23 +/- 202.92
Eval num_timesteps=14500, episode_reward=-310.05 +/- 200.03
Episode length: 310.53 +/- 199.53
Eval num_timesteps=15000, episode_reward=-254.78 +/- 201.06
Episode length: 255.39 +/- 200.58
Eval num_timesteps=15500, episode_reward=-342.07 +/- 195.74
Episode length: 342.47 +/- 195.26
Eval num_timesteps=16000, episode_reward=-317.65 +/- 198.45
Episode length: 318.12 +/- 197.97
Eval num_timesteps=16500, episode_reward=-404.97 +/- 169.33
Episode length: 405.21 +/- 168.90
Eval num_timesteps=17000, episode_reward=-431.65 +/- 151.16
Episode length: 431.82 +/- 150.78
Eval num_timesteps=17500, episode_reward=-388.40 +/- 179.26
Episode length: 388.68 +/- 178.81
Eval num_timesteps=18000, episode_reward=-354.87 +/- 194.26
Episode length: 355.23 +/- 193.78
Eval num_timesteps=18500, episode_reward=-349.61 +/- 197.03
Episode length: 349.98 +/- 196.55
Eval num_timesteps=19000, episode_reward=-303.68 +/- 203.13
Episode length: 304.17 +/- 202.64
Eval num_timesteps=19500, episode_reward=-296.27 +/- 205.69
Episode length: 296.77 +/- 205.19
Eval num_timesteps=20000, episode_reward=-257.04 +/- 205.55
Episode length: 257.63 +/- 205.06
Eval num_timesteps=20500, episode_reward=-195.07 +/- 183.29
Episode length: 195.82 +/- 182.87
New best mean reward!
Eval num_timesteps=21000, episode_reward=-221.80 +/- 193.29
Episode length: 222.48 +/- 192.83
Eval num_timesteps=21500, episode_reward=-203.65 +/- 184.64
Episode length: 204.38 +/- 184.21
Eval num_timesteps=22000, episode_reward=-170.85 +/- 166.19
Episode length: 171.65 +/- 165.80
New best mean reward!
Eval num_timesteps=22500, episode_reward=-133.66 +/- 125.89
Episode length: 134.56 +/- 125.59
New best mean reward!
Eval num_timesteps=23000, episode_reward=-128.07 +/- 121.90
Episode length: 128.99 +/- 121.65
New best mean reward!
Eval num_timesteps=23500, episode_reward=-111.08 +/- 93.49
Episode length: 112.05 +/- 93.37
New best mean reward!
Eval num_timesteps=24000, episode_reward=-96.33 +/- 63.82
Episode length: 97.32 +/- 63.76
New best mean reward!
FINISHED IN 558.9458555959864 s


starting seed  10050 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-422.69 +/- 141.06
Episode length: 422.93 +/- 140.64
New best mean reward!
Eval num_timesteps=8500, episode_reward=-484.68 +/- 48.49
Episode length: 484.80 +/- 48.21
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-492.41 +/- 30.79
Episode length: 492.49 +/- 30.56
Eval num_timesteps=10000, episode_reward=-430.02 +/- 85.44
Episode length: 430.53 +/- 85.04
Eval num_timesteps=10500, episode_reward=-474.63 +/- 82.21
Episode length: 474.72 +/- 81.93
Eval num_timesteps=11000, episode_reward=-473.15 +/- 54.12
Episode length: 473.46 +/- 53.78
Eval num_timesteps=11500, episode_reward=-122.47 +/- 24.19
Episode length: 123.47 +/- 24.19
New best mean reward!
Eval num_timesteps=12000, episode_reward=-154.24 +/- 52.87
Episode length: 155.23 +/- 52.81
Eval num_timesteps=12500, episode_reward=-470.32 +/- 75.65
Episode length: 470.49 +/- 75.33
Eval num_timesteps=13000, episode_reward=-101.67 +/- 42.30
Episode length: 102.66 +/- 42.21
New best mean reward!
Eval num_timesteps=13500, episode_reward=-112.59 +/- 33.76
Episode length: 113.59 +/- 33.76
Eval num_timesteps=14000, episode_reward=-109.79 +/- 32.01
Episode length: 110.79 +/- 32.01
Eval num_timesteps=14500, episode_reward=-97.32 +/- 26.90
Episode length: 98.32 +/- 26.90
New best mean reward!
FINISHED IN 321.0914091920131 s


starting seed  10051 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-145.87 +/- 112.52
Episode length: 146.80 +/- 112.30
New best mean reward!
Eval num_timesteps=11500, episode_reward=-111.46 +/- 58.10
Episode length: 112.45 +/- 58.03
New best mean reward!
Eval num_timesteps=12000, episode_reward=-107.61 +/- 64.98
Episode length: 108.59 +/- 64.86
New best mean reward!
Eval num_timesteps=12500, episode_reward=-95.87 +/- 31.17
Episode length: 96.87 +/- 31.17
New best mean reward!
FINISHED IN 271.2018667749944 s


starting seed  10052 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-82.33 +/- 16.32
Episode length: 83.33 +/- 16.32
New best mean reward!
FINISHED IN 15.847029372001998 s


starting seed  10053 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-346.35 +/- 193.15
Episode length: 346.74 +/- 192.67
New best mean reward!
Eval num_timesteps=10000, episode_reward=-196.89 +/- 172.81
Episode length: 197.65 +/- 172.39
New best mean reward!
Eval num_timesteps=10500, episode_reward=-212.85 +/- 178.11
Episode length: 213.58 +/- 177.67
Eval num_timesteps=11000, episode_reward=-177.11 +/- 163.20
Episode length: 177.91 +/- 162.81
New best mean reward!
Eval num_timesteps=11500, episode_reward=-188.30 +/- 166.25
Episode length: 189.09 +/- 165.86
Eval num_timesteps=12000, episode_reward=-146.27 +/- 133.01
Episode length: 147.15 +/- 132.69
New best mean reward!
Eval num_timesteps=12500, episode_reward=-200.61 +/- 172.48
Episode length: 201.38 +/- 172.08
Eval num_timesteps=13000, episode_reward=-88.35 +/- 25.08
Episode length: 89.35 +/- 25.08
New best mean reward!
FINISHED IN 283.7768702430185 s


starting seed  10054 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-86.40 +/- 24.74
Episode length: 87.40 +/- 24.74
New best mean reward!
FINISHED IN 213.4226422329666 s


starting seed  10055 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-300.04 +/- 133.98
Episode length: 300.76 +/- 133.56
New best mean reward!
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-497.73 +/- 22.59
Episode length: 497.74 +/- 22.49
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-194.97 +/- 62.08
Episode length: 195.94 +/- 61.93
New best mean reward!
Eval num_timesteps=5500, episode_reward=-145.10 +/- 83.54
Episode length: 146.05 +/- 83.33
New best mean reward!
Eval num_timesteps=6000, episode_reward=-113.29 +/- 48.39
Episode length: 114.28 +/- 48.31
New best mean reward!
Eval num_timesteps=6500, episode_reward=-328.47 +/- 186.19
Episode length: 328.94 +/- 185.70
Eval num_timesteps=7000, episode_reward=-301.16 +/- 181.70
Episode length: 301.71 +/- 181.21
Eval num_timesteps=7500, episode_reward=-431.93 +/- 139.06
Episode length: 432.13 +/- 138.67
Eval num_timesteps=8000, episode_reward=-128.75 +/- 30.67
Episode length: 129.75 +/- 30.67
Eval num_timesteps=8500, episode_reward=-216.75 +/- 111.78
Episode length: 217.63 +/- 111.47
Eval num_timesteps=9000, episode_reward=-194.33 +/- 60.90
Episode length: 195.31 +/- 60.80
Eval num_timesteps=9500, episode_reward=-216.88 +/- 100.36
Episode length: 217.78 +/- 100.08
Eval num_timesteps=10000, episode_reward=-318.07 +/- 160.15
Episode length: 318.65 +/- 159.67
Eval num_timesteps=10500, episode_reward=-191.72 +/- 54.89
Episode length: 192.71 +/- 54.84
Eval num_timesteps=11000, episode_reward=-178.44 +/- 39.22
Episode length: 179.44 +/- 39.22
Eval num_timesteps=11500, episode_reward=-252.24 +/- 131.09
Episode length: 253.04 +/- 130.71
Eval num_timesteps=12000, episode_reward=-493.51 +/- 45.43
Episode length: 493.53 +/- 45.29
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-317.34 +/- 177.61
Episode length: 317.86 +/- 177.12
Eval num_timesteps=14500, episode_reward=-294.66 +/- 167.06
Episode length: 295.27 +/- 166.58
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-388.24 +/- 179.37
Episode length: 388.52 +/- 178.92
Eval num_timesteps=16000, episode_reward=-402.18 +/- 173.20
Episode length: 402.43 +/- 172.78
Eval num_timesteps=16500, episode_reward=-322.46 +/- 197.88
Episode length: 322.91 +/- 197.39
Eval num_timesteps=17000, episode_reward=-246.22 +/- 187.80
Episode length: 246.88 +/- 187.34
Eval num_timesteps=17500, episode_reward=-239.35 +/- 188.94
Episode length: 240.02 +/- 188.49
Eval num_timesteps=18000, episode_reward=-183.09 +/- 165.33
Episode length: 183.88 +/- 164.93
Eval num_timesteps=18500, episode_reward=-146.43 +/- 133.08
Episode length: 147.32 +/- 132.79
Eval num_timesteps=19000, episode_reward=-138.92 +/- 125.53
Episode length: 139.82 +/- 125.24
Eval num_timesteps=19500, episode_reward=-156.83 +/- 145.35
Episode length: 157.70 +/- 145.04
Eval num_timesteps=20000, episode_reward=-139.59 +/- 125.98
Episode length: 140.49 +/- 125.69
Eval num_timesteps=20500, episode_reward=-153.10 +/- 139.40
Episode length: 153.97 +/- 139.08
Eval num_timesteps=21000, episode_reward=-162.00 +/- 144.66
Episode length: 162.85 +/- 144.31
Eval num_timesteps=21500, episode_reward=-168.95 +/- 156.44
Episode length: 169.77 +/- 156.06
Eval num_timesteps=22000, episode_reward=-181.20 +/- 161.78
Episode length: 182.00 +/- 161.38
Eval num_timesteps=22500, episode_reward=-181.50 +/- 162.12
Episode length: 182.30 +/- 161.73
Eval num_timesteps=23000, episode_reward=-180.22 +/- 161.53
Episode length: 181.03 +/- 161.16
Eval num_timesteps=23500, episode_reward=-209.31 +/- 175.70
Episode length: 210.06 +/- 175.29
Eval num_timesteps=24000, episode_reward=-208.98 +/- 176.45
Episode length: 209.72 +/- 176.02
Eval num_timesteps=24500, episode_reward=-195.55 +/- 172.60
Episode length: 196.31 +/- 172.18
Eval num_timesteps=25000, episode_reward=-212.11 +/- 184.75
Episode length: 212.82 +/- 184.30
Eval num_timesteps=25500, episode_reward=-188.71 +/- 167.34
Episode length: 189.50 +/- 166.95
Eval num_timesteps=26000, episode_reward=-149.11 +/- 133.35
Episode length: 149.99 +/- 133.03
Eval num_timesteps=26500, episode_reward=-136.96 +/- 118.39
Episode length: 137.87 +/- 118.12
Eval num_timesteps=27000, episode_reward=-135.41 +/- 116.66
Episode length: 136.32 +/- 116.38
Eval num_timesteps=27500, episode_reward=-153.13 +/- 138.48
Episode length: 154.00 +/- 138.15
Eval num_timesteps=28000, episode_reward=-150.56 +/- 138.76
Episode length: 151.43 +/- 138.43
Eval num_timesteps=28500, episode_reward=-117.11 +/- 99.32
Episode length: 118.06 +/- 99.13
Eval num_timesteps=29000, episode_reward=-171.08 +/- 154.86
Episode length: 171.91 +/- 154.50
Eval num_timesteps=29500, episode_reward=-139.02 +/- 122.21
Episode length: 139.93 +/- 121.95
Eval num_timesteps=30000, episode_reward=-110.61 +/- 92.13
Episode length: 111.56 +/- 91.92
New best mean reward!
Eval num_timesteps=30500, episode_reward=-126.16 +/- 108.28
Episode length: 127.09 +/- 108.04
Eval num_timesteps=31000, episode_reward=-124.93 +/- 108.36
Episode length: 125.87 +/- 108.16
Eval num_timesteps=31500, episode_reward=-116.24 +/- 95.27
Episode length: 117.19 +/- 95.07
Eval num_timesteps=32000, episode_reward=-103.28 +/- 65.19
Episode length: 104.26 +/- 65.06
New best mean reward!
Eval num_timesteps=32500, episode_reward=-90.91 +/- 44.52
Episode length: 91.90 +/- 44.43
New best mean reward!
FINISHED IN 536.4841327610193 s


starting seed  10056 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-466.58 +/- 113.39
Episode length: 466.66 +/- 113.11
New best mean reward!
Eval num_timesteps=14500, episode_reward=-330.01 +/- 200.90
Episode length: 330.43 +/- 200.41
New best mean reward!
Eval num_timesteps=15000, episode_reward=-292.57 +/- 201.17
Episode length: 293.09 +/- 200.68
New best mean reward!
Eval num_timesteps=15500, episode_reward=-146.49 +/- 139.95
Episode length: 147.36 +/- 139.62
New best mean reward!
Eval num_timesteps=16000, episode_reward=-152.63 +/- 144.07
Episode length: 153.49 +/- 143.74
Eval num_timesteps=16500, episode_reward=-156.02 +/- 143.07
Episode length: 156.88 +/- 142.73
Eval num_timesteps=17000, episode_reward=-171.76 +/- 160.89
Episode length: 172.57 +/- 160.50
Eval num_timesteps=17500, episode_reward=-135.38 +/- 130.43
Episode length: 136.27 +/- 130.12
New best mean reward!
Eval num_timesteps=18000, episode_reward=-147.24 +/- 140.95
Episode length: 148.11 +/- 140.62
Eval num_timesteps=18500, episode_reward=-145.52 +/- 139.03
Episode length: 146.40 +/- 138.73
Eval num_timesteps=19000, episode_reward=-137.45 +/- 130.24
Episode length: 138.34 +/- 129.93
Eval num_timesteps=19500, episode_reward=-118.07 +/- 102.06
Episode length: 119.02 +/- 101.88
New best mean reward!
Eval num_timesteps=20000, episode_reward=-146.30 +/- 139.98
Episode length: 147.17 +/- 139.65
Eval num_timesteps=20500, episode_reward=-158.64 +/- 155.94
Episode length: 159.47 +/- 155.56
Eval num_timesteps=21000, episode_reward=-130.04 +/- 118.11
Episode length: 130.96 +/- 117.86
Eval num_timesteps=21500, episode_reward=-114.26 +/- 102.47
Episode length: 115.20 +/- 102.24
New best mean reward!
Eval num_timesteps=22000, episode_reward=-98.15 +/- 73.23
Episode length: 99.12 +/- 73.07
New best mean reward!
FINISHED IN 444.0125631020055 s


starting seed  10057 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-388.96 +/- 87.46
Episode length: 389.69 +/- 87.12
New best mean reward!
Eval num_timesteps=8500, episode_reward=-477.78 +/- 71.34
Episode length: 477.88 +/- 71.06
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-456.10 +/- 116.71
Episode length: 456.23 +/- 116.38
Eval num_timesteps=10000, episode_reward=-473.99 +/- 96.11
Episode length: 474.06 +/- 95.86
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-472.56 +/- 100.37
Episode length: 472.63 +/- 100.11
Eval num_timesteps=12500, episode_reward=-247.45 +/- 199.94
Episode length: 248.08 +/- 199.47
New best mean reward!
Eval num_timesteps=13000, episode_reward=-149.82 +/- 138.98
Episode length: 150.69 +/- 138.65
New best mean reward!
Eval num_timesteps=13500, episode_reward=-116.00 +/- 66.81
Episode length: 116.98 +/- 66.70
New best mean reward!
Eval num_timesteps=14000, episode_reward=-118.31 +/- 89.41
Episode length: 119.27 +/- 89.24
Eval num_timesteps=14500, episode_reward=-117.01 +/- 99.33
Episode length: 117.95 +/- 99.10
Eval num_timesteps=15000, episode_reward=-104.43 +/- 51.56
Episode length: 105.42 +/- 51.49
New best mean reward!
Eval num_timesteps=15500, episode_reward=-95.67 +/- 28.00
Episode length: 96.67 +/- 28.00
New best mean reward!
FINISHED IN 345.5847103570122 s


starting seed  10058 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-293.26 +/- 199.20
Episode length: 293.78 +/- 198.70
New best mean reward!
Eval num_timesteps=9000, episode_reward=-279.42 +/- 197.20
Episode length: 279.98 +/- 196.71
New best mean reward!
Eval num_timesteps=9500, episode_reward=-417.55 +/- 154.93
Episode length: 417.78 +/- 154.52
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-492.85 +/- 50.11
Episode length: 492.87 +/- 49.97
Eval num_timesteps=11000, episode_reward=-419.15 +/- 153.55
Episode length: 419.37 +/- 153.14
Eval num_timesteps=11500, episode_reward=-416.77 +/- 157.64
Episode length: 416.99 +/- 157.23
Eval num_timesteps=12000, episode_reward=-347.51 +/- 188.36
Episode length: 347.91 +/- 187.87
Eval num_timesteps=12500, episode_reward=-366.57 +/- 178.32
Episode length: 366.94 +/- 177.85
Eval num_timesteps=13000, episode_reward=-379.96 +/- 180.12
Episode length: 380.27 +/- 179.66
Eval num_timesteps=13500, episode_reward=-135.95 +/- 120.94
Episode length: 136.86 +/- 120.67
New best mean reward!
Eval num_timesteps=14000, episode_reward=-138.31 +/- 110.17
Episode length: 139.25 +/- 109.97
Eval num_timesteps=14500, episode_reward=-136.90 +/- 112.88
Episode length: 137.83 +/- 112.66
Eval num_timesteps=15000, episode_reward=-188.92 +/- 171.58
Episode length: 189.69 +/- 171.16
Eval num_timesteps=15500, episode_reward=-205.27 +/- 173.07
Episode length: 206.03 +/- 172.66
Eval num_timesteps=16000, episode_reward=-199.16 +/- 176.26
Episode length: 199.91 +/- 175.83
Eval num_timesteps=16500, episode_reward=-197.70 +/- 175.32
Episode length: 198.45 +/- 174.89
Eval num_timesteps=17000, episode_reward=-242.21 +/- 195.58
Episode length: 242.85 +/- 195.11
Eval num_timesteps=17500, episode_reward=-317.30 +/- 200.97
Episode length: 317.76 +/- 200.48
Eval num_timesteps=18000, episode_reward=-171.16 +/- 154.96
Episode length: 172.00 +/- 154.62
Eval num_timesteps=18500, episode_reward=-187.69 +/- 168.95
Episode length: 188.47 +/- 168.54
Eval num_timesteps=19000, episode_reward=-132.09 +/- 112.50
Episode length: 133.01 +/- 112.24
New best mean reward!
Eval num_timesteps=19500, episode_reward=-163.38 +/- 153.92
Episode length: 164.21 +/- 153.55
Eval num_timesteps=20000, episode_reward=-163.15 +/- 148.57
Episode length: 164.00 +/- 148.23
Eval num_timesteps=20500, episode_reward=-110.98 +/- 79.33
Episode length: 111.95 +/- 79.19
New best mean reward!
Eval num_timesteps=21000, episode_reward=-118.38 +/- 101.80
Episode length: 119.32 +/- 101.57
Eval num_timesteps=21500, episode_reward=-125.43 +/- 106.51
Episode length: 126.36 +/- 106.26
Eval num_timesteps=22000, episode_reward=-104.01 +/- 63.20
Episode length: 104.99 +/- 63.08
New best mean reward!
Eval num_timesteps=22500, episode_reward=-99.65 +/- 58.16
Episode length: 100.64 +/- 58.09
New best mean reward!
FINISHED IN 438.896109497 s


starting seed  10059 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-173.93 +/- 91.61
Episode length: 174.86 +/- 91.36
New best mean reward!
Eval num_timesteps=8000, episode_reward=-136.12 +/- 48.67
Episode length: 137.12 +/- 48.67
New best mean reward!
Eval num_timesteps=8500, episode_reward=-107.74 +/- 27.51
Episode length: 108.74 +/- 27.51
New best mean reward!
Eval num_timesteps=9000, episode_reward=-142.13 +/- 41.01
Episode length: 143.13 +/- 41.01
Eval num_timesteps=9500, episode_reward=-149.10 +/- 68.76
Episode length: 150.07 +/- 68.61
Eval num_timesteps=10000, episode_reward=-173.88 +/- 89.10
Episode length: 174.82 +/- 88.88
Eval num_timesteps=10500, episode_reward=-207.97 +/- 121.82
Episode length: 208.83 +/- 121.49
Eval num_timesteps=11000, episode_reward=-163.40 +/- 139.02
Episode length: 164.26 +/- 138.68
Eval num_timesteps=11500, episode_reward=-132.23 +/- 102.88
Episode length: 133.17 +/- 102.67
Eval num_timesteps=12000, episode_reward=-194.32 +/- 165.43
Episode length: 195.10 +/- 165.02
Eval num_timesteps=12500, episode_reward=-123.06 +/- 76.90
Episode length: 124.03 +/- 76.75
Eval num_timesteps=13000, episode_reward=-103.09 +/- 50.25
Episode length: 104.09 +/- 50.25
New best mean reward!
Eval num_timesteps=13500, episode_reward=-100.53 +/- 47.99
Episode length: 101.53 +/- 47.99
New best mean reward!
Eval num_timesteps=14000, episode_reward=-120.97 +/- 94.18
Episode length: 121.93 +/- 94.02
Eval num_timesteps=14500, episode_reward=-119.69 +/- 86.78
Episode length: 120.65 +/- 86.60
Eval num_timesteps=15000, episode_reward=-137.88 +/- 119.17
Episode length: 138.79 +/- 118.90
Eval num_timesteps=15500, episode_reward=-106.65 +/- 63.04
Episode length: 107.63 +/- 62.92
Eval num_timesteps=16000, episode_reward=-110.14 +/- 74.88
Episode length: 111.11 +/- 74.72
Eval num_timesteps=16500, episode_reward=-135.51 +/- 117.76
Episode length: 136.42 +/- 117.48
Eval num_timesteps=17000, episode_reward=-152.38 +/- 131.97
Episode length: 153.26 +/- 131.66
Eval num_timesteps=17500, episode_reward=-117.33 +/- 95.77
Episode length: 118.28 +/- 95.57
Eval num_timesteps=18000, episode_reward=-105.11 +/- 51.44
Episode length: 106.10 +/- 51.36
Eval num_timesteps=18500, episode_reward=-101.35 +/- 56.51
Episode length: 102.34 +/- 56.44
Eval num_timesteps=19000, episode_reward=-120.61 +/- 92.91
Episode length: 121.56 +/- 92.71
Eval num_timesteps=19500, episode_reward=-96.53 +/- 37.11
Episode length: 97.53 +/- 37.11
New best mean reward!
FINISHED IN 306.1142046479508 s


starting seed  10060 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-414.59 +/- 114.98
Episode length: 415.00 +/- 114.54
New best mean reward!
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-492.54 +/- 44.92
Episode length: 492.57 +/- 44.76
Eval num_timesteps=7000, episode_reward=-428.08 +/- 139.47
Episode length: 428.31 +/- 139.07
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-477.80 +/- 83.26
Episode length: 477.87 +/- 83.01
Eval num_timesteps=8500, episode_reward=-369.00 +/- 172.61
Episode length: 369.38 +/- 172.14
New best mean reward!
Eval num_timesteps=9000, episode_reward=-282.35 +/- 187.19
Episode length: 282.93 +/- 186.70
New best mean reward!
Eval num_timesteps=9500, episode_reward=-437.16 +/- 139.07
Episode length: 437.33 +/- 138.70
Eval num_timesteps=10000, episode_reward=-401.17 +/- 163.26
Episode length: 401.44 +/- 162.82
Eval num_timesteps=10500, episode_reward=-383.93 +/- 167.98
Episode length: 384.27 +/- 167.52
Eval num_timesteps=11000, episode_reward=-144.73 +/- 108.52
Episode length: 145.66 +/- 108.29
New best mean reward!
Eval num_timesteps=11500, episode_reward=-151.47 +/- 100.24
Episode length: 152.41 +/- 100.03
Eval num_timesteps=12000, episode_reward=-150.00 +/- 113.63
Episode length: 150.92 +/- 113.38
Eval num_timesteps=12500, episode_reward=-151.32 +/- 115.28
Episode length: 152.24 +/- 115.04
Eval num_timesteps=13000, episode_reward=-145.62 +/- 97.99
Episode length: 146.57 +/- 97.81
Eval num_timesteps=13500, episode_reward=-139.56 +/- 93.72
Episode length: 140.51 +/- 93.53
New best mean reward!
Eval num_timesteps=14000, episode_reward=-155.20 +/- 119.78
Episode length: 156.10 +/- 119.49
Eval num_timesteps=14500, episode_reward=-121.31 +/- 71.46
Episode length: 122.29 +/- 71.35
New best mean reward!
Eval num_timesteps=15000, episode_reward=-168.04 +/- 135.91
Episode length: 168.91 +/- 135.59
Eval num_timesteps=15500, episode_reward=-231.43 +/- 168.23
Episode length: 232.16 +/- 167.80
Eval num_timesteps=16000, episode_reward=-247.66 +/- 178.02
Episode length: 248.35 +/- 177.58
Eval num_timesteps=16500, episode_reward=-110.20 +/- 40.28
Episode length: 111.20 +/- 40.28
New best mean reward!
Eval num_timesteps=17000, episode_reward=-119.70 +/- 62.47
Episode length: 120.70 +/- 62.47
Eval num_timesteps=17500, episode_reward=-107.63 +/- 42.45
Episode length: 108.63 +/- 42.45
New best mean reward!
Eval num_timesteps=18000, episode_reward=-110.63 +/- 51.87
Episode length: 111.62 +/- 51.79
Eval num_timesteps=18500, episode_reward=-106.50 +/- 63.03
Episode length: 107.48 +/- 62.90
New best mean reward!
Eval num_timesteps=19000, episode_reward=-99.27 +/- 26.77
Episode length: 100.27 +/- 26.77
New best mean reward!
FINISHED IN 333.40988454502076 s


starting seed  10061 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-190.98 +/- 64.46
Episode length: 191.95 +/- 64.32
New best mean reward!
Eval num_timesteps=12000, episode_reward=-145.22 +/- 75.91
Episode length: 146.18 +/- 75.72
New best mean reward!
Eval num_timesteps=12500, episode_reward=-150.87 +/- 82.48
Episode length: 151.83 +/- 82.31
Eval num_timesteps=13000, episode_reward=-182.16 +/- 78.03
Episode length: 183.11 +/- 77.83
Eval num_timesteps=13500, episode_reward=-190.52 +/- 93.92
Episode length: 191.45 +/- 93.69
Eval num_timesteps=14000, episode_reward=-235.47 +/- 174.60
Episode length: 236.17 +/- 174.15
Eval num_timesteps=14500, episode_reward=-377.79 +/- 178.62
Episode length: 378.11 +/- 178.15
Eval num_timesteps=15000, episode_reward=-308.19 +/- 192.86
Episode length: 308.69 +/- 192.37
Eval num_timesteps=15500, episode_reward=-412.56 +/- 165.50
Episode length: 412.78 +/- 165.09
Eval num_timesteps=16000, episode_reward=-373.59 +/- 184.80
Episode length: 373.92 +/- 184.34
Eval num_timesteps=16500, episode_reward=-207.73 +/- 183.17
Episode length: 208.45 +/- 182.72
Eval num_timesteps=17000, episode_reward=-158.43 +/- 146.42
Episode length: 159.28 +/- 146.07
Eval num_timesteps=17500, episode_reward=-141.07 +/- 129.17
Episode length: 141.96 +/- 128.87
New best mean reward!
Eval num_timesteps=18000, episode_reward=-151.65 +/- 142.30
Episode length: 152.51 +/- 141.96
Eval num_timesteps=18500, episode_reward=-186.14 +/- 172.98
Episode length: 186.91 +/- 172.57
Eval num_timesteps=19000, episode_reward=-258.89 +/- 197.62
Episode length: 259.50 +/- 197.14
Eval num_timesteps=19500, episode_reward=-393.49 +/- 175.78
Episode length: 393.76 +/- 175.33
Eval num_timesteps=20000, episode_reward=-339.60 +/- 198.24
Episode length: 340.00 +/- 197.75
Eval num_timesteps=20500, episode_reward=-303.20 +/- 205.45
Episode length: 303.68 +/- 204.95
Eval num_timesteps=21000, episode_reward=-247.08 +/- 197.55
Episode length: 247.71 +/- 197.08
Eval num_timesteps=21500, episode_reward=-286.03 +/- 202.32
Episode length: 286.56 +/- 201.83
Eval num_timesteps=22000, episode_reward=-230.62 +/- 190.32
Episode length: 231.29 +/- 189.85
Eval num_timesteps=22500, episode_reward=-247.59 +/- 198.48
Episode length: 248.21 +/- 198.00
Eval num_timesteps=23000, episode_reward=-232.04 +/- 193.28
Episode length: 232.70 +/- 192.81
Eval num_timesteps=23500, episode_reward=-219.48 +/- 185.10
Episode length: 220.18 +/- 184.64
Eval num_timesteps=24000, episode_reward=-224.96 +/- 188.98
Episode length: 225.65 +/- 188.53
Eval num_timesteps=24500, episode_reward=-234.81 +/- 194.86
Episode length: 235.47 +/- 194.40
Eval num_timesteps=25000, episode_reward=-237.46 +/- 192.45
Episode length: 238.12 +/- 191.99
Eval num_timesteps=25500, episode_reward=-246.21 +/- 196.33
Episode length: 246.85 +/- 195.86
Eval num_timesteps=26000, episode_reward=-203.15 +/- 181.48
Episode length: 203.88 +/- 181.03
Eval num_timesteps=26500, episode_reward=-184.83 +/- 173.27
Episode length: 185.60 +/- 172.85
Eval num_timesteps=27000, episode_reward=-182.50 +/- 171.03
Episode length: 183.28 +/- 170.62
Eval num_timesteps=27500, episode_reward=-220.78 +/- 192.25
Episode length: 221.46 +/- 191.78
Eval num_timesteps=28000, episode_reward=-182.33 +/- 170.55
Episode length: 183.11 +/- 170.14
Eval num_timesteps=28500, episode_reward=-183.25 +/- 170.56
Episode length: 184.03 +/- 170.15
Eval num_timesteps=29000, episode_reward=-183.31 +/- 170.67
Episode length: 184.09 +/- 170.26
Eval num_timesteps=29500, episode_reward=-158.69 +/- 147.53
Episode length: 159.54 +/- 147.19
Eval num_timesteps=30000, episode_reward=-172.36 +/- 159.60
Episode length: 173.19 +/- 159.25
Eval num_timesteps=30500, episode_reward=-155.11 +/- 153.16
Episode length: 155.95 +/- 152.80
Eval num_timesteps=31000, episode_reward=-167.42 +/- 162.95
Episode length: 168.23 +/- 162.56
Eval num_timesteps=31500, episode_reward=-152.61 +/- 152.73
Episode length: 153.45 +/- 152.37
Eval num_timesteps=32000, episode_reward=-178.12 +/- 171.93
Episode length: 178.90 +/- 171.52
Eval num_timesteps=32500, episode_reward=-180.20 +/- 170.20
Episode length: 180.99 +/- 169.80
Eval num_timesteps=33000, episode_reward=-159.27 +/- 155.33
Episode length: 160.10 +/- 154.96
Eval num_timesteps=33500, episode_reward=-141.43 +/- 140.99
Episode length: 142.30 +/- 140.66
Eval num_timesteps=34000, episode_reward=-139.60 +/- 130.06
Episode length: 140.50 +/- 129.78
New best mean reward!
Eval num_timesteps=34500, episode_reward=-124.33 +/- 122.55
Episode length: 125.24 +/- 122.28
New best mean reward!
Eval num_timesteps=35000, episode_reward=-156.12 +/- 151.17
Episode length: 156.96 +/- 150.81
Eval num_timesteps=35500, episode_reward=-161.14 +/- 152.49
Episode length: 161.99 +/- 152.16
Eval num_timesteps=36000, episode_reward=-143.63 +/- 134.56
Episode length: 144.51 +/- 134.25
Eval num_timesteps=36500, episode_reward=-119.69 +/- 108.23
Episode length: 120.62 +/- 107.98
New best mean reward!
Eval num_timesteps=37000, episode_reward=-131.01 +/- 123.28
Episode length: 131.92 +/- 123.01
Eval num_timesteps=37500, episode_reward=-122.04 +/- 108.20
Episode length: 122.97 +/- 107.96
Eval num_timesteps=38000, episode_reward=-118.96 +/- 107.61
Episode length: 119.89 +/- 107.37
New best mean reward!
Eval num_timesteps=38500, episode_reward=-121.00 +/- 120.14
Episode length: 121.91 +/- 119.86
Eval num_timesteps=39000, episode_reward=-132.27 +/- 131.66
Episode length: 133.16 +/- 131.35
Eval num_timesteps=39500, episode_reward=-138.43 +/- 136.41
Episode length: 139.31 +/- 136.09
Eval num_timesteps=40000, episode_reward=-162.88 +/- 160.31
Episode length: 163.70 +/- 159.93
Eval num_timesteps=40500, episode_reward=-129.15 +/- 126.09
Episode length: 130.05 +/- 125.80
Eval num_timesteps=41000, episode_reward=-157.61 +/- 151.47
Episode length: 158.45 +/- 151.11
Eval num_timesteps=41500, episode_reward=-151.18 +/- 151.82
Episode length: 152.03 +/- 151.48
Eval num_timesteps=42000, episode_reward=-170.48 +/- 164.62
Episode length: 171.29 +/- 164.24
Eval num_timesteps=42500, episode_reward=-152.52 +/- 152.75
Episode length: 153.36 +/- 152.39
Eval num_timesteps=43000, episode_reward=-156.61 +/- 156.38
Episode length: 157.44 +/- 156.00
Eval num_timesteps=43500, episode_reward=-183.26 +/- 174.64
Episode length: 184.03 +/- 174.22
Eval num_timesteps=44000, episode_reward=-178.78 +/- 176.80
Episode length: 179.55 +/- 176.38
Eval num_timesteps=44500, episode_reward=-171.47 +/- 161.49
Episode length: 172.29 +/- 161.12
Eval num_timesteps=45000, episode_reward=-170.53 +/- 164.05
Episode length: 171.34 +/- 163.67
Eval num_timesteps=45500, episode_reward=-153.91 +/- 152.86
Episode length: 154.75 +/- 152.50
Eval num_timesteps=46000, episode_reward=-163.50 +/- 164.21
Episode length: 164.31 +/- 163.82
Eval num_timesteps=46500, episode_reward=-168.84 +/- 167.15
Episode length: 169.64 +/- 166.75
Eval num_timesteps=47000, episode_reward=-167.21 +/- 161.99
Episode length: 168.03 +/- 161.62
Eval num_timesteps=47500, episode_reward=-148.19 +/- 148.71
Episode length: 149.05 +/- 148.37
Eval num_timesteps=48000, episode_reward=-162.23 +/- 159.10
Episode length: 163.06 +/- 158.74
Eval num_timesteps=48500, episode_reward=-174.24 +/- 169.25
Episode length: 175.03 +/- 168.84
Eval num_timesteps=49000, episode_reward=-180.85 +/- 174.02
Episode length: 181.63 +/- 173.61
Eval num_timesteps=49500, episode_reward=-148.88 +/- 150.73
Episode length: 149.73 +/- 150.38
Eval num_timesteps=50000, episode_reward=-180.92 +/- 167.33
Episode length: 181.71 +/- 166.93
FINISHED IN 886.3577631890075 s


starting seed  10062 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=17000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=17500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=18000, episode_reward=-243.88 +/- 198.67
Episode length: 244.51 +/- 198.19
New best mean reward!
Eval num_timesteps=18500, episode_reward=-197.96 +/- 178.85
Episode length: 198.71 +/- 178.43
New best mean reward!
Eval num_timesteps=19000, episode_reward=-215.02 +/- 182.88
Episode length: 215.77 +/- 182.49
Eval num_timesteps=19500, episode_reward=-185.08 +/- 147.76
Episode length: 185.91 +/- 147.40
New best mean reward!
Eval num_timesteps=20000, episode_reward=-240.00 +/- 155.18
Episode length: 240.75 +/- 154.76
Eval num_timesteps=20500, episode_reward=-227.02 +/- 146.16
Episode length: 227.82 +/- 145.79
Eval num_timesteps=21000, episode_reward=-284.85 +/- 166.88
Episode length: 285.48 +/- 166.41
Eval num_timesteps=21500, episode_reward=-286.44 +/- 164.74
Episode length: 287.08 +/- 164.27
Eval num_timesteps=22000, episode_reward=-257.22 +/- 153.36
Episode length: 257.94 +/- 152.91
Eval num_timesteps=22500, episode_reward=-271.04 +/- 160.80
Episode length: 271.73 +/- 160.36
Eval num_timesteps=23000, episode_reward=-247.27 +/- 156.11
Episode length: 248.00 +/- 155.67
Eval num_timesteps=23500, episode_reward=-241.13 +/- 154.68
Episode length: 241.87 +/- 154.24
Eval num_timesteps=24000, episode_reward=-206.30 +/- 132.80
Episode length: 207.15 +/- 132.47
Eval num_timesteps=24500, episode_reward=-193.37 +/- 143.60
Episode length: 194.21 +/- 143.26
Eval num_timesteps=25000, episode_reward=-171.54 +/- 135.83
Episode length: 172.40 +/- 135.49
New best mean reward!
Eval num_timesteps=25500, episode_reward=-165.73 +/- 130.73
Episode length: 166.60 +/- 130.40
New best mean reward!
Eval num_timesteps=26000, episode_reward=-127.78 +/- 120.31
Episode length: 128.69 +/- 120.03
New best mean reward!
Eval num_timesteps=26500, episode_reward=-154.41 +/- 118.07
Episode length: 155.31 +/- 117.78
Eval num_timesteps=27000, episode_reward=-140.53 +/- 130.88
Episode length: 141.44 +/- 130.63
Eval num_timesteps=27500, episode_reward=-157.39 +/- 146.52
Episode length: 158.24 +/- 146.17
Eval num_timesteps=28000, episode_reward=-167.36 +/- 142.60
Episode length: 168.21 +/- 142.25
Eval num_timesteps=28500, episode_reward=-189.74 +/- 127.90
Episode length: 190.60 +/- 127.56
Eval num_timesteps=29000, episode_reward=-185.61 +/- 148.10
Episode length: 186.44 +/- 147.74
Eval num_timesteps=29500, episode_reward=-171.70 +/- 123.98
Episode length: 172.58 +/- 123.66
Eval num_timesteps=30000, episode_reward=-188.57 +/- 120.53
Episode length: 189.45 +/- 120.22
Eval num_timesteps=30500, episode_reward=-185.02 +/- 122.36
Episode length: 185.90 +/- 122.05
Eval num_timesteps=31000, episode_reward=-204.30 +/- 169.79
Episode length: 205.06 +/- 169.37
Eval num_timesteps=31500, episode_reward=-182.76 +/- 166.43
Episode length: 183.55 +/- 166.03
Eval num_timesteps=32000, episode_reward=-148.59 +/- 139.64
Episode length: 149.46 +/- 139.31
Eval num_timesteps=32500, episode_reward=-155.49 +/- 149.62
Episode length: 156.34 +/- 149.28
Eval num_timesteps=33000, episode_reward=-148.23 +/- 141.74
Episode length: 149.10 +/- 141.41
Eval num_timesteps=33500, episode_reward=-175.80 +/- 160.37
Episode length: 176.62 +/- 160.01
Eval num_timesteps=34000, episode_reward=-143.05 +/- 134.71
Episode length: 143.93 +/- 134.39
Eval num_timesteps=34500, episode_reward=-148.82 +/- 144.86
Episode length: 149.68 +/- 144.52
Eval num_timesteps=35000, episode_reward=-136.08 +/- 129.66
Episode length: 136.97 +/- 129.35
Eval num_timesteps=35500, episode_reward=-165.12 +/- 157.99
Episode length: 165.96 +/- 157.65
Eval num_timesteps=36000, episode_reward=-170.93 +/- 159.21
Episode length: 171.75 +/- 158.84
Eval num_timesteps=36500, episode_reward=-135.45 +/- 125.09
Episode length: 136.36 +/- 124.83
Eval num_timesteps=37000, episode_reward=-99.35 +/- 74.57
Episode length: 100.32 +/- 74.41
New best mean reward!
FINISHED IN 734.4588450919837 s


starting seed  10063 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-495.82 +/- 41.59
Episode length: 495.83 +/- 41.49
New best mean reward!
Eval num_timesteps=10500, episode_reward=-304.78 +/- 200.25
Episode length: 305.27 +/- 199.75
New best mean reward!
Eval num_timesteps=11000, episode_reward=-272.23 +/- 202.66
Episode length: 272.79 +/- 202.17
New best mean reward!
Eval num_timesteps=11500, episode_reward=-267.01 +/- 198.18
Episode length: 267.60 +/- 197.70
New best mean reward!
Eval num_timesteps=12000, episode_reward=-265.92 +/- 198.96
Episode length: 266.51 +/- 198.47
New best mean reward!
Eval num_timesteps=12500, episode_reward=-248.99 +/- 197.48
Episode length: 249.61 +/- 197.00
New best mean reward!
Eval num_timesteps=13000, episode_reward=-293.17 +/- 200.15
Episode length: 293.69 +/- 199.65
Eval num_timesteps=13500, episode_reward=-245.37 +/- 195.95
Episode length: 246.01 +/- 195.49
New best mean reward!
Eval num_timesteps=14000, episode_reward=-219.64 +/- 189.41
Episode length: 220.33 +/- 188.95
New best mean reward!
Eval num_timesteps=14500, episode_reward=-205.37 +/- 184.60
Episode length: 206.10 +/- 184.16
New best mean reward!
Eval num_timesteps=15000, episode_reward=-177.73 +/- 168.29
Episode length: 178.52 +/- 167.88
New best mean reward!
Eval num_timesteps=15500, episode_reward=-190.18 +/- 170.63
Episode length: 190.97 +/- 170.25
Eval num_timesteps=16000, episode_reward=-149.88 +/- 148.59
Episode length: 150.73 +/- 148.24
New best mean reward!
Eval num_timesteps=16500, episode_reward=-151.61 +/- 153.22
Episode length: 152.45 +/- 152.85
Eval num_timesteps=17000, episode_reward=-168.70 +/- 161.44
Episode length: 169.52 +/- 161.07
Eval num_timesteps=17500, episode_reward=-137.79 +/- 126.96
Episode length: 138.69 +/- 126.68
New best mean reward!
Eval num_timesteps=18000, episode_reward=-139.57 +/- 135.70
Episode length: 140.45 +/- 135.39
Eval num_timesteps=18500, episode_reward=-138.85 +/- 128.21
Episode length: 139.75 +/- 127.92
Eval num_timesteps=19000, episode_reward=-115.37 +/- 102.32
Episode length: 116.31 +/- 102.10
New best mean reward!
Eval num_timesteps=19500, episode_reward=-118.22 +/- 91.47
Episode length: 119.18 +/- 91.31
Eval num_timesteps=20000, episode_reward=-123.35 +/- 109.06
Episode length: 124.28 +/- 108.82
Eval num_timesteps=20500, episode_reward=-121.61 +/- 104.81
Episode length: 122.55 +/- 104.59
Eval num_timesteps=21000, episode_reward=-110.31 +/- 89.10
Episode length: 111.27 +/- 88.92
New best mean reward!
Eval num_timesteps=21500, episode_reward=-105.84 +/- 75.77
Episode length: 106.81 +/- 75.61
New best mean reward!
Eval num_timesteps=22000, episode_reward=-111.48 +/- 91.10
Episode length: 112.44 +/- 90.93
Eval num_timesteps=22500, episode_reward=-146.08 +/- 140.63
Episode length: 146.95 +/- 140.30
Eval num_timesteps=23000, episode_reward=-106.03 +/- 75.32
Episode length: 107.00 +/- 75.16
Eval num_timesteps=23500, episode_reward=-104.95 +/- 67.32
Episode length: 105.93 +/- 67.21
New best mean reward!
Eval num_timesteps=24000, episode_reward=-103.40 +/- 80.37
Episode length: 104.37 +/- 80.23
New best mean reward!
Eval num_timesteps=24500, episode_reward=-113.35 +/- 90.13
Episode length: 114.31 +/- 89.96
Eval num_timesteps=25000, episode_reward=-101.47 +/- 45.27
Episode length: 102.47 +/- 45.27
New best mean reward!
Eval num_timesteps=25500, episode_reward=-113.13 +/- 81.28
Episode length: 114.11 +/- 81.18
Eval num_timesteps=26000, episode_reward=-129.41 +/- 111.28
Episode length: 130.34 +/- 111.04
Eval num_timesteps=26500, episode_reward=-100.26 +/- 66.54
Episode length: 101.24 +/- 66.42
New best mean reward!
Eval num_timesteps=27000, episode_reward=-97.45 +/- 44.16
Episode length: 98.45 +/- 44.16
New best mean reward!
FINISHED IN 480.7099868590012 s


starting seed  10064 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-446.21 +/- 128.46
Episode length: 446.36 +/- 128.10
New best mean reward!
Eval num_timesteps=1500, episode_reward=-496.54 +/- 34.43
Episode length: 496.55 +/- 34.33
Eval num_timesteps=2000, episode_reward=-453.79 +/- 119.64
Episode length: 453.92 +/- 119.30
Eval num_timesteps=2500, episode_reward=-252.73 +/- 94.93
Episode length: 253.62 +/- 94.64
New best mean reward!
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-467.79 +/- 59.00
Episode length: 468.10 +/- 58.62
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-387.17 +/- 98.08
Episode length: 387.87 +/- 97.74
Eval num_timesteps=5000, episode_reward=-442.11 +/- 71.93
Episode length: 442.65 +/- 71.56
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-498.61 +/- 13.83
Episode length: 498.62 +/- 13.73
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-493.14 +/- 34.32
Episode length: 493.18 +/- 34.13
Eval num_timesteps=8500, episode_reward=-421.12 +/- 131.17
Episode length: 421.39 +/- 130.73
Eval num_timesteps=9000, episode_reward=-250.53 +/- 116.26
Episode length: 251.42 +/- 116.02
New best mean reward!
Eval num_timesteps=9500, episode_reward=-272.32 +/- 130.20
Episode length: 273.19 +/- 129.97
Eval num_timesteps=10000, episode_reward=-328.73 +/- 161.63
Episode length: 329.33 +/- 161.21
Eval num_timesteps=10500, episode_reward=-230.27 +/- 54.45
Episode length: 231.27 +/- 54.45
New best mean reward!
Eval num_timesteps=11000, episode_reward=-236.82 +/- 76.37
Episode length: 237.80 +/- 76.31
Eval num_timesteps=11500, episode_reward=-206.31 +/- 50.02
Episode length: 207.31 +/- 50.02
New best mean reward!
Eval num_timesteps=12000, episode_reward=-156.71 +/- 50.65
Episode length: 157.70 +/- 50.58
New best mean reward!
Eval num_timesteps=12500, episode_reward=-129.34 +/- 68.89
Episode length: 130.31 +/- 68.72
New best mean reward!
Eval num_timesteps=13000, episode_reward=-257.53 +/- 60.54
Episode length: 258.51 +/- 60.46
Eval num_timesteps=13500, episode_reward=-240.56 +/- 66.05
Episode length: 241.53 +/- 65.93
Eval num_timesteps=14000, episode_reward=-270.47 +/- 123.31
Episode length: 271.28 +/- 122.96
Eval num_timesteps=14500, episode_reward=-221.27 +/- 117.82
Episode length: 222.15 +/- 117.53
Eval num_timesteps=15000, episode_reward=-196.78 +/- 116.88
Episode length: 197.66 +/- 116.56
Eval num_timesteps=15500, episode_reward=-233.52 +/- 135.90
Episode length: 234.33 +/- 135.52
Eval num_timesteps=16000, episode_reward=-231.27 +/- 164.73
Episode length: 232.02 +/- 164.33
Eval num_timesteps=16500, episode_reward=-216.22 +/- 163.62
Episode length: 216.98 +/- 163.21
Eval num_timesteps=17000, episode_reward=-214.43 +/- 98.17
Episode length: 215.34 +/- 97.91
Eval num_timesteps=17500, episode_reward=-235.27 +/- 99.18
Episode length: 236.18 +/- 98.94
Eval num_timesteps=18000, episode_reward=-210.82 +/- 122.71
Episode length: 211.68 +/- 122.38
Eval num_timesteps=18500, episode_reward=-263.55 +/- 135.63
Episode length: 264.31 +/- 135.21
Eval num_timesteps=19000, episode_reward=-238.16 +/- 125.64
Episode length: 238.99 +/- 125.28
Eval num_timesteps=19500, episode_reward=-237.75 +/- 152.07
Episode length: 238.51 +/- 151.66
Eval num_timesteps=20000, episode_reward=-228.09 +/- 136.69
Episode length: 228.92 +/- 136.35
Eval num_timesteps=20500, episode_reward=-189.65 +/- 158.60
Episode length: 190.45 +/- 158.21
Eval num_timesteps=21000, episode_reward=-147.97 +/- 123.13
Episode length: 148.87 +/- 122.84
Eval num_timesteps=21500, episode_reward=-178.87 +/- 152.02
Episode length: 179.69 +/- 151.64
Eval num_timesteps=22000, episode_reward=-197.41 +/- 169.16
Episode length: 198.18 +/- 168.75
Eval num_timesteps=22500, episode_reward=-153.21 +/- 134.69
Episode length: 154.09 +/- 134.38
Eval num_timesteps=23000, episode_reward=-116.16 +/- 80.96
Episode length: 117.12 +/- 80.77
New best mean reward!
Eval num_timesteps=23500, episode_reward=-104.59 +/- 60.01
Episode length: 105.57 +/- 59.88
New best mean reward!
Eval num_timesteps=24000, episode_reward=-104.25 +/- 48.64
Episode length: 105.24 +/- 48.56
New best mean reward!
Eval num_timesteps=24500, episode_reward=-95.42 +/- 32.26
Episode length: 96.42 +/- 32.26
New best mean reward!
FINISHED IN 445.04490784200607 s


starting seed  10065 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-368.53 +/- 180.93
Episode length: 368.88 +/- 180.46
New best mean reward!
Eval num_timesteps=9500, episode_reward=-444.83 +/- 132.49
Episode length: 444.98 +/- 132.13
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-488.50 +/- 65.42
Episode length: 488.53 +/- 65.25
Eval num_timesteps=12500, episode_reward=-422.94 +/- 155.23
Episode length: 423.14 +/- 154.83
Eval num_timesteps=13000, episode_reward=-353.15 +/- 192.10
Episode length: 353.52 +/- 191.61
New best mean reward!
Eval num_timesteps=13500, episode_reward=-185.65 +/- 173.17
Episode length: 186.42 +/- 172.76
New best mean reward!
Eval num_timesteps=14000, episode_reward=-173.41 +/- 163.58
Episode length: 174.22 +/- 163.20
New best mean reward!
Eval num_timesteps=14500, episode_reward=-157.17 +/- 149.64
Episode length: 158.02 +/- 149.29
New best mean reward!
Eval num_timesteps=15000, episode_reward=-143.76 +/- 135.88
Episode length: 144.64 +/- 135.57
New best mean reward!
Eval num_timesteps=15500, episode_reward=-152.79 +/- 138.98
Episode length: 153.66 +/- 138.65
Eval num_timesteps=16000, episode_reward=-158.36 +/- 148.12
Episode length: 159.22 +/- 147.80
Eval num_timesteps=16500, episode_reward=-147.34 +/- 140.78
Episode length: 148.21 +/- 140.45
Eval num_timesteps=17000, episode_reward=-129.20 +/- 118.84
Episode length: 130.11 +/- 118.56
New best mean reward!
Eval num_timesteps=17500, episode_reward=-146.11 +/- 136.08
Episode length: 146.99 +/- 135.77
Eval num_timesteps=18000, episode_reward=-138.62 +/- 129.79
Episode length: 139.51 +/- 129.48
Eval num_timesteps=18500, episode_reward=-138.75 +/- 129.64
Episode length: 139.64 +/- 129.33
Eval num_timesteps=19000, episode_reward=-111.87 +/- 86.84
Episode length: 112.83 +/- 86.66
New best mean reward!
Eval num_timesteps=19500, episode_reward=-152.39 +/- 148.09
Episode length: 153.24 +/- 147.74
Eval num_timesteps=20000, episode_reward=-138.88 +/- 131.63
Episode length: 139.78 +/- 131.35
Eval num_timesteps=20500, episode_reward=-112.65 +/- 100.58
Episode length: 113.59 +/- 100.35
Eval num_timesteps=21000, episode_reward=-110.51 +/- 85.48
Episode length: 111.47 +/- 85.29
New best mean reward!
Eval num_timesteps=21500, episode_reward=-109.19 +/- 73.26
Episode length: 110.17 +/- 73.15
New best mean reward!
Eval num_timesteps=22000, episode_reward=-107.48 +/- 70.16
Episode length: 108.46 +/- 70.05
New best mean reward!
Eval num_timesteps=22500, episode_reward=-100.58 +/- 65.68
Episode length: 101.56 +/- 65.56
New best mean reward!
Eval num_timesteps=23000, episode_reward=-101.51 +/- 73.85
Episode length: 102.48 +/- 73.69
Eval num_timesteps=23500, episode_reward=-109.60 +/- 77.84
Episode length: 110.57 +/- 77.68
Eval num_timesteps=24000, episode_reward=-103.27 +/- 54.52
Episode length: 104.26 +/- 54.45
Eval num_timesteps=24500, episode_reward=-109.05 +/- 84.87
Episode length: 110.01 +/- 84.68
Eval num_timesteps=25000, episode_reward=-113.53 +/- 91.65
Episode length: 114.50 +/- 91.53
Eval num_timesteps=25500, episode_reward=-98.57 +/- 60.58
Episode length: 99.55 +/- 60.44
New best mean reward!
FINISHED IN 573.3441689639585 s


starting seed  10066 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-488.11 +/- 67.63
Episode length: 488.14 +/- 67.46
New best mean reward!
Eval num_timesteps=15000, episode_reward=-385.14 +/- 180.01
Episode length: 385.43 +/- 179.55
New best mean reward!
Eval num_timesteps=15500, episode_reward=-395.18 +/- 177.12
Episode length: 395.44 +/- 176.68
Eval num_timesteps=16000, episode_reward=-383.66 +/- 182.56
Episode length: 383.95 +/- 182.11
New best mean reward!
Eval num_timesteps=16500, episode_reward=-377.69 +/- 186.72
Episode length: 378.00 +/- 186.27
New best mean reward!
Eval num_timesteps=17000, episode_reward=-376.60 +/- 185.24
Episode length: 376.91 +/- 184.78
New best mean reward!
Eval num_timesteps=17500, episode_reward=-182.57 +/- 172.04
Episode length: 183.35 +/- 171.63
New best mean reward!
Eval num_timesteps=18000, episode_reward=-181.77 +/- 171.38
Episode length: 182.55 +/- 170.97
New best mean reward!
Eval num_timesteps=18500, episode_reward=-157.98 +/- 155.82
Episode length: 158.81 +/- 155.44
New best mean reward!
Eval num_timesteps=19000, episode_reward=-140.27 +/- 136.37
Episode length: 141.15 +/- 136.05
New best mean reward!
Eval num_timesteps=19500, episode_reward=-109.88 +/- 83.69
Episode length: 110.85 +/- 83.55
New best mean reward!
Eval num_timesteps=20000, episode_reward=-102.58 +/- 73.65
Episode length: 103.55 +/- 73.49
New best mean reward!
Eval num_timesteps=20500, episode_reward=-115.22 +/- 94.84
Episode length: 116.17 +/- 94.64
Eval num_timesteps=21000, episode_reward=-94.04 +/- 33.63
Episode length: 95.04 +/- 33.63
New best mean reward!
FINISHED IN 617.5924454000196 s


starting seed  10067 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-292.92 +/- 203.46
Episode length: 293.43 +/- 202.96
New best mean reward!
Eval num_timesteps=9000, episode_reward=-336.66 +/- 192.48
Episode length: 337.08 +/- 191.99
Eval num_timesteps=9500, episode_reward=-168.74 +/- 161.23
Episode length: 169.56 +/- 160.86
New best mean reward!
Eval num_timesteps=10000, episode_reward=-153.23 +/- 145.49
Episode length: 154.09 +/- 145.16
New best mean reward!
Eval num_timesteps=10500, episode_reward=-143.10 +/- 131.88
Episode length: 143.99 +/- 131.58
New best mean reward!
Eval num_timesteps=11000, episode_reward=-105.97 +/- 72.03
Episode length: 106.94 +/- 71.87
New best mean reward!
Eval num_timesteps=11500, episode_reward=-97.55 +/- 55.27
Episode length: 98.54 +/- 55.20
New best mean reward!
FINISHED IN 329.22233608795796 s


starting seed  10068 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-304.87 +/- 197.49
Episode length: 305.37 +/- 196.99
New best mean reward!
Eval num_timesteps=14000, episode_reward=-89.11 +/- 31.28
Episode length: 90.11 +/- 31.28
New best mean reward!
FINISHED IN 348.8143602240016 s


starting seed  10069 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-92.87 +/- 42.90
Episode length: 93.87 +/- 42.90
New best mean reward!
FINISHED IN 3.9504715889925137 s


starting seed  10070 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-484.35 +/- 69.21
Episode length: 484.40 +/- 69.00
New best mean reward!
Eval num_timesteps=7000, episode_reward=-493.63 +/- 44.71
Episode length: 493.65 +/- 44.57
Eval num_timesteps=7500, episode_reward=-402.24 +/- 146.01
Episode length: 402.56 +/- 145.56
New best mean reward!
Eval num_timesteps=8000, episode_reward=-358.01 +/- 161.16
Episode length: 358.45 +/- 160.67
New best mean reward!
Eval num_timesteps=8500, episode_reward=-428.52 +/- 126.52
Episode length: 428.77 +/- 126.10
Eval num_timesteps=9000, episode_reward=-483.43 +/- 56.94
Episode length: 483.52 +/- 56.68
Eval num_timesteps=9500, episode_reward=-302.72 +/- 146.21
Episode length: 303.38 +/- 145.75
New best mean reward!
Eval num_timesteps=10000, episode_reward=-367.25 +/- 128.02
Episode length: 367.80 +/- 127.55
Eval num_timesteps=10500, episode_reward=-497.69 +/- 16.21
Episode length: 497.71 +/- 16.08
Eval num_timesteps=11000, episode_reward=-344.56 +/- 178.34
Episode length: 345.00 +/- 177.85
Eval num_timesteps=11500, episode_reward=-120.91 +/- 91.60
Episode length: 121.86 +/- 91.39
New best mean reward!
Eval num_timesteps=12000, episode_reward=-162.09 +/- 58.82
Episode length: 163.07 +/- 58.71
Eval num_timesteps=12500, episode_reward=-169.03 +/- 32.39
Episode length: 170.03 +/- 32.39
Eval num_timesteps=13000, episode_reward=-167.48 +/- 27.38
Episode length: 168.48 +/- 27.38
Eval num_timesteps=13500, episode_reward=-177.45 +/- 47.14
Episode length: 178.44 +/- 47.07
Eval num_timesteps=14000, episode_reward=-170.14 +/- 32.98
Episode length: 171.14 +/- 32.98
Eval num_timesteps=14500, episode_reward=-175.92 +/- 55.58
Episode length: 176.90 +/- 55.46
Eval num_timesteps=15000, episode_reward=-170.94 +/- 43.68
Episode length: 171.93 +/- 43.61
Eval num_timesteps=15500, episode_reward=-160.12 +/- 26.74
Episode length: 161.12 +/- 26.74
Eval num_timesteps=16000, episode_reward=-149.64 +/- 38.27
Episode length: 150.64 +/- 38.27
Eval num_timesteps=16500, episode_reward=-161.16 +/- 38.25
Episode length: 162.16 +/- 38.25
Eval num_timesteps=17000, episode_reward=-152.02 +/- 31.58
Episode length: 153.02 +/- 31.58
Eval num_timesteps=17500, episode_reward=-141.16 +/- 44.10
Episode length: 142.16 +/- 44.10
Eval num_timesteps=18000, episode_reward=-118.69 +/- 41.21
Episode length: 119.69 +/- 41.21
New best mean reward!
Eval num_timesteps=18500, episode_reward=-151.64 +/- 28.29
Episode length: 152.64 +/- 28.29
Eval num_timesteps=19000, episode_reward=-183.35 +/- 53.46
Episode length: 184.33 +/- 53.34
Eval num_timesteps=19500, episode_reward=-180.47 +/- 40.80
Episode length: 181.46 +/- 40.72
Eval num_timesteps=20000, episode_reward=-172.68 +/- 37.97
Episode length: 173.67 +/- 37.88
Eval num_timesteps=20500, episode_reward=-176.64 +/- 53.14
Episode length: 177.62 +/- 53.01
Eval num_timesteps=21000, episode_reward=-136.65 +/- 27.43
Episode length: 137.65 +/- 27.43
Eval num_timesteps=21500, episode_reward=-121.08 +/- 34.04
Episode length: 122.08 +/- 34.04
Eval num_timesteps=22000, episode_reward=-101.58 +/- 27.20
Episode length: 102.58 +/- 27.20
New best mean reward!
Eval num_timesteps=22500, episode_reward=-94.69 +/- 28.81
Episode length: 95.69 +/- 28.81
New best mean reward!
FINISHED IN 404.735427939042 s


starting seed  10071 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-215.21 +/- 121.79
Episode length: 216.07 +/- 121.46
New best mean reward!
Eval num_timesteps=7500, episode_reward=-344.08 +/- 162.70
Episode length: 344.57 +/- 162.21
Eval num_timesteps=8000, episode_reward=-260.13 +/- 149.86
Episode length: 260.87 +/- 149.44
Eval num_timesteps=8500, episode_reward=-247.66 +/- 146.78
Episode length: 248.41 +/- 146.36
Eval num_timesteps=9000, episode_reward=-185.26 +/- 88.02
Episode length: 186.20 +/- 87.80
New best mean reward!
Eval num_timesteps=9500, episode_reward=-173.09 +/- 53.42
Episode length: 174.08 +/- 53.36
New best mean reward!
Eval num_timesteps=10000, episode_reward=-176.88 +/- 25.83
Episode length: 177.88 +/- 25.83
Eval num_timesteps=10500, episode_reward=-497.81 +/- 15.17
Episode length: 497.84 +/- 15.03
Eval num_timesteps=11000, episode_reward=-211.90 +/- 53.83
Episode length: 212.88 +/- 53.73
Eval num_timesteps=11500, episode_reward=-172.46 +/- 55.15
Episode length: 173.44 +/- 55.03
New best mean reward!
Eval num_timesteps=12000, episode_reward=-176.97 +/- 66.42
Episode length: 177.94 +/- 66.27
Eval num_timesteps=12500, episode_reward=-168.06 +/- 42.92
Episode length: 169.05 +/- 42.84
New best mean reward!
Eval num_timesteps=13000, episode_reward=-160.01 +/- 39.08
Episode length: 161.01 +/- 39.08
New best mean reward!
Eval num_timesteps=13500, episode_reward=-159.59 +/- 43.53
Episode length: 160.58 +/- 43.45
New best mean reward!
Eval num_timesteps=14000, episode_reward=-103.95 +/- 36.67
Episode length: 104.95 +/- 36.67
New best mean reward!
Eval num_timesteps=14500, episode_reward=-97.47 +/- 27.34
Episode length: 98.47 +/- 27.34
New best mean reward!
FINISHED IN 302.4413699589786 s


starting seed  10072 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-237.09 +/- 132.25
Episode length: 237.91 +/- 131.90
New best mean reward!
Eval num_timesteps=8000, episode_reward=-161.34 +/- 38.36
Episode length: 162.34 +/- 38.36
New best mean reward!
Eval num_timesteps=8500, episode_reward=-196.41 +/- 98.77
Episode length: 197.33 +/- 98.53
Eval num_timesteps=9000, episode_reward=-159.85 +/- 29.40
Episode length: 160.85 +/- 29.40
New best mean reward!
Eval num_timesteps=9500, episode_reward=-164.71 +/- 47.20
Episode length: 165.71 +/- 47.20
Eval num_timesteps=10000, episode_reward=-169.71 +/- 49.06
Episode length: 170.70 +/- 48.99
Eval num_timesteps=10500, episode_reward=-180.62 +/- 48.84
Episode length: 181.61 +/- 48.78
Eval num_timesteps=11000, episode_reward=-195.35 +/- 65.05
Episode length: 196.33 +/- 64.95
Eval num_timesteps=11500, episode_reward=-194.10 +/- 75.97
Episode length: 195.06 +/- 75.81
Eval num_timesteps=12000, episode_reward=-172.95 +/- 48.43
Episode length: 173.95 +/- 48.43
Eval num_timesteps=12500, episode_reward=-187.48 +/- 65.24
Episode length: 188.46 +/- 65.15
Eval num_timesteps=13000, episode_reward=-171.22 +/- 41.12
Episode length: 172.22 +/- 41.12
Eval num_timesteps=13500, episode_reward=-183.02 +/- 51.13
Episode length: 184.01 +/- 51.07
Eval num_timesteps=14000, episode_reward=-186.00 +/- 60.29
Episode length: 186.99 +/- 60.24
Eval num_timesteps=14500, episode_reward=-185.21 +/- 46.59
Episode length: 186.21 +/- 46.59
Eval num_timesteps=15000, episode_reward=-170.27 +/- 34.74
Episode length: 171.27 +/- 34.74
Eval num_timesteps=15500, episode_reward=-177.85 +/- 40.17
Episode length: 178.85 +/- 40.17
Eval num_timesteps=16000, episode_reward=-178.64 +/- 38.89
Episode length: 179.64 +/- 38.89
Eval num_timesteps=16500, episode_reward=-164.61 +/- 35.52
Episode length: 165.61 +/- 35.52
Eval num_timesteps=17000, episode_reward=-172.17 +/- 40.52
Episode length: 173.17 +/- 40.52
Eval num_timesteps=17500, episode_reward=-163.44 +/- 45.06
Episode length: 164.44 +/- 45.06
Eval num_timesteps=18000, episode_reward=-166.31 +/- 45.21
Episode length: 167.31 +/- 45.21
Eval num_timesteps=18500, episode_reward=-172.55 +/- 53.31
Episode length: 173.55 +/- 53.31
Eval num_timesteps=19000, episode_reward=-163.40 +/- 41.71
Episode length: 164.40 +/- 41.71
Eval num_timesteps=19500, episode_reward=-161.30 +/- 48.63
Episode length: 162.30 +/- 48.63
Eval num_timesteps=20000, episode_reward=-157.72 +/- 38.29
Episode length: 158.72 +/- 38.29
New best mean reward!
Eval num_timesteps=20500, episode_reward=-168.30 +/- 41.51
Episode length: 169.30 +/- 41.51
Eval num_timesteps=21000, episode_reward=-162.90 +/- 43.56
Episode length: 163.90 +/- 43.56
Eval num_timesteps=21500, episode_reward=-166.45 +/- 57.16
Episode length: 167.44 +/- 57.10
Eval num_timesteps=22000, episode_reward=-163.57 +/- 41.49
Episode length: 164.57 +/- 41.49
Eval num_timesteps=22500, episode_reward=-160.15 +/- 39.40
Episode length: 161.15 +/- 39.40
Eval num_timesteps=23000, episode_reward=-165.48 +/- 40.59
Episode length: 166.48 +/- 40.59
Eval num_timesteps=23500, episode_reward=-158.39 +/- 40.40
Episode length: 159.39 +/- 40.40
Eval num_timesteps=24000, episode_reward=-166.73 +/- 44.60
Episode length: 167.73 +/- 44.60
Eval num_timesteps=24500, episode_reward=-168.50 +/- 47.09
Episode length: 169.50 +/- 47.09
Eval num_timesteps=25000, episode_reward=-158.89 +/- 36.18
Episode length: 159.89 +/- 36.18
Eval num_timesteps=25500, episode_reward=-160.76 +/- 42.25
Episode length: 161.76 +/- 42.25
Eval num_timesteps=26000, episode_reward=-161.40 +/- 44.92
Episode length: 162.40 +/- 44.92
Eval num_timesteps=26500, episode_reward=-165.44 +/- 40.97
Episode length: 166.44 +/- 40.97
Eval num_timesteps=27000, episode_reward=-157.77 +/- 35.53
Episode length: 158.77 +/- 35.53
Eval num_timesteps=27500, episode_reward=-162.03 +/- 45.57
Episode length: 163.03 +/- 45.57
Eval num_timesteps=28000, episode_reward=-160.50 +/- 46.96
Episode length: 161.50 +/- 46.96
Eval num_timesteps=28500, episode_reward=-161.07 +/- 44.71
Episode length: 162.07 +/- 44.71
Eval num_timesteps=29000, episode_reward=-165.86 +/- 46.33
Episode length: 166.86 +/- 46.33
Eval num_timesteps=29500, episode_reward=-161.51 +/- 35.82
Episode length: 162.51 +/- 35.82
Eval num_timesteps=30000, episode_reward=-162.16 +/- 44.01
Episode length: 163.16 +/- 44.01
Eval num_timesteps=30500, episode_reward=-170.20 +/- 53.42
Episode length: 171.20 +/- 53.42
Eval num_timesteps=31000, episode_reward=-171.97 +/- 48.42
Episode length: 172.97 +/- 48.42
Eval num_timesteps=31500, episode_reward=-165.37 +/- 43.78
Episode length: 166.37 +/- 43.78
Eval num_timesteps=32000, episode_reward=-157.20 +/- 33.40
Episode length: 158.20 +/- 33.40
New best mean reward!
Eval num_timesteps=32500, episode_reward=-155.44 +/- 37.85
Episode length: 156.44 +/- 37.85
New best mean reward!
Eval num_timesteps=33000, episode_reward=-159.78 +/- 36.16
Episode length: 160.78 +/- 36.16
Eval num_timesteps=33500, episode_reward=-166.06 +/- 45.65
Episode length: 167.06 +/- 45.65
Eval num_timesteps=34000, episode_reward=-151.34 +/- 37.23
Episode length: 152.34 +/- 37.23
New best mean reward!
Eval num_timesteps=34500, episode_reward=-160.13 +/- 30.27
Episode length: 161.13 +/- 30.27
Eval num_timesteps=35000, episode_reward=-163.89 +/- 43.82
Episode length: 164.89 +/- 43.82
Eval num_timesteps=35500, episode_reward=-157.66 +/- 44.33
Episode length: 158.65 +/- 44.25
Eval num_timesteps=36000, episode_reward=-156.59 +/- 28.73
Episode length: 157.59 +/- 28.73
Eval num_timesteps=36500, episode_reward=-154.65 +/- 44.81
Episode length: 155.64 +/- 44.73
Eval num_timesteps=37000, episode_reward=-153.52 +/- 36.88
Episode length: 154.52 +/- 36.88
Eval num_timesteps=37500, episode_reward=-152.41 +/- 30.70
Episode length: 153.41 +/- 30.70
Eval num_timesteps=38000, episode_reward=-150.39 +/- 31.32
Episode length: 151.39 +/- 31.32
New best mean reward!
Eval num_timesteps=38500, episode_reward=-142.88 +/- 21.99
Episode length: 143.88 +/- 21.99
New best mean reward!
Eval num_timesteps=39000, episode_reward=-137.77 +/- 25.91
Episode length: 138.77 +/- 25.91
New best mean reward!
Eval num_timesteps=39500, episode_reward=-130.96 +/- 23.52
Episode length: 131.96 +/- 23.52
New best mean reward!
Eval num_timesteps=40000, episode_reward=-144.51 +/- 32.46
Episode length: 145.51 +/- 32.46
Eval num_timesteps=40500, episode_reward=-125.18 +/- 21.62
Episode length: 126.18 +/- 21.62
New best mean reward!
Eval num_timesteps=41000, episode_reward=-130.69 +/- 36.58
Episode length: 131.69 +/- 36.58
Eval num_timesteps=41500, episode_reward=-125.21 +/- 26.33
Episode length: 126.21 +/- 26.33
Eval num_timesteps=42000, episode_reward=-122.36 +/- 29.76
Episode length: 123.36 +/- 29.76
New best mean reward!
Eval num_timesteps=42500, episode_reward=-120.71 +/- 30.49
Episode length: 121.71 +/- 30.49
New best mean reward!
Eval num_timesteps=43000, episode_reward=-121.06 +/- 31.06
Episode length: 122.06 +/- 31.06
Eval num_timesteps=43500, episode_reward=-121.52 +/- 32.64
Episode length: 122.52 +/- 32.64
Eval num_timesteps=44000, episode_reward=-123.16 +/- 32.80
Episode length: 124.16 +/- 32.80
Eval num_timesteps=44500, episode_reward=-124.66 +/- 46.67
Episode length: 125.65 +/- 46.58
Eval num_timesteps=45000, episode_reward=-127.50 +/- 57.61
Episode length: 128.49 +/- 57.55
Eval num_timesteps=45500, episode_reward=-116.32 +/- 26.44
Episode length: 117.32 +/- 26.44
New best mean reward!
Eval num_timesteps=46000, episode_reward=-128.30 +/- 43.35
Episode length: 129.30 +/- 43.35
Eval num_timesteps=46500, episode_reward=-126.51 +/- 67.34
Episode length: 127.49 +/- 67.23
Eval num_timesteps=47000, episode_reward=-113.24 +/- 28.50
Episode length: 114.24 +/- 28.50
New best mean reward!
Eval num_timesteps=47500, episode_reward=-111.99 +/- 27.58
Episode length: 112.99 +/- 27.58
New best mean reward!
Eval num_timesteps=48000, episode_reward=-118.81 +/- 52.79
Episode length: 119.80 +/- 52.72
Eval num_timesteps=48500, episode_reward=-114.22 +/- 32.26
Episode length: 115.22 +/- 32.26
Eval num_timesteps=49000, episode_reward=-116.56 +/- 49.52
Episode length: 117.55 +/- 49.44
Eval num_timesteps=49500, episode_reward=-115.33 +/- 35.65
Episode length: 116.33 +/- 35.65
Eval num_timesteps=50000, episode_reward=-111.21 +/- 33.38
Episode length: 112.21 +/- 33.38
New best mean reward!
FINISHED IN 612.4514967210125 s


starting seed  10073 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-118.86 +/- 51.14
Episode length: 119.85 +/- 51.07
New best mean reward!
Eval num_timesteps=5000, episode_reward=-97.85 +/- 34.70
Episode length: 98.85 +/- 34.70
New best mean reward!
FINISHED IN 109.73930851300247 s


starting seed  10074 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-374.84 +/- 163.38
Episode length: 375.22 +/- 162.90
New best mean reward!
Eval num_timesteps=7000, episode_reward=-346.19 +/- 170.95
Episode length: 346.64 +/- 170.46
New best mean reward!
Eval num_timesteps=7500, episode_reward=-317.73 +/- 170.42
Episode length: 318.27 +/- 169.93
New best mean reward!
Eval num_timesteps=8000, episode_reward=-210.96 +/- 116.97
Episode length: 211.83 +/- 116.65
New best mean reward!
Eval num_timesteps=8500, episode_reward=-163.42 +/- 41.44
Episode length: 164.42 +/- 41.44
New best mean reward!
Eval num_timesteps=9000, episode_reward=-148.61 +/- 24.43
Episode length: 149.61 +/- 24.43
New best mean reward!
Eval num_timesteps=9500, episode_reward=-161.21 +/- 62.45
Episode length: 162.19 +/- 62.34
Eval num_timesteps=10000, episode_reward=-152.38 +/- 30.54
Episode length: 153.38 +/- 30.54
Eval num_timesteps=10500, episode_reward=-113.55 +/- 22.72
Episode length: 114.55 +/- 22.72
New best mean reward!
Eval num_timesteps=11000, episode_reward=-147.06 +/- 20.25
Episode length: 148.06 +/- 20.25
Eval num_timesteps=11500, episode_reward=-99.32 +/- 32.60
Episode length: 100.32 +/- 32.60
New best mean reward!
FINISHED IN 222.29670542804524 s


starting seed  10075 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-307.13 +/- 201.34
Episode length: 307.61 +/- 200.84
New best mean reward!
Eval num_timesteps=7500, episode_reward=-353.70 +/- 164.47
Episode length: 354.17 +/- 163.99
Eval num_timesteps=8000, episode_reward=-190.45 +/- 136.59
Episode length: 191.30 +/- 136.25
New best mean reward!
Eval num_timesteps=8500, episode_reward=-194.10 +/- 164.44
Episode length: 194.88 +/- 164.03
Eval num_timesteps=9000, episode_reward=-159.98 +/- 134.24
Episode length: 160.85 +/- 133.91
New best mean reward!
Eval num_timesteps=9500, episode_reward=-169.17 +/- 147.87
Episode length: 170.01 +/- 147.51
Eval num_timesteps=10000, episode_reward=-271.36 +/- 181.06
Episode length: 271.98 +/- 180.58
Eval num_timesteps=10500, episode_reward=-123.66 +/- 85.45
Episode length: 124.62 +/- 85.28
New best mean reward!
Eval num_timesteps=11000, episode_reward=-203.94 +/- 172.12
Episode length: 204.69 +/- 171.69
Eval num_timesteps=11500, episode_reward=-155.86 +/- 140.32
Episode length: 156.72 +/- 139.98
Eval num_timesteps=12000, episode_reward=-185.51 +/- 161.12
Episode length: 186.31 +/- 160.73
Eval num_timesteps=12500, episode_reward=-235.66 +/- 195.91
Episode length: 236.31 +/- 195.44
Eval num_timesteps=13000, episode_reward=-158.15 +/- 147.25
Episode length: 159.01 +/- 146.93
Eval num_timesteps=13500, episode_reward=-92.09 +/- 47.56
Episode length: 93.08 +/- 47.48
New best mean reward!
FINISHED IN 259.2064234119607 s


starting seed  10076 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-469.14 +/- 104.84
Episode length: 469.22 +/- 104.57
New best mean reward!
Eval num_timesteps=11000, episode_reward=-278.51 +/- 201.95
Episode length: 279.07 +/- 201.47
New best mean reward!
Eval num_timesteps=11500, episode_reward=-229.98 +/- 192.62
Episode length: 230.65 +/- 192.16
New best mean reward!
Eval num_timesteps=12000, episode_reward=-174.40 +/- 165.65
Episode length: 175.21 +/- 165.27
New best mean reward!
Eval num_timesteps=12500, episode_reward=-176.13 +/- 166.37
Episode length: 176.93 +/- 165.99
Eval num_timesteps=13000, episode_reward=-139.07 +/- 132.86
Episode length: 139.96 +/- 132.56
New best mean reward!
Eval num_timesteps=13500, episode_reward=-110.85 +/- 92.76
Episode length: 111.81 +/- 92.59
New best mean reward!
Eval num_timesteps=14000, episode_reward=-136.52 +/- 127.61
Episode length: 137.42 +/- 127.32
Eval num_timesteps=14500, episode_reward=-100.09 +/- 68.64
Episode length: 101.07 +/- 68.52
New best mean reward!
Eval num_timesteps=15000, episode_reward=-100.03 +/- 67.22
Episode length: 101.01 +/- 67.10
New best mean reward!
Eval num_timesteps=15500, episode_reward=-106.65 +/- 81.93
Episode length: 107.62 +/- 81.78
Eval num_timesteps=16000, episode_reward=-111.72 +/- 100.58
Episode length: 112.66 +/- 100.35
Eval num_timesteps=16500, episode_reward=-119.92 +/- 103.81
Episode length: 120.87 +/- 103.62
Eval num_timesteps=17000, episode_reward=-101.37 +/- 69.03
Episode length: 102.35 +/- 68.91
Eval num_timesteps=17500, episode_reward=-122.02 +/- 117.54
Episode length: 122.94 +/- 117.28
Eval num_timesteps=18000, episode_reward=-103.01 +/- 69.98
Episode length: 103.99 +/- 69.87
Eval num_timesteps=18500, episode_reward=-118.79 +/- 105.10
Episode length: 119.73 +/- 104.88
Eval num_timesteps=19000, episode_reward=-109.10 +/- 85.28
Episode length: 110.06 +/- 85.10
Eval num_timesteps=19500, episode_reward=-104.17 +/- 82.87
Episode length: 105.14 +/- 82.72
Eval num_timesteps=20000, episode_reward=-103.58 +/- 67.82
Episode length: 104.56 +/- 67.71
Eval num_timesteps=20500, episode_reward=-95.35 +/- 48.35
Episode length: 96.34 +/- 48.27
New best mean reward!
FINISHED IN 382.07471170299686 s


starting seed  10077 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-209.21 +/- 59.79
Episode length: 210.20 +/- 59.74
New best mean reward!
Eval num_timesteps=8500, episode_reward=-200.11 +/- 98.26
Episode length: 201.03 +/- 98.02
New best mean reward!
Eval num_timesteps=9000, episode_reward=-196.29 +/- 156.56
Episode length: 197.09 +/- 156.17
New best mean reward!
Eval num_timesteps=9500, episode_reward=-119.49 +/- 23.38
Episode length: 120.49 +/- 23.38
New best mean reward!
Eval num_timesteps=10000, episode_reward=-120.68 +/- 34.22
Episode length: 121.68 +/- 34.22
Eval num_timesteps=10500, episode_reward=-166.25 +/- 49.50
Episode length: 167.24 +/- 49.44
Eval num_timesteps=11000, episode_reward=-234.25 +/- 146.13
Episode length: 235.05 +/- 145.77
Eval num_timesteps=11500, episode_reward=-443.59 +/- 125.48
Episode length: 443.77 +/- 125.11
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-442.12 +/- 136.14
Episode length: 442.28 +/- 135.78
Eval num_timesteps=13000, episode_reward=-92.31 +/- 35.65
Episode length: 93.31 +/- 35.65
New best mean reward!
FINISHED IN 264.54262922302587 s


starting seed  10078 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-496.24 +/- 37.41
Episode length: 496.25 +/- 37.31
New best mean reward!
Eval num_timesteps=10500, episode_reward=-341.68 +/- 198.03
Episode length: 342.08 +/- 197.55
New best mean reward!
Eval num_timesteps=11000, episode_reward=-174.24 +/- 160.45
Episode length: 175.06 +/- 160.08
New best mean reward!
Eval num_timesteps=11500, episode_reward=-153.12 +/- 135.49
Episode length: 154.01 +/- 135.20
New best mean reward!
Eval num_timesteps=12000, episode_reward=-167.78 +/- 146.05
Episode length: 168.63 +/- 145.71
Eval num_timesteps=12500, episode_reward=-122.52 +/- 104.04
Episode length: 123.46 +/- 103.82
New best mean reward!
Eval num_timesteps=13000, episode_reward=-115.04 +/- 99.85
Episode length: 115.98 +/- 99.62
New best mean reward!
Eval num_timesteps=13500, episode_reward=-129.28 +/- 125.04
Episode length: 130.18 +/- 124.74
Eval num_timesteps=14000, episode_reward=-116.00 +/- 97.30
Episode length: 116.95 +/- 97.10
Eval num_timesteps=14500, episode_reward=-172.67 +/- 163.46
Episode length: 173.49 +/- 163.10
Eval num_timesteps=15000, episode_reward=-160.03 +/- 150.39
Episode length: 160.88 +/- 150.05
Eval num_timesteps=15500, episode_reward=-123.15 +/- 109.50
Episode length: 124.08 +/- 109.26
Eval num_timesteps=16000, episode_reward=-142.95 +/- 139.86
Episode length: 143.82 +/- 139.53
Eval num_timesteps=16500, episode_reward=-203.31 +/- 183.95
Episode length: 204.04 +/- 183.52
Eval num_timesteps=17000, episode_reward=-182.68 +/- 166.46
Episode length: 183.48 +/- 166.08
Eval num_timesteps=17500, episode_reward=-221.95 +/- 191.76
Episode length: 222.63 +/- 191.29
Eval num_timesteps=18000, episode_reward=-135.56 +/- 129.07
Episode length: 136.46 +/- 128.79
Eval num_timesteps=18500, episode_reward=-148.35 +/- 142.46
Episode length: 149.22 +/- 142.14
Eval num_timesteps=19000, episode_reward=-92.34 +/- 49.63
Episode length: 93.33 +/- 49.55
New best mean reward!
FINISHED IN 364.42289032699773 s


starting seed  10079 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-497.70 +/- 15.03
Episode length: 497.74 +/- 14.89
New best mean reward!
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-305.23 +/- 178.05
Episode length: 305.78 +/- 177.55
New best mean reward!
Eval num_timesteps=12000, episode_reward=-288.81 +/- 197.37
Episode length: 289.35 +/- 196.88
New best mean reward!
Eval num_timesteps=12500, episode_reward=-291.27 +/- 189.60
Episode length: 291.82 +/- 189.10
Eval num_timesteps=13000, episode_reward=-235.67 +/- 166.99
Episode length: 236.39 +/- 166.54
New best mean reward!
Eval num_timesteps=13500, episode_reward=-141.45 +/- 115.00
Episode length: 142.36 +/- 114.72
New best mean reward!
Eval num_timesteps=14000, episode_reward=-129.27 +/- 81.95
Episode length: 130.23 +/- 81.77
New best mean reward!
Eval num_timesteps=14500, episode_reward=-156.57 +/- 52.96
Episode length: 157.55 +/- 52.83
Eval num_timesteps=15000, episode_reward=-198.05 +/- 60.09
Episode length: 199.03 +/- 59.99
Eval num_timesteps=15500, episode_reward=-166.79 +/- 44.00
Episode length: 167.78 +/- 43.93
Eval num_timesteps=16000, episode_reward=-176.97 +/- 63.49
Episode length: 177.94 +/- 63.34
Eval num_timesteps=16500, episode_reward=-166.62 +/- 107.42
Episode length: 167.54 +/- 107.18
Eval num_timesteps=17000, episode_reward=-149.88 +/- 20.50
Episode length: 150.88 +/- 20.50
Eval num_timesteps=17500, episode_reward=-144.94 +/- 108.29
Episode length: 145.87 +/- 108.06
Eval num_timesteps=18000, episode_reward=-196.61 +/- 91.82
Episode length: 197.53 +/- 91.55
Eval num_timesteps=18500, episode_reward=-178.88 +/- 71.58
Episode length: 179.85 +/- 71.44
Eval num_timesteps=19000, episode_reward=-194.93 +/- 100.88
Episode length: 195.84 +/- 100.60
Eval num_timesteps=19500, episode_reward=-186.74 +/- 91.74
Episode length: 187.67 +/- 91.50
Eval num_timesteps=20000, episode_reward=-197.34 +/- 109.48
Episode length: 198.23 +/- 109.17
Eval num_timesteps=20500, episode_reward=-234.84 +/- 139.21
Episode length: 235.64 +/- 138.83
Eval num_timesteps=21000, episode_reward=-293.21 +/- 159.99
Episode length: 293.85 +/- 159.53
Eval num_timesteps=21500, episode_reward=-325.70 +/- 168.64
Episode length: 326.22 +/- 168.14
Eval num_timesteps=22000, episode_reward=-314.44 +/- 169.33
Episode length: 314.99 +/- 168.83
Eval num_timesteps=22500, episode_reward=-291.80 +/- 177.61
Episode length: 292.38 +/- 177.12
Eval num_timesteps=23000, episode_reward=-242.47 +/- 190.10
Episode length: 243.12 +/- 189.63
Eval num_timesteps=23500, episode_reward=-284.55 +/- 195.86
Episode length: 285.10 +/- 195.36
Eval num_timesteps=24000, episode_reward=-262.00 +/- 194.87
Episode length: 262.60 +/- 194.38
Eval num_timesteps=24500, episode_reward=-247.59 +/- 190.39
Episode length: 248.23 +/- 189.92
Eval num_timesteps=25000, episode_reward=-293.65 +/- 199.43
Episode length: 294.17 +/- 198.93
Eval num_timesteps=25500, episode_reward=-268.66 +/- 201.61
Episode length: 269.23 +/- 201.11
Eval num_timesteps=26000, episode_reward=-273.44 +/- 201.52
Episode length: 274.00 +/- 201.02
Eval num_timesteps=26500, episode_reward=-311.78 +/- 204.34
Episode length: 312.24 +/- 203.84
Eval num_timesteps=27000, episode_reward=-225.96 +/- 188.86
Episode length: 226.65 +/- 188.41
Eval num_timesteps=27500, episode_reward=-225.63 +/- 193.61
Episode length: 226.30 +/- 193.14
Eval num_timesteps=28000, episode_reward=-205.14 +/- 184.80
Episode length: 205.86 +/- 184.36
Eval num_timesteps=28500, episode_reward=-209.78 +/- 186.44
Episode length: 210.49 +/- 185.99
Eval num_timesteps=29000, episode_reward=-229.35 +/- 195.60
Episode length: 230.01 +/- 195.13
Eval num_timesteps=29500, episode_reward=-294.12 +/- 199.87
Episode length: 294.64 +/- 199.38
Eval num_timesteps=30000, episode_reward=-244.51 +/- 198.15
Episode length: 245.14 +/- 197.67
Eval num_timesteps=30500, episode_reward=-237.99 +/- 198.34
Episode length: 238.63 +/- 197.86
Eval num_timesteps=31000, episode_reward=-220.38 +/- 194.08
Episode length: 221.06 +/- 193.62
Eval num_timesteps=31500, episode_reward=-221.84 +/- 191.68
Episode length: 222.53 +/- 191.23
Eval num_timesteps=32000, episode_reward=-205.22 +/- 185.46
Episode length: 205.94 +/- 185.01
Eval num_timesteps=32500, episode_reward=-244.44 +/- 198.01
Episode length: 245.07 +/- 197.53
Eval num_timesteps=33000, episode_reward=-222.60 +/- 192.62
Episode length: 223.28 +/- 192.16
Eval num_timesteps=33500, episode_reward=-259.42 +/- 199.67
Episode length: 260.02 +/- 199.19
Eval num_timesteps=34000, episode_reward=-264.92 +/- 201.81
Episode length: 265.50 +/- 201.32
Eval num_timesteps=34500, episode_reward=-228.32 +/- 192.02
Episode length: 228.99 +/- 191.55
Eval num_timesteps=35000, episode_reward=-250.64 +/- 201.27
Episode length: 251.26 +/- 200.80
Eval num_timesteps=35500, episode_reward=-258.00 +/- 202.61
Episode length: 258.59 +/- 202.12
Eval num_timesteps=36000, episode_reward=-266.91 +/- 201.47
Episode length: 267.49 +/- 200.99
Eval num_timesteps=36500, episode_reward=-218.19 +/- 190.95
Episode length: 218.88 +/- 190.50
Eval num_timesteps=37000, episode_reward=-216.75 +/- 190.86
Episode length: 217.44 +/- 190.40
Eval num_timesteps=37500, episode_reward=-209.47 +/- 190.80
Episode length: 210.18 +/- 190.36
Eval num_timesteps=38000, episode_reward=-216.70 +/- 186.73
Episode length: 217.40 +/- 186.28
Eval num_timesteps=38500, episode_reward=-175.44 +/- 168.59
Episode length: 176.23 +/- 168.19
Eval num_timesteps=39000, episode_reward=-191.88 +/- 175.40
Episode length: 192.65 +/- 175.00
Eval num_timesteps=39500, episode_reward=-153.35 +/- 152.23
Episode length: 154.19 +/- 151.86
Eval num_timesteps=40000, episode_reward=-168.97 +/- 164.29
Episode length: 169.78 +/- 163.91
Eval num_timesteps=40500, episode_reward=-171.25 +/- 164.78
Episode length: 172.06 +/- 164.40
Eval num_timesteps=41000, episode_reward=-185.36 +/- 177.61
Episode length: 186.12 +/- 177.19
Eval num_timesteps=41500, episode_reward=-151.64 +/- 145.10
Episode length: 152.50 +/- 144.77
Eval num_timesteps=42000, episode_reward=-177.32 +/- 167.68
Episode length: 178.11 +/- 167.28
Eval num_timesteps=42500, episode_reward=-163.37 +/- 158.01
Episode length: 164.20 +/- 157.65
Eval num_timesteps=43000, episode_reward=-149.01 +/- 146.55
Episode length: 149.87 +/- 146.22
Eval num_timesteps=43500, episode_reward=-164.05 +/- 159.28
Episode length: 164.87 +/- 158.90
Eval num_timesteps=44000, episode_reward=-151.94 +/- 148.01
Episode length: 152.79 +/- 147.66
Eval num_timesteps=44500, episode_reward=-122.30 +/- 116.76
Episode length: 123.22 +/- 116.50
New best mean reward!
Eval num_timesteps=45000, episode_reward=-152.92 +/- 153.20
Episode length: 153.76 +/- 152.83
Eval num_timesteps=45500, episode_reward=-156.52 +/- 156.92
Episode length: 157.35 +/- 156.55
Eval num_timesteps=46000, episode_reward=-133.83 +/- 129.28
Episode length: 134.73 +/- 129.00
Eval num_timesteps=46500, episode_reward=-128.45 +/- 119.89
Episode length: 129.36 +/- 119.61
Eval num_timesteps=47000, episode_reward=-161.63 +/- 159.66
Episode length: 162.45 +/- 159.28
Eval num_timesteps=47500, episode_reward=-152.55 +/- 149.73
Episode length: 153.40 +/- 149.38
Eval num_timesteps=48000, episode_reward=-134.92 +/- 133.94
Episode length: 135.81 +/- 133.64
Eval num_timesteps=48500, episode_reward=-132.74 +/- 130.36
Episode length: 133.63 +/- 130.05
Eval num_timesteps=49000, episode_reward=-149.87 +/- 144.51
Episode length: 150.73 +/- 144.17
Eval num_timesteps=49500, episode_reward=-156.94 +/- 151.76
Episode length: 157.78 +/- 151.39
Eval num_timesteps=50000, episode_reward=-133.55 +/- 125.65
Episode length: 134.45 +/- 125.36
FINISHED IN 946.7624030829757 s


starting seed  10080 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-376.24 +/- 189.23
Episode length: 376.54 +/- 188.78
New best mean reward!
Eval num_timesteps=13500, episode_reward=-276.99 +/- 206.66
Episode length: 277.53 +/- 206.16
New best mean reward!
Eval num_timesteps=14000, episode_reward=-250.56 +/- 202.04
Episode length: 251.17 +/- 201.55
New best mean reward!
Eval num_timesteps=14500, episode_reward=-228.02 +/- 192.84
Episode length: 228.69 +/- 192.38
New best mean reward!
Eval num_timesteps=15000, episode_reward=-265.72 +/- 202.54
Episode length: 266.30 +/- 202.06
Eval num_timesteps=15500, episode_reward=-191.58 +/- 179.19
Episode length: 192.33 +/- 178.76
New best mean reward!
Eval num_timesteps=16000, episode_reward=-173.95 +/- 161.35
Episode length: 174.76 +/- 160.97
New best mean reward!
Eval num_timesteps=16500, episode_reward=-172.77 +/- 162.60
Episode length: 173.58 +/- 162.21
New best mean reward!
Eval num_timesteps=17000, episode_reward=-134.38 +/- 128.16
Episode length: 135.28 +/- 127.87
New best mean reward!
Eval num_timesteps=17500, episode_reward=-93.38 +/- 60.24
Episode length: 94.36 +/- 60.11
New best mean reward!
FINISHED IN 377.99169688497204 s


starting seed  10081 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-175.47 +/- 56.06
Episode length: 176.46 +/- 56.00
New best mean reward!
Eval num_timesteps=13500, episode_reward=-191.40 +/- 47.96
Episode length: 192.40 +/- 47.96
Eval num_timesteps=14000, episode_reward=-174.00 +/- 38.71
Episode length: 175.00 +/- 38.71
New best mean reward!
Eval num_timesteps=14500, episode_reward=-169.49 +/- 33.84
Episode length: 170.49 +/- 33.84
New best mean reward!
Eval num_timesteps=15000, episode_reward=-164.73 +/- 31.39
Episode length: 165.73 +/- 31.39
New best mean reward!
Eval num_timesteps=15500, episode_reward=-196.84 +/- 54.65
Episode length: 197.83 +/- 54.60
Eval num_timesteps=16000, episode_reward=-184.53 +/- 54.73
Episode length: 185.52 +/- 54.67
Eval num_timesteps=16500, episode_reward=-177.55 +/- 32.27
Episode length: 178.55 +/- 32.27
Eval num_timesteps=17000, episode_reward=-192.05 +/- 38.49
Episode length: 193.05 +/- 38.49
Eval num_timesteps=17500, episode_reward=-456.90 +/- 91.16
Episode length: 457.10 +/- 90.78
Eval num_timesteps=18000, episode_reward=-385.18 +/- 180.08
Episode length: 385.47 +/- 179.63
Eval num_timesteps=18500, episode_reward=-353.48 +/- 195.70
Episode length: 353.84 +/- 195.23
Eval num_timesteps=19000, episode_reward=-280.21 +/- 197.43
Episode length: 280.77 +/- 196.94
Eval num_timesteps=19500, episode_reward=-319.38 +/- 198.34
Episode length: 319.84 +/- 197.85
Eval num_timesteps=20000, episode_reward=-231.12 +/- 187.92
Episode length: 231.81 +/- 187.47
Eval num_timesteps=20500, episode_reward=-195.10 +/- 169.46
Episode length: 195.87 +/- 169.05
Eval num_timesteps=21000, episode_reward=-205.98 +/- 172.83
Episode length: 206.73 +/- 172.41
Eval num_timesteps=21500, episode_reward=-216.73 +/- 185.69
Episode length: 217.44 +/- 185.25
Eval num_timesteps=22000, episode_reward=-215.25 +/- 185.78
Episode length: 215.97 +/- 185.35
Eval num_timesteps=22500, episode_reward=-166.32 +/- 142.09
Episode length: 167.18 +/- 141.76
Eval num_timesteps=23000, episode_reward=-211.45 +/- 185.99
Episode length: 212.17 +/- 185.56
Eval num_timesteps=23500, episode_reward=-235.39 +/- 192.53
Episode length: 236.06 +/- 192.07
Eval num_timesteps=24000, episode_reward=-219.31 +/- 190.50
Episode length: 220.01 +/- 190.06
Eval num_timesteps=24500, episode_reward=-160.07 +/- 151.37
Episode length: 160.91 +/- 151.01
New best mean reward!
Eval num_timesteps=25000, episode_reward=-173.79 +/- 162.61
Episode length: 174.60 +/- 162.23
Eval num_timesteps=25500, episode_reward=-209.59 +/- 186.59
Episode length: 210.30 +/- 186.13
Eval num_timesteps=26000, episode_reward=-129.69 +/- 117.42
Episode length: 130.61 +/- 117.17
New best mean reward!
Eval num_timesteps=26500, episode_reward=-135.69 +/- 130.21
Episode length: 136.58 +/- 129.90
Eval num_timesteps=27000, episode_reward=-153.41 +/- 134.24
Episode length: 154.29 +/- 133.93
Eval num_timesteps=27500, episode_reward=-108.61 +/- 81.87
Episode length: 109.59 +/- 81.78
New best mean reward!
Eval num_timesteps=28000, episode_reward=-126.27 +/- 100.50
Episode length: 127.22 +/- 100.31
Eval num_timesteps=28500, episode_reward=-114.34 +/- 95.31
Episode length: 115.29 +/- 95.11
Eval num_timesteps=29000, episode_reward=-106.32 +/- 76.39
Episode length: 107.30 +/- 76.28
New best mean reward!
Eval num_timesteps=29500, episode_reward=-110.25 +/- 72.48
Episode length: 111.23 +/- 72.38
Eval num_timesteps=30000, episode_reward=-102.64 +/- 63.19
Episode length: 103.62 +/- 63.06
New best mean reward!
Eval num_timesteps=30500, episode_reward=-95.49 +/- 51.76
Episode length: 96.48 +/- 51.68
New best mean reward!
FINISHED IN 570.7132504199981 s


starting seed  10082 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-171.68 +/- 39.37
Episode length: 172.68 +/- 39.37
New best mean reward!
Eval num_timesteps=1500, episode_reward=-411.23 +/- 123.92
Episode length: 411.59 +/- 123.46
Eval num_timesteps=2000, episode_reward=-443.96 +/- 116.20
Episode length: 444.15 +/- 115.81
Eval num_timesteps=2500, episode_reward=-422.44 +/- 128.83
Episode length: 422.72 +/- 128.39
Eval num_timesteps=3000, episode_reward=-496.99 +/- 29.95
Episode length: 497.00 +/- 29.85
Eval num_timesteps=3500, episode_reward=-494.27 +/- 40.12
Episode length: 494.29 +/- 39.98
Eval num_timesteps=4000, episode_reward=-491.52 +/- 48.26
Episode length: 491.55 +/- 48.09
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-466.65 +/- 95.21
Episode length: 466.76 +/- 94.90
Eval num_timesteps=7000, episode_reward=-442.63 +/- 119.65
Episode length: 442.82 +/- 119.26
Eval num_timesteps=7500, episode_reward=-388.67 +/- 155.28
Episode length: 389.02 +/- 154.82
Eval num_timesteps=8000, episode_reward=-414.04 +/- 138.79
Episode length: 414.33 +/- 138.35
Eval num_timesteps=8500, episode_reward=-390.74 +/- 153.92
Episode length: 391.09 +/- 153.46
Eval num_timesteps=9000, episode_reward=-341.97 +/- 165.44
Episode length: 342.46 +/- 164.95
Eval num_timesteps=9500, episode_reward=-376.20 +/- 157.55
Episode length: 376.59 +/- 157.07
Eval num_timesteps=10000, episode_reward=-439.96 +/- 121.82
Episode length: 440.18 +/- 121.43
Eval num_timesteps=10500, episode_reward=-447.11 +/- 119.69
Episode length: 447.29 +/- 119.33
Eval num_timesteps=11000, episode_reward=-290.39 +/- 165.23
Episode length: 291.01 +/- 164.75
Eval num_timesteps=11500, episode_reward=-316.44 +/- 161.16
Episode length: 317.02 +/- 160.68
Eval num_timesteps=12000, episode_reward=-292.85 +/- 165.34
Episode length: 293.48 +/- 164.88
Eval num_timesteps=12500, episode_reward=-325.41 +/- 172.02
Episode length: 325.92 +/- 171.52
Eval num_timesteps=13000, episode_reward=-268.99 +/- 164.46
Episode length: 269.66 +/- 164.00
Eval num_timesteps=13500, episode_reward=-276.26 +/- 163.64
Episode length: 276.92 +/- 163.18
Eval num_timesteps=14000, episode_reward=-340.63 +/- 176.80
Episode length: 341.08 +/- 176.30
Eval num_timesteps=14500, episode_reward=-297.75 +/- 177.82
Episode length: 298.34 +/- 177.35
Eval num_timesteps=15000, episode_reward=-277.75 +/- 196.32
Episode length: 278.32 +/- 195.83
Eval num_timesteps=15500, episode_reward=-352.74 +/- 192.10
Episode length: 353.12 +/- 191.62
Eval num_timesteps=16000, episode_reward=-311.10 +/- 197.33
Episode length: 311.58 +/- 196.84
Eval num_timesteps=16500, episode_reward=-297.03 +/- 195.01
Episode length: 297.56 +/- 194.52
Eval num_timesteps=17000, episode_reward=-176.09 +/- 162.36
Episode length: 176.90 +/- 161.98
Eval num_timesteps=17500, episode_reward=-239.93 +/- 190.57
Episode length: 240.59 +/- 190.11
Eval num_timesteps=18000, episode_reward=-182.45 +/- 172.49
Episode length: 183.23 +/- 172.08
Eval num_timesteps=18500, episode_reward=-185.64 +/- 169.03
Episode length: 186.42 +/- 168.63
Eval num_timesteps=19000, episode_reward=-156.76 +/- 152.18
Episode length: 157.60 +/- 151.82
New best mean reward!
Eval num_timesteps=19500, episode_reward=-162.75 +/- 148.67
Episode length: 163.61 +/- 148.35
Eval num_timesteps=20000, episode_reward=-147.81 +/- 136.47
Episode length: 148.69 +/- 136.17
New best mean reward!
Eval num_timesteps=20500, episode_reward=-128.88 +/- 110.42
Episode length: 129.81 +/- 110.18
New best mean reward!
Eval num_timesteps=21000, episode_reward=-148.73 +/- 133.89
Episode length: 149.61 +/- 133.57
Eval num_timesteps=21500, episode_reward=-141.53 +/- 130.90
Episode length: 142.42 +/- 130.60
Eval num_timesteps=22000, episode_reward=-148.69 +/- 135.26
Episode length: 149.58 +/- 134.97
Eval num_timesteps=22500, episode_reward=-117.66 +/- 96.68
Episode length: 118.61 +/- 96.48
New best mean reward!
Eval num_timesteps=23000, episode_reward=-99.44 +/- 36.37
Episode length: 100.44 +/- 36.37
New best mean reward!
FINISHED IN 458.8785847569816 s


starting seed  10083 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-468.40 +/- 101.41
Episode length: 468.49 +/- 101.13
New best mean reward!
Eval num_timesteps=11500, episode_reward=-369.52 +/- 190.39
Episode length: 369.84 +/- 189.92
New best mean reward!
Eval num_timesteps=12000, episode_reward=-352.68 +/- 193.29
Episode length: 353.06 +/- 192.82
New best mean reward!
Eval num_timesteps=12500, episode_reward=-292.42 +/- 200.67
Episode length: 292.94 +/- 200.17
New best mean reward!
Eval num_timesteps=13000, episode_reward=-139.05 +/- 134.83
Episode length: 139.94 +/- 134.54
New best mean reward!
Eval num_timesteps=13500, episode_reward=-206.83 +/- 184.06
Episode length: 207.55 +/- 183.61
Eval num_timesteps=14000, episode_reward=-242.62 +/- 198.44
Episode length: 243.25 +/- 197.96
Eval num_timesteps=14500, episode_reward=-103.77 +/- 75.25
Episode length: 104.74 +/- 75.09
New best mean reward!
Eval num_timesteps=15000, episode_reward=-99.70 +/- 62.43
Episode length: 100.68 +/- 62.30
New best mean reward!
FINISHED IN 350.0521739899996 s


starting seed  10084 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-421.10 +/- 144.81
Episode length: 421.33 +/- 144.39
New best mean reward!
Eval num_timesteps=11000, episode_reward=-493.16 +/- 47.89
Episode length: 493.18 +/- 47.75
Eval num_timesteps=11500, episode_reward=-469.66 +/- 102.99
Episode length: 469.74 +/- 102.72
Eval num_timesteps=12000, episode_reward=-305.93 +/- 195.75
Episode length: 306.43 +/- 195.25
New best mean reward!
Eval num_timesteps=12500, episode_reward=-221.96 +/- 174.67
Episode length: 222.70 +/- 174.26
New best mean reward!
Eval num_timesteps=13000, episode_reward=-216.46 +/- 186.36
Episode length: 217.17 +/- 185.92
New best mean reward!
Eval num_timesteps=13500, episode_reward=-216.60 +/- 179.99
Episode length: 217.34 +/- 179.58
Eval num_timesteps=14000, episode_reward=-162.31 +/- 115.01
Episode length: 163.21 +/- 114.71
New best mean reward!
Eval num_timesteps=14500, episode_reward=-188.70 +/- 96.58
Episode length: 189.64 +/- 96.38
Eval num_timesteps=15000, episode_reward=-159.81 +/- 30.58
Episode length: 160.81 +/- 30.58
New best mean reward!
Eval num_timesteps=15500, episode_reward=-180.17 +/- 93.56
Episode length: 181.10 +/- 93.32
Eval num_timesteps=16000, episode_reward=-174.59 +/- 66.34
Episode length: 175.56 +/- 66.19
Eval num_timesteps=16500, episode_reward=-166.56 +/- 64.41
Episode length: 167.53 +/- 64.25
Eval num_timesteps=17000, episode_reward=-127.90 +/- 24.19
Episode length: 128.90 +/- 24.19
New best mean reward!
Eval num_timesteps=17500, episode_reward=-143.80 +/- 44.61
Episode length: 144.79 +/- 44.53
Eval num_timesteps=18000, episode_reward=-98.94 +/- 47.46
Episode length: 99.93 +/- 47.38
New best mean reward!
FINISHED IN 392.68128311197506 s


starting seed  10085 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-483.97 +/- 78.54
Episode length: 484.01 +/- 78.34
New best mean reward!
Eval num_timesteps=10000, episode_reward=-294.51 +/- 194.84
Episode length: 295.05 +/- 194.36
New best mean reward!
Eval num_timesteps=10500, episode_reward=-234.23 +/- 188.55
Episode length: 234.90 +/- 188.09
New best mean reward!
Eval num_timesteps=11000, episode_reward=-214.24 +/- 177.03
Episode length: 214.97 +/- 176.59
New best mean reward!
Eval num_timesteps=11500, episode_reward=-176.66 +/- 153.26
Episode length: 177.49 +/- 152.90
New best mean reward!
Eval num_timesteps=12000, episode_reward=-160.28 +/- 135.89
Episode length: 161.15 +/- 135.56
New best mean reward!
Eval num_timesteps=12500, episode_reward=-206.10 +/- 171.29
Episode length: 206.87 +/- 170.90
Eval num_timesteps=13000, episode_reward=-137.40 +/- 107.08
Episode length: 138.34 +/- 106.88
New best mean reward!
Eval num_timesteps=13500, episode_reward=-203.71 +/- 165.71
Episode length: 204.49 +/- 165.31
Eval num_timesteps=14000, episode_reward=-193.92 +/- 165.16
Episode length: 194.70 +/- 164.75
Eval num_timesteps=14500, episode_reward=-195.64 +/- 164.30
Episode length: 196.42 +/- 163.89
Eval num_timesteps=15000, episode_reward=-193.53 +/- 165.04
Episode length: 194.33 +/- 164.67
Eval num_timesteps=15500, episode_reward=-142.25 +/- 118.50
Episode length: 143.16 +/- 118.23
Eval num_timesteps=16000, episode_reward=-142.03 +/- 116.17
Episode length: 142.95 +/- 115.92
Eval num_timesteps=16500, episode_reward=-114.68 +/- 82.00
Episode length: 115.65 +/- 81.86
New best mean reward!
Eval num_timesteps=17000, episode_reward=-118.30 +/- 89.76
Episode length: 119.26 +/- 89.59
Eval num_timesteps=17500, episode_reward=-107.52 +/- 66.38
Episode length: 108.50 +/- 66.27
New best mean reward!
Eval num_timesteps=18000, episode_reward=-115.16 +/- 72.17
Episode length: 116.14 +/- 72.06
Eval num_timesteps=18500, episode_reward=-108.46 +/- 75.31
Episode length: 109.43 +/- 75.15
Eval num_timesteps=19000, episode_reward=-101.42 +/- 32.48
Episode length: 102.42 +/- 32.48
New best mean reward!
Eval num_timesteps=19500, episode_reward=-106.57 +/- 37.27
Episode length: 107.57 +/- 37.27
Eval num_timesteps=20000, episode_reward=-101.66 +/- 39.20
Episode length: 102.66 +/- 39.20
Eval num_timesteps=20500, episode_reward=-102.41 +/- 37.83
Episode length: 103.41 +/- 37.83
Eval num_timesteps=21000, episode_reward=-112.93 +/- 57.50
Episode length: 113.92 +/- 57.44
Eval num_timesteps=21500, episode_reward=-113.56 +/- 61.35
Episode length: 114.55 +/- 61.29
Eval num_timesteps=22000, episode_reward=-98.33 +/- 45.92
Episode length: 99.33 +/- 45.92
New best mean reward!
FINISHED IN 399.19762127898866 s


starting seed  10086 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-392.69 +/- 172.95
Episode length: 392.97 +/- 172.51
New best mean reward!
Eval num_timesteps=12500, episode_reward=-279.20 +/- 194.73
Episode length: 279.77 +/- 194.24
New best mean reward!
Eval num_timesteps=13000, episode_reward=-144.59 +/- 130.87
Episode length: 145.48 +/- 130.58
New best mean reward!
Eval num_timesteps=13500, episode_reward=-94.26 +/- 48.75
Episode length: 95.25 +/- 48.67
New best mean reward!
FINISHED IN 278.00739434798015 s


starting seed  10087 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-236.32 +/- 191.05
Episode length: 236.98 +/- 190.58
New best mean reward!
Eval num_timesteps=10000, episode_reward=-142.72 +/- 127.02
Episode length: 143.61 +/- 126.71
New best mean reward!
Eval num_timesteps=10500, episode_reward=-105.07 +/- 63.67
Episode length: 106.05 +/- 63.54
New best mean reward!
Eval num_timesteps=11000, episode_reward=-116.62 +/- 91.81
Episode length: 117.58 +/- 91.64
Eval num_timesteps=11500, episode_reward=-119.92 +/- 91.92
Episode length: 120.87 +/- 91.71
Eval num_timesteps=12000, episode_reward=-136.68 +/- 111.82
Episode length: 137.60 +/- 111.56
Eval num_timesteps=12500, episode_reward=-142.89 +/- 124.99
Episode length: 143.79 +/- 124.70
Eval num_timesteps=13000, episode_reward=-148.13 +/- 133.33
Episode length: 149.01 +/- 133.01
Eval num_timesteps=13500, episode_reward=-161.52 +/- 148.06
Episode length: 162.37 +/- 147.71
Eval num_timesteps=14000, episode_reward=-125.25 +/- 107.50
Episode length: 126.18 +/- 107.26
Eval num_timesteps=14500, episode_reward=-178.81 +/- 163.12
Episode length: 179.62 +/- 162.74
Eval num_timesteps=15000, episode_reward=-149.60 +/- 134.65
Episode length: 150.49 +/- 134.37
Eval num_timesteps=15500, episode_reward=-147.05 +/- 132.50
Episode length: 147.93 +/- 132.18
Eval num_timesteps=16000, episode_reward=-104.08 +/- 72.78
Episode length: 105.05 +/- 72.62
New best mean reward!
Eval num_timesteps=16500, episode_reward=-113.64 +/- 84.97
Episode length: 114.60 +/- 84.79
Eval num_timesteps=17000, episode_reward=-105.18 +/- 57.89
Episode length: 106.17 +/- 57.82
Eval num_timesteps=17500, episode_reward=-101.41 +/- 68.05
Episode length: 102.40 +/- 67.99
New best mean reward!
Eval num_timesteps=18000, episode_reward=-94.31 +/- 45.47
Episode length: 95.30 +/- 45.38
New best mean reward!
FINISHED IN 244.40443123900332 s


starting seed  10088 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-461.23 +/- 105.24
Episode length: 461.35 +/- 104.92
New best mean reward!
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-438.54 +/- 123.24
Episode length: 438.75 +/- 122.84
New best mean reward!
Eval num_timesteps=6500, episode_reward=-486.96 +/- 64.13
Episode length: 487.00 +/- 63.94
Eval num_timesteps=7000, episode_reward=-394.22 +/- 174.09
Episode length: 394.49 +/- 173.64
New best mean reward!
Eval num_timesteps=7500, episode_reward=-275.04 +/- 188.28
Episode length: 275.64 +/- 187.80
New best mean reward!
Eval num_timesteps=8000, episode_reward=-365.85 +/- 183.69
Episode length: 366.20 +/- 183.21
Eval num_timesteps=8500, episode_reward=-345.26 +/- 193.86
Episode length: 345.65 +/- 193.37
Eval num_timesteps=9000, episode_reward=-100.43 +/- 78.66
Episode length: 101.40 +/- 78.51
New best mean reward!
Eval num_timesteps=9500, episode_reward=-101.35 +/- 67.30
Episode length: 102.33 +/- 67.18
Eval num_timesteps=10000, episode_reward=-124.93 +/- 113.70
Episode length: 125.85 +/- 113.44
Eval num_timesteps=10500, episode_reward=-143.02 +/- 130.84
Episode length: 143.92 +/- 130.56
Eval num_timesteps=11000, episode_reward=-168.37 +/- 162.31
Episode length: 169.19 +/- 161.94
Eval num_timesteps=11500, episode_reward=-151.51 +/- 144.78
Episode length: 152.37 +/- 144.44
Eval num_timesteps=12000, episode_reward=-168.04 +/- 158.61
Episode length: 168.87 +/- 158.25
Eval num_timesteps=12500, episode_reward=-129.82 +/- 118.61
Episode length: 130.73 +/- 118.33
Eval num_timesteps=13000, episode_reward=-128.65 +/- 109.90
Episode length: 129.59 +/- 109.70
Eval num_timesteps=13500, episode_reward=-106.11 +/- 69.92
Episode length: 107.09 +/- 69.81
Eval num_timesteps=14000, episode_reward=-102.92 +/- 74.57
Episode length: 103.89 +/- 74.41
Eval num_timesteps=14500, episode_reward=-95.18 +/- 45.90
Episode length: 96.17 +/- 45.82
New best mean reward!
FINISHED IN 194.27334427903406 s


starting seed  10089 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-480.29 +/- 73.26
Episode length: 480.36 +/- 73.01
New best mean reward!
Eval num_timesteps=9500, episode_reward=-480.90 +/- 75.93
Episode length: 480.96 +/- 75.69
Eval num_timesteps=10000, episode_reward=-280.01 +/- 148.22
Episode length: 280.72 +/- 147.78
New best mean reward!
Eval num_timesteps=10500, episode_reward=-234.11 +/- 119.00
Episode length: 234.97 +/- 118.69
New best mean reward!
Eval num_timesteps=11000, episode_reward=-198.30 +/- 85.68
Episode length: 199.24 +/- 85.47
New best mean reward!
Eval num_timesteps=11500, episode_reward=-202.77 +/- 80.09
Episode length: 203.72 +/- 79.90
Eval num_timesteps=12000, episode_reward=-213.16 +/- 96.71
Episode length: 214.07 +/- 96.44
Eval num_timesteps=12500, episode_reward=-190.76 +/- 63.21
Episode length: 191.73 +/- 63.07
New best mean reward!
Eval num_timesteps=13000, episode_reward=-209.11 +/- 88.75
Episode length: 210.04 +/- 88.52
Eval num_timesteps=13500, episode_reward=-183.48 +/- 48.38
Episode length: 184.47 +/- 48.31
New best mean reward!
Eval num_timesteps=14000, episode_reward=-179.27 +/- 46.89
Episode length: 180.26 +/- 46.83
New best mean reward!
Eval num_timesteps=14500, episode_reward=-182.16 +/- 46.39
Episode length: 183.15 +/- 46.32
Eval num_timesteps=15000, episode_reward=-180.99 +/- 46.83
Episode length: 181.98 +/- 46.76
Eval num_timesteps=15500, episode_reward=-207.96 +/- 85.67
Episode length: 208.90 +/- 85.46
Eval num_timesteps=16000, episode_reward=-185.28 +/- 58.53
Episode length: 186.26 +/- 58.42
Eval num_timesteps=16500, episode_reward=-207.62 +/- 84.09
Episode length: 208.56 +/- 83.88
Eval num_timesteps=17000, episode_reward=-179.57 +/- 70.36
Episode length: 180.54 +/- 70.22
Eval num_timesteps=17500, episode_reward=-96.78 +/- 33.84
Episode length: 97.78 +/- 33.84
New best mean reward!
FINISHED IN 285.4698631489882 s


starting seed  10090 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-443.53 +/- 131.22
Episode length: 443.69 +/- 130.85
New best mean reward!
Eval num_timesteps=10500, episode_reward=-82.67 +/- 26.15
Episode length: 83.67 +/- 26.15
New best mean reward!
FINISHED IN 281.6113733489765 s


starting seed  10091 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-240.93 +/- 150.04
Episode length: 241.70 +/- 149.64
New best mean reward!
Eval num_timesteps=1000, episode_reward=-367.71 +/- 140.89
Episode length: 368.22 +/- 140.43
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-290.62 +/- 133.91
Episode length: 291.38 +/- 133.53
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-156.42 +/- 45.81
Episode length: 157.42 +/- 45.81
New best mean reward!
Eval num_timesteps=6500, episode_reward=-342.88 +/- 166.39
Episode length: 343.36 +/- 165.90
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-434.92 +/- 144.11
Episode length: 435.09 +/- 143.73
Eval num_timesteps=8000, episode_reward=-461.06 +/- 116.96
Episode length: 461.16 +/- 116.66
Eval num_timesteps=8500, episode_reward=-345.44 +/- 193.55
Episode length: 345.83 +/- 193.07
Eval num_timesteps=9000, episode_reward=-167.62 +/- 156.62
Episode length: 168.44 +/- 156.24
Eval num_timesteps=9500, episode_reward=-158.35 +/- 144.38
Episode length: 159.20 +/- 144.03
Eval num_timesteps=10000, episode_reward=-191.98 +/- 164.58
Episode length: 192.77 +/- 164.19
Eval num_timesteps=10500, episode_reward=-341.64 +/- 193.23
Episode length: 342.05 +/- 192.75
Eval num_timesteps=11000, episode_reward=-232.74 +/- 188.50
Episode length: 233.41 +/- 188.04
Eval num_timesteps=11500, episode_reward=-175.58 +/- 152.45
Episode length: 176.41 +/- 152.09
Eval num_timesteps=12000, episode_reward=-156.42 +/- 146.07
Episode length: 157.27 +/- 145.72
Eval num_timesteps=12500, episode_reward=-160.07 +/- 153.65
Episode length: 160.91 +/- 153.30
Eval num_timesteps=13000, episode_reward=-219.56 +/- 190.97
Episode length: 220.25 +/- 190.52
Eval num_timesteps=13500, episode_reward=-114.81 +/- 101.86
Episode length: 115.75 +/- 101.63
New best mean reward!
Eval num_timesteps=14000, episode_reward=-147.23 +/- 139.20
Episode length: 148.10 +/- 138.87
Eval num_timesteps=14500, episode_reward=-139.55 +/- 141.96
Episode length: 140.42 +/- 141.63
Eval num_timesteps=15000, episode_reward=-195.53 +/- 182.84
Episode length: 196.27 +/- 182.41
Eval num_timesteps=15500, episode_reward=-144.30 +/- 136.95
Episode length: 145.18 +/- 136.64
Eval num_timesteps=16000, episode_reward=-131.81 +/- 130.94
Episode length: 132.70 +/- 130.63
Eval num_timesteps=16500, episode_reward=-105.94 +/- 85.05
Episode length: 106.91 +/- 84.91
New best mean reward!
Eval num_timesteps=17000, episode_reward=-95.73 +/- 68.60
Episode length: 96.71 +/- 68.49
New best mean reward!
FINISHED IN 238.37500125099905 s


starting seed  10092 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-347.63 +/- 91.15
Episode length: 348.46 +/- 90.87
New best mean reward!
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-458.81 +/- 67.32
Episode length: 459.14 +/- 66.91
Eval num_timesteps=6000, episode_reward=-252.60 +/- 95.62
Episode length: 253.50 +/- 95.36
New best mean reward!
Eval num_timesteps=6500, episode_reward=-206.60 +/- 74.42
Episode length: 207.55 +/- 74.22
New best mean reward!
Eval num_timesteps=7000, episode_reward=-422.63 +/- 72.58
Episode length: 423.33 +/- 72.26
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-121.49 +/- 71.69
Episode length: 122.46 +/- 71.53
New best mean reward!
Eval num_timesteps=9000, episode_reward=-98.34 +/- 35.70
Episode length: 99.34 +/- 35.70
New best mean reward!
FINISHED IN 211.06714637804544 s


starting seed  10093 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-111.12 +/- 52.52
Episode length: 112.11 +/- 52.45
New best mean reward!
Eval num_timesteps=8000, episode_reward=-139.51 +/- 128.51
Episode length: 140.41 +/- 128.23
Eval num_timesteps=8500, episode_reward=-120.17 +/- 50.79
Episode length: 121.17 +/- 50.79
Eval num_timesteps=9000, episode_reward=-94.63 +/- 29.50
Episode length: 95.63 +/- 29.50
New best mean reward!
FINISHED IN 172.08032496698434 s


starting seed  10094 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-484.16 +/- 77.64
Episode length: 484.20 +/- 77.44
New best mean reward!
Eval num_timesteps=13500, episode_reward=-473.19 +/- 98.39
Episode length: 473.26 +/- 98.13
New best mean reward!
Eval num_timesteps=14000, episode_reward=-436.08 +/- 143.44
Episode length: 436.25 +/- 143.07
New best mean reward!
Eval num_timesteps=14500, episode_reward=-371.21 +/- 185.38
Episode length: 371.54 +/- 184.92
New best mean reward!
Eval num_timesteps=15000, episode_reward=-343.85 +/- 193.31
Episode length: 344.26 +/- 192.83
New best mean reward!
Eval num_timesteps=15500, episode_reward=-270.61 +/- 199.56
Episode length: 271.19 +/- 199.07
New best mean reward!
Eval num_timesteps=16000, episode_reward=-285.52 +/- 203.44
Episode length: 286.06 +/- 202.95
Eval num_timesteps=16500, episode_reward=-285.98 +/- 203.66
Episode length: 286.52 +/- 203.18
Eval num_timesteps=17000, episode_reward=-315.52 +/- 200.20
Episode length: 315.99 +/- 199.71
Eval num_timesteps=17500, episode_reward=-336.00 +/- 198.45
Episode length: 336.41 +/- 197.96
Eval num_timesteps=18000, episode_reward=-371.55 +/- 188.45
Episode length: 371.87 +/- 187.98
Eval num_timesteps=18500, episode_reward=-338.98 +/- 194.79
Episode length: 339.39 +/- 194.30
Eval num_timesteps=19000, episode_reward=-324.20 +/- 199.52
Episode length: 324.65 +/- 199.03
Eval num_timesteps=19500, episode_reward=-300.62 +/- 203.36
Episode length: 301.12 +/- 202.87
Eval num_timesteps=20000, episode_reward=-290.07 +/- 203.80
Episode length: 290.59 +/- 203.31
Eval num_timesteps=20500, episode_reward=-216.69 +/- 191.09
Episode length: 217.38 +/- 190.63
New best mean reward!
Eval num_timesteps=21000, episode_reward=-230.15 +/- 194.34
Episode length: 230.82 +/- 193.89
Eval num_timesteps=21500, episode_reward=-159.30 +/- 156.59
Episode length: 160.13 +/- 156.22
New best mean reward!
Eval num_timesteps=22000, episode_reward=-212.32 +/- 185.60
Episode length: 213.03 +/- 185.15
Eval num_timesteps=22500, episode_reward=-220.45 +/- 193.24
Episode length: 221.13 +/- 192.78
Eval num_timesteps=23000, episode_reward=-144.34 +/- 138.05
Episode length: 145.22 +/- 137.74
New best mean reward!
Eval num_timesteps=23500, episode_reward=-150.26 +/- 146.77
Episode length: 151.12 +/- 146.43
Eval num_timesteps=24000, episode_reward=-162.42 +/- 156.00
Episode length: 163.25 +/- 155.63
Eval num_timesteps=24500, episode_reward=-174.54 +/- 164.37
Episode length: 175.34 +/- 163.98
Eval num_timesteps=25000, episode_reward=-171.67 +/- 162.25
Episode length: 172.48 +/- 161.87
Eval num_timesteps=25500, episode_reward=-174.36 +/- 161.49
Episode length: 175.17 +/- 161.11
Eval num_timesteps=26000, episode_reward=-178.70 +/- 161.81
Episode length: 179.54 +/- 161.49
Eval num_timesteps=26500, episode_reward=-175.29 +/- 166.99
Episode length: 176.09 +/- 166.60
Eval num_timesteps=27000, episode_reward=-145.26 +/- 143.95
Episode length: 146.13 +/- 143.63
Eval num_timesteps=27500, episode_reward=-163.23 +/- 159.33
Episode length: 164.05 +/- 158.95
Eval num_timesteps=28000, episode_reward=-174.10 +/- 164.31
Episode length: 174.91 +/- 163.93
Eval num_timesteps=28500, episode_reward=-142.44 +/- 135.20
Episode length: 143.32 +/- 134.88
New best mean reward!
Eval num_timesteps=29000, episode_reward=-127.68 +/- 126.12
Episode length: 128.58 +/- 125.82
New best mean reward!
Eval num_timesteps=29500, episode_reward=-123.24 +/- 113.62
Episode length: 124.17 +/- 113.39
New best mean reward!
Eval num_timesteps=30000, episode_reward=-145.48 +/- 141.50
Episode length: 146.35 +/- 141.17
Eval num_timesteps=30500, episode_reward=-115.34 +/- 100.43
Episode length: 116.28 +/- 100.20
New best mean reward!
Eval num_timesteps=31000, episode_reward=-141.11 +/- 134.02
Episode length: 141.99 +/- 133.70
Eval num_timesteps=31500, episode_reward=-124.23 +/- 101.91
Episode length: 125.17 +/- 101.69
Eval num_timesteps=32000, episode_reward=-119.80 +/- 100.93
Episode length: 120.74 +/- 100.70
Eval num_timesteps=32500, episode_reward=-126.16 +/- 113.43
Episode length: 127.09 +/- 113.20
Eval num_timesteps=33000, episode_reward=-121.71 +/- 106.37
Episode length: 122.65 +/- 106.15
Eval num_timesteps=33500, episode_reward=-114.48 +/- 104.10
Episode length: 115.42 +/- 103.88
New best mean reward!
Eval num_timesteps=34000, episode_reward=-132.45 +/- 131.58
Episode length: 133.34 +/- 131.27
Eval num_timesteps=34500, episode_reward=-114.95 +/- 95.77
Episode length: 115.90 +/- 95.57
Eval num_timesteps=35000, episode_reward=-117.05 +/- 99.25
Episode length: 118.00 +/- 99.05
Eval num_timesteps=35500, episode_reward=-122.19 +/- 105.64
Episode length: 123.13 +/- 105.43
Eval num_timesteps=36000, episode_reward=-102.82 +/- 84.11
Episode length: 103.78 +/- 83.92
New best mean reward!
Eval num_timesteps=36500, episode_reward=-107.93 +/- 85.18
Episode length: 108.89 +/- 85.00
Eval num_timesteps=37000, episode_reward=-108.26 +/- 81.67
Episode length: 109.23 +/- 81.53
Eval num_timesteps=37500, episode_reward=-120.98 +/- 112.34
Episode length: 121.91 +/- 112.10
Eval num_timesteps=38000, episode_reward=-101.39 +/- 75.25
Episode length: 102.36 +/- 75.09
New best mean reward!
Eval num_timesteps=38500, episode_reward=-116.22 +/- 107.71
Episode length: 117.15 +/- 107.46
Eval num_timesteps=39000, episode_reward=-106.34 +/- 87.25
Episode length: 107.30 +/- 87.07
Eval num_timesteps=39500, episode_reward=-107.04 +/- 75.48
Episode length: 108.01 +/- 75.32
Eval num_timesteps=40000, episode_reward=-108.65 +/- 80.09
Episode length: 109.62 +/- 79.95
Eval num_timesteps=40500, episode_reward=-103.21 +/- 83.88
Episode length: 104.17 +/- 83.69
Eval num_timesteps=41000, episode_reward=-112.95 +/- 100.16
Episode length: 113.89 +/- 99.93
Eval num_timesteps=41500, episode_reward=-113.33 +/- 97.54
Episode length: 114.28 +/- 97.34
Eval num_timesteps=42000, episode_reward=-100.06 +/- 83.96
Episode length: 101.02 +/- 83.77
New best mean reward!
Eval num_timesteps=42500, episode_reward=-95.69 +/- 60.41
Episode length: 96.68 +/- 60.34
New best mean reward!
FINISHED IN 481.7069579119561 s


starting seed  10095 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-493.28 +/- 47.06
Episode length: 493.30 +/- 46.92
New best mean reward!
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-251.08 +/- 150.57
Episode length: 251.83 +/- 150.16
New best mean reward!
Eval num_timesteps=3500, episode_reward=-119.85 +/- 39.20
Episode length: 120.85 +/- 39.20
New best mean reward!
Eval num_timesteps=4000, episode_reward=-165.87 +/- 51.44
Episode length: 166.86 +/- 51.37
Eval num_timesteps=4500, episode_reward=-220.39 +/- 88.56
Episode length: 221.36 +/- 88.47
Eval num_timesteps=5000, episode_reward=-236.80 +/- 142.07
Episode length: 237.58 +/- 141.66
Eval num_timesteps=5500, episode_reward=-168.62 +/- 46.30
Episode length: 169.61 +/- 46.23
Eval num_timesteps=6000, episode_reward=-190.92 +/- 81.15
Episode length: 191.87 +/- 80.96
Eval num_timesteps=6500, episode_reward=-199.20 +/- 103.82
Episode length: 200.10 +/- 103.53
Eval num_timesteps=7000, episode_reward=-225.05 +/- 135.65
Episode length: 225.86 +/- 135.27
Eval num_timesteps=7500, episode_reward=-221.33 +/- 125.97
Episode length: 222.17 +/- 125.62
Eval num_timesteps=8000, episode_reward=-311.54 +/- 178.03
Episode length: 312.07 +/- 177.54
Eval num_timesteps=8500, episode_reward=-188.19 +/- 107.31
Episode length: 189.09 +/- 107.02
Eval num_timesteps=9000, episode_reward=-439.44 +/- 129.60
Episode length: 439.62 +/- 129.22
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-496.56 +/- 34.23
Episode length: 496.57 +/- 34.13
Eval num_timesteps=11500, episode_reward=-447.74 +/- 124.98
Episode length: 447.89 +/- 124.63
Eval num_timesteps=12000, episode_reward=-355.41 +/- 174.22
Episode length: 355.82 +/- 173.73
Eval num_timesteps=12500, episode_reward=-339.52 +/- 174.69
Episode length: 339.98 +/- 174.20
Eval num_timesteps=13000, episode_reward=-352.66 +/- 172.20
Episode length: 353.09 +/- 171.71
Eval num_timesteps=13500, episode_reward=-289.85 +/- 170.86
Episode length: 290.46 +/- 170.38
Eval num_timesteps=14000, episode_reward=-266.22 +/- 162.53
Episode length: 266.90 +/- 162.07
Eval num_timesteps=14500, episode_reward=-229.48 +/- 147.41
Episode length: 230.26 +/- 147.00
Eval num_timesteps=15000, episode_reward=-279.13 +/- 167.37
Episode length: 279.78 +/- 166.90
Eval num_timesteps=15500, episode_reward=-262.58 +/- 171.68
Episode length: 263.24 +/- 171.21
Eval num_timesteps=16000, episode_reward=-255.34 +/- 172.61
Episode length: 256.01 +/- 172.14
Eval num_timesteps=16500, episode_reward=-239.65 +/- 159.58
Episode length: 240.38 +/- 159.14
Eval num_timesteps=17000, episode_reward=-251.79 +/- 164.09
Episode length: 252.49 +/- 163.63
Eval num_timesteps=17500, episode_reward=-247.22 +/- 159.91
Episode length: 247.94 +/- 159.47
Eval num_timesteps=18000, episode_reward=-293.79 +/- 174.30
Episode length: 294.38 +/- 173.82
Eval num_timesteps=18500, episode_reward=-286.35 +/- 169.37
Episode length: 286.97 +/- 168.89
Eval num_timesteps=19000, episode_reward=-277.83 +/- 173.44
Episode length: 278.46 +/- 172.96
Eval num_timesteps=19500, episode_reward=-252.62 +/- 168.54
Episode length: 253.32 +/- 168.10
Eval num_timesteps=20000, episode_reward=-247.94 +/- 174.20
Episode length: 248.63 +/- 173.75
Eval num_timesteps=20500, episode_reward=-162.33 +/- 98.44
Episode length: 163.26 +/- 98.20
Eval num_timesteps=21000, episode_reward=-203.06 +/- 141.98
Episode length: 203.88 +/- 141.61
Eval num_timesteps=21500, episode_reward=-188.72 +/- 124.42
Episode length: 189.60 +/- 124.12
Eval num_timesteps=22000, episode_reward=-155.90 +/- 99.22
Episode length: 156.83 +/- 98.98
Eval num_timesteps=22500, episode_reward=-166.91 +/- 97.26
Episode length: 167.84 +/- 97.02
Eval num_timesteps=23000, episode_reward=-191.40 +/- 135.47
Episode length: 192.25 +/- 135.13
Eval num_timesteps=23500, episode_reward=-180.54 +/- 131.23
Episode length: 181.40 +/- 130.89
Eval num_timesteps=24000, episode_reward=-115.19 +/- 99.06
Episode length: 116.13 +/- 98.82
New best mean reward!
Eval num_timesteps=24500, episode_reward=-148.29 +/- 95.66
Episode length: 149.23 +/- 95.44
Eval num_timesteps=25000, episode_reward=-115.81 +/- 60.21
Episode length: 116.79 +/- 60.08
Eval num_timesteps=25500, episode_reward=-119.14 +/- 108.37
Episode length: 120.07 +/- 108.12
Eval num_timesteps=26000, episode_reward=-106.27 +/- 92.25
Episode length: 107.23 +/- 92.08
New best mean reward!
Eval num_timesteps=26500, episode_reward=-125.57 +/- 126.85
Episode length: 126.47 +/- 126.56
Eval num_timesteps=27000, episode_reward=-112.29 +/- 93.02
Episode length: 113.24 +/- 92.81
Eval num_timesteps=27500, episode_reward=-101.29 +/- 74.70
Episode length: 102.27 +/- 74.59
New best mean reward!
Eval num_timesteps=28000, episode_reward=-107.16 +/- 96.55
Episode length: 108.11 +/- 96.34
Eval num_timesteps=28500, episode_reward=-87.45 +/- 44.88
Episode length: 88.44 +/- 44.79
New best mean reward!
FINISHED IN 319.1079369029612 s


starting seed  10096 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-427.20 +/- 152.14
Episode length: 427.39 +/- 151.75
New best mean reward!
Eval num_timesteps=10000, episode_reward=-362.97 +/- 191.10
Episode length: 363.31 +/- 190.62
New best mean reward!
Eval num_timesteps=10500, episode_reward=-436.08 +/- 146.77
Episode length: 436.24 +/- 146.40
Eval num_timesteps=11000, episode_reward=-317.68 +/- 201.89
Episode length: 318.13 +/- 201.40
New best mean reward!
Eval num_timesteps=11500, episode_reward=-257.13 +/- 205.48
Episode length: 257.72 +/- 205.00
New best mean reward!
Eval num_timesteps=12000, episode_reward=-281.64 +/- 202.64
Episode length: 282.18 +/- 202.15
Eval num_timesteps=12500, episode_reward=-307.69 +/- 205.47
Episode length: 308.16 +/- 204.97
Eval num_timesteps=13000, episode_reward=-339.91 +/- 204.74
Episode length: 340.29 +/- 204.26
Eval num_timesteps=13500, episode_reward=-285.38 +/- 206.66
Episode length: 285.90 +/- 206.16
Eval num_timesteps=14000, episode_reward=-245.31 +/- 193.57
Episode length: 245.96 +/- 193.11
New best mean reward!
Eval num_timesteps=14500, episode_reward=-230.63 +/- 194.85
Episode length: 231.29 +/- 194.38
New best mean reward!
Eval num_timesteps=15000, episode_reward=-232.61 +/- 195.43
Episode length: 233.27 +/- 194.97
Eval num_timesteps=15500, episode_reward=-157.07 +/- 151.63
Episode length: 157.91 +/- 151.27
New best mean reward!
Eval num_timesteps=16000, episode_reward=-164.14 +/- 158.84
Episode length: 164.96 +/- 158.46
Eval num_timesteps=16500, episode_reward=-242.32 +/- 200.55
Episode length: 242.95 +/- 200.08
Eval num_timesteps=17000, episode_reward=-212.12 +/- 187.33
Episode length: 212.83 +/- 186.88
Eval num_timesteps=17500, episode_reward=-150.33 +/- 140.44
Episode length: 151.20 +/- 140.12
New best mean reward!
Eval num_timesteps=18000, episode_reward=-141.69 +/- 137.71
Episode length: 142.57 +/- 137.40
New best mean reward!
Eval num_timesteps=18500, episode_reward=-143.88 +/- 141.62
Episode length: 144.75 +/- 141.29
Eval num_timesteps=19000, episode_reward=-152.65 +/- 144.24
Episode length: 153.51 +/- 143.90
Eval num_timesteps=19500, episode_reward=-163.80 +/- 160.55
Episode length: 164.62 +/- 160.18
Eval num_timesteps=20000, episode_reward=-128.21 +/- 120.63
Episode length: 129.12 +/- 120.35
New best mean reward!
Eval num_timesteps=20500, episode_reward=-115.00 +/- 109.00
Episode length: 115.93 +/- 108.75
New best mean reward!
Eval num_timesteps=21000, episode_reward=-130.13 +/- 120.96
Episode length: 131.05 +/- 120.71
Eval num_timesteps=21500, episode_reward=-106.10 +/- 90.08
Episode length: 107.06 +/- 89.90
New best mean reward!
Eval num_timesteps=22000, episode_reward=-97.21 +/- 68.36
Episode length: 98.19 +/- 68.25
New best mean reward!
FINISHED IN 301.2370766909444 s


starting seed  10097 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-86.32 +/- 18.51
Episode length: 87.32 +/- 18.51
New best mean reward!
FINISHED IN 167.1395616980153 s


starting seed  10098 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-146.31 +/- 55.06
Episode length: 147.30 +/- 54.99
New best mean reward!
Eval num_timesteps=2500, episode_reward=-483.64 +/- 65.06
Episode length: 483.71 +/- 64.82
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-456.56 +/- 110.18
Episode length: 456.70 +/- 109.84
Eval num_timesteps=11000, episode_reward=-408.07 +/- 146.99
Episode length: 408.36 +/- 146.55
Eval num_timesteps=11500, episode_reward=-410.39 +/- 151.55
Episode length: 410.65 +/- 151.11
Eval num_timesteps=12000, episode_reward=-344.80 +/- 202.64
Episode length: 345.17 +/- 202.16
Eval num_timesteps=12500, episode_reward=-293.42 +/- 205.46
Episode length: 293.93 +/- 204.97
Eval num_timesteps=13000, episode_reward=-297.12 +/- 207.24
Episode length: 297.61 +/- 206.74
Eval num_timesteps=13500, episode_reward=-316.36 +/- 200.76
Episode length: 316.82 +/- 200.27
Eval num_timesteps=14000, episode_reward=-282.26 +/- 205.47
Episode length: 282.79 +/- 204.97
Eval num_timesteps=14500, episode_reward=-261.61 +/- 200.72
Episode length: 262.20 +/- 200.23
Eval num_timesteps=15000, episode_reward=-219.52 +/- 193.03
Episode length: 220.20 +/- 192.56
Eval num_timesteps=15500, episode_reward=-178.65 +/- 172.11
Episode length: 179.43 +/- 171.70
Eval num_timesteps=16000, episode_reward=-156.70 +/- 156.98
Episode length: 157.54 +/- 156.63
Eval num_timesteps=16500, episode_reward=-185.72 +/- 174.09
Episode length: 186.49 +/- 173.67
Eval num_timesteps=17000, episode_reward=-177.58 +/- 141.88
Episode length: 178.42 +/- 141.51
Eval num_timesteps=17500, episode_reward=-129.23 +/- 115.24
Episode length: 130.15 +/- 114.98
New best mean reward!
Eval num_timesteps=18000, episode_reward=-129.60 +/- 125.28
Episode length: 130.50 +/- 124.98
Eval num_timesteps=18500, episode_reward=-109.80 +/- 102.74
Episode length: 110.74 +/- 102.51
New best mean reward!
Eval num_timesteps=19000, episode_reward=-114.06 +/- 109.02
Episode length: 114.99 +/- 108.77
Eval num_timesteps=19500, episode_reward=-92.38 +/- 66.53
Episode length: 93.36 +/- 66.40
New best mean reward!
FINISHED IN 231.25213440100197 s


starting seed  10099 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-314.65 +/- 169.75
Episode length: 315.20 +/- 169.26
New best mean reward!
Eval num_timesteps=6500, episode_reward=-490.71 +/- 40.84
Episode length: 490.77 +/- 40.63
Eval num_timesteps=7000, episode_reward=-179.31 +/- 84.79
Episode length: 180.26 +/- 84.61
New best mean reward!
Eval num_timesteps=7500, episode_reward=-228.90 +/- 111.94
Episode length: 229.77 +/- 111.62
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-247.68 +/- 65.78
Episode length: 248.65 +/- 65.66
Eval num_timesteps=9000, episode_reward=-303.55 +/- 98.42
Episode length: 304.49 +/- 98.30
Eval num_timesteps=9500, episode_reward=-497.43 +/- 25.57
Episode length: 497.44 +/- 25.47
Eval num_timesteps=10000, episode_reward=-201.94 +/- 33.39
Episode length: 202.94 +/- 33.39
Eval num_timesteps=10500, episode_reward=-183.15 +/- 46.87
Episode length: 184.14 +/- 46.80
Eval num_timesteps=11000, episode_reward=-180.64 +/- 47.08
Episode length: 181.64 +/- 47.08
Eval num_timesteps=11500, episode_reward=-177.23 +/- 36.82
Episode length: 178.23 +/- 36.82
New best mean reward!
Eval num_timesteps=12000, episode_reward=-175.08 +/- 33.71
Episode length: 176.08 +/- 33.71
New best mean reward!
Eval num_timesteps=12500, episode_reward=-173.36 +/- 39.55
Episode length: 174.36 +/- 39.55
New best mean reward!
Eval num_timesteps=13000, episode_reward=-176.36 +/- 38.15
Episode length: 177.36 +/- 38.15
Eval num_timesteps=13500, episode_reward=-175.85 +/- 36.46
Episode length: 176.85 +/- 36.46
Eval num_timesteps=14000, episode_reward=-298.86 +/- 163.50
Episode length: 299.47 +/- 163.02
Eval num_timesteps=14500, episode_reward=-201.40 +/- 93.01
Episode length: 202.33 +/- 92.78
Eval num_timesteps=15000, episode_reward=-222.87 +/- 116.37
Episode length: 223.74 +/- 116.06
Eval num_timesteps=15500, episode_reward=-287.31 +/- 155.03
Episode length: 287.97 +/- 154.56
Eval num_timesteps=16000, episode_reward=-369.24 +/- 160.86
Episode length: 369.64 +/- 160.37
Eval num_timesteps=16500, episode_reward=-285.40 +/- 153.71
Episode length: 286.08 +/- 153.27
Eval num_timesteps=17000, episode_reward=-378.50 +/- 158.21
Episode length: 378.88 +/- 157.74
Eval num_timesteps=17500, episode_reward=-398.10 +/- 149.81
Episode length: 398.42 +/- 149.35
Eval num_timesteps=18000, episode_reward=-475.32 +/- 85.15
Episode length: 475.40 +/- 84.88
Eval num_timesteps=18500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=19000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=19500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=20000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=20500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=21000, episode_reward=-496.40 +/- 35.82
Episode length: 496.41 +/- 35.72
Eval num_timesteps=21500, episode_reward=-456.00 +/- 119.30
Episode length: 456.12 +/- 118.97
Eval num_timesteps=22000, episode_reward=-406.98 +/- 168.44
Episode length: 407.22 +/- 168.02
Eval num_timesteps=22500, episode_reward=-379.90 +/- 180.14
Episode length: 380.21 +/- 179.68
Eval num_timesteps=23000, episode_reward=-262.24 +/- 194.86
Episode length: 262.84 +/- 194.37
Eval num_timesteps=23500, episode_reward=-275.57 +/- 202.03
Episode length: 276.14 +/- 201.55
Eval num_timesteps=24000, episode_reward=-301.21 +/- 202.75
Episode length: 301.71 +/- 202.26
Eval num_timesteps=24500, episode_reward=-301.56 +/- 197.84
Episode length: 302.07 +/- 197.35
Eval num_timesteps=25000, episode_reward=-285.77 +/- 201.69
Episode length: 286.32 +/- 201.21
Eval num_timesteps=25500, episode_reward=-285.30 +/- 203.22
Episode length: 285.83 +/- 202.72
Eval num_timesteps=26000, episode_reward=-256.59 +/- 200.15
Episode length: 257.19 +/- 199.66
Eval num_timesteps=26500, episode_reward=-253.85 +/- 198.24
Episode length: 254.48 +/- 197.78
Eval num_timesteps=27000, episode_reward=-234.46 +/- 190.52
Episode length: 235.13 +/- 190.06
Eval num_timesteps=27500, episode_reward=-200.44 +/- 174.41
Episode length: 201.20 +/- 174.00
Eval num_timesteps=28000, episode_reward=-145.82 +/- 132.38
Episode length: 146.71 +/- 132.08
New best mean reward!
Eval num_timesteps=28500, episode_reward=-169.98 +/- 147.77
Episode length: 170.83 +/- 147.43
Eval num_timesteps=29000, episode_reward=-167.84 +/- 153.08
Episode length: 168.68 +/- 152.73
Eval num_timesteps=29500, episode_reward=-147.93 +/- 131.25
Episode length: 148.82 +/- 130.95
Eval num_timesteps=30000, episode_reward=-132.24 +/- 111.36
Episode length: 133.17 +/- 111.12
New best mean reward!
Eval num_timesteps=30500, episode_reward=-99.79 +/- 41.13
Episode length: 100.79 +/- 41.13
New best mean reward!
FINISHED IN 333.8694926839671 s
AVG TIME: 372.9817155042681
