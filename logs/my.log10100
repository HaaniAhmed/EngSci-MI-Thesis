nohup: ignoring input


starting seed  10100 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-115.03 +/- 80.95
Episode length: 115.99 +/- 80.76
New best mean reward!
Eval num_timesteps=8500, episode_reward=-115.83 +/- 91.53
Episode length: 116.78 +/- 91.32
Eval num_timesteps=9000, episode_reward=-423.02 +/- 159.08
Episode length: 423.21 +/- 158.69
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-459.25 +/- 122.27
Episode length: 459.35 +/- 121.97
Eval num_timesteps=11000, episode_reward=-340.13 +/- 200.31
Episode length: 340.52 +/- 199.82
Eval num_timesteps=11500, episode_reward=-203.43 +/- 182.05
Episode length: 204.16 +/- 181.61
Eval num_timesteps=12000, episode_reward=-176.99 +/- 166.76
Episode length: 177.79 +/- 166.38
Eval num_timesteps=12500, episode_reward=-134.62 +/- 129.47
Episode length: 135.52 +/- 129.19
Eval num_timesteps=13000, episode_reward=-108.45 +/- 92.40
Episode length: 109.40 +/- 92.19
New best mean reward!
Eval num_timesteps=13500, episode_reward=-154.34 +/- 151.30
Episode length: 155.19 +/- 150.96
Eval num_timesteps=14000, episode_reward=-114.17 +/- 101.19
Episode length: 115.11 +/- 100.97
Eval num_timesteps=14500, episode_reward=-119.12 +/- 106.14
Episode length: 120.06 +/- 105.92
Eval num_timesteps=15000, episode_reward=-85.17 +/- 18.43
Episode length: 86.17 +/- 18.43
New best mean reward!
FINISHED IN 223.76216346700676 s


starting seed  10101 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-490.78 +/- 52.45
Episode length: 490.81 +/- 52.28
New best mean reward!
Eval num_timesteps=10500, episode_reward=-340.48 +/- 191.89
Episode length: 340.89 +/- 191.40
New best mean reward!
Eval num_timesteps=11000, episode_reward=-385.58 +/- 179.35
Episode length: 385.87 +/- 178.90
Eval num_timesteps=11500, episode_reward=-290.16 +/- 195.05
Episode length: 290.70 +/- 194.55
New best mean reward!
Eval num_timesteps=12000, episode_reward=-311.21 +/- 205.47
Episode length: 311.67 +/- 204.97
Eval num_timesteps=12500, episode_reward=-304.86 +/- 196.26
Episode length: 305.36 +/- 195.76
Eval num_timesteps=13000, episode_reward=-269.75 +/- 191.50
Episode length: 270.35 +/- 191.02
New best mean reward!
Eval num_timesteps=13500, episode_reward=-237.19 +/- 197.18
Episode length: 237.84 +/- 196.72
New best mean reward!
Eval num_timesteps=14000, episode_reward=-256.59 +/- 199.31
Episode length: 257.20 +/- 198.83
Eval num_timesteps=14500, episode_reward=-179.38 +/- 164.71
Episode length: 180.18 +/- 164.32
New best mean reward!
Eval num_timesteps=15000, episode_reward=-183.18 +/- 171.97
Episode length: 183.96 +/- 171.57
Eval num_timesteps=15500, episode_reward=-188.64 +/- 169.39
Episode length: 189.43 +/- 169.00
Eval num_timesteps=16000, episode_reward=-194.96 +/- 168.94
Episode length: 195.74 +/- 168.54
Eval num_timesteps=16500, episode_reward=-249.05 +/- 191.11
Episode length: 249.69 +/- 190.63
Eval num_timesteps=17000, episode_reward=-207.59 +/- 181.98
Episode length: 208.32 +/- 181.55
Eval num_timesteps=17500, episode_reward=-227.01 +/- 189.91
Episode length: 227.70 +/- 189.46
Eval num_timesteps=18000, episode_reward=-185.77 +/- 170.79
Episode length: 186.55 +/- 170.39
Eval num_timesteps=18500, episode_reward=-130.00 +/- 122.46
Episode length: 130.91 +/- 122.19
New best mean reward!
Eval num_timesteps=19000, episode_reward=-129.08 +/- 125.72
Episode length: 129.99 +/- 125.46
New best mean reward!
Eval num_timesteps=19500, episode_reward=-142.72 +/- 139.45
Episode length: 143.59 +/- 139.12
Eval num_timesteps=20000, episode_reward=-100.21 +/- 75.97
Episode length: 101.18 +/- 75.81
New best mean reward!
Eval num_timesteps=20500, episode_reward=-110.55 +/- 90.70
Episode length: 111.51 +/- 90.53
Eval num_timesteps=21000, episode_reward=-125.28 +/- 114.56
Episode length: 126.20 +/- 114.30
Eval num_timesteps=21500, episode_reward=-99.81 +/- 69.87
Episode length: 100.79 +/- 69.76
New best mean reward!
FINISHED IN 502.66614832996856 s


starting seed  10102 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-263.09 +/- 187.52
Episode length: 263.71 +/- 187.04
New best mean reward!
Eval num_timesteps=9000, episode_reward=-182.64 +/- 148.23
Episode length: 183.47 +/- 147.87
New best mean reward!
Eval num_timesteps=9500, episode_reward=-223.66 +/- 178.24
Episode length: 224.37 +/- 177.79
Eval num_timesteps=10000, episode_reward=-163.71 +/- 146.30
Episode length: 164.56 +/- 145.95
New best mean reward!
Eval num_timesteps=10500, episode_reward=-193.63 +/- 167.21
Episode length: 194.41 +/- 166.81
Eval num_timesteps=11000, episode_reward=-169.16 +/- 145.57
Episode length: 170.01 +/- 145.23
Eval num_timesteps=11500, episode_reward=-170.20 +/- 140.17
Episode length: 171.06 +/- 139.84
Eval num_timesteps=12000, episode_reward=-143.19 +/- 123.11
Episode length: 144.09 +/- 122.82
New best mean reward!
Eval num_timesteps=12500, episode_reward=-211.93 +/- 176.42
Episode length: 212.67 +/- 175.99
Eval num_timesteps=13000, episode_reward=-211.71 +/- 178.39
Episode length: 212.45 +/- 177.97
Eval num_timesteps=13500, episode_reward=-293.22 +/- 195.32
Episode length: 293.76 +/- 194.83
Eval num_timesteps=14000, episode_reward=-299.00 +/- 192.47
Episode length: 299.53 +/- 191.98
Eval num_timesteps=14500, episode_reward=-281.41 +/- 194.91
Episode length: 281.99 +/- 194.44
Eval num_timesteps=15000, episode_reward=-200.62 +/- 171.20
Episode length: 201.39 +/- 170.80
Eval num_timesteps=15500, episode_reward=-213.28 +/- 175.70
Episode length: 214.02 +/- 175.28
Eval num_timesteps=16000, episode_reward=-149.80 +/- 121.62
Episode length: 150.71 +/- 121.36
Eval num_timesteps=16500, episode_reward=-143.31 +/- 122.93
Episode length: 144.21 +/- 122.64
Eval num_timesteps=17000, episode_reward=-158.86 +/- 135.99
Episode length: 159.74 +/- 135.69
Eval num_timesteps=17500, episode_reward=-128.96 +/- 99.54
Episode length: 129.91 +/- 99.36
New best mean reward!
Eval num_timesteps=18000, episode_reward=-119.67 +/- 83.45
Episode length: 120.65 +/- 83.36
New best mean reward!
Eval num_timesteps=18500, episode_reward=-119.44 +/- 82.09
Episode length: 120.41 +/- 81.95
New best mean reward!
Eval num_timesteps=19000, episode_reward=-114.42 +/- 71.69
Episode length: 115.40 +/- 71.59
New best mean reward!
Eval num_timesteps=19500, episode_reward=-99.94 +/- 36.70
Episode length: 100.94 +/- 36.70
New best mean reward!
FINISHED IN 466.5645659560105 s


starting seed  10103 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-468.55 +/- 75.81
Episode length: 468.73 +/- 75.47
New best mean reward!
Eval num_timesteps=6000, episode_reward=-232.19 +/- 164.58
Episode length: 232.95 +/- 164.19
New best mean reward!
Eval num_timesteps=6500, episode_reward=-89.90 +/- 31.32
Episode length: 90.90 +/- 31.32
New best mean reward!
FINISHED IN 202.31276065699058 s


starting seed  10104 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-496.02 +/- 20.24
Episode length: 496.07 +/- 20.05
New best mean reward!
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-180.16 +/- 135.24
Episode length: 181.02 +/- 134.91
New best mean reward!
Eval num_timesteps=7000, episode_reward=-98.65 +/- 31.42
Episode length: 99.65 +/- 31.42
New best mean reward!
FINISHED IN 201.59866061800858 s


starting seed  10105 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-223.20 +/- 182.35
Episode length: 223.91 +/- 181.91
New best mean reward!
Eval num_timesteps=6500, episode_reward=-103.72 +/- 32.39
Episode length: 104.72 +/- 32.39
New best mean reward!
Eval num_timesteps=7000, episode_reward=-101.76 +/- 50.61
Episode length: 102.75 +/- 50.53
New best mean reward!
Eval num_timesteps=7500, episode_reward=-90.80 +/- 30.27
Episode length: 91.80 +/- 30.27
New best mean reward!
FINISHED IN 205.60399187600706 s


starting seed  10106 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-248.18 +/- 76.12
Episode length: 249.13 +/- 75.96
New best mean reward!
Eval num_timesteps=8000, episode_reward=-313.21 +/- 146.04
Episode length: 313.85 +/- 145.58
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-368.01 +/- 163.60
Episode length: 368.41 +/- 163.12
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-405.73 +/- 149.12
Episode length: 406.02 +/- 148.67
Eval num_timesteps=11000, episode_reward=-318.90 +/- 175.77
Episode length: 319.42 +/- 175.28
Eval num_timesteps=11500, episode_reward=-382.64 +/- 164.03
Episode length: 382.98 +/- 163.56
Eval num_timesteps=12000, episode_reward=-477.41 +/- 82.74
Episode length: 477.48 +/- 82.49
Eval num_timesteps=12500, episode_reward=-292.03 +/- 175.01
Episode length: 292.62 +/- 174.52
Eval num_timesteps=13000, episode_reward=-183.21 +/- 131.16
Episode length: 184.07 +/- 130.82
New best mean reward!
Eval num_timesteps=13500, episode_reward=-283.49 +/- 169.67
Episode length: 284.12 +/- 169.20
Eval num_timesteps=14000, episode_reward=-129.03 +/- 60.39
Episode length: 130.01 +/- 60.26
New best mean reward!
Eval num_timesteps=14500, episode_reward=-120.16 +/- 30.99
Episode length: 121.16 +/- 30.99
New best mean reward!
Eval num_timesteps=15000, episode_reward=-119.97 +/- 45.42
Episode length: 120.97 +/- 45.42
New best mean reward!
Eval num_timesteps=15500, episode_reward=-98.43 +/- 35.26
Episode length: 99.43 +/- 35.26
New best mean reward!
FINISHED IN 318.20657151803607 s


starting seed  10107 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-351.46 +/- 196.08
Episode length: 351.83 +/- 195.60
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-487.51 +/- 61.34
Episode length: 487.55 +/- 61.14
Eval num_timesteps=8000, episode_reward=-234.31 +/- 121.87
Episode length: 235.15 +/- 121.52
New best mean reward!
Eval num_timesteps=8500, episode_reward=-245.04 +/- 130.21
Episode length: 245.84 +/- 129.82
Eval num_timesteps=9000, episode_reward=-247.08 +/- 131.38
Episode length: 247.89 +/- 131.01
Eval num_timesteps=9500, episode_reward=-166.74 +/- 43.45
Episode length: 167.73 +/- 43.38
New best mean reward!
Eval num_timesteps=10000, episode_reward=-147.39 +/- 76.37
Episode length: 148.35 +/- 76.19
New best mean reward!
Eval num_timesteps=10500, episode_reward=-168.42 +/- 23.30
Episode length: 169.42 +/- 23.30
Eval num_timesteps=11000, episode_reward=-126.78 +/- 41.53
Episode length: 127.77 +/- 41.44
New best mean reward!
Eval num_timesteps=11500, episode_reward=-112.68 +/- 83.37
Episode length: 113.64 +/- 83.18
New best mean reward!
Eval num_timesteps=12000, episode_reward=-172.32 +/- 145.63
Episode length: 173.16 +/- 145.27
Eval num_timesteps=12500, episode_reward=-151.17 +/- 136.74
Episode length: 152.04 +/- 136.41
Eval num_timesteps=13000, episode_reward=-155.65 +/- 151.27
Episode length: 156.49 +/- 150.91
Eval num_timesteps=13500, episode_reward=-119.70 +/- 86.39
Episode length: 120.66 +/- 86.22
Eval num_timesteps=14000, episode_reward=-110.07 +/- 53.14
Episode length: 111.06 +/- 53.07
New best mean reward!
Eval num_timesteps=14500, episode_reward=-107.59 +/- 50.06
Episode length: 108.58 +/- 49.98
New best mean reward!
Eval num_timesteps=15000, episode_reward=-107.59 +/- 60.10
Episode length: 108.58 +/- 60.04
Eval num_timesteps=15500, episode_reward=-92.48 +/- 28.44
Episode length: 93.48 +/- 28.44
New best mean reward!
FINISHED IN 271.33354189200327 s


starting seed  10108 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-393.82 +/- 157.47
Episode length: 394.14 +/- 157.01
New best mean reward!
Eval num_timesteps=9000, episode_reward=-212.54 +/- 165.41
Episode length: 213.30 +/- 164.99
New best mean reward!
Eval num_timesteps=9500, episode_reward=-161.37 +/- 132.44
Episode length: 162.25 +/- 132.14
New best mean reward!
Eval num_timesteps=10000, episode_reward=-205.25 +/- 166.30
Episode length: 206.02 +/- 165.89
Eval num_timesteps=10500, episode_reward=-219.84 +/- 182.09
Episode length: 220.55 +/- 181.64
Eval num_timesteps=11000, episode_reward=-213.45 +/- 179.15
Episode length: 214.18 +/- 178.71
Eval num_timesteps=11500, episode_reward=-178.86 +/- 160.56
Episode length: 179.67 +/- 160.18
Eval num_timesteps=12000, episode_reward=-230.51 +/- 188.76
Episode length: 231.19 +/- 188.30
Eval num_timesteps=12500, episode_reward=-122.86 +/- 98.92
Episode length: 123.80 +/- 98.69
New best mean reward!
Eval num_timesteps=13000, episode_reward=-119.92 +/- 106.26
Episode length: 120.85 +/- 106.01
New best mean reward!
Eval num_timesteps=13500, episode_reward=-130.48 +/- 117.53
Episode length: 131.40 +/- 117.28
Eval num_timesteps=14000, episode_reward=-99.11 +/- 35.67
Episode length: 100.11 +/- 35.67
New best mean reward!
FINISHED IN 283.7343924269662 s


starting seed  10109 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-351.45 +/- 194.08
Episode length: 351.82 +/- 193.60
New best mean reward!
Eval num_timesteps=6500, episode_reward=-271.22 +/- 199.39
Episode length: 271.79 +/- 198.90
New best mean reward!
Eval num_timesteps=7000, episode_reward=-351.32 +/- 192.39
Episode length: 351.70 +/- 191.91
Eval num_timesteps=7500, episode_reward=-108.28 +/- 27.55
Episode length: 109.28 +/- 27.55
New best mean reward!
Eval num_timesteps=8000, episode_reward=-153.91 +/- 112.48
Episode length: 154.82 +/- 112.20
Eval num_timesteps=8500, episode_reward=-159.88 +/- 124.07
Episode length: 160.77 +/- 123.77
Eval num_timesteps=9000, episode_reward=-295.49 +/- 187.44
Episode length: 296.04 +/- 186.95
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-214.55 +/- 170.83
Episode length: 215.29 +/- 170.40
Eval num_timesteps=10500, episode_reward=-141.21 +/- 48.92
Episode length: 142.20 +/- 48.85
Eval num_timesteps=11000, episode_reward=-247.15 +/- 141.51
Episode length: 247.92 +/- 141.10
Eval num_timesteps=11500, episode_reward=-150.12 +/- 29.42
Episode length: 151.12 +/- 29.42
Eval num_timesteps=12000, episode_reward=-178.52 +/- 67.02
Episode length: 179.49 +/- 66.87
Eval num_timesteps=12500, episode_reward=-390.03 +/- 150.65
Episode length: 390.39 +/- 150.18
Eval num_timesteps=13000, episode_reward=-90.65 +/- 27.91
Episode length: 91.65 +/- 27.91
New best mean reward!
FINISHED IN 274.1297585730208 s


starting seed  10110 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-465.80 +/- 110.00
Episode length: 465.89 +/- 109.72
New best mean reward!
Eval num_timesteps=7500, episode_reward=-391.37 +/- 175.00
Episode length: 391.65 +/- 174.56
New best mean reward!
Eval num_timesteps=8000, episode_reward=-302.65 +/- 195.30
Episode length: 303.16 +/- 194.80
New best mean reward!
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-263.62 +/- 186.01
Episode length: 264.24 +/- 185.53
New best mean reward!
Eval num_timesteps=10000, episode_reward=-119.55 +/- 74.77
Episode length: 120.52 +/- 74.62
New best mean reward!
Eval num_timesteps=10500, episode_reward=-91.64 +/- 35.45
Episode length: 92.64 +/- 35.45
New best mean reward!
FINISHED IN 239.482768679969 s


starting seed  10111 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-174.25 +/- 58.42
Episode length: 175.24 +/- 58.36
New best mean reward!
Eval num_timesteps=3000, episode_reward=-172.50 +/- 32.27
Episode length: 173.50 +/- 32.27
New best mean reward!
Eval num_timesteps=3500, episode_reward=-178.27 +/- 71.97
Episode length: 179.25 +/- 71.88
Eval num_timesteps=4000, episode_reward=-172.39 +/- 29.68
Episode length: 173.39 +/- 29.68
New best mean reward!
Eval num_timesteps=4500, episode_reward=-366.46 +/- 109.56
Episode length: 367.21 +/- 109.26
Eval num_timesteps=5000, episode_reward=-175.72 +/- 29.51
Episode length: 176.72 +/- 29.51
Eval num_timesteps=5500, episode_reward=-161.21 +/- 50.89
Episode length: 162.21 +/- 50.89
New best mean reward!
Eval num_timesteps=6000, episode_reward=-160.08 +/- 35.38
Episode length: 161.08 +/- 35.38
New best mean reward!
Eval num_timesteps=6500, episode_reward=-177.80 +/- 49.05
Episode length: 178.79 +/- 48.98
Eval num_timesteps=7000, episode_reward=-188.13 +/- 70.03
Episode length: 189.11 +/- 69.94
Eval num_timesteps=7500, episode_reward=-178.78 +/- 63.21
Episode length: 179.76 +/- 63.11
Eval num_timesteps=8000, episode_reward=-167.61 +/- 37.02
Episode length: 168.61 +/- 37.02
Eval num_timesteps=8500, episode_reward=-157.15 +/- 35.65
Episode length: 158.15 +/- 35.65
New best mean reward!
Eval num_timesteps=9000, episode_reward=-171.79 +/- 48.68
Episode length: 172.78 +/- 48.62
Eval num_timesteps=9500, episode_reward=-178.50 +/- 53.36
Episode length: 179.49 +/- 53.30
Eval num_timesteps=10000, episode_reward=-173.76 +/- 53.80
Episode length: 174.75 +/- 53.73
Eval num_timesteps=10500, episode_reward=-194.20 +/- 83.26
Episode length: 195.15 +/- 83.08
Eval num_timesteps=11000, episode_reward=-258.74 +/- 141.90
Episode length: 259.50 +/- 141.49
Eval num_timesteps=11500, episode_reward=-281.96 +/- 151.08
Episode length: 282.65 +/- 150.63
Eval num_timesteps=12000, episode_reward=-263.62 +/- 145.00
Episode length: 264.37 +/- 144.59
Eval num_timesteps=12500, episode_reward=-227.45 +/- 123.48
Episode length: 228.29 +/- 123.13
Eval num_timesteps=13000, episode_reward=-257.66 +/- 138.49
Episode length: 258.44 +/- 138.10
Eval num_timesteps=13500, episode_reward=-252.29 +/- 141.07
Episode length: 253.06 +/- 140.66
Eval num_timesteps=14000, episode_reward=-237.96 +/- 134.19
Episode length: 238.76 +/- 133.80
Eval num_timesteps=14500, episode_reward=-232.36 +/- 127.40
Episode length: 233.19 +/- 127.04
Eval num_timesteps=15000, episode_reward=-183.54 +/- 101.88
Episode length: 184.46 +/- 101.63
Eval num_timesteps=15500, episode_reward=-189.65 +/- 108.93
Episode length: 190.55 +/- 108.64
Eval num_timesteps=16000, episode_reward=-157.15 +/- 98.08
Episode length: 158.08 +/- 97.84
Eval num_timesteps=16500, episode_reward=-176.82 +/- 124.75
Episode length: 177.70 +/- 124.44
Eval num_timesteps=17000, episode_reward=-185.41 +/- 149.79
Episode length: 186.23 +/- 149.41
Eval num_timesteps=17500, episode_reward=-141.59 +/- 109.16
Episode length: 142.51 +/- 108.90
New best mean reward!
Eval num_timesteps=18000, episode_reward=-142.20 +/- 103.80
Episode length: 143.14 +/- 103.59
Eval num_timesteps=18500, episode_reward=-163.87 +/- 103.99
Episode length: 164.79 +/- 103.73
Eval num_timesteps=19000, episode_reward=-143.97 +/- 95.75
Episode length: 144.91 +/- 95.53
Eval num_timesteps=19500, episode_reward=-160.40 +/- 138.53
Episode length: 161.26 +/- 138.19
Eval num_timesteps=20000, episode_reward=-128.67 +/- 106.87
Episode length: 129.60 +/- 106.63
New best mean reward!
Eval num_timesteps=20500, episode_reward=-187.67 +/- 156.08
Episode length: 188.49 +/- 155.72
Eval num_timesteps=21000, episode_reward=-152.96 +/- 143.00
Episode length: 153.83 +/- 142.69
Eval num_timesteps=21500, episode_reward=-117.92 +/- 100.16
Episode length: 118.86 +/- 99.93
New best mean reward!
Eval num_timesteps=22000, episode_reward=-145.32 +/- 126.36
Episode length: 146.23 +/- 126.11
Eval num_timesteps=22500, episode_reward=-136.95 +/- 128.73
Episode length: 137.85 +/- 128.45
Eval num_timesteps=23000, episode_reward=-134.07 +/- 120.19
Episode length: 134.99 +/- 119.95
Eval num_timesteps=23500, episode_reward=-102.24 +/- 68.15
Episode length: 103.22 +/- 68.03
New best mean reward!
Eval num_timesteps=24000, episode_reward=-116.06 +/- 94.96
Episode length: 117.01 +/- 94.76
Eval num_timesteps=24500, episode_reward=-112.54 +/- 83.89
Episode length: 113.51 +/- 83.75
Eval num_timesteps=25000, episode_reward=-114.79 +/- 94.38
Episode length: 115.75 +/- 94.21
Eval num_timesteps=25500, episode_reward=-94.91 +/- 41.24
Episode length: 95.91 +/- 41.24
New best mean reward!
FINISHED IN 380.9998575389618 s


starting seed  10112 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-366.07 +/- 186.01
Episode length: 366.42 +/- 185.54
New best mean reward!
Eval num_timesteps=6500, episode_reward=-108.89 +/- 37.39
Episode length: 109.89 +/- 37.39
New best mean reward!
Eval num_timesteps=7000, episode_reward=-93.96 +/- 20.42
Episode length: 94.96 +/- 20.42
New best mean reward!
FINISHED IN 163.00990578200435 s


starting seed  10113 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-333.04 +/- 170.44
Episode length: 333.54 +/- 169.95
New best mean reward!
Eval num_timesteps=10500, episode_reward=-287.86 +/- 158.39
Episode length: 288.51 +/- 157.93
New best mean reward!
Eval num_timesteps=11000, episode_reward=-248.49 +/- 143.84
Episode length: 249.25 +/- 143.42
New best mean reward!
Eval num_timesteps=11500, episode_reward=-317.10 +/- 164.67
Episode length: 317.66 +/- 164.18
Eval num_timesteps=12000, episode_reward=-294.37 +/- 161.25
Episode length: 295.00 +/- 160.78
Eval num_timesteps=12500, episode_reward=-220.66 +/- 121.10
Episode length: 221.51 +/- 120.76
New best mean reward!
Eval num_timesteps=13000, episode_reward=-170.11 +/- 45.84
Episode length: 171.10 +/- 45.77
New best mean reward!
Eval num_timesteps=13500, episode_reward=-175.91 +/- 57.40
Episode length: 176.89 +/- 57.29
Eval num_timesteps=14000, episode_reward=-179.08 +/- 61.70
Episode length: 180.06 +/- 61.59
Eval num_timesteps=14500, episode_reward=-171.86 +/- 42.83
Episode length: 172.86 +/- 42.83
Eval num_timesteps=15000, episode_reward=-186.42 +/- 67.98
Episode length: 187.40 +/- 67.89
Eval num_timesteps=15500, episode_reward=-174.48 +/- 64.02
Episode length: 175.45 +/- 63.86
Eval num_timesteps=16000, episode_reward=-151.96 +/- 28.71
Episode length: 152.96 +/- 28.71
New best mean reward!
Eval num_timesteps=16500, episode_reward=-111.48 +/- 17.28
Episode length: 112.48 +/- 17.28
New best mean reward!
Eval num_timesteps=17000, episode_reward=-157.15 +/- 38.89
Episode length: 158.15 +/- 38.89
Eval num_timesteps=17500, episode_reward=-106.37 +/- 28.98
Episode length: 107.37 +/- 28.98
New best mean reward!
Eval num_timesteps=18000, episode_reward=-121.86 +/- 38.08
Episode length: 122.86 +/- 38.08
Eval num_timesteps=18500, episode_reward=-122.35 +/- 44.42
Episode length: 123.35 +/- 44.42
Eval num_timesteps=19000, episode_reward=-100.30 +/- 29.00
Episode length: 101.30 +/- 29.00
New best mean reward!
Eval num_timesteps=19500, episode_reward=-100.77 +/- 48.19
Episode length: 101.76 +/- 48.11
Eval num_timesteps=20000, episode_reward=-84.84 +/- 16.79
Episode length: 85.84 +/- 16.79
New best mean reward!
FINISHED IN 386.33406385197304 s


starting seed  10114 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-479.74 +/- 52.52
Episode length: 479.92 +/- 52.20
New best mean reward!
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-350.50 +/- 121.87
Episode length: 351.22 +/- 121.53
New best mean reward!
Eval num_timesteps=14000, episode_reward=-497.90 +/- 20.89
Episode length: 497.91 +/- 20.80
Eval num_timesteps=14500, episode_reward=-369.52 +/- 136.53
Episode length: 370.06 +/- 136.09
Eval num_timesteps=15000, episode_reward=-351.56 +/- 115.97
Episode length: 352.28 +/- 115.61
Eval num_timesteps=15500, episode_reward=-499.95 +/- 0.50
Episode length: 499.96 +/- 0.40
Eval num_timesteps=16000, episode_reward=-296.81 +/- 95.72
Episode length: 297.74 +/- 95.57
New best mean reward!
Eval num_timesteps=16500, episode_reward=-269.48 +/- 86.15
Episode length: 270.45 +/- 86.07
New best mean reward!
Eval num_timesteps=17000, episode_reward=-301.49 +/- 100.06
Episode length: 302.42 +/- 99.92
Eval num_timesteps=17500, episode_reward=-145.24 +/- 54.43
Episode length: 146.24 +/- 54.43
New best mean reward!
Eval num_timesteps=18000, episode_reward=-135.37 +/- 34.50
Episode length: 136.37 +/- 34.50
New best mean reward!
Eval num_timesteps=18500, episode_reward=-190.30 +/- 58.72
Episode length: 191.29 +/- 58.67
Eval num_timesteps=19000, episode_reward=-126.80 +/- 48.09
Episode length: 127.80 +/- 48.09
New best mean reward!
Eval num_timesteps=19500, episode_reward=-86.60 +/- 32.00
Episode length: 87.60 +/- 32.00
New best mean reward!
FINISHED IN 429.5383841819712 s


starting seed  10115 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-337.65 +/- 185.34
Episode length: 338.09 +/- 184.85
New best mean reward!
Eval num_timesteps=6000, episode_reward=-492.42 +/- 53.06
Episode length: 492.44 +/- 52.92
Eval num_timesteps=6500, episode_reward=-139.73 +/- 47.57
Episode length: 140.72 +/- 47.50
New best mean reward!
Eval num_timesteps=7000, episode_reward=-296.37 +/- 161.04
Episode length: 297.02 +/- 160.60
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-184.74 +/- 102.55
Episode length: 185.66 +/- 102.31
Eval num_timesteps=8500, episode_reward=-198.22 +/- 103.41
Episode length: 199.13 +/- 103.15
Eval num_timesteps=9000, episode_reward=-208.23 +/- 111.64
Episode length: 209.13 +/- 111.38
Eval num_timesteps=9500, episode_reward=-419.38 +/- 137.87
Episode length: 419.64 +/- 137.44
Eval num_timesteps=10000, episode_reward=-442.17 +/- 132.72
Episode length: 442.33 +/- 132.35
Eval num_timesteps=10500, episode_reward=-467.19 +/- 98.62
Episode length: 467.29 +/- 98.32
Eval num_timesteps=11000, episode_reward=-492.14 +/- 46.23
Episode length: 492.17 +/- 46.06
Eval num_timesteps=11500, episode_reward=-311.03 +/- 162.27
Episode length: 311.62 +/- 161.80
Eval num_timesteps=12000, episode_reward=-364.26 +/- 179.47
Episode length: 364.63 +/- 178.99
Eval num_timesteps=12500, episode_reward=-239.84 +/- 172.70
Episode length: 240.55 +/- 172.26
Eval num_timesteps=13000, episode_reward=-262.84 +/- 151.01
Episode length: 263.56 +/- 150.57
Eval num_timesteps=13500, episode_reward=-295.16 +/- 198.20
Episode length: 295.68 +/- 197.70
Eval num_timesteps=14000, episode_reward=-141.82 +/- 123.61
Episode length: 142.73 +/- 123.35
Eval num_timesteps=14500, episode_reward=-154.23 +/- 139.74
Episode length: 155.11 +/- 139.45
Eval num_timesteps=15000, episode_reward=-124.99 +/- 113.80
Episode length: 125.91 +/- 113.53
New best mean reward!
Eval num_timesteps=15500, episode_reward=-158.41 +/- 152.08
Episode length: 159.25 +/- 151.72
Eval num_timesteps=16000, episode_reward=-112.08 +/- 90.75
Episode length: 113.04 +/- 90.57
New best mean reward!
Eval num_timesteps=16500, episode_reward=-104.12 +/- 77.69
Episode length: 105.09 +/- 77.54
New best mean reward!
Eval num_timesteps=17000, episode_reward=-88.42 +/- 28.07
Episode length: 89.42 +/- 28.07
New best mean reward!
FINISHED IN 320.2815089300275 s


starting seed  10116 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-426.92 +/- 64.29
Episode length: 427.65 +/- 63.98
New best mean reward!
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-426.48 +/- 133.26
Episode length: 426.73 +/- 132.84
New best mean reward!
Eval num_timesteps=7000, episode_reward=-189.25 +/- 56.58
Episode length: 190.23 +/- 56.47
New best mean reward!
Eval num_timesteps=7500, episode_reward=-184.82 +/- 50.59
Episode length: 185.81 +/- 50.53
New best mean reward!
Eval num_timesteps=8000, episode_reward=-210.78 +/- 79.12
Episode length: 211.73 +/- 78.94
Eval num_timesteps=8500, episode_reward=-155.10 +/- 47.67
Episode length: 156.09 +/- 47.60
New best mean reward!
Eval num_timesteps=9000, episode_reward=-169.38 +/- 40.98
Episode length: 170.38 +/- 40.98
Eval num_timesteps=9500, episode_reward=-154.35 +/- 33.52
Episode length: 155.35 +/- 33.52
New best mean reward!
Eval num_timesteps=10000, episode_reward=-165.19 +/- 45.24
Episode length: 166.19 +/- 45.24
Eval num_timesteps=10500, episode_reward=-156.73 +/- 34.64
Episode length: 157.73 +/- 34.64
Eval num_timesteps=11000, episode_reward=-164.57 +/- 45.10
Episode length: 165.57 +/- 45.10
Eval num_timesteps=11500, episode_reward=-165.04 +/- 45.35
Episode length: 166.04 +/- 45.35
Eval num_timesteps=12000, episode_reward=-155.15 +/- 36.84
Episode length: 156.15 +/- 36.84
Eval num_timesteps=12500, episode_reward=-168.41 +/- 54.70
Episode length: 169.39 +/- 54.58
Eval num_timesteps=13000, episode_reward=-156.31 +/- 41.53
Episode length: 157.31 +/- 41.53
Eval num_timesteps=13500, episode_reward=-159.95 +/- 59.38
Episode length: 160.93 +/- 59.27
Eval num_timesteps=14000, episode_reward=-156.85 +/- 41.67
Episode length: 157.85 +/- 41.67
Eval num_timesteps=14500, episode_reward=-156.96 +/- 50.63
Episode length: 157.95 +/- 50.56
Eval num_timesteps=15000, episode_reward=-147.38 +/- 33.35
Episode length: 148.38 +/- 33.35
New best mean reward!
Eval num_timesteps=15500, episode_reward=-232.80 +/- 145.89
Episode length: 233.58 +/- 145.48
Eval num_timesteps=16000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16500, episode_reward=-322.93 +/- 172.17
Episode length: 323.45 +/- 171.67
Eval num_timesteps=17000, episode_reward=-279.97 +/- 159.87
Episode length: 280.64 +/- 159.42
Eval num_timesteps=17500, episode_reward=-254.19 +/- 151.65
Episode length: 254.93 +/- 151.23
Eval num_timesteps=18000, episode_reward=-251.19 +/- 153.06
Episode length: 251.93 +/- 152.63
Eval num_timesteps=18500, episode_reward=-165.72 +/- 51.59
Episode length: 166.71 +/- 51.52
Eval num_timesteps=19000, episode_reward=-178.87 +/- 86.94
Episode length: 179.81 +/- 86.72
Eval num_timesteps=19500, episode_reward=-310.50 +/- 169.87
Episode length: 311.06 +/- 169.38
Eval num_timesteps=20000, episode_reward=-418.60 +/- 145.99
Episode length: 418.84 +/- 145.57
Eval num_timesteps=20500, episode_reward=-153.54 +/- 35.77
Episode length: 154.54 +/- 35.77
Eval num_timesteps=21000, episode_reward=-154.47 +/- 35.58
Episode length: 155.47 +/- 35.58
Eval num_timesteps=21500, episode_reward=-164.45 +/- 50.81
Episode length: 165.44 +/- 50.75
Eval num_timesteps=22000, episode_reward=-158.66 +/- 34.58
Episode length: 159.66 +/- 34.58
Eval num_timesteps=22500, episode_reward=-155.83 +/- 35.78
Episode length: 156.83 +/- 35.78
Eval num_timesteps=23000, episode_reward=-166.54 +/- 55.51
Episode length: 167.52 +/- 55.39
Eval num_timesteps=23500, episode_reward=-162.24 +/- 47.74
Episode length: 163.24 +/- 47.74
Eval num_timesteps=24000, episode_reward=-140.85 +/- 38.60
Episode length: 141.85 +/- 38.60
New best mean reward!
Eval num_timesteps=24500, episode_reward=-106.08 +/- 61.37
Episode length: 107.06 +/- 61.24
New best mean reward!
Eval num_timesteps=25000, episode_reward=-93.83 +/- 21.66
Episode length: 94.83 +/- 21.66
New best mean reward!
FINISHED IN 401.7565093510202 s


starting seed  10117 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-331.29 +/- 170.63
Episode length: 331.80 +/- 170.15
New best mean reward!
Eval num_timesteps=7000, episode_reward=-231.20 +/- 133.66
Episode length: 232.01 +/- 133.28
New best mean reward!
Eval num_timesteps=7500, episode_reward=-125.74 +/- 99.08
Episode length: 126.68 +/- 98.85
New best mean reward!
Eval num_timesteps=8000, episode_reward=-166.22 +/- 30.88
Episode length: 167.22 +/- 30.88
Eval num_timesteps=8500, episode_reward=-171.29 +/- 52.72
Episode length: 172.28 +/- 52.66
Eval num_timesteps=9000, episode_reward=-166.14 +/- 54.67
Episode length: 167.12 +/- 54.54
Eval num_timesteps=9500, episode_reward=-161.21 +/- 32.91
Episode length: 162.21 +/- 32.91
Eval num_timesteps=10000, episode_reward=-176.16 +/- 46.80
Episode length: 177.16 +/- 46.80
Eval num_timesteps=10500, episode_reward=-176.58 +/- 48.78
Episode length: 177.58 +/- 48.78
Eval num_timesteps=11000, episode_reward=-121.15 +/- 37.87
Episode length: 122.15 +/- 37.87
New best mean reward!
Eval num_timesteps=11500, episode_reward=-100.21 +/- 47.48
Episode length: 101.20 +/- 47.40
New best mean reward!
Eval num_timesteps=12000, episode_reward=-84.17 +/- 13.11
Episode length: 85.17 +/- 13.11
New best mean reward!
FINISHED IN 238.3256262820214 s


starting seed  10118 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-471.41 +/- 104.26
Episode length: 471.48 +/- 104.00
New best mean reward!
Eval num_timesteps=9500, episode_reward=-455.24 +/- 127.49
Episode length: 455.35 +/- 127.18
New best mean reward!
Eval num_timesteps=10000, episode_reward=-354.74 +/- 194.46
Episode length: 355.10 +/- 193.99
New best mean reward!
Eval num_timesteps=10500, episode_reward=-317.46 +/- 206.29
Episode length: 317.90 +/- 205.79
New best mean reward!
Eval num_timesteps=11000, episode_reward=-339.55 +/- 200.20
Episode length: 339.95 +/- 199.72
Eval num_timesteps=11500, episode_reward=-405.76 +/- 172.64
Episode length: 405.99 +/- 172.22
Eval num_timesteps=12000, episode_reward=-289.94 +/- 203.87
Episode length: 290.46 +/- 203.37
New best mean reward!
Eval num_timesteps=12500, episode_reward=-255.96 +/- 202.53
Episode length: 256.57 +/- 202.06
New best mean reward!
Eval num_timesteps=13000, episode_reward=-220.40 +/- 193.37
Episode length: 221.08 +/- 192.90
New best mean reward!
Eval num_timesteps=13500, episode_reward=-221.92 +/- 192.53
Episode length: 222.60 +/- 192.07
Eval num_timesteps=14000, episode_reward=-183.88 +/- 168.83
Episode length: 184.67 +/- 168.44
New best mean reward!
Eval num_timesteps=14500, episode_reward=-161.87 +/- 157.60
Episode length: 162.70 +/- 157.24
New best mean reward!
Eval num_timesteps=15000, episode_reward=-175.28 +/- 173.11
Episode length: 176.06 +/- 172.70
Eval num_timesteps=15500, episode_reward=-163.12 +/- 155.21
Episode length: 163.97 +/- 154.89
Eval num_timesteps=16000, episode_reward=-145.77 +/- 149.70
Episode length: 146.62 +/- 149.35
New best mean reward!
Eval num_timesteps=16500, episode_reward=-180.15 +/- 169.69
Episode length: 180.94 +/- 169.29
Eval num_timesteps=17000, episode_reward=-172.03 +/- 157.70
Episode length: 172.87 +/- 157.36
Eval num_timesteps=17500, episode_reward=-176.06 +/- 168.47
Episode length: 176.85 +/- 168.06
Eval num_timesteps=18000, episode_reward=-167.39 +/- 167.03
Episode length: 168.19 +/- 166.63
Eval num_timesteps=18500, episode_reward=-131.32 +/- 128.32
Episode length: 132.22 +/- 128.04
New best mean reward!
Eval num_timesteps=19000, episode_reward=-148.54 +/- 141.09
Episode length: 149.41 +/- 140.77
Eval num_timesteps=19500, episode_reward=-116.57 +/- 97.87
Episode length: 117.52 +/- 97.68
New best mean reward!
Eval num_timesteps=20000, episode_reward=-112.91 +/- 101.73
Episode length: 113.85 +/- 101.50
New best mean reward!
Eval num_timesteps=20500, episode_reward=-114.37 +/- 92.98
Episode length: 115.32 +/- 92.77
Eval num_timesteps=21000, episode_reward=-111.69 +/- 95.29
Episode length: 112.64 +/- 95.09
New best mean reward!
Eval num_timesteps=21500, episode_reward=-120.77 +/- 111.09
Episode length: 121.70 +/- 110.85
Eval num_timesteps=22000, episode_reward=-97.53 +/- 65.54
Episode length: 98.51 +/- 65.42
New best mean reward!
FINISHED IN 445.00033956900006 s


starting seed  10119 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-470.25 +/- 101.02
Episode length: 470.33 +/- 100.75
New best mean reward!
Eval num_timesteps=9500, episode_reward=-109.99 +/- 91.64
Episode length: 110.94 +/- 91.43
New best mean reward!
Eval num_timesteps=10000, episode_reward=-80.25 +/- 17.14
Episode length: 81.25 +/- 17.14
New best mean reward!
FINISHED IN 306.7035522930091 s


starting seed  10120 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-455.16 +/- 111.76
Episode length: 455.30 +/- 111.42
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-416.29 +/- 145.62
Episode length: 416.54 +/- 145.19
New best mean reward!
Eval num_timesteps=10500, episode_reward=-464.09 +/- 102.46
Episode length: 464.20 +/- 102.14
Eval num_timesteps=11000, episode_reward=-436.43 +/- 131.63
Episode length: 436.62 +/- 131.24
Eval num_timesteps=11500, episode_reward=-387.99 +/- 157.14
Episode length: 388.33 +/- 156.67
New best mean reward!
Eval num_timesteps=12000, episode_reward=-270.50 +/- 181.99
Episode length: 271.12 +/- 181.51
New best mean reward!
Eval num_timesteps=12500, episode_reward=-215.05 +/- 186.73
Episode length: 215.76 +/- 186.28
New best mean reward!
Eval num_timesteps=13000, episode_reward=-193.16 +/- 173.15
Episode length: 193.93 +/- 172.74
New best mean reward!
Eval num_timesteps=13500, episode_reward=-188.48 +/- 168.86
Episode length: 189.27 +/- 168.47
New best mean reward!
Eval num_timesteps=14000, episode_reward=-174.97 +/- 163.51
Episode length: 175.79 +/- 163.16
New best mean reward!
Eval num_timesteps=14500, episode_reward=-188.92 +/- 175.43
Episode length: 189.70 +/- 175.04
Eval num_timesteps=15000, episode_reward=-148.17 +/- 140.11
Episode length: 149.04 +/- 139.78
New best mean reward!
Eval num_timesteps=15500, episode_reward=-162.49 +/- 156.41
Episode length: 163.32 +/- 156.05
Eval num_timesteps=16000, episode_reward=-136.13 +/- 130.29
Episode length: 137.04 +/- 130.04
New best mean reward!
Eval num_timesteps=16500, episode_reward=-138.47 +/- 134.46
Episode length: 139.36 +/- 134.17
Eval num_timesteps=17000, episode_reward=-157.14 +/- 151.48
Episode length: 157.98 +/- 151.12
Eval num_timesteps=17500, episode_reward=-132.52 +/- 129.33
Episode length: 133.42 +/- 129.05
New best mean reward!
Eval num_timesteps=18000, episode_reward=-135.03 +/- 126.82
Episode length: 135.93 +/- 126.54
Eval num_timesteps=18500, episode_reward=-109.78 +/- 84.30
Episode length: 110.74 +/- 84.11
New best mean reward!
Eval num_timesteps=19000, episode_reward=-126.61 +/- 123.67
Episode length: 127.52 +/- 123.40
Eval num_timesteps=19500, episode_reward=-102.93 +/- 74.10
Episode length: 103.90 +/- 73.94
New best mean reward!
Eval num_timesteps=20000, episode_reward=-121.47 +/- 110.94
Episode length: 122.40 +/- 110.70
Eval num_timesteps=20500, episode_reward=-152.09 +/- 146.39
Episode length: 152.95 +/- 146.06
Eval num_timesteps=21000, episode_reward=-141.69 +/- 132.35
Episode length: 142.58 +/- 132.05
Eval num_timesteps=21500, episode_reward=-160.82 +/- 151.63
Episode length: 161.66 +/- 151.27
Eval num_timesteps=22000, episode_reward=-124.77 +/- 112.73
Episode length: 125.70 +/- 112.49
Eval num_timesteps=22500, episode_reward=-102.27 +/- 73.72
Episode length: 103.24 +/- 73.55
New best mean reward!
Eval num_timesteps=23000, episode_reward=-102.79 +/- 76.41
Episode length: 103.76 +/- 76.26
Eval num_timesteps=23500, episode_reward=-99.01 +/- 64.70
Episode length: 99.99 +/- 64.58
New best mean reward!
FINISHED IN 554.9340773110162 s


starting seed  10121 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-469.33 +/- 104.18
Episode length: 469.41 +/- 103.91
New best mean reward!
Eval num_timesteps=11000, episode_reward=-408.64 +/- 163.73
Episode length: 408.88 +/- 163.30
New best mean reward!
Eval num_timesteps=11500, episode_reward=-312.81 +/- 207.23
Episode length: 313.26 +/- 206.73
New best mean reward!
Eval num_timesteps=12000, episode_reward=-262.47 +/- 201.24
Episode length: 263.06 +/- 200.75
New best mean reward!
Eval num_timesteps=12500, episode_reward=-270.36 +/- 204.42
Episode length: 270.92 +/- 203.92
Eval num_timesteps=13000, episode_reward=-204.13 +/- 184.11
Episode length: 204.86 +/- 183.67
New best mean reward!
Eval num_timesteps=13500, episode_reward=-252.15 +/- 202.20
Episode length: 252.76 +/- 201.72
Eval num_timesteps=14000, episode_reward=-174.67 +/- 164.16
Episode length: 175.48 +/- 163.79
New best mean reward!
Eval num_timesteps=14500, episode_reward=-120.65 +/- 114.77
Episode length: 121.57 +/- 114.51
New best mean reward!
Eval num_timesteps=15000, episode_reward=-117.36 +/- 105.58
Episode length: 118.30 +/- 105.36
New best mean reward!
Eval num_timesteps=15500, episode_reward=-128.40 +/- 120.55
Episode length: 129.31 +/- 120.27
Eval num_timesteps=16000, episode_reward=-115.11 +/- 99.53
Episode length: 116.05 +/- 99.30
New best mean reward!
Eval num_timesteps=16500, episode_reward=-139.47 +/- 130.10
Episode length: 140.36 +/- 129.79
Eval num_timesteps=17000, episode_reward=-156.77 +/- 147.01
Episode length: 157.62 +/- 146.66
Eval num_timesteps=17500, episode_reward=-152.77 +/- 138.25
Episode length: 153.65 +/- 137.95
Eval num_timesteps=18000, episode_reward=-164.24 +/- 159.71
Episode length: 165.07 +/- 159.35
Eval num_timesteps=18500, episode_reward=-117.18 +/- 101.52
Episode length: 118.12 +/- 101.29
Eval num_timesteps=19000, episode_reward=-110.36 +/- 84.48
Episode length: 111.32 +/- 84.29
New best mean reward!
Eval num_timesteps=19500, episode_reward=-94.82 +/- 61.75
Episode length: 95.80 +/- 61.62
New best mean reward!
FINISHED IN 439.1532077000011 s


starting seed  10122 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-321.20 +/- 165.83
Episode length: 321.76 +/- 165.36
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-186.92 +/- 96.15
Episode length: 187.84 +/- 95.89
New best mean reward!
Eval num_timesteps=8000, episode_reward=-487.84 +/- 18.65
Episode length: 488.28 +/- 18.29
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-99.41 +/- 47.94
Episode length: 100.40 +/- 47.85
New best mean reward!
FINISHED IN 268.6570788709796 s


starting seed  10123 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-497.03 +/- 29.55
Episode length: 497.04 +/- 29.45
New best mean reward!
Eval num_timesteps=8000, episode_reward=-139.39 +/- 29.81
Episode length: 140.39 +/- 29.81
New best mean reward!
Eval num_timesteps=8500, episode_reward=-125.87 +/- 20.02
Episode length: 126.87 +/- 20.02
New best mean reward!
Eval num_timesteps=9000, episode_reward=-159.49 +/- 32.91
Episode length: 160.49 +/- 32.91
Eval num_timesteps=9500, episode_reward=-118.67 +/- 25.77
Episode length: 119.67 +/- 25.77
New best mean reward!
Eval num_timesteps=10000, episode_reward=-135.45 +/- 96.15
Episode length: 136.39 +/- 95.93
Eval num_timesteps=10500, episode_reward=-117.51 +/- 40.20
Episode length: 118.51 +/- 40.20
New best mean reward!
Eval num_timesteps=11000, episode_reward=-119.78 +/- 39.77
Episode length: 120.78 +/- 39.77
Eval num_timesteps=11500, episode_reward=-131.64 +/- 44.55
Episode length: 132.63 +/- 44.47
Eval num_timesteps=12000, episode_reward=-111.78 +/- 47.88
Episode length: 112.77 +/- 47.80
New best mean reward!
Eval num_timesteps=12500, episode_reward=-113.60 +/- 18.15
Episode length: 114.60 +/- 18.15
Eval num_timesteps=13000, episode_reward=-109.12 +/- 43.95
Episode length: 110.11 +/- 43.86
New best mean reward!
Eval num_timesteps=13500, episode_reward=-101.03 +/- 20.63
Episode length: 102.03 +/- 20.63
New best mean reward!
Eval num_timesteps=14000, episode_reward=-95.16 +/- 31.10
Episode length: 96.16 +/- 31.10
New best mean reward!
FINISHED IN 247.68499124096707 s


starting seed  10124 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-156.73 +/- 32.27
Episode length: 157.73 +/- 32.27
New best mean reward!
Eval num_timesteps=9500, episode_reward=-184.62 +/- 84.42
Episode length: 185.56 +/- 84.20
Eval num_timesteps=10000, episode_reward=-260.87 +/- 151.05
Episode length: 261.60 +/- 150.62
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-200.95 +/- 108.13
Episode length: 201.85 +/- 107.85
Eval num_timesteps=11500, episode_reward=-366.57 +/- 170.16
Episode length: 366.97 +/- 169.69
Eval num_timesteps=12000, episode_reward=-84.53 +/- 18.24
Episode length: 85.53 +/- 18.24
New best mean reward!
FINISHED IN 301.3140787870507 s


starting seed  10125 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-324.79 +/- 202.05
Episode length: 325.22 +/- 201.56
New best mean reward!
Eval num_timesteps=10000, episode_reward=-116.66 +/- 101.74
Episode length: 117.60 +/- 101.52
New best mean reward!
Eval num_timesteps=10500, episode_reward=-92.89 +/- 32.06
Episode length: 93.89 +/- 32.06
New best mean reward!
FINISHED IN 309.32302400801564 s


starting seed  10126 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-191.40 +/- 40.96
Episode length: 192.39 +/- 40.89
New best mean reward!
Eval num_timesteps=4500, episode_reward=-261.70 +/- 176.74
Episode length: 262.38 +/- 176.31
Eval num_timesteps=5000, episode_reward=-132.23 +/- 89.81
Episode length: 133.18 +/- 89.60
New best mean reward!
Eval num_timesteps=5500, episode_reward=-122.14 +/- 47.70
Episode length: 123.13 +/- 47.63
New best mean reward!
Eval num_timesteps=6000, episode_reward=-109.90 +/- 22.65
Episode length: 110.90 +/- 22.65
New best mean reward!
Eval num_timesteps=6500, episode_reward=-181.10 +/- 42.20
Episode length: 182.10 +/- 42.20
Eval num_timesteps=7000, episode_reward=-246.03 +/- 115.83
Episode length: 246.87 +/- 115.48
Eval num_timesteps=7500, episode_reward=-178.55 +/- 37.58
Episode length: 179.55 +/- 37.58
Eval num_timesteps=8000, episode_reward=-183.28 +/- 37.26
Episode length: 184.28 +/- 37.26
Eval num_timesteps=8500, episode_reward=-175.45 +/- 32.77
Episode length: 176.45 +/- 32.77
Eval num_timesteps=9000, episode_reward=-182.99 +/- 50.75
Episode length: 183.98 +/- 50.68
Eval num_timesteps=9500, episode_reward=-175.44 +/- 34.91
Episode length: 176.44 +/- 34.91
Eval num_timesteps=10000, episode_reward=-189.48 +/- 62.57
Episode length: 190.46 +/- 62.47
Eval num_timesteps=10500, episode_reward=-180.26 +/- 56.46
Episode length: 181.24 +/- 56.34
Eval num_timesteps=11000, episode_reward=-172.38 +/- 36.94
Episode length: 173.38 +/- 36.94
Eval num_timesteps=11500, episode_reward=-138.63 +/- 34.45
Episode length: 139.63 +/- 34.45
Eval num_timesteps=12000, episode_reward=-118.49 +/- 50.77
Episode length: 119.49 +/- 50.77
Eval num_timesteps=12500, episode_reward=-164.30 +/- 143.42
Episode length: 165.15 +/- 143.07
Eval num_timesteps=13000, episode_reward=-209.35 +/- 160.64
Episode length: 210.13 +/- 160.24
Eval num_timesteps=13500, episode_reward=-236.05 +/- 184.13
Episode length: 236.74 +/- 183.68
Eval num_timesteps=14000, episode_reward=-241.29 +/- 186.93
Episode length: 241.95 +/- 186.46
Eval num_timesteps=14500, episode_reward=-289.65 +/- 189.99
Episode length: 290.22 +/- 189.51
Eval num_timesteps=15000, episode_reward=-400.84 +/- 167.44
Episode length: 401.10 +/- 167.00
Eval num_timesteps=15500, episode_reward=-371.30 +/- 187.74
Episode length: 371.62 +/- 187.27
Eval num_timesteps=16000, episode_reward=-380.77 +/- 182.52
Episode length: 381.07 +/- 182.06
Eval num_timesteps=16500, episode_reward=-292.74 +/- 204.74
Episode length: 293.25 +/- 204.24
Eval num_timesteps=17000, episode_reward=-247.84 +/- 195.63
Episode length: 248.47 +/- 195.15
Eval num_timesteps=17500, episode_reward=-180.35 +/- 164.78
Episode length: 181.15 +/- 164.40
Eval num_timesteps=18000, episode_reward=-155.21 +/- 147.81
Episode length: 156.06 +/- 147.46
Eval num_timesteps=18500, episode_reward=-145.59 +/- 142.26
Episode length: 146.46 +/- 141.93
Eval num_timesteps=19000, episode_reward=-163.30 +/- 155.00
Episode length: 164.15 +/- 154.67
Eval num_timesteps=19500, episode_reward=-141.88 +/- 136.36
Episode length: 142.77 +/- 136.07
Eval num_timesteps=20000, episode_reward=-116.87 +/- 108.09
Episode length: 117.80 +/- 107.84
Eval num_timesteps=20500, episode_reward=-114.36 +/- 107.66
Episode length: 115.29 +/- 107.41
Eval num_timesteps=21000, episode_reward=-127.46 +/- 125.60
Episode length: 128.36 +/- 125.31
Eval num_timesteps=21500, episode_reward=-117.87 +/- 101.07
Episode length: 118.81 +/- 100.85
Eval num_timesteps=22000, episode_reward=-109.88 +/- 91.12
Episode length: 110.84 +/- 90.94
New best mean reward!
Eval num_timesteps=22500, episode_reward=-156.85 +/- 156.16
Episode length: 157.68 +/- 155.78
Eval num_timesteps=23000, episode_reward=-99.87 +/- 74.20
Episode length: 100.84 +/- 74.04
New best mean reward!
FINISHED IN 432.703258743044 s


starting seed  10127 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-188.68 +/- 116.48
Episode length: 189.57 +/- 116.18
New best mean reward!
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-202.37 +/- 151.00
Episode length: 203.18 +/- 150.62
Eval num_timesteps=11500, episode_reward=-184.83 +/- 76.31
Episode length: 185.79 +/- 76.14
New best mean reward!
Eval num_timesteps=12000, episode_reward=-203.74 +/- 106.01
Episode length: 204.64 +/- 105.73
Eval num_timesteps=12500, episode_reward=-201.20 +/- 81.31
Episode length: 202.16 +/- 81.16
Eval num_timesteps=13000, episode_reward=-322.88 +/- 170.08
Episode length: 323.41 +/- 169.59
Eval num_timesteps=13500, episode_reward=-329.45 +/- 183.67
Episode length: 329.92 +/- 183.18
Eval num_timesteps=14000, episode_reward=-340.06 +/- 188.77
Episode length: 340.50 +/- 188.30
Eval num_timesteps=14500, episode_reward=-307.92 +/- 201.29
Episode length: 308.41 +/- 200.80
Eval num_timesteps=15000, episode_reward=-148.52 +/- 139.45
Episode length: 149.39 +/- 139.13
New best mean reward!
Eval num_timesteps=15500, episode_reward=-177.25 +/- 160.69
Episode length: 178.06 +/- 160.31
Eval num_timesteps=16000, episode_reward=-141.05 +/- 125.95
Episode length: 141.96 +/- 125.69
New best mean reward!
Eval num_timesteps=16500, episode_reward=-181.66 +/- 169.66
Episode length: 182.44 +/- 169.24
Eval num_timesteps=17000, episode_reward=-150.01 +/- 139.39
Episode length: 150.88 +/- 139.06
Eval num_timesteps=17500, episode_reward=-137.54 +/- 128.81
Episode length: 138.43 +/- 128.50
New best mean reward!
Eval num_timesteps=18000, episode_reward=-106.78 +/- 76.88
Episode length: 107.75 +/- 76.73
New best mean reward!
Eval num_timesteps=18500, episode_reward=-111.60 +/- 84.10
Episode length: 112.56 +/- 83.91
Eval num_timesteps=19000, episode_reward=-157.99 +/- 147.26
Episode length: 158.84 +/- 146.91
Eval num_timesteps=19500, episode_reward=-135.83 +/- 125.15
Episode length: 136.73 +/- 124.86
Eval num_timesteps=20000, episode_reward=-118.50 +/- 89.49
Episode length: 119.46 +/- 89.32
Eval num_timesteps=20500, episode_reward=-132.56 +/- 113.43
Episode length: 133.49 +/- 113.20
Eval num_timesteps=21000, episode_reward=-112.31 +/- 84.69
Episode length: 113.27 +/- 84.51
Eval num_timesteps=21500, episode_reward=-117.40 +/- 92.88
Episode length: 118.36 +/- 92.72
Eval num_timesteps=22000, episode_reward=-96.00 +/- 47.07
Episode length: 96.99 +/- 46.98
New best mean reward!
FINISHED IN 427.61305847298354 s


starting seed  10128 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-488.77 +/- 64.34
Episode length: 488.80 +/- 64.17
New best mean reward!
Eval num_timesteps=10500, episode_reward=-131.75 +/- 106.11
Episode length: 132.69 +/- 105.90
New best mean reward!
Eval num_timesteps=11000, episode_reward=-124.59 +/- 105.36
Episode length: 125.52 +/- 105.11
New best mean reward!
Eval num_timesteps=11500, episode_reward=-90.89 +/- 34.56
Episode length: 91.89 +/- 34.56
New best mean reward!
FINISHED IN 322.48022688901983 s


starting seed  10129 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-416.72 +/- 94.17
Episode length: 417.23 +/- 93.74
New best mean reward!
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-428.20 +/- 153.56
Episode length: 428.38 +/- 153.18
Eval num_timesteps=8500, episode_reward=-219.25 +/- 188.87
Episode length: 219.94 +/- 188.41
New best mean reward!
Eval num_timesteps=9000, episode_reward=-140.59 +/- 135.15
Episode length: 141.47 +/- 134.83
New best mean reward!
Eval num_timesteps=9500, episode_reward=-91.30 +/- 44.74
Episode length: 92.30 +/- 44.74
New best mean reward!
FINISHED IN 248.06638062099228 s


starting seed  10130 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-89.04 +/- 26.07
Episode length: 90.04 +/- 26.07
New best mean reward!
FINISHED IN 2.7894984040176496 s


starting seed  10131 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-392.78 +/- 111.41
Episode length: 393.36 +/- 111.00
New best mean reward!
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-222.21 +/- 177.56
Episode length: 222.93 +/- 177.12
New best mean reward!
Eval num_timesteps=6500, episode_reward=-418.78 +/- 149.61
Episode length: 419.01 +/- 149.19
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-352.12 +/- 189.80
Episode length: 352.50 +/- 189.32
Eval num_timesteps=8000, episode_reward=-95.32 +/- 48.71
Episode length: 96.31 +/- 48.62
New best mean reward!
FINISHED IN 211.56926966499304 s


starting seed  10132 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-165.21 +/- 88.16
Episode length: 166.15 +/- 87.93
New best mean reward!
Eval num_timesteps=4500, episode_reward=-480.41 +/- 85.59
Episode length: 480.46 +/- 85.37
Eval num_timesteps=5000, episode_reward=-111.43 +/- 95.77
Episode length: 112.38 +/- 95.57
New best mean reward!
Eval num_timesteps=5500, episode_reward=-484.89 +/- 47.48
Episode length: 485.02 +/- 47.21
Eval num_timesteps=6000, episode_reward=-139.05 +/- 47.78
Episode length: 140.04 +/- 47.70
Eval num_timesteps=6500, episode_reward=-269.23 +/- 70.42
Episode length: 270.21 +/- 70.36
Eval num_timesteps=7000, episode_reward=-150.71 +/- 53.55
Episode length: 151.70 +/- 53.48
Eval num_timesteps=7500, episode_reward=-134.79 +/- 26.67
Episode length: 135.79 +/- 26.67
Eval num_timesteps=8000, episode_reward=-155.35 +/- 32.06
Episode length: 156.35 +/- 32.06
Eval num_timesteps=8500, episode_reward=-158.45 +/- 50.62
Episode length: 159.44 +/- 50.56
Eval num_timesteps=9000, episode_reward=-156.90 +/- 47.34
Episode length: 157.90 +/- 47.34
Eval num_timesteps=9500, episode_reward=-175.88 +/- 77.58
Episode length: 176.85 +/- 77.46
Eval num_timesteps=10000, episode_reward=-157.46 +/- 34.53
Episode length: 158.46 +/- 34.53
Eval num_timesteps=10500, episode_reward=-168.03 +/- 54.72
Episode length: 169.03 +/- 54.72
Eval num_timesteps=11000, episode_reward=-143.88 +/- 44.81
Episode length: 144.88 +/- 44.81
Eval num_timesteps=11500, episode_reward=-133.25 +/- 27.37
Episode length: 134.25 +/- 27.37
Eval num_timesteps=12000, episode_reward=-115.48 +/- 30.25
Episode length: 116.48 +/- 30.25
Eval num_timesteps=12500, episode_reward=-136.03 +/- 31.32
Episode length: 137.03 +/- 31.32
Eval num_timesteps=13000, episode_reward=-129.35 +/- 28.94
Episode length: 130.35 +/- 28.94
Eval num_timesteps=13500, episode_reward=-121.70 +/- 19.31
Episode length: 122.70 +/- 19.31
Eval num_timesteps=14000, episode_reward=-153.50 +/- 85.24
Episode length: 154.45 +/- 85.04
Eval num_timesteps=14500, episode_reward=-122.56 +/- 81.54
Episode length: 123.52 +/- 81.36
Eval num_timesteps=15000, episode_reward=-123.71 +/- 112.42
Episode length: 124.63 +/- 112.15
Eval num_timesteps=15500, episode_reward=-95.08 +/- 30.14
Episode length: 96.08 +/- 30.14
New best mean reward!
FINISHED IN 274.01936439599376 s


starting seed  10133 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-366.05 +/- 135.12
Episode length: 366.63 +/- 134.71
New best mean reward!
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-493.79 +/- 30.91
Episode length: 493.85 +/- 30.72
Eval num_timesteps=8000, episode_reward=-498.81 +/- 7.49
Episode length: 498.86 +/- 7.34
Eval num_timesteps=8500, episode_reward=-430.11 +/- 100.37
Episode length: 430.51 +/- 99.96
Eval num_timesteps=9000, episode_reward=-171.35 +/- 153.04
Episode length: 172.18 +/- 152.67
New best mean reward!
Eval num_timesteps=9500, episode_reward=-172.77 +/- 159.88
Episode length: 173.58 +/- 159.49
Eval num_timesteps=10000, episode_reward=-156.78 +/- 145.96
Episode length: 157.63 +/- 145.61
New best mean reward!
Eval num_timesteps=10500, episode_reward=-165.01 +/- 146.72
Episode length: 165.86 +/- 146.38
Eval num_timesteps=11000, episode_reward=-206.32 +/- 181.73
Episode length: 207.05 +/- 181.29
Eval num_timesteps=11500, episode_reward=-165.48 +/- 150.95
Episode length: 166.32 +/- 150.60
Eval num_timesteps=12000, episode_reward=-152.12 +/- 144.01
Episode length: 152.98 +/- 143.67
New best mean reward!
Eval num_timesteps=12500, episode_reward=-163.66 +/- 151.19
Episode length: 164.50 +/- 150.84
Eval num_timesteps=13000, episode_reward=-110.10 +/- 84.53
Episode length: 111.06 +/- 84.34
New best mean reward!
Eval num_timesteps=13500, episode_reward=-87.03 +/- 25.51
Episode length: 88.03 +/- 25.51
New best mean reward!
FINISHED IN 258.2024137590197 s


starting seed  10134 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-461.84 +/- 118.00
Episode length: 461.94 +/- 117.70
New best mean reward!
Eval num_timesteps=16000, episode_reward=-352.48 +/- 196.93
Episode length: 352.84 +/- 196.45
New best mean reward!
Eval num_timesteps=16500, episode_reward=-306.72 +/- 206.08
Episode length: 307.19 +/- 205.58
New best mean reward!
Eval num_timesteps=17000, episode_reward=-329.38 +/- 204.98
Episode length: 329.79 +/- 204.48
Eval num_timesteps=17500, episode_reward=-303.14 +/- 207.07
Episode length: 303.62 +/- 206.58
New best mean reward!
Eval num_timesteps=18000, episode_reward=-314.70 +/- 203.36
Episode length: 315.16 +/- 202.86
Eval num_timesteps=18500, episode_reward=-250.32 +/- 204.07
Episode length: 250.92 +/- 203.58
New best mean reward!
Eval num_timesteps=19000, episode_reward=-212.63 +/- 189.00
Episode length: 213.33 +/- 188.54
New best mean reward!
Eval num_timesteps=19500, episode_reward=-206.79 +/- 185.27
Episode length: 207.51 +/- 184.83
New best mean reward!
Eval num_timesteps=20000, episode_reward=-239.67 +/- 196.73
Episode length: 240.31 +/- 196.25
Eval num_timesteps=20500, episode_reward=-197.97 +/- 178.70
Episode length: 198.72 +/- 178.28
New best mean reward!
Eval num_timesteps=21000, episode_reward=-220.20 +/- 189.39
Episode length: 220.90 +/- 188.95
Eval num_timesteps=21500, episode_reward=-203.32 +/- 181.88
Episode length: 204.06 +/- 181.45
Eval num_timesteps=22000, episode_reward=-170.83 +/- 162.43
Episode length: 171.65 +/- 162.07
New best mean reward!
Eval num_timesteps=22500, episode_reward=-133.90 +/- 133.89
Episode length: 134.79 +/- 133.59
New best mean reward!
Eval num_timesteps=23000, episode_reward=-119.09 +/- 116.11
Episode length: 120.01 +/- 115.85
New best mean reward!
Eval num_timesteps=23500, episode_reward=-111.00 +/- 100.04
Episode length: 111.94 +/- 99.81
New best mean reward!
Eval num_timesteps=24000, episode_reward=-119.42 +/- 114.32
Episode length: 120.34 +/- 114.06
Eval num_timesteps=24500, episode_reward=-126.36 +/- 126.75
Episode length: 127.26 +/- 126.46
Eval num_timesteps=25000, episode_reward=-91.69 +/- 61.41
Episode length: 92.67 +/- 61.28
New best mean reward!
FINISHED IN 684.0066544479923 s


starting seed  10135 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-176.51 +/- 158.45
Episode length: 177.33 +/- 158.08
New best mean reward!
Eval num_timesteps=13500, episode_reward=-148.39 +/- 132.74
Episode length: 149.27 +/- 132.43
New best mean reward!
Eval num_timesteps=14000, episode_reward=-415.54 +/- 159.46
Episode length: 415.76 +/- 159.05
Eval num_timesteps=14500, episode_reward=-159.89 +/- 140.36
Episode length: 160.76 +/- 140.05
Eval num_timesteps=15000, episode_reward=-115.26 +/- 90.34
Episode length: 116.22 +/- 90.17
New best mean reward!
Eval num_timesteps=15500, episode_reward=-184.99 +/- 159.69
Episode length: 185.79 +/- 159.30
Eval num_timesteps=16000, episode_reward=-192.49 +/- 158.37
Episode length: 193.30 +/- 158.00
Eval num_timesteps=16500, episode_reward=-190.25 +/- 159.98
Episode length: 191.05 +/- 159.59
Eval num_timesteps=17000, episode_reward=-142.55 +/- 104.55
Episode length: 143.48 +/- 104.31
Eval num_timesteps=17500, episode_reward=-219.61 +/- 168.29
Episode length: 220.35 +/- 167.86
Eval num_timesteps=18000, episode_reward=-264.92 +/- 183.26
Episode length: 265.55 +/- 182.79
Eval num_timesteps=18500, episode_reward=-343.97 +/- 188.39
Episode length: 344.38 +/- 187.90
Eval num_timesteps=19000, episode_reward=-228.99 +/- 168.28
Episode length: 229.72 +/- 167.84
Eval num_timesteps=19500, episode_reward=-179.77 +/- 139.76
Episode length: 180.62 +/- 139.42
Eval num_timesteps=20000, episode_reward=-100.06 +/- 46.80
Episode length: 101.05 +/- 46.71
New best mean reward!
Eval num_timesteps=20500, episode_reward=-107.31 +/- 73.84
Episode length: 108.28 +/- 73.68
Eval num_timesteps=21000, episode_reward=-103.41 +/- 68.28
Episode length: 104.39 +/- 68.16
Eval num_timesteps=21500, episode_reward=-92.44 +/- 24.26
Episode length: 93.44 +/- 24.26
New best mean reward!
FINISHED IN 456.5464075669879 s


starting seed  10136 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-444.78 +/- 118.93
Episode length: 444.96 +/- 118.55
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-462.14 +/- 64.06
Episode length: 462.42 +/- 63.64
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=17000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=17500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=18000, episode_reward=-488.32 +/- 66.45
Episode length: 488.35 +/- 66.28
Eval num_timesteps=18500, episode_reward=-440.46 +/- 142.01
Episode length: 440.61 +/- 141.65
New best mean reward!
Eval num_timesteps=19000, episode_reward=-317.78 +/- 197.95
Episode length: 318.24 +/- 197.45
New best mean reward!
Eval num_timesteps=19500, episode_reward=-297.00 +/- 195.87
Episode length: 297.53 +/- 195.39
New best mean reward!
Eval num_timesteps=20000, episode_reward=-218.04 +/- 180.09
Episode length: 218.77 +/- 179.67
New best mean reward!
Eval num_timesteps=20500, episode_reward=-256.01 +/- 195.79
Episode length: 256.62 +/- 195.30
Eval num_timesteps=21000, episode_reward=-236.56 +/- 186.83
Episode length: 237.23 +/- 186.36
Eval num_timesteps=21500, episode_reward=-219.79 +/- 179.88
Episode length: 220.51 +/- 179.45
Eval num_timesteps=22000, episode_reward=-315.94 +/- 197.43
Episode length: 316.41 +/- 196.94
Eval num_timesteps=22500, episode_reward=-319.90 +/- 196.19
Episode length: 320.36 +/- 195.69
Eval num_timesteps=23000, episode_reward=-278.71 +/- 194.44
Episode length: 279.29 +/- 193.96
Eval num_timesteps=23500, episode_reward=-224.21 +/- 186.40
Episode length: 224.90 +/- 185.94
Eval num_timesteps=24000, episode_reward=-176.45 +/- 162.39
Episode length: 177.26 +/- 162.02
New best mean reward!
Eval num_timesteps=24500, episode_reward=-205.28 +/- 175.85
Episode length: 206.03 +/- 175.43
Eval num_timesteps=25000, episode_reward=-165.88 +/- 149.16
Episode length: 166.72 +/- 148.80
New best mean reward!
Eval num_timesteps=25500, episode_reward=-176.77 +/- 162.78
Episode length: 177.57 +/- 162.38
Eval num_timesteps=26000, episode_reward=-155.46 +/- 146.98
Episode length: 156.31 +/- 146.63
New best mean reward!
Eval num_timesteps=26500, episode_reward=-138.50 +/- 129.07
Episode length: 139.39 +/- 128.77
New best mean reward!
Eval num_timesteps=27000, episode_reward=-125.41 +/- 108.84
Episode length: 126.34 +/- 108.59
New best mean reward!
Eval num_timesteps=27500, episode_reward=-109.94 +/- 66.28
Episode length: 110.92 +/- 66.17
New best mean reward!
Eval num_timesteps=28000, episode_reward=-139.74 +/- 131.62
Episode length: 140.63 +/- 131.32
Eval num_timesteps=28500, episode_reward=-164.52 +/- 150.60
Episode length: 165.36 +/- 150.25
Eval num_timesteps=29000, episode_reward=-150.38 +/- 139.30
Episode length: 151.25 +/- 138.97
Eval num_timesteps=29500, episode_reward=-135.24 +/- 119.83
Episode length: 136.15 +/- 119.55
Eval num_timesteps=30000, episode_reward=-110.31 +/- 65.50
Episode length: 111.29 +/- 65.38
Eval num_timesteps=30500, episode_reward=-116.20 +/- 91.44
Episode length: 117.15 +/- 91.23
Eval num_timesteps=31000, episode_reward=-105.83 +/- 82.71
Episode length: 106.79 +/- 82.52
New best mean reward!
Eval num_timesteps=31500, episode_reward=-119.72 +/- 103.20
Episode length: 120.66 +/- 102.98
Eval num_timesteps=32000, episode_reward=-132.79 +/- 124.94
Episode length: 133.69 +/- 124.65
Eval num_timesteps=32500, episode_reward=-101.46 +/- 73.05
Episode length: 102.43 +/- 72.88
New best mean reward!
Eval num_timesteps=33000, episode_reward=-117.55 +/- 104.94
Episode length: 118.49 +/- 104.72
Eval num_timesteps=33500, episode_reward=-89.14 +/- 24.58
Episode length: 90.14 +/- 24.58
New best mean reward!
FINISHED IN 837.195876867976 s


starting seed  10137 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-277.44 +/- 201.68
Episode length: 277.99 +/- 201.19
New best mean reward!
Eval num_timesteps=10500, episode_reward=-110.50 +/- 72.39
Episode length: 111.47 +/- 72.23
New best mean reward!
Eval num_timesteps=11000, episode_reward=-107.45 +/- 76.08
Episode length: 108.42 +/- 75.93
New best mean reward!
Eval num_timesteps=11500, episode_reward=-98.62 +/- 29.83
Episode length: 99.62 +/- 29.83
New best mean reward!
FINISHED IN 268.8349809939973 s


starting seed  10138 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-419.58 +/- 88.26
Episode length: 420.31 +/- 88.01
New best mean reward!
Eval num_timesteps=8500, episode_reward=-375.15 +/- 70.17
Episode length: 376.02 +/- 69.94
New best mean reward!
Eval num_timesteps=9000, episode_reward=-180.65 +/- 53.02
Episode length: 181.64 +/- 52.96
New best mean reward!
Eval num_timesteps=9500, episode_reward=-389.27 +/- 159.15
Episode length: 389.60 +/- 158.69
Eval num_timesteps=10000, episode_reward=-435.07 +/- 131.50
Episode length: 435.27 +/- 131.11
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-408.04 +/- 152.23
Episode length: 408.31 +/- 151.79
Eval num_timesteps=12000, episode_reward=-194.62 +/- 97.65
Episode length: 195.54 +/- 97.40
Eval num_timesteps=12500, episode_reward=-165.86 +/- 37.75
Episode length: 166.86 +/- 37.75
New best mean reward!
Eval num_timesteps=13000, episode_reward=-122.50 +/- 29.77
Episode length: 123.50 +/- 29.77
New best mean reward!
Eval num_timesteps=13500, episode_reward=-123.55 +/- 22.50
Episode length: 124.55 +/- 22.50
Eval num_timesteps=14000, episode_reward=-119.98 +/- 26.87
Episode length: 120.98 +/- 26.87
New best mean reward!
Eval num_timesteps=14500, episode_reward=-147.06 +/- 21.74
Episode length: 148.06 +/- 21.74
Eval num_timesteps=15000, episode_reward=-151.69 +/- 21.36
Episode length: 152.69 +/- 21.36
Eval num_timesteps=15500, episode_reward=-139.88 +/- 24.05
Episode length: 140.88 +/- 24.05
Eval num_timesteps=16000, episode_reward=-151.98 +/- 26.56
Episode length: 152.98 +/- 26.56
Eval num_timesteps=16500, episode_reward=-148.87 +/- 29.23
Episode length: 149.87 +/- 29.23
Eval num_timesteps=17000, episode_reward=-148.42 +/- 22.73
Episode length: 149.42 +/- 22.73
Eval num_timesteps=17500, episode_reward=-150.57 +/- 30.80
Episode length: 151.57 +/- 30.80
Eval num_timesteps=18000, episode_reward=-167.41 +/- 60.94
Episode length: 168.39 +/- 60.84
Eval num_timesteps=18500, episode_reward=-151.32 +/- 32.67
Episode length: 152.32 +/- 32.67
Eval num_timesteps=19000, episode_reward=-146.09 +/- 86.31
Episode length: 147.04 +/- 86.11
Eval num_timesteps=19500, episode_reward=-142.13 +/- 83.08
Episode length: 143.10 +/- 82.95
Eval num_timesteps=20000, episode_reward=-178.77 +/- 107.40
Episode length: 179.68 +/- 107.13
Eval num_timesteps=20500, episode_reward=-194.73 +/- 123.29
Episode length: 195.60 +/- 122.96
Eval num_timesteps=21000, episode_reward=-161.54 +/- 106.83
Episode length: 162.46 +/- 106.58
Eval num_timesteps=21500, episode_reward=-235.21 +/- 137.16
Episode length: 236.02 +/- 136.79
Eval num_timesteps=22000, episode_reward=-225.36 +/- 132.99
Episode length: 226.18 +/- 132.62
Eval num_timesteps=22500, episode_reward=-257.73 +/- 153.06
Episode length: 258.45 +/- 152.62
Eval num_timesteps=23000, episode_reward=-227.42 +/- 132.27
Episode length: 228.24 +/- 131.90
Eval num_timesteps=23500, episode_reward=-224.52 +/- 158.75
Episode length: 225.28 +/- 158.33
Eval num_timesteps=24000, episode_reward=-180.05 +/- 122.06
Episode length: 180.93 +/- 121.75
Eval num_timesteps=24500, episode_reward=-186.46 +/- 143.65
Episode length: 187.29 +/- 143.28
Eval num_timesteps=25000, episode_reward=-157.70 +/- 122.37
Episode length: 158.59 +/- 122.06
Eval num_timesteps=25500, episode_reward=-134.31 +/- 96.64
Episode length: 135.25 +/- 96.41
Eval num_timesteps=26000, episode_reward=-114.78 +/- 45.39
Episode length: 115.77 +/- 45.30
New best mean reward!
Eval num_timesteps=26500, episode_reward=-121.21 +/- 69.45
Episode length: 122.19 +/- 69.34
Eval num_timesteps=27000, episode_reward=-101.23 +/- 24.59
Episode length: 102.23 +/- 24.59
New best mean reward!
Eval num_timesteps=27500, episode_reward=-105.14 +/- 35.39
Episode length: 106.14 +/- 35.39
Eval num_timesteps=28000, episode_reward=-104.78 +/- 76.61
Episode length: 105.75 +/- 76.45
Eval num_timesteps=28500, episode_reward=-89.06 +/- 33.00
Episode length: 90.06 +/- 33.00
New best mean reward!
FINISHED IN 480.8359040739597 s


starting seed  10139 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-203.64 +/- 181.96
Episode length: 204.37 +/- 181.52
New best mean reward!
Eval num_timesteps=9500, episode_reward=-319.99 +/- 200.57
Episode length: 320.44 +/- 200.07
Eval num_timesteps=10000, episode_reward=-340.07 +/- 201.37
Episode length: 340.46 +/- 200.89
Eval num_timesteps=10500, episode_reward=-414.42 +/- 166.11
Episode length: 414.63 +/- 165.71
Eval num_timesteps=11000, episode_reward=-89.00 +/- 23.19
Episode length: 90.00 +/- 23.19
New best mean reward!
FINISHED IN 260.7079189189826 s


starting seed  10140 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-386.73 +/- 182.09
Episode length: 387.01 +/- 181.64
New best mean reward!
Eval num_timesteps=8500, episode_reward=-129.85 +/- 125.31
Episode length: 130.75 +/- 125.02
New best mean reward!
Eval num_timesteps=9000, episode_reward=-87.53 +/- 23.05
Episode length: 88.53 +/- 23.05
New best mean reward!
FINISHED IN 193.3640396670089 s


starting seed  10141 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-91.44 +/- 19.09
Episode length: 92.44 +/- 19.09
New best mean reward!
FINISHED IN 186.68677511101123 s


starting seed  10142 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-496.97 +/- 30.15
Episode length: 496.98 +/- 30.05
New best mean reward!
Eval num_timesteps=1000, episode_reward=-313.53 +/- 150.54
Episode length: 314.15 +/- 150.07
New best mean reward!
Eval num_timesteps=1500, episode_reward=-499.04 +/- 9.55
Episode length: 499.05 +/- 9.45
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-455.07 +/- 81.51
Episode length: 455.38 +/- 81.13
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-183.39 +/- 40.37
Episode length: 184.39 +/- 40.37
New best mean reward!
Eval num_timesteps=6500, episode_reward=-184.33 +/- 51.95
Episode length: 185.33 +/- 51.95
Eval num_timesteps=7000, episode_reward=-175.15 +/- 50.47
Episode length: 176.14 +/- 50.40
New best mean reward!
Eval num_timesteps=7500, episode_reward=-167.18 +/- 37.05
Episode length: 168.18 +/- 37.05
New best mean reward!
Eval num_timesteps=8000, episode_reward=-127.56 +/- 38.18
Episode length: 128.56 +/- 38.18
New best mean reward!
Eval num_timesteps=8500, episode_reward=-120.23 +/- 23.30
Episode length: 121.23 +/- 23.30
New best mean reward!
Eval num_timesteps=9000, episode_reward=-114.91 +/- 63.22
Episode length: 115.89 +/- 63.10
New best mean reward!
Eval num_timesteps=9500, episode_reward=-116.55 +/- 83.07
Episode length: 117.51 +/- 82.88
Eval num_timesteps=10000, episode_reward=-111.76 +/- 62.69
Episode length: 112.74 +/- 62.56
New best mean reward!
Eval num_timesteps=10500, episode_reward=-107.85 +/- 68.37
Episode length: 108.83 +/- 68.26
New best mean reward!
Eval num_timesteps=11000, episode_reward=-97.44 +/- 36.03
Episode length: 98.44 +/- 36.03
New best mean reward!
FINISHED IN 177.2284132920322 s


starting seed  10143 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-254.09 +/- 177.54
Episode length: 254.79 +/- 177.12
New best mean reward!
Eval num_timesteps=6500, episode_reward=-108.35 +/- 46.34
Episode length: 109.35 +/- 46.34
New best mean reward!
Eval num_timesteps=7000, episode_reward=-106.82 +/- 28.95
Episode length: 107.82 +/- 28.95
New best mean reward!
Eval num_timesteps=7500, episode_reward=-161.20 +/- 122.30
Episode length: 162.09 +/- 122.00
Eval num_timesteps=8000, episode_reward=-246.65 +/- 179.65
Episode length: 247.32 +/- 179.19
Eval num_timesteps=8500, episode_reward=-277.16 +/- 188.83
Episode length: 277.75 +/- 188.34
Eval num_timesteps=9000, episode_reward=-252.34 +/- 172.94
Episode length: 253.02 +/- 172.48
Eval num_timesteps=9500, episode_reward=-116.19 +/- 22.07
Episode length: 117.19 +/- 22.07
Eval num_timesteps=10000, episode_reward=-106.54 +/- 32.73
Episode length: 107.54 +/- 32.73
New best mean reward!
Eval num_timesteps=10500, episode_reward=-93.12 +/- 29.71
Episode length: 94.12 +/- 29.71
New best mean reward!
FINISHED IN 192.987398494035 s


starting seed  10144 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-492.79 +/- 50.47
Episode length: 492.81 +/- 50.33
New best mean reward!
Eval num_timesteps=8000, episode_reward=-409.95 +/- 160.50
Episode length: 410.19 +/- 160.07
New best mean reward!
Eval num_timesteps=8500, episode_reward=-457.70 +/- 114.66
Episode length: 457.82 +/- 114.34
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-468.21 +/- 107.89
Episode length: 468.29 +/- 107.62
Eval num_timesteps=11500, episode_reward=-374.69 +/- 187.59
Episode length: 375.00 +/- 187.13
New best mean reward!
Eval num_timesteps=12000, episode_reward=-307.79 +/- 201.32
Episode length: 308.28 +/- 200.84
New best mean reward!
Eval num_timesteps=12500, episode_reward=-342.05 +/- 194.54
Episode length: 342.46 +/- 194.06
Eval num_timesteps=13000, episode_reward=-280.74 +/- 197.72
Episode length: 281.30 +/- 197.23
New best mean reward!
Eval num_timesteps=13500, episode_reward=-337.80 +/- 199.21
Episode length: 338.20 +/- 198.72
Eval num_timesteps=14000, episode_reward=-208.03 +/- 177.68
Episode length: 208.77 +/- 177.25
New best mean reward!
Eval num_timesteps=14500, episode_reward=-241.45 +/- 191.74
Episode length: 242.10 +/- 191.27
Eval num_timesteps=15000, episode_reward=-214.04 +/- 178.51
Episode length: 214.77 +/- 178.08
Eval num_timesteps=15500, episode_reward=-320.75 +/- 195.77
Episode length: 321.21 +/- 195.27
Eval num_timesteps=16000, episode_reward=-316.17 +/- 199.99
Episode length: 316.64 +/- 199.51
Eval num_timesteps=16500, episode_reward=-321.36 +/- 195.99
Episode length: 321.82 +/- 195.50
Eval num_timesteps=17000, episode_reward=-285.43 +/- 195.38
Episode length: 285.99 +/- 194.89
Eval num_timesteps=17500, episode_reward=-283.78 +/- 199.52
Episode length: 284.33 +/- 199.03
Eval num_timesteps=18000, episode_reward=-296.45 +/- 196.95
Episode length: 296.99 +/- 196.47
Eval num_timesteps=18500, episode_reward=-223.88 +/- 182.91
Episode length: 224.59 +/- 182.47
Eval num_timesteps=19000, episode_reward=-216.75 +/- 181.30
Episode length: 217.47 +/- 180.87
Eval num_timesteps=19500, episode_reward=-214.03 +/- 183.03
Episode length: 214.75 +/- 182.59
Eval num_timesteps=20000, episode_reward=-219.96 +/- 179.01
Episode length: 220.69 +/- 178.59
Eval num_timesteps=20500, episode_reward=-217.79 +/- 181.49
Episode length: 218.51 +/- 181.06
Eval num_timesteps=21000, episode_reward=-180.85 +/- 160.80
Episode length: 181.66 +/- 160.42
New best mean reward!
Eval num_timesteps=21500, episode_reward=-193.60 +/- 165.27
Episode length: 194.39 +/- 164.88
Eval num_timesteps=22000, episode_reward=-196.18 +/- 169.72
Episode length: 196.95 +/- 169.31
Eval num_timesteps=22500, episode_reward=-180.35 +/- 162.07
Episode length: 181.15 +/- 161.67
New best mean reward!
Eval num_timesteps=23000, episode_reward=-184.89 +/- 164.63
Episode length: 185.68 +/- 164.23
Eval num_timesteps=23500, episode_reward=-223.12 +/- 183.58
Episode length: 223.83 +/- 183.14
Eval num_timesteps=24000, episode_reward=-220.74 +/- 182.76
Episode length: 221.45 +/- 182.31
Eval num_timesteps=24500, episode_reward=-195.40 +/- 166.22
Episode length: 196.18 +/- 165.81
Eval num_timesteps=25000, episode_reward=-164.36 +/- 142.74
Episode length: 165.22 +/- 142.41
New best mean reward!
Eval num_timesteps=25500, episode_reward=-133.13 +/- 114.28
Episode length: 134.05 +/- 114.03
New best mean reward!
Eval num_timesteps=26000, episode_reward=-167.41 +/- 154.94
Episode length: 168.25 +/- 154.60
Eval num_timesteps=26500, episode_reward=-198.49 +/- 173.85
Episode length: 199.26 +/- 173.45
Eval num_timesteps=27000, episode_reward=-197.42 +/- 169.02
Episode length: 198.20 +/- 168.63
Eval num_timesteps=27500, episode_reward=-226.80 +/- 185.64
Episode length: 227.50 +/- 185.20
Eval num_timesteps=28000, episode_reward=-199.02 +/- 174.30
Episode length: 199.78 +/- 173.89
Eval num_timesteps=28500, episode_reward=-165.97 +/- 149.97
Episode length: 166.81 +/- 149.62
Eval num_timesteps=29000, episode_reward=-162.08 +/- 137.83
Episode length: 162.96 +/- 137.54
Eval num_timesteps=29500, episode_reward=-149.97 +/- 141.79
Episode length: 150.84 +/- 141.47
Eval num_timesteps=30000, episode_reward=-175.77 +/- 158.78
Episode length: 176.59 +/- 158.41
Eval num_timesteps=30500, episode_reward=-141.30 +/- 129.98
Episode length: 142.19 +/- 129.68
Eval num_timesteps=31000, episode_reward=-154.62 +/- 142.20
Episode length: 155.49 +/- 141.88
Eval num_timesteps=31500, episode_reward=-140.66 +/- 131.19
Episode length: 141.55 +/- 130.88
Eval num_timesteps=32000, episode_reward=-137.01 +/- 115.18
Episode length: 137.95 +/- 114.99
Eval num_timesteps=32500, episode_reward=-157.15 +/- 142.54
Episode length: 158.01 +/- 142.20
Eval num_timesteps=33000, episode_reward=-122.72 +/- 92.71
Episode length: 123.68 +/- 92.54
New best mean reward!
Eval num_timesteps=33500, episode_reward=-104.52 +/- 59.46
Episode length: 105.51 +/- 59.39
New best mean reward!
Eval num_timesteps=34000, episode_reward=-125.96 +/- 107.10
Episode length: 126.89 +/- 106.85
Eval num_timesteps=34500, episode_reward=-148.84 +/- 137.07
Episode length: 149.72 +/- 136.76
Eval num_timesteps=35000, episode_reward=-126.33 +/- 113.62
Episode length: 127.25 +/- 113.36
Eval num_timesteps=35500, episode_reward=-145.71 +/- 133.21
Episode length: 146.60 +/- 132.92
Eval num_timesteps=36000, episode_reward=-139.56 +/- 124.88
Episode length: 140.46 +/- 124.59
Eval num_timesteps=36500, episode_reward=-147.47 +/- 137.90
Episode length: 148.36 +/- 137.62
Eval num_timesteps=37000, episode_reward=-143.98 +/- 124.19
Episode length: 144.88 +/- 123.90
Eval num_timesteps=37500, episode_reward=-128.10 +/- 118.95
Episode length: 129.01 +/- 118.67
Eval num_timesteps=38000, episode_reward=-116.82 +/- 101.89
Episode length: 117.76 +/- 101.66
Eval num_timesteps=38500, episode_reward=-144.51 +/- 130.73
Episode length: 145.40 +/- 130.43
Eval num_timesteps=39000, episode_reward=-141.78 +/- 124.17
Episode length: 142.69 +/- 123.91
Eval num_timesteps=39500, episode_reward=-153.94 +/- 143.28
Episode length: 154.80 +/- 142.95
Eval num_timesteps=40000, episode_reward=-167.60 +/- 155.33
Episode length: 168.43 +/- 154.97
Eval num_timesteps=40500, episode_reward=-163.88 +/- 151.18
Episode length: 164.72 +/- 150.82
Eval num_timesteps=41000, episode_reward=-154.36 +/- 139.32
Episode length: 155.23 +/- 139.00
Eval num_timesteps=41500, episode_reward=-186.48 +/- 163.12
Episode length: 187.28 +/- 162.73
Eval num_timesteps=42000, episode_reward=-151.32 +/- 139.10
Episode length: 152.19 +/- 138.77
Eval num_timesteps=42500, episode_reward=-183.18 +/- 168.34
Episode length: 183.98 +/- 167.96
Eval num_timesteps=43000, episode_reward=-162.17 +/- 155.49
Episode length: 163.00 +/- 155.12
Eval num_timesteps=43500, episode_reward=-163.37 +/- 152.62
Episode length: 164.21 +/- 152.27
Eval num_timesteps=44000, episode_reward=-180.24 +/- 166.41
Episode length: 181.03 +/- 166.01
Eval num_timesteps=44500, episode_reward=-192.30 +/- 169.16
Episode length: 193.08 +/- 168.76
Eval num_timesteps=45000, episode_reward=-165.23 +/- 159.75
Episode length: 166.06 +/- 159.39
Eval num_timesteps=45500, episode_reward=-169.76 +/- 153.09
Episode length: 170.59 +/- 152.73
Eval num_timesteps=46000, episode_reward=-144.73 +/- 136.26
Episode length: 145.61 +/- 135.95
Eval num_timesteps=46500, episode_reward=-166.95 +/- 153.29
Episode length: 167.80 +/- 152.97
Eval num_timesteps=47000, episode_reward=-142.91 +/- 132.52
Episode length: 143.80 +/- 132.22
Eval num_timesteps=47500, episode_reward=-177.36 +/- 163.99
Episode length: 178.18 +/- 163.64
Eval num_timesteps=48000, episode_reward=-155.81 +/- 152.59
Episode length: 156.65 +/- 152.23
Eval num_timesteps=48500, episode_reward=-158.34 +/- 151.81
Episode length: 159.18 +/- 151.45
Eval num_timesteps=49000, episode_reward=-158.78 +/- 143.73
Episode length: 159.64 +/- 143.40
Eval num_timesteps=49500, episode_reward=-123.53 +/- 109.05
Episode length: 124.46 +/- 108.81
Eval num_timesteps=50000, episode_reward=-127.58 +/- 110.19
Episode length: 128.51 +/- 109.96
FINISHED IN 845.9867854969925 s


starting seed  10145 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-476.66 +/- 85.25
Episode length: 476.73 +/- 84.99
New best mean reward!
Eval num_timesteps=7500, episode_reward=-114.74 +/- 36.01
Episode length: 115.74 +/- 36.01
New best mean reward!
Eval num_timesteps=8000, episode_reward=-114.56 +/- 50.03
Episode length: 115.55 +/- 49.96
New best mean reward!
Eval num_timesteps=8500, episode_reward=-113.12 +/- 35.85
Episode length: 114.12 +/- 35.85
New best mean reward!
Eval num_timesteps=9000, episode_reward=-119.36 +/- 37.06
Episode length: 120.36 +/- 37.06
Eval num_timesteps=9500, episode_reward=-108.62 +/- 40.50
Episode length: 109.62 +/- 40.50
New best mean reward!
Eval num_timesteps=10000, episode_reward=-116.92 +/- 28.13
Episode length: 117.92 +/- 28.13
Eval num_timesteps=10500, episode_reward=-114.72 +/- 54.56
Episode length: 115.71 +/- 54.49
Eval num_timesteps=11000, episode_reward=-108.22 +/- 49.67
Episode length: 109.21 +/- 49.59
New best mean reward!
Eval num_timesteps=11500, episode_reward=-108.80 +/- 42.97
Episode length: 109.80 +/- 42.97
Eval num_timesteps=12000, episode_reward=-103.91 +/- 48.50
Episode length: 104.90 +/- 48.42
New best mean reward!
Eval num_timesteps=12500, episode_reward=-95.88 +/- 25.61
Episode length: 96.88 +/- 25.61
New best mean reward!
FINISHED IN 239.19053157797316 s


starting seed  10146 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-159.98 +/- 33.68
Episode length: 160.98 +/- 33.68
New best mean reward!
Eval num_timesteps=10500, episode_reward=-150.23 +/- 31.08
Episode length: 151.23 +/- 31.08
New best mean reward!
Eval num_timesteps=11000, episode_reward=-153.29 +/- 31.50
Episode length: 154.29 +/- 31.50
Eval num_timesteps=11500, episode_reward=-152.07 +/- 42.98
Episode length: 153.06 +/- 42.90
Eval num_timesteps=12000, episode_reward=-151.59 +/- 42.62
Episode length: 152.59 +/- 42.62
Eval num_timesteps=12500, episode_reward=-224.54 +/- 151.99
Episode length: 225.32 +/- 151.59
Eval num_timesteps=13000, episode_reward=-140.57 +/- 29.15
Episode length: 141.57 +/- 29.15
New best mean reward!
Eval num_timesteps=13500, episode_reward=-139.30 +/- 37.22
Episode length: 140.30 +/- 37.22
New best mean reward!
Eval num_timesteps=14000, episode_reward=-132.19 +/- 60.08
Episode length: 133.18 +/- 60.02
New best mean reward!
Eval num_timesteps=14500, episode_reward=-141.73 +/- 65.45
Episode length: 142.71 +/- 65.34
Eval num_timesteps=15000, episode_reward=-172.35 +/- 107.26
Episode length: 173.28 +/- 107.04
Eval num_timesteps=15500, episode_reward=-212.99 +/- 154.95
Episode length: 213.78 +/- 154.56
Eval num_timesteps=16000, episode_reward=-212.11 +/- 166.20
Episode length: 212.87 +/- 165.79
Eval num_timesteps=16500, episode_reward=-290.62 +/- 184.71
Episode length: 291.19 +/- 184.22
Eval num_timesteps=17000, episode_reward=-300.14 +/- 191.58
Episode length: 300.67 +/- 191.09
Eval num_timesteps=17500, episode_reward=-274.54 +/- 191.25
Episode length: 275.13 +/- 190.77
Eval num_timesteps=18000, episode_reward=-232.94 +/- 177.47
Episode length: 233.64 +/- 177.02
Eval num_timesteps=18500, episode_reward=-167.67 +/- 144.35
Episode length: 168.53 +/- 144.03
Eval num_timesteps=19000, episode_reward=-172.76 +/- 164.48
Episode length: 173.56 +/- 164.08
Eval num_timesteps=19500, episode_reward=-121.64 +/- 103.97
Episode length: 122.58 +/- 103.75
New best mean reward!
Eval num_timesteps=20000, episode_reward=-132.91 +/- 119.61
Episode length: 133.82 +/- 119.34
Eval num_timesteps=20500, episode_reward=-104.66 +/- 75.83
Episode length: 105.63 +/- 75.67
New best mean reward!
Eval num_timesteps=21000, episode_reward=-119.43 +/- 98.79
Episode length: 120.38 +/- 98.60
Eval num_timesteps=21500, episode_reward=-110.73 +/- 78.83
Episode length: 111.70 +/- 78.69
Eval num_timesteps=22000, episode_reward=-114.75 +/- 81.65
Episode length: 115.72 +/- 81.51
Eval num_timesteps=22500, episode_reward=-107.71 +/- 71.96
Episode length: 108.69 +/- 71.85
Eval num_timesteps=23000, episode_reward=-104.07 +/- 64.72
Episode length: 105.06 +/- 64.66
New best mean reward!
Eval num_timesteps=23500, episode_reward=-86.05 +/- 26.40
Episode length: 87.05 +/- 26.40
New best mean reward!
FINISHED IN 421.1780080299941 s


starting seed  10147 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-83.26 +/- 20.96
Episode length: 84.26 +/- 20.96
New best mean reward!
FINISHED IN 2.514219482021872 s


starting seed  10148 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-472.13 +/- 72.90
Episode length: 472.26 +/- 72.57
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-195.43 +/- 53.90
Episode length: 196.41 +/- 53.79
New best mean reward!
Eval num_timesteps=9500, episode_reward=-285.17 +/- 61.09
Episode length: 286.13 +/- 60.95
Eval num_timesteps=10000, episode_reward=-478.81 +/- 33.13
Episode length: 479.23 +/- 32.76
Eval num_timesteps=10500, episode_reward=-234.72 +/- 35.02
Episode length: 235.72 +/- 35.02
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-240.74 +/- 120.32
Episode length: 241.58 +/- 119.97
Eval num_timesteps=12000, episode_reward=-121.43 +/- 43.34
Episode length: 122.42 +/- 43.26
New best mean reward!
Eval num_timesteps=12500, episode_reward=-99.60 +/- 35.37
Episode length: 100.60 +/- 35.37
New best mean reward!
FINISHED IN 296.2821014590445 s


starting seed  10149 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-451.56 +/- 114.92
Episode length: 451.72 +/- 114.57
New best mean reward!
Eval num_timesteps=9000, episode_reward=-493.35 +/- 46.59
Episode length: 493.37 +/- 46.45
Eval num_timesteps=9500, episode_reward=-417.59 +/- 139.42
Episode length: 417.86 +/- 138.99
New best mean reward!
Eval num_timesteps=10000, episode_reward=-187.15 +/- 108.24
Episode length: 188.05 +/- 107.95
New best mean reward!
Eval num_timesteps=10500, episode_reward=-203.28 +/- 110.17
Episode length: 204.17 +/- 109.88
Eval num_timesteps=11000, episode_reward=-182.07 +/- 144.16
Episode length: 182.92 +/- 143.82
New best mean reward!
Eval num_timesteps=11500, episode_reward=-194.18 +/- 133.75
Episode length: 195.03 +/- 133.40
Eval num_timesteps=12000, episode_reward=-196.69 +/- 104.00
Episode length: 197.61 +/- 103.77
Eval num_timesteps=12500, episode_reward=-247.16 +/- 147.25
Episode length: 247.93 +/- 146.86
Eval num_timesteps=13000, episode_reward=-249.73 +/- 147.79
Episode length: 250.50 +/- 147.40
Eval num_timesteps=13500, episode_reward=-259.69 +/- 148.51
Episode length: 260.43 +/- 148.09
Eval num_timesteps=14000, episode_reward=-187.35 +/- 99.71
Episode length: 188.28 +/- 99.50
Eval num_timesteps=14500, episode_reward=-184.51 +/- 92.74
Episode length: 185.44 +/- 92.50
Eval num_timesteps=15000, episode_reward=-201.40 +/- 113.24
Episode length: 202.29 +/- 112.95
Eval num_timesteps=15500, episode_reward=-191.91 +/- 104.91
Episode length: 192.82 +/- 104.65
Eval num_timesteps=16000, episode_reward=-195.48 +/- 106.10
Episode length: 196.39 +/- 105.84
Eval num_timesteps=16500, episode_reward=-193.82 +/- 96.60
Episode length: 194.76 +/- 96.41
Eval num_timesteps=17000, episode_reward=-180.69 +/- 74.74
Episode length: 181.66 +/- 74.61
New best mean reward!
Eval num_timesteps=17500, episode_reward=-172.73 +/- 62.05
Episode length: 173.71 +/- 61.94
New best mean reward!
Eval num_timesteps=18000, episode_reward=-186.32 +/- 75.65
Episode length: 187.29 +/- 75.52
Eval num_timesteps=18500, episode_reward=-195.89 +/- 100.40
Episode length: 196.81 +/- 100.15
Eval num_timesteps=19000, episode_reward=-206.69 +/- 112.73
Episode length: 207.60 +/- 112.50
Eval num_timesteps=19500, episode_reward=-201.49 +/- 122.17
Episode length: 202.36 +/- 121.85
Eval num_timesteps=20000, episode_reward=-141.29 +/- 128.95
Episode length: 142.20 +/- 128.70
New best mean reward!
Eval num_timesteps=20500, episode_reward=-145.26 +/- 135.35
Episode length: 146.14 +/- 135.04
Eval num_timesteps=21000, episode_reward=-174.62 +/- 164.92
Episode length: 175.42 +/- 164.53
Eval num_timesteps=21500, episode_reward=-198.66 +/- 178.99
Episode length: 199.41 +/- 178.57
Eval num_timesteps=22000, episode_reward=-141.41 +/- 133.03
Episode length: 142.32 +/- 132.78
Eval num_timesteps=22500, episode_reward=-170.20 +/- 162.32
Episode length: 171.03 +/- 161.97
Eval num_timesteps=23000, episode_reward=-144.10 +/- 137.32
Episode length: 144.98 +/- 137.01
Eval num_timesteps=23500, episode_reward=-94.93 +/- 55.32
Episode length: 95.92 +/- 55.25
New best mean reward!
FINISHED IN 496.4037659179885 s


starting seed  10150 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-393.90 +/- 106.20
Episode length: 394.48 +/- 105.78
New best mean reward!
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-346.58 +/- 200.49
Episode length: 346.95 +/- 200.01
New best mean reward!
Eval num_timesteps=10000, episode_reward=-156.72 +/- 154.24
Episode length: 157.56 +/- 153.88
New best mean reward!
Eval num_timesteps=10500, episode_reward=-150.96 +/- 148.88
Episode length: 151.81 +/- 148.53
New best mean reward!
Eval num_timesteps=11000, episode_reward=-93.67 +/- 38.08
Episode length: 94.67 +/- 38.08
New best mean reward!
FINISHED IN 330.00659271801123 s


starting seed  10151 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-130.20 +/- 112.04
Episode length: 131.12 +/- 111.78
New best mean reward!
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-312.60 +/- 171.66
Episode length: 313.16 +/- 171.18
Eval num_timesteps=6500, episode_reward=-200.01 +/- 130.55
Episode length: 200.86 +/- 130.20
Eval num_timesteps=7000, episode_reward=-204.27 +/- 126.77
Episode length: 205.13 +/- 126.44
Eval num_timesteps=7500, episode_reward=-289.09 +/- 165.97
Episode length: 289.73 +/- 165.52
Eval num_timesteps=8000, episode_reward=-217.34 +/- 131.64
Episode length: 218.18 +/- 131.29
Eval num_timesteps=8500, episode_reward=-180.58 +/- 72.74
Episode length: 181.56 +/- 72.65
Eval num_timesteps=9000, episode_reward=-207.27 +/- 118.97
Episode length: 208.15 +/- 118.67
Eval num_timesteps=9500, episode_reward=-175.36 +/- 80.66
Episode length: 176.32 +/- 80.50
Eval num_timesteps=10000, episode_reward=-176.16 +/- 102.44
Episode length: 177.08 +/- 102.19
Eval num_timesteps=10500, episode_reward=-221.36 +/- 133.17
Episode length: 222.19 +/- 132.81
Eval num_timesteps=11000, episode_reward=-227.65 +/- 123.82
Episode length: 228.49 +/- 123.47
Eval num_timesteps=11500, episode_reward=-294.13 +/- 166.57
Episode length: 294.75 +/- 166.10
Eval num_timesteps=12000, episode_reward=-292.75 +/- 169.92
Episode length: 293.36 +/- 169.44
Eval num_timesteps=12500, episode_reward=-243.99 +/- 153.86
Episode length: 244.75 +/- 153.46
Eval num_timesteps=13000, episode_reward=-171.96 +/- 99.87
Episode length: 172.89 +/- 99.64
Eval num_timesteps=13500, episode_reward=-164.15 +/- 89.24
Episode length: 165.09 +/- 89.02
Eval num_timesteps=14000, episode_reward=-199.99 +/- 148.43
Episode length: 200.80 +/- 148.04
Eval num_timesteps=14500, episode_reward=-221.87 +/- 140.78
Episode length: 222.68 +/- 140.41
Eval num_timesteps=15000, episode_reward=-259.29 +/- 155.87
Episode length: 260.01 +/- 155.44
Eval num_timesteps=15500, episode_reward=-235.56 +/- 165.88
Episode length: 236.29 +/- 165.45
Eval num_timesteps=16000, episode_reward=-231.07 +/- 175.27
Episode length: 231.79 +/- 174.84
Eval num_timesteps=16500, episode_reward=-260.04 +/- 182.91
Episode length: 260.69 +/- 182.45
Eval num_timesteps=17000, episode_reward=-244.32 +/- 184.30
Episode length: 244.99 +/- 183.84
Eval num_timesteps=17500, episode_reward=-196.05 +/- 158.29
Episode length: 196.85 +/- 157.91
Eval num_timesteps=18000, episode_reward=-166.11 +/- 136.33
Episode length: 166.98 +/- 136.01
Eval num_timesteps=18500, episode_reward=-188.48 +/- 155.00
Episode length: 189.29 +/- 154.62
Eval num_timesteps=19000, episode_reward=-240.53 +/- 181.87
Episode length: 241.21 +/- 181.41
Eval num_timesteps=19500, episode_reward=-233.03 +/- 178.04
Episode length: 233.73 +/- 177.59
Eval num_timesteps=20000, episode_reward=-232.79 +/- 182.16
Episode length: 233.49 +/- 181.72
Eval num_timesteps=20500, episode_reward=-185.51 +/- 152.61
Episode length: 186.34 +/- 152.26
Eval num_timesteps=21000, episode_reward=-192.99 +/- 161.17
Episode length: 193.79 +/- 160.79
Eval num_timesteps=21500, episode_reward=-170.94 +/- 144.07
Episode length: 171.79 +/- 143.73
Eval num_timesteps=22000, episode_reward=-166.95 +/- 137.84
Episode length: 167.81 +/- 137.50
Eval num_timesteps=22500, episode_reward=-164.80 +/- 138.18
Episode length: 165.67 +/- 137.86
Eval num_timesteps=23000, episode_reward=-224.15 +/- 177.57
Episode length: 224.88 +/- 177.15
Eval num_timesteps=23500, episode_reward=-172.56 +/- 147.19
Episode length: 173.41 +/- 146.85
Eval num_timesteps=24000, episode_reward=-176.01 +/- 148.20
Episode length: 176.85 +/- 147.85
Eval num_timesteps=24500, episode_reward=-154.36 +/- 127.72
Episode length: 155.25 +/- 127.42
Eval num_timesteps=25000, episode_reward=-120.92 +/- 86.56
Episode length: 121.88 +/- 86.39
New best mean reward!
Eval num_timesteps=25500, episode_reward=-115.19 +/- 74.82
Episode length: 116.17 +/- 74.71
New best mean reward!
Eval num_timesteps=26000, episode_reward=-109.43 +/- 64.50
Episode length: 110.41 +/- 64.38
New best mean reward!
Eval num_timesteps=26500, episode_reward=-122.08 +/- 85.06
Episode length: 123.05 +/- 84.92
Eval num_timesteps=27000, episode_reward=-122.02 +/- 83.68
Episode length: 122.99 +/- 83.54
Eval num_timesteps=27500, episode_reward=-98.18 +/- 46.26
Episode length: 99.17 +/- 46.17
New best mean reward!
FINISHED IN 572.1641260549659 s


starting seed  10152 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-87.05 +/- 21.94
Episode length: 88.05 +/- 21.94
New best mean reward!
FINISHED IN 211.15504577901447 s


starting seed  10153 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-486.81 +/- 54.95
Episode length: 486.88 +/- 54.72
New best mean reward!
Eval num_timesteps=7000, episode_reward=-487.16 +/- 56.78
Episode length: 487.22 +/- 56.57
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-404.07 +/- 123.80
Episode length: 404.48 +/- 123.35
New best mean reward!
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-254.82 +/- 122.58
Episode length: 255.65 +/- 122.24
New best mean reward!
Eval num_timesteps=10500, episode_reward=-191.93 +/- 66.87
Episode length: 192.91 +/- 66.77
New best mean reward!
Eval num_timesteps=11000, episode_reward=-195.77 +/- 65.42
Episode length: 196.75 +/- 65.33
Eval num_timesteps=11500, episode_reward=-164.66 +/- 52.72
Episode length: 165.65 +/- 52.66
New best mean reward!
Eval num_timesteps=12000, episode_reward=-153.52 +/- 42.05
Episode length: 154.51 +/- 41.97
New best mean reward!
Eval num_timesteps=12500, episode_reward=-163.91 +/- 87.92
Episode length: 164.86 +/- 87.73
Eval num_timesteps=13000, episode_reward=-315.38 +/- 178.10
Episode length: 315.91 +/- 177.61
Eval num_timesteps=13500, episode_reward=-311.05 +/- 187.02
Episode length: 311.57 +/- 186.53
Eval num_timesteps=14000, episode_reward=-306.10 +/- 183.16
Episode length: 306.64 +/- 182.68
Eval num_timesteps=14500, episode_reward=-350.62 +/- 183.16
Episode length: 351.03 +/- 182.67
Eval num_timesteps=15000, episode_reward=-230.71 +/- 189.64
Episode length: 231.38 +/- 189.17
Eval num_timesteps=15500, episode_reward=-114.33 +/- 82.69
Episode length: 115.29 +/- 82.51
New best mean reward!
Eval num_timesteps=16000, episode_reward=-140.07 +/- 126.10
Episode length: 140.97 +/- 125.81
Eval num_timesteps=16500, episode_reward=-101.24 +/- 49.28
Episode length: 102.23 +/- 49.20
New best mean reward!
Eval num_timesteps=17000, episode_reward=-124.09 +/- 106.17
Episode length: 125.02 +/- 105.92
Eval num_timesteps=17500, episode_reward=-113.31 +/- 93.87
Episode length: 114.26 +/- 93.67
Eval num_timesteps=18000, episode_reward=-101.01 +/- 60.95
Episode length: 101.99 +/- 60.82
New best mean reward!
Eval num_timesteps=18500, episode_reward=-97.25 +/- 35.47
Episode length: 98.25 +/- 35.47
New best mean reward!
FINISHED IN 383.5300020609866 s


starting seed  10154 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-359.04 +/- 188.81
Episode length: 359.40 +/- 188.33
New best mean reward!
Eval num_timesteps=11000, episode_reward=-259.74 +/- 188.52
Episode length: 260.37 +/- 188.05
New best mean reward!
Eval num_timesteps=11500, episode_reward=-111.23 +/- 65.48
Episode length: 112.22 +/- 65.42
New best mean reward!
Eval num_timesteps=12000, episode_reward=-96.73 +/- 39.01
Episode length: 97.73 +/- 39.01
New best mean reward!
FINISHED IN 273.9946672680089 s


starting seed  10155 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-415.65 +/- 164.28
Episode length: 415.86 +/- 163.87
New best mean reward!
Eval num_timesteps=14000, episode_reward=-279.40 +/- 196.80
Episode length: 279.98 +/- 196.33
New best mean reward!
Eval num_timesteps=14500, episode_reward=-208.27 +/- 189.84
Episode length: 208.98 +/- 189.39
New best mean reward!
Eval num_timesteps=15000, episode_reward=-154.31 +/- 143.68
Episode length: 155.17 +/- 143.34
New best mean reward!
Eval num_timesteps=15500, episode_reward=-125.72 +/- 106.02
Episode length: 126.65 +/- 105.77
New best mean reward!
Eval num_timesteps=16000, episode_reward=-122.69 +/- 106.25
Episode length: 123.62 +/- 106.01
New best mean reward!
Eval num_timesteps=16500, episode_reward=-108.30 +/- 75.59
Episode length: 109.27 +/- 75.44
New best mean reward!
Eval num_timesteps=17000, episode_reward=-106.23 +/- 74.77
Episode length: 107.20 +/- 74.61
New best mean reward!
Eval num_timesteps=17500, episode_reward=-107.49 +/- 86.65
Episode length: 108.45 +/- 86.47
Eval num_timesteps=18000, episode_reward=-124.88 +/- 106.54
Episode length: 125.81 +/- 106.30
Eval num_timesteps=18500, episode_reward=-102.09 +/- 54.56
Episode length: 103.08 +/- 54.49
New best mean reward!
Eval num_timesteps=19000, episode_reward=-98.61 +/- 49.88
Episode length: 99.60 +/- 49.80
New best mean reward!
FINISHED IN 410.4784460759838 s


starting seed  10156 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-103.46 +/- 51.78
Episode length: 104.45 +/- 51.70
New best mean reward!
Eval num_timesteps=10000, episode_reward=-105.26 +/- 29.46
Episode length: 106.26 +/- 29.46
Eval num_timesteps=10500, episode_reward=-105.86 +/- 30.06
Episode length: 106.86 +/- 30.06
Eval num_timesteps=11000, episode_reward=-148.20 +/- 22.55
Episode length: 149.20 +/- 22.55
Eval num_timesteps=11500, episode_reward=-173.13 +/- 39.75
Episode length: 174.13 +/- 39.75
Eval num_timesteps=12000, episode_reward=-198.01 +/- 94.32
Episode length: 198.94 +/- 94.10
Eval num_timesteps=12500, episode_reward=-167.34 +/- 31.89
Episode length: 168.34 +/- 31.89
Eval num_timesteps=13000, episode_reward=-162.72 +/- 39.64
Episode length: 163.72 +/- 39.64
Eval num_timesteps=13500, episode_reward=-170.70 +/- 50.36
Episode length: 171.69 +/- 50.30
Eval num_timesteps=14000, episode_reward=-176.30 +/- 34.85
Episode length: 177.30 +/- 34.85
Eval num_timesteps=14500, episode_reward=-167.03 +/- 40.55
Episode length: 168.03 +/- 40.55
Eval num_timesteps=15000, episode_reward=-174.19 +/- 56.34
Episode length: 175.17 +/- 56.22
Eval num_timesteps=15500, episode_reward=-170.07 +/- 48.47
Episode length: 171.06 +/- 48.40
Eval num_timesteps=16000, episode_reward=-175.37 +/- 46.90
Episode length: 176.37 +/- 46.90
Eval num_timesteps=16500, episode_reward=-165.81 +/- 47.37
Episode length: 166.80 +/- 47.30
Eval num_timesteps=17000, episode_reward=-164.24 +/- 51.14
Episode length: 165.24 +/- 51.14
Eval num_timesteps=17500, episode_reward=-168.68 +/- 35.61
Episode length: 169.68 +/- 35.61
Eval num_timesteps=18000, episode_reward=-171.55 +/- 49.93
Episode length: 172.54 +/- 49.87
Eval num_timesteps=18500, episode_reward=-245.30 +/- 133.68
Episode length: 246.11 +/- 133.32
Eval num_timesteps=19000, episode_reward=-172.44 +/- 52.61
Episode length: 173.43 +/- 52.55
Eval num_timesteps=19500, episode_reward=-169.89 +/- 44.93
Episode length: 170.89 +/- 44.93
Eval num_timesteps=20000, episode_reward=-176.45 +/- 51.46
Episode length: 177.45 +/- 51.46
Eval num_timesteps=20500, episode_reward=-169.11 +/- 45.94
Episode length: 170.11 +/- 45.94
Eval num_timesteps=21000, episode_reward=-181.45 +/- 59.47
Episode length: 182.44 +/- 59.42
Eval num_timesteps=21500, episode_reward=-172.09 +/- 49.61
Episode length: 173.08 +/- 49.54
Eval num_timesteps=22000, episode_reward=-164.65 +/- 33.60
Episode length: 165.65 +/- 33.60
Eval num_timesteps=22500, episode_reward=-173.70 +/- 51.54
Episode length: 174.70 +/- 51.54
Eval num_timesteps=23000, episode_reward=-168.74 +/- 42.77
Episode length: 169.74 +/- 42.77
Eval num_timesteps=23500, episode_reward=-171.00 +/- 38.49
Episode length: 172.00 +/- 38.49
Eval num_timesteps=24000, episode_reward=-173.66 +/- 57.05
Episode length: 174.65 +/- 56.99
Eval num_timesteps=24500, episode_reward=-168.46 +/- 39.32
Episode length: 169.46 +/- 39.32
Eval num_timesteps=25000, episode_reward=-164.23 +/- 35.71
Episode length: 165.23 +/- 35.71
Eval num_timesteps=25500, episode_reward=-162.48 +/- 48.71
Episode length: 163.47 +/- 48.65
Eval num_timesteps=26000, episode_reward=-161.58 +/- 31.85
Episode length: 162.58 +/- 31.85
Eval num_timesteps=26500, episode_reward=-163.62 +/- 31.20
Episode length: 164.62 +/- 31.20
Eval num_timesteps=27000, episode_reward=-168.54 +/- 45.28
Episode length: 169.54 +/- 45.28
Eval num_timesteps=27500, episode_reward=-168.65 +/- 36.28
Episode length: 169.65 +/- 36.28
Eval num_timesteps=28000, episode_reward=-171.98 +/- 39.57
Episode length: 172.98 +/- 39.57
Eval num_timesteps=28500, episode_reward=-166.19 +/- 31.66
Episode length: 167.19 +/- 31.66
Eval num_timesteps=29000, episode_reward=-169.46 +/- 49.46
Episode length: 170.45 +/- 49.39
Eval num_timesteps=29500, episode_reward=-191.57 +/- 87.68
Episode length: 192.51 +/- 87.47
Eval num_timesteps=30000, episode_reward=-190.80 +/- 95.95
Episode length: 191.73 +/- 95.72
Eval num_timesteps=30500, episode_reward=-201.54 +/- 107.65
Episode length: 202.44 +/- 107.37
Eval num_timesteps=31000, episode_reward=-202.66 +/- 104.89
Episode length: 203.57 +/- 104.63
Eval num_timesteps=31500, episode_reward=-217.51 +/- 121.61
Episode length: 218.37 +/- 121.29
Eval num_timesteps=32000, episode_reward=-204.85 +/- 104.95
Episode length: 205.75 +/- 104.67
Eval num_timesteps=32500, episode_reward=-202.59 +/- 105.44
Episode length: 203.50 +/- 105.19
Eval num_timesteps=33000, episode_reward=-180.93 +/- 93.65
Episode length: 181.87 +/- 93.44
Eval num_timesteps=33500, episode_reward=-165.17 +/- 88.43
Episode length: 166.12 +/- 88.24
Eval num_timesteps=34000, episode_reward=-177.91 +/- 104.85
Episode length: 178.82 +/- 104.57
Eval num_timesteps=34500, episode_reward=-155.32 +/- 70.23
Episode length: 156.29 +/- 70.08
Eval num_timesteps=35000, episode_reward=-170.70 +/- 95.08
Episode length: 171.63 +/- 94.83
Eval num_timesteps=35500, episode_reward=-169.22 +/- 83.20
Episode length: 170.17 +/- 83.00
Eval num_timesteps=36000, episode_reward=-172.51 +/- 96.82
Episode length: 173.44 +/- 96.59
Eval num_timesteps=36500, episode_reward=-161.20 +/- 90.22
Episode length: 162.14 +/- 90.00
Eval num_timesteps=37000, episode_reward=-148.18 +/- 71.87
Episode length: 149.15 +/- 71.72
Eval num_timesteps=37500, episode_reward=-168.98 +/- 103.14
Episode length: 169.90 +/- 102.88
Eval num_timesteps=38000, episode_reward=-165.19 +/- 102.33
Episode length: 166.11 +/- 102.07
Eval num_timesteps=38500, episode_reward=-144.29 +/- 60.23
Episode length: 145.27 +/- 60.11
Eval num_timesteps=39000, episode_reward=-123.01 +/- 45.21
Episode length: 124.00 +/- 45.12
Eval num_timesteps=39500, episode_reward=-147.30 +/- 100.59
Episode length: 148.24 +/- 100.38
Eval num_timesteps=40000, episode_reward=-140.34 +/- 77.74
Episode length: 141.31 +/- 77.60
Eval num_timesteps=40500, episode_reward=-140.86 +/- 82.51
Episode length: 141.82 +/- 82.34
Eval num_timesteps=41000, episode_reward=-133.88 +/- 72.03
Episode length: 134.85 +/- 71.88
Eval num_timesteps=41500, episode_reward=-142.90 +/- 88.09
Episode length: 143.85 +/- 87.88
Eval num_timesteps=42000, episode_reward=-144.87 +/- 88.65
Episode length: 145.82 +/- 88.45
Eval num_timesteps=42500, episode_reward=-131.42 +/- 63.70
Episode length: 132.40 +/- 63.59
Eval num_timesteps=43000, episode_reward=-129.51 +/- 73.14
Episode length: 130.48 +/- 72.99
Eval num_timesteps=43500, episode_reward=-134.38 +/- 91.98
Episode length: 135.33 +/- 91.78
Eval num_timesteps=44000, episode_reward=-133.70 +/- 87.45
Episode length: 134.66 +/- 87.28
Eval num_timesteps=44500, episode_reward=-115.40 +/- 45.82
Episode length: 116.39 +/- 45.73
Eval num_timesteps=45000, episode_reward=-123.39 +/- 74.65
Episode length: 124.36 +/- 74.50
Eval num_timesteps=45500, episode_reward=-122.61 +/- 79.19
Episode length: 123.57 +/- 79.00
Eval num_timesteps=46000, episode_reward=-127.07 +/- 85.12
Episode length: 128.03 +/- 84.95
Eval num_timesteps=46500, episode_reward=-130.96 +/- 78.77
Episode length: 131.93 +/- 78.63
Eval num_timesteps=47000, episode_reward=-124.17 +/- 72.03
Episode length: 125.14 +/- 71.87
Eval num_timesteps=47500, episode_reward=-130.97 +/- 82.16
Episode length: 131.93 +/- 81.98
Eval num_timesteps=48000, episode_reward=-117.14 +/- 49.65
Episode length: 118.13 +/- 49.57
Eval num_timesteps=48500, episode_reward=-131.45 +/- 85.24
Episode length: 132.41 +/- 85.06
Eval num_timesteps=49000, episode_reward=-131.87 +/- 95.48
Episode length: 132.81 +/- 95.25
Eval num_timesteps=49500, episode_reward=-121.03 +/- 69.39
Episode length: 122.01 +/- 69.28
Eval num_timesteps=50000, episode_reward=-120.83 +/- 56.31
Episode length: 121.82 +/- 56.24
FINISHED IN 749.7994246649905 s


starting seed  10157 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-388.28 +/- 152.07
Episode length: 388.64 +/- 151.60
New best mean reward!
Eval num_timesteps=6000, episode_reward=-213.12 +/- 56.47
Episode length: 214.10 +/- 56.37
New best mean reward!
Eval num_timesteps=6500, episode_reward=-184.29 +/- 107.20
Episode length: 185.19 +/- 106.91
New best mean reward!
Eval num_timesteps=7000, episode_reward=-165.75 +/- 38.98
Episode length: 166.75 +/- 38.98
New best mean reward!
Eval num_timesteps=7500, episode_reward=-255.75 +/- 59.05
Episode length: 256.73 +/- 58.97
Eval num_timesteps=8000, episode_reward=-248.94 +/- 47.06
Episode length: 249.94 +/- 47.06
Eval num_timesteps=8500, episode_reward=-200.60 +/- 79.64
Episode length: 201.55 +/- 79.45
Eval num_timesteps=9000, episode_reward=-153.94 +/- 36.00
Episode length: 154.94 +/- 36.00
New best mean reward!
Eval num_timesteps=9500, episode_reward=-193.63 +/- 51.18
Episode length: 194.62 +/- 51.12
Eval num_timesteps=10000, episode_reward=-270.82 +/- 125.52
Episode length: 271.61 +/- 125.14
Eval num_timesteps=10500, episode_reward=-149.84 +/- 26.45
Episode length: 150.84 +/- 26.45
New best mean reward!
Eval num_timesteps=11000, episode_reward=-144.35 +/- 42.89
Episode length: 145.35 +/- 42.89
New best mean reward!
Eval num_timesteps=11500, episode_reward=-144.97 +/- 52.47
Episode length: 145.96 +/- 52.40
Eval num_timesteps=12000, episode_reward=-188.81 +/- 118.55
Episode length: 189.69 +/- 118.23
Eval num_timesteps=12500, episode_reward=-138.79 +/- 43.47
Episode length: 139.78 +/- 43.38
New best mean reward!
Eval num_timesteps=13000, episode_reward=-138.83 +/- 25.74
Episode length: 139.83 +/- 25.74
Eval num_timesteps=13500, episode_reward=-152.57 +/- 109.77
Episode length: 153.49 +/- 109.52
Eval num_timesteps=14000, episode_reward=-123.45 +/- 49.92
Episode length: 124.44 +/- 49.84
New best mean reward!
Eval num_timesteps=14500, episode_reward=-132.91 +/- 27.70
Episode length: 133.91 +/- 27.70
Eval num_timesteps=15000, episode_reward=-129.32 +/- 29.16
Episode length: 130.32 +/- 29.16
Eval num_timesteps=15500, episode_reward=-117.79 +/- 30.02
Episode length: 118.79 +/- 30.02
New best mean reward!
Eval num_timesteps=16000, episode_reward=-152.89 +/- 45.81
Episode length: 153.88 +/- 45.74
Eval num_timesteps=16500, episode_reward=-172.83 +/- 42.92
Episode length: 173.82 +/- 42.84
Eval num_timesteps=17000, episode_reward=-175.13 +/- 39.23
Episode length: 176.13 +/- 39.23
Eval num_timesteps=17500, episode_reward=-143.30 +/- 36.95
Episode length: 144.30 +/- 36.95
Eval num_timesteps=18000, episode_reward=-185.31 +/- 37.31
Episode length: 186.31 +/- 37.31
Eval num_timesteps=18500, episode_reward=-179.43 +/- 31.34
Episode length: 180.43 +/- 31.34
Eval num_timesteps=19000, episode_reward=-195.41 +/- 58.15
Episode length: 196.39 +/- 58.05
Eval num_timesteps=19500, episode_reward=-186.86 +/- 45.17
Episode length: 187.85 +/- 45.10
Eval num_timesteps=20000, episode_reward=-187.40 +/- 45.60
Episode length: 188.39 +/- 45.53
Eval num_timesteps=20500, episode_reward=-165.00 +/- 29.51
Episode length: 166.00 +/- 29.51
Eval num_timesteps=21000, episode_reward=-144.35 +/- 43.21
Episode length: 145.35 +/- 43.21
Eval num_timesteps=21500, episode_reward=-135.61 +/- 19.27
Episode length: 136.61 +/- 19.27
Eval num_timesteps=22000, episode_reward=-119.72 +/- 28.39
Episode length: 120.72 +/- 28.39
Eval num_timesteps=22500, episode_reward=-124.65 +/- 60.34
Episode length: 125.63 +/- 60.22
Eval num_timesteps=23000, episode_reward=-115.96 +/- 30.14
Episode length: 116.96 +/- 30.14
New best mean reward!
Eval num_timesteps=23500, episode_reward=-122.95 +/- 34.69
Episode length: 123.95 +/- 34.69
Eval num_timesteps=24000, episode_reward=-114.85 +/- 23.22
Episode length: 115.85 +/- 23.22
New best mean reward!
Eval num_timesteps=24500, episode_reward=-119.76 +/- 34.48
Episode length: 120.76 +/- 34.48
Eval num_timesteps=25000, episode_reward=-114.28 +/- 49.09
Episode length: 115.27 +/- 49.01
New best mean reward!
Eval num_timesteps=25500, episode_reward=-108.62 +/- 26.27
Episode length: 109.62 +/- 26.27
New best mean reward!
Eval num_timesteps=26000, episode_reward=-125.87 +/- 55.80
Episode length: 126.86 +/- 55.73
Eval num_timesteps=26500, episode_reward=-116.19 +/- 35.76
Episode length: 117.19 +/- 35.76
Eval num_timesteps=27000, episode_reward=-120.82 +/- 34.36
Episode length: 121.82 +/- 34.36
Eval num_timesteps=27500, episode_reward=-119.77 +/- 60.57
Episode length: 120.75 +/- 60.44
Eval num_timesteps=28000, episode_reward=-115.03 +/- 69.43
Episode length: 116.01 +/- 69.32
Eval num_timesteps=28500, episode_reward=-109.50 +/- 44.54
Episode length: 110.50 +/- 44.54
Eval num_timesteps=29000, episode_reward=-108.40 +/- 53.84
Episode length: 109.39 +/- 53.77
New best mean reward!
Eval num_timesteps=29500, episode_reward=-113.04 +/- 57.36
Episode length: 114.04 +/- 57.36
Eval num_timesteps=30000, episode_reward=-115.09 +/- 76.55
Episode length: 116.06 +/- 76.40
Eval num_timesteps=30500, episode_reward=-122.16 +/- 80.32
Episode length: 123.14 +/- 80.23
Eval num_timesteps=31000, episode_reward=-97.59 +/- 27.58
Episode length: 98.59 +/- 27.58
New best mean reward!
FINISHED IN 407.5323261069716 s


starting seed  10158 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-434.34 +/- 128.90
Episode length: 434.55 +/- 128.50
New best mean reward!
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-364.00 +/- 163.51
Episode length: 364.41 +/- 163.02
New best mean reward!
Eval num_timesteps=2500, episode_reward=-298.49 +/- 159.68
Episode length: 299.11 +/- 159.20
New best mean reward!
Eval num_timesteps=3000, episode_reward=-182.59 +/- 69.41
Episode length: 183.56 +/- 69.28
New best mean reward!
Eval num_timesteps=3500, episode_reward=-182.09 +/- 48.94
Episode length: 183.08 +/- 48.88
New best mean reward!
Eval num_timesteps=4000, episode_reward=-168.26 +/- 23.20
Episode length: 169.26 +/- 23.20
New best mean reward!
Eval num_timesteps=4500, episode_reward=-180.01 +/- 47.97
Episode length: 181.00 +/- 47.90
Eval num_timesteps=5000, episode_reward=-185.57 +/- 66.69
Episode length: 186.54 +/- 66.55
Eval num_timesteps=5500, episode_reward=-188.65 +/- 29.56
Episode length: 189.65 +/- 29.56
Eval num_timesteps=6000, episode_reward=-370.41 +/- 148.01
Episode length: 370.85 +/- 147.52
Eval num_timesteps=6500, episode_reward=-384.46 +/- 143.31
Episode length: 384.86 +/- 142.83
Eval num_timesteps=7000, episode_reward=-284.41 +/- 142.19
Episode length: 285.12 +/- 141.75
Eval num_timesteps=7500, episode_reward=-234.96 +/- 127.88
Episode length: 235.80 +/- 127.55
Eval num_timesteps=8000, episode_reward=-195.51 +/- 85.45
Episode length: 196.45 +/- 85.24
Eval num_timesteps=8500, episode_reward=-177.03 +/- 65.69
Episode length: 178.00 +/- 65.54
Eval num_timesteps=9000, episode_reward=-179.52 +/- 142.05
Episode length: 180.37 +/- 141.72
Eval num_timesteps=9500, episode_reward=-168.15 +/- 151.81
Episode length: 168.98 +/- 151.44
New best mean reward!
Eval num_timesteps=10000, episode_reward=-89.27 +/- 27.61
Episode length: 90.27 +/- 27.61
New best mean reward!
FINISHED IN 159.78947272204095 s


starting seed  10159 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-251.68 +/- 163.67
Episode length: 252.39 +/- 163.23
New best mean reward!
Eval num_timesteps=1500, episode_reward=-408.88 +/- 158.86
Episode length: 409.13 +/- 158.43
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-499.19 +/- 8.06
Episode length: 499.20 +/- 7.96
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-486.04 +/- 43.94
Episode length: 486.16 +/- 43.66
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-495.00 +/- 20.17
Episode length: 495.07 +/- 19.94
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-159.01 +/- 58.12
Episode length: 159.99 +/- 58.00
New best mean reward!
Eval num_timesteps=10000, episode_reward=-150.63 +/- 71.51
Episode length: 151.60 +/- 71.36
New best mean reward!
Eval num_timesteps=10500, episode_reward=-147.88 +/- 61.61
Episode length: 148.86 +/- 61.50
New best mean reward!
Eval num_timesteps=11000, episode_reward=-176.50 +/- 106.87
Episode length: 177.41 +/- 106.60
Eval num_timesteps=11500, episode_reward=-226.17 +/- 112.73
Episode length: 227.04 +/- 112.41
Eval num_timesteps=12000, episode_reward=-205.76 +/- 106.62
Episode length: 206.66 +/- 106.35
Eval num_timesteps=12500, episode_reward=-184.92 +/- 66.37
Episode length: 185.89 +/- 66.23
Eval num_timesteps=13000, episode_reward=-193.80 +/- 105.38
Episode length: 194.70 +/- 105.09
Eval num_timesteps=13500, episode_reward=-194.14 +/- 101.06
Episode length: 195.05 +/- 100.79
Eval num_timesteps=14000, episode_reward=-285.04 +/- 162.53
Episode length: 285.68 +/- 162.06
Eval num_timesteps=14500, episode_reward=-356.68 +/- 169.93
Episode length: 357.10 +/- 169.44
Eval num_timesteps=15000, episode_reward=-293.54 +/- 166.69
Episode length: 294.15 +/- 166.21
Eval num_timesteps=15500, episode_reward=-201.21 +/- 95.42
Episode length: 202.13 +/- 95.17
Eval num_timesteps=16000, episode_reward=-210.38 +/- 110.19
Episode length: 211.27 +/- 109.90
Eval num_timesteps=16500, episode_reward=-176.37 +/- 75.87
Episode length: 177.33 +/- 75.70
Eval num_timesteps=17000, episode_reward=-150.88 +/- 68.63
Episode length: 151.85 +/- 68.48
Eval num_timesteps=17500, episode_reward=-138.08 +/- 36.31
Episode length: 139.08 +/- 36.31
New best mean reward!
Eval num_timesteps=18000, episode_reward=-127.73 +/- 29.09
Episode length: 128.73 +/- 29.09
New best mean reward!
Eval num_timesteps=18500, episode_reward=-98.01 +/- 21.94
Episode length: 99.01 +/- 21.94
New best mean reward!
FINISHED IN 345.87844867503736 s


starting seed  10160 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-484.81 +/- 74.44
Episode length: 484.85 +/- 74.25
New best mean reward!
Eval num_timesteps=11500, episode_reward=-470.62 +/- 94.55
Episode length: 470.71 +/- 94.26
New best mean reward!
Eval num_timesteps=12000, episode_reward=-320.08 +/- 190.55
Episode length: 320.56 +/- 190.06
New best mean reward!
Eval num_timesteps=12500, episode_reward=-293.71 +/- 191.82
Episode length: 294.26 +/- 191.34
New best mean reward!
Eval num_timesteps=13000, episode_reward=-310.66 +/- 184.46
Episode length: 311.18 +/- 183.96
Eval num_timesteps=13500, episode_reward=-320.67 +/- 186.88
Episode length: 321.16 +/- 186.39
Eval num_timesteps=14000, episode_reward=-199.73 +/- 177.93
Episode length: 200.48 +/- 177.51
New best mean reward!
Eval num_timesteps=14500, episode_reward=-182.02 +/- 167.41
Episode length: 182.81 +/- 167.01
New best mean reward!
Eval num_timesteps=15000, episode_reward=-185.74 +/- 169.07
Episode length: 186.53 +/- 168.68
Eval num_timesteps=15500, episode_reward=-328.93 +/- 194.11
Episode length: 329.37 +/- 193.61
Eval num_timesteps=16000, episode_reward=-217.15 +/- 187.67
Episode length: 217.85 +/- 187.22
Eval num_timesteps=16500, episode_reward=-267.19 +/- 196.68
Episode length: 267.79 +/- 196.21
Eval num_timesteps=17000, episode_reward=-207.05 +/- 178.53
Episode length: 207.79 +/- 178.10
Eval num_timesteps=17500, episode_reward=-278.75 +/- 201.94
Episode length: 279.30 +/- 201.44
Eval num_timesteps=18000, episode_reward=-107.35 +/- 81.09
Episode length: 108.32 +/- 80.94
New best mean reward!
Eval num_timesteps=18500, episode_reward=-112.47 +/- 87.96
Episode length: 113.43 +/- 87.78
Eval num_timesteps=19000, episode_reward=-107.64 +/- 79.13
Episode length: 108.61 +/- 78.98
Eval num_timesteps=19500, episode_reward=-96.95 +/- 47.31
Episode length: 97.94 +/- 47.22
New best mean reward!
FINISHED IN 419.549486842996 s


starting seed  10161 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-441.74 +/- 144.45
Episode length: 441.88 +/- 144.10
New best mean reward!
Eval num_timesteps=9500, episode_reward=-399.98 +/- 174.56
Episode length: 400.23 +/- 174.13
New best mean reward!
Eval num_timesteps=10000, episode_reward=-467.89 +/- 108.97
Episode length: 467.97 +/- 108.70
Eval num_timesteps=10500, episode_reward=-418.86 +/- 162.81
Episode length: 419.06 +/- 162.41
Eval num_timesteps=11000, episode_reward=-102.71 +/- 65.42
Episode length: 103.70 +/- 65.36
New best mean reward!
Eval num_timesteps=11500, episode_reward=-196.06 +/- 178.10
Episode length: 196.81 +/- 177.67
Eval num_timesteps=12000, episode_reward=-104.73 +/- 86.48
Episode length: 105.69 +/- 86.30
Eval num_timesteps=12500, episode_reward=-108.71 +/- 84.05
Episode length: 109.68 +/- 83.91
Eval num_timesteps=13000, episode_reward=-109.71 +/- 80.55
Episode length: 110.68 +/- 80.40
Eval num_timesteps=13500, episode_reward=-104.12 +/- 75.09
Episode length: 105.09 +/- 74.93
Eval num_timesteps=14000, episode_reward=-115.88 +/- 100.86
Episode length: 116.83 +/- 100.67
Eval num_timesteps=14500, episode_reward=-90.64 +/- 30.43
Episode length: 91.64 +/- 30.43
New best mean reward!
FINISHED IN 375.1319562589633 s


starting seed  10162 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=17000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=17500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=18000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=18500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=19000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=19500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=20000, episode_reward=-495.89 +/- 40.89
Episode length: 495.90 +/- 40.79
New best mean reward!
Eval num_timesteps=20500, episode_reward=-463.90 +/- 114.85
Episode length: 463.99 +/- 114.56
New best mean reward!
Eval num_timesteps=21000, episode_reward=-467.85 +/- 109.07
Episode length: 467.93 +/- 108.80
Eval num_timesteps=21500, episode_reward=-492.67 +/- 51.58
Episode length: 492.69 +/- 51.44
Eval num_timesteps=22000, episode_reward=-467.42 +/- 110.53
Episode length: 467.50 +/- 110.26
Eval num_timesteps=22500, episode_reward=-417.52 +/- 165.06
Episode length: 417.72 +/- 164.66
New best mean reward!
Eval num_timesteps=23000, episode_reward=-386.38 +/- 182.42
Episode length: 386.66 +/- 181.97
New best mean reward!
Eval num_timesteps=23500, episode_reward=-385.02 +/- 184.54
Episode length: 385.30 +/- 184.09
New best mean reward!
Eval num_timesteps=24000, episode_reward=-365.27 +/- 188.75
Episode length: 365.61 +/- 188.28
New best mean reward!
Eval num_timesteps=24500, episode_reward=-349.90 +/- 196.41
Episode length: 350.27 +/- 195.92
New best mean reward!
Eval num_timesteps=25000, episode_reward=-327.56 +/- 202.98
Episode length: 327.98 +/- 202.49
New best mean reward!
Eval num_timesteps=25500, episode_reward=-261.73 +/- 203.92
Episode length: 262.31 +/- 203.43
New best mean reward!
Eval num_timesteps=26000, episode_reward=-238.07 +/- 195.83
Episode length: 238.72 +/- 195.36
New best mean reward!
Eval num_timesteps=26500, episode_reward=-224.43 +/- 194.54
Episode length: 225.10 +/- 194.07
New best mean reward!
Eval num_timesteps=27000, episode_reward=-189.25 +/- 180.03
Episode length: 190.01 +/- 179.62
New best mean reward!
Eval num_timesteps=27500, episode_reward=-214.92 +/- 187.52
Episode length: 215.64 +/- 187.09
Eval num_timesteps=28000, episode_reward=-191.03 +/- 175.30
Episode length: 191.80 +/- 174.90
Eval num_timesteps=28500, episode_reward=-161.57 +/- 150.80
Episode length: 162.42 +/- 150.46
New best mean reward!
Eval num_timesteps=29000, episode_reward=-150.78 +/- 143.35
Episode length: 151.64 +/- 143.01
New best mean reward!
Eval num_timesteps=29500, episode_reward=-150.12 +/- 148.29
Episode length: 150.97 +/- 147.94
New best mean reward!
Eval num_timesteps=30000, episode_reward=-160.80 +/- 160.89
Episode length: 161.63 +/- 160.54
Eval num_timesteps=30500, episode_reward=-167.43 +/- 158.12
Episode length: 168.25 +/- 157.74
Eval num_timesteps=31000, episode_reward=-170.28 +/- 167.60
Episode length: 171.08 +/- 167.21
Eval num_timesteps=31500, episode_reward=-152.72 +/- 150.33
Episode length: 153.58 +/- 150.00
Eval num_timesteps=32000, episode_reward=-130.87 +/- 123.45
Episode length: 131.78 +/- 123.18
New best mean reward!
Eval num_timesteps=32500, episode_reward=-161.22 +/- 155.48
Episode length: 162.05 +/- 155.11
Eval num_timesteps=33000, episode_reward=-123.53 +/- 116.34
Episode length: 124.45 +/- 116.08
New best mean reward!
Eval num_timesteps=33500, episode_reward=-131.63 +/- 126.44
Episode length: 132.53 +/- 126.15
Eval num_timesteps=34000, episode_reward=-142.79 +/- 138.93
Episode length: 143.67 +/- 138.62
Eval num_timesteps=34500, episode_reward=-133.54 +/- 129.01
Episode length: 134.44 +/- 128.72
Eval num_timesteps=35000, episode_reward=-141.58 +/- 141.24
Episode length: 142.45 +/- 140.91
Eval num_timesteps=35500, episode_reward=-128.13 +/- 120.82
Episode length: 129.04 +/- 120.54
Eval num_timesteps=36000, episode_reward=-117.14 +/- 108.52
Episode length: 118.07 +/- 108.27
New best mean reward!
Eval num_timesteps=36500, episode_reward=-117.92 +/- 110.07
Episode length: 118.85 +/- 109.83
Eval num_timesteps=37000, episode_reward=-113.77 +/- 101.69
Episode length: 114.71 +/- 101.47
New best mean reward!
Eval num_timesteps=37500, episode_reward=-130.97 +/- 120.61
Episode length: 131.88 +/- 120.33
Eval num_timesteps=38000, episode_reward=-131.97 +/- 127.42
Episode length: 132.87 +/- 127.13
Eval num_timesteps=38500, episode_reward=-137.32 +/- 139.05
Episode length: 138.20 +/- 138.74
Eval num_timesteps=39000, episode_reward=-128.51 +/- 125.91
Episode length: 129.41 +/- 125.62
Eval num_timesteps=39500, episode_reward=-110.54 +/- 94.33
Episode length: 111.49 +/- 94.12
New best mean reward!
Eval num_timesteps=40000, episode_reward=-123.82 +/- 115.37
Episode length: 124.74 +/- 115.11
Eval num_timesteps=40500, episode_reward=-116.00 +/- 107.02
Episode length: 116.93 +/- 106.77
Eval num_timesteps=41000, episode_reward=-122.11 +/- 107.76
Episode length: 123.04 +/- 107.52
Eval num_timesteps=41500, episode_reward=-110.65 +/- 98.28
Episode length: 111.60 +/- 98.08
Eval num_timesteps=42000, episode_reward=-121.53 +/- 113.96
Episode length: 122.45 +/- 113.69
Eval num_timesteps=42500, episode_reward=-102.79 +/- 72.41
Episode length: 103.77 +/- 72.30
New best mean reward!
Eval num_timesteps=43000, episode_reward=-101.49 +/- 79.92
Episode length: 102.46 +/- 79.77
New best mean reward!
Eval num_timesteps=43500, episode_reward=-110.76 +/- 88.00
Episode length: 111.72 +/- 87.82
Eval num_timesteps=44000, episode_reward=-123.78 +/- 118.04
Episode length: 124.70 +/- 117.78
Eval num_timesteps=44500, episode_reward=-94.58 +/- 65.79
Episode length: 95.56 +/- 65.67
New best mean reward!
FINISHED IN 915.5167464210535 s


starting seed  10163 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-380.27 +/- 149.23
Episode length: 380.67 +/- 148.75
New best mean reward!
Eval num_timesteps=5500, episode_reward=-467.95 +/- 51.48
Episode length: 468.32 +/- 51.08
Eval num_timesteps=6000, episode_reward=-494.34 +/- 40.00
Episode length: 494.36 +/- 39.86
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-496.73 +/- 32.54
Episode length: 496.74 +/- 32.44
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-491.82 +/- 47.22
Episode length: 491.85 +/- 47.05
Eval num_timesteps=8500, episode_reward=-477.24 +/- 83.36
Episode length: 477.31 +/- 83.11
Eval num_timesteps=9000, episode_reward=-356.69 +/- 163.52
Episode length: 357.13 +/- 163.03
New best mean reward!
Eval num_timesteps=9500, episode_reward=-459.65 +/- 105.44
Episode length: 459.78 +/- 105.11
Eval num_timesteps=10000, episode_reward=-255.57 +/- 155.58
Episode length: 256.30 +/- 155.16
New best mean reward!
Eval num_timesteps=10500, episode_reward=-201.37 +/- 111.33
Episode length: 202.26 +/- 111.04
New best mean reward!
Eval num_timesteps=11000, episode_reward=-173.08 +/- 63.86
Episode length: 174.06 +/- 63.75
New best mean reward!
Eval num_timesteps=11500, episode_reward=-172.10 +/- 56.88
Episode length: 173.08 +/- 56.76
New best mean reward!
Eval num_timesteps=12000, episode_reward=-176.63 +/- 77.09
Episode length: 177.59 +/- 76.92
Eval num_timesteps=12500, episode_reward=-202.01 +/- 115.25
Episode length: 202.89 +/- 114.94
Eval num_timesteps=13000, episode_reward=-162.40 +/- 45.91
Episode length: 163.39 +/- 45.83
New best mean reward!
Eval num_timesteps=13500, episode_reward=-169.58 +/- 65.69
Episode length: 170.56 +/- 65.59
Eval num_timesteps=14000, episode_reward=-169.22 +/- 66.40
Episode length: 170.19 +/- 66.25
Eval num_timesteps=14500, episode_reward=-178.53 +/- 82.53
Episode length: 179.48 +/- 82.34
Eval num_timesteps=15000, episode_reward=-186.93 +/- 106.08
Episode length: 187.84 +/- 105.81
Eval num_timesteps=15500, episode_reward=-141.68 +/- 53.71
Episode length: 142.67 +/- 53.64
New best mean reward!
Eval num_timesteps=16000, episode_reward=-158.45 +/- 45.36
Episode length: 159.44 +/- 45.28
Eval num_timesteps=16500, episode_reward=-162.03 +/- 65.54
Episode length: 163.00 +/- 65.39
Eval num_timesteps=17000, episode_reward=-182.94 +/- 95.13
Episode length: 183.87 +/- 94.90
Eval num_timesteps=17500, episode_reward=-149.36 +/- 30.88
Episode length: 150.36 +/- 30.88
Eval num_timesteps=18000, episode_reward=-161.69 +/- 42.50
Episode length: 162.69 +/- 42.50
Eval num_timesteps=18500, episode_reward=-172.54 +/- 52.51
Episode length: 173.53 +/- 52.45
Eval num_timesteps=19000, episode_reward=-161.69 +/- 42.36
Episode length: 162.68 +/- 42.28
Eval num_timesteps=19500, episode_reward=-167.75 +/- 95.52
Episode length: 168.69 +/- 95.31
Eval num_timesteps=20000, episode_reward=-156.48 +/- 64.09
Episode length: 157.45 +/- 63.93
Eval num_timesteps=20500, episode_reward=-165.25 +/- 75.00
Episode length: 166.21 +/- 74.82
Eval num_timesteps=21000, episode_reward=-117.36 +/- 53.53
Episode length: 118.35 +/- 53.46
New best mean reward!
Eval num_timesteps=21500, episode_reward=-100.94 +/- 33.41
Episode length: 101.94 +/- 33.41
New best mean reward!
Eval num_timesteps=22000, episode_reward=-85.92 +/- 23.18
Episode length: 86.92 +/- 23.18
New best mean reward!
FINISHED IN 394.6696785319946 s


starting seed  10164 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-183.13 +/- 169.37
Episode length: 183.91 +/- 168.96
New best mean reward!
Eval num_timesteps=9000, episode_reward=-276.92 +/- 201.69
Episode length: 277.48 +/- 201.20
Eval num_timesteps=9500, episode_reward=-283.85 +/- 201.44
Episode length: 284.40 +/- 200.95
Eval num_timesteps=10000, episode_reward=-299.11 +/- 199.64
Episode length: 299.63 +/- 199.15
Eval num_timesteps=10500, episode_reward=-379.23 +/- 184.83
Episode length: 379.53 +/- 184.37
Eval num_timesteps=11000, episode_reward=-359.32 +/- 190.82
Episode length: 359.68 +/- 190.35
Eval num_timesteps=11500, episode_reward=-390.57 +/- 177.17
Episode length: 390.85 +/- 176.72
Eval num_timesteps=12000, episode_reward=-136.32 +/- 123.71
Episode length: 137.23 +/- 123.44
New best mean reward!
Eval num_timesteps=12500, episode_reward=-194.89 +/- 173.98
Episode length: 195.65 +/- 173.56
Eval num_timesteps=13000, episode_reward=-130.39 +/- 114.19
Episode length: 131.31 +/- 113.93
New best mean reward!
Eval num_timesteps=13500, episode_reward=-93.43 +/- 38.76
Episode length: 94.43 +/- 38.76
New best mean reward!
FINISHED IN 289.8875024309964 s


starting seed  10165 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-294.82 +/- 193.75
Episode length: 295.35 +/- 193.25
New best mean reward!
Eval num_timesteps=9000, episode_reward=-492.29 +/- 53.97
Episode length: 492.31 +/- 53.83
Eval num_timesteps=9500, episode_reward=-436.05 +/- 147.47
Episode length: 436.21 +/- 147.11
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-455.71 +/- 126.55
Episode length: 455.82 +/- 126.24
Eval num_timesteps=11500, episode_reward=-371.98 +/- 187.35
Episode length: 372.30 +/- 186.89
Eval num_timesteps=12000, episode_reward=-361.56 +/- 193.18
Episode length: 361.90 +/- 192.71
Eval num_timesteps=12500, episode_reward=-363.46 +/- 192.07
Episode length: 363.80 +/- 191.61
Eval num_timesteps=13000, episode_reward=-242.55 +/- 202.84
Episode length: 243.17 +/- 202.36
New best mean reward!
Eval num_timesteps=13500, episode_reward=-175.05 +/- 166.00
Episode length: 175.85 +/- 165.61
New best mean reward!
Eval num_timesteps=14000, episode_reward=-181.10 +/- 171.93
Episode length: 181.88 +/- 171.53
Eval num_timesteps=14500, episode_reward=-172.37 +/- 166.44
Episode length: 173.17 +/- 166.05
New best mean reward!
Eval num_timesteps=15000, episode_reward=-292.41 +/- 207.43
Episode length: 292.92 +/- 206.94
Eval num_timesteps=15500, episode_reward=-138.04 +/- 134.62
Episode length: 138.93 +/- 134.33
New best mean reward!
Eval num_timesteps=16000, episode_reward=-142.91 +/- 139.48
Episode length: 143.78 +/- 139.15
Eval num_timesteps=16500, episode_reward=-95.26 +/- 48.42
Episode length: 96.25 +/- 48.33
New best mean reward!
FINISHED IN 396.20800836599665 s


starting seed  10166 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-428.45 +/- 131.59
Episode length: 428.68 +/- 131.17
New best mean reward!
Eval num_timesteps=1500, episode_reward=-491.97 +/- 45.89
Episode length: 492.00 +/- 45.72
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-490.65 +/- 53.36
Episode length: 490.68 +/- 53.19
Eval num_timesteps=6500, episode_reward=-488.22 +/- 58.58
Episode length: 488.26 +/- 58.39
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-493.95 +/- 42.36
Episode length: 493.97 +/- 42.22
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-90.61 +/- 27.21
Episode length: 91.61 +/- 27.21
New best mean reward!
FINISHED IN 215.41274671297288 s


starting seed  10167 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-137.13 +/- 71.49
Episode length: 138.10 +/- 71.34
New best mean reward!
Eval num_timesteps=11000, episode_reward=-231.79 +/- 162.92
Episode length: 232.53 +/- 162.49
Eval num_timesteps=11500, episode_reward=-158.17 +/- 75.40
Episode length: 159.14 +/- 75.26
Eval num_timesteps=12000, episode_reward=-187.92 +/- 123.05
Episode length: 188.79 +/- 122.72
Eval num_timesteps=12500, episode_reward=-169.82 +/- 93.14
Episode length: 170.76 +/- 92.93
Eval num_timesteps=13000, episode_reward=-123.95 +/- 20.80
Episode length: 124.95 +/- 20.80
New best mean reward!
Eval num_timesteps=13500, episode_reward=-127.11 +/- 28.23
Episode length: 128.11 +/- 28.23
Eval num_timesteps=14000, episode_reward=-133.50 +/- 58.91
Episode length: 134.48 +/- 58.79
Eval num_timesteps=14500, episode_reward=-149.07 +/- 42.82
Episode length: 150.06 +/- 42.74
Eval num_timesteps=15000, episode_reward=-139.96 +/- 27.12
Episode length: 140.96 +/- 27.12
Eval num_timesteps=15500, episode_reward=-156.09 +/- 40.42
Episode length: 157.09 +/- 40.42
Eval num_timesteps=16000, episode_reward=-127.46 +/- 26.41
Episode length: 128.46 +/- 26.41
Eval num_timesteps=16500, episode_reward=-119.13 +/- 34.53
Episode length: 120.13 +/- 34.53
New best mean reward!
Eval num_timesteps=17000, episode_reward=-119.90 +/- 29.66
Episode length: 120.90 +/- 29.66
Eval num_timesteps=17500, episode_reward=-128.32 +/- 41.31
Episode length: 129.32 +/- 41.31
Eval num_timesteps=18000, episode_reward=-111.47 +/- 27.42
Episode length: 112.47 +/- 27.42
New best mean reward!
Eval num_timesteps=18500, episode_reward=-110.87 +/- 25.78
Episode length: 111.87 +/- 25.78
New best mean reward!
Eval num_timesteps=19000, episode_reward=-128.23 +/- 82.65
Episode length: 129.19 +/- 82.47
Eval num_timesteps=19500, episode_reward=-116.16 +/- 45.15
Episode length: 117.16 +/- 45.15
Eval num_timesteps=20000, episode_reward=-163.37 +/- 133.00
Episode length: 164.25 +/- 132.69
Eval num_timesteps=20500, episode_reward=-106.23 +/- 49.13
Episode length: 107.23 +/- 49.13
New best mean reward!
Eval num_timesteps=21000, episode_reward=-122.07 +/- 94.81
Episode length: 123.03 +/- 94.65
Eval num_timesteps=21500, episode_reward=-150.01 +/- 122.13
Episode length: 150.91 +/- 121.84
Eval num_timesteps=22000, episode_reward=-177.33 +/- 155.02
Episode length: 178.15 +/- 154.65
Eval num_timesteps=22500, episode_reward=-210.55 +/- 179.06
Episode length: 211.28 +/- 178.63
Eval num_timesteps=23000, episode_reward=-159.34 +/- 148.91
Episode length: 160.19 +/- 148.56
Eval num_timesteps=23500, episode_reward=-107.32 +/- 81.90
Episode length: 108.28 +/- 81.71
Eval num_timesteps=24000, episode_reward=-181.05 +/- 164.26
Episode length: 181.85 +/- 163.87
Eval num_timesteps=24500, episode_reward=-178.38 +/- 163.68
Episode length: 179.18 +/- 163.28
Eval num_timesteps=25000, episode_reward=-214.90 +/- 183.51
Episode length: 215.62 +/- 183.07
Eval num_timesteps=25500, episode_reward=-177.90 +/- 167.03
Episode length: 178.69 +/- 166.63
Eval num_timesteps=26000, episode_reward=-216.13 +/- 189.76
Episode length: 216.83 +/- 189.32
Eval num_timesteps=26500, episode_reward=-242.38 +/- 195.39
Episode length: 243.02 +/- 194.91
Eval num_timesteps=27000, episode_reward=-192.22 +/- 178.77
Episode length: 192.98 +/- 178.35
Eval num_timesteps=27500, episode_reward=-186.96 +/- 171.35
Episode length: 187.74 +/- 170.95
Eval num_timesteps=28000, episode_reward=-177.35 +/- 164.66
Episode length: 178.17 +/- 164.30
Eval num_timesteps=28500, episode_reward=-147.51 +/- 139.81
Episode length: 148.39 +/- 139.50
Eval num_timesteps=29000, episode_reward=-202.49 +/- 179.13
Episode length: 203.23 +/- 178.69
Eval num_timesteps=29500, episode_reward=-191.06 +/- 169.89
Episode length: 191.84 +/- 169.49
Eval num_timesteps=30000, episode_reward=-189.91 +/- 175.36
Episode length: 190.67 +/- 174.94
Eval num_timesteps=30500, episode_reward=-154.77 +/- 147.32
Episode length: 155.62 +/- 146.97
Eval num_timesteps=31000, episode_reward=-162.02 +/- 155.90
Episode length: 162.85 +/- 155.54
Eval num_timesteps=31500, episode_reward=-134.86 +/- 126.03
Episode length: 135.76 +/- 125.74
Eval num_timesteps=32000, episode_reward=-159.81 +/- 155.52
Episode length: 160.64 +/- 155.15
Eval num_timesteps=32500, episode_reward=-121.83 +/- 109.15
Episode length: 122.76 +/- 108.91
Eval num_timesteps=33000, episode_reward=-122.90 +/- 117.65
Episode length: 123.82 +/- 117.39
Eval num_timesteps=33500, episode_reward=-118.20 +/- 102.49
Episode length: 119.14 +/- 102.26
Eval num_timesteps=34000, episode_reward=-116.09 +/- 107.67
Episode length: 117.02 +/- 107.42
Eval num_timesteps=34500, episode_reward=-141.18 +/- 135.82
Episode length: 142.07 +/- 135.53
Eval num_timesteps=35000, episode_reward=-113.84 +/- 85.29
Episode length: 114.80 +/- 85.11
Eval num_timesteps=35500, episode_reward=-106.34 +/- 86.45
Episode length: 107.30 +/- 86.26
Eval num_timesteps=36000, episode_reward=-114.86 +/- 103.88
Episode length: 115.80 +/- 103.66
Eval num_timesteps=36500, episode_reward=-122.86 +/- 118.07
Episode length: 123.78 +/- 117.81
Eval num_timesteps=37000, episode_reward=-90.13 +/- 44.97
Episode length: 91.12 +/- 44.88
New best mean reward!
FINISHED IN 541.6547994730063 s


starting seed  10168 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-95.65 +/- 37.82
Episode length: 96.65 +/- 37.82
New best mean reward!
FINISHED IN 221.79963927698554 s


starting seed  10169 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-493.20 +/- 48.30
Episode length: 493.22 +/- 48.16
New best mean reward!
Eval num_timesteps=12500, episode_reward=-406.30 +/- 171.53
Episode length: 406.53 +/- 171.11
New best mean reward!
Eval num_timesteps=13000, episode_reward=-325.80 +/- 194.94
Episode length: 326.26 +/- 194.46
New best mean reward!
Eval num_timesteps=13500, episode_reward=-248.81 +/- 187.59
Episode length: 249.47 +/- 187.13
New best mean reward!
Eval num_timesteps=14000, episode_reward=-275.65 +/- 191.30
Episode length: 276.24 +/- 190.82
Eval num_timesteps=14500, episode_reward=-231.87 +/- 185.61
Episode length: 232.55 +/- 185.15
New best mean reward!
Eval num_timesteps=15000, episode_reward=-292.60 +/- 197.60
Episode length: 293.15 +/- 197.13
Eval num_timesteps=15500, episode_reward=-288.49 +/- 192.86
Episode length: 289.05 +/- 192.38
Eval num_timesteps=16000, episode_reward=-306.49 +/- 198.30
Episode length: 306.98 +/- 197.81
Eval num_timesteps=16500, episode_reward=-311.34 +/- 197.98
Episode length: 311.82 +/- 197.49
Eval num_timesteps=17000, episode_reward=-294.18 +/- 194.86
Episode length: 294.72 +/- 194.37
Eval num_timesteps=17500, episode_reward=-183.95 +/- 162.34
Episode length: 184.75 +/- 161.96
New best mean reward!
Eval num_timesteps=18000, episode_reward=-162.54 +/- 146.31
Episode length: 163.39 +/- 145.97
New best mean reward!
Eval num_timesteps=18500, episode_reward=-177.37 +/- 162.11
Episode length: 178.18 +/- 161.73
Eval num_timesteps=19000, episode_reward=-224.97 +/- 187.43
Episode length: 225.67 +/- 186.99
Eval num_timesteps=19500, episode_reward=-151.99 +/- 142.93
Episode length: 152.86 +/- 142.61
New best mean reward!
Eval num_timesteps=20000, episode_reward=-115.26 +/- 77.33
Episode length: 116.23 +/- 77.18
New best mean reward!
Eval num_timesteps=20500, episode_reward=-124.87 +/- 107.32
Episode length: 125.80 +/- 107.08
Eval num_timesteps=21000, episode_reward=-134.21 +/- 110.71
Episode length: 135.14 +/- 110.48
Eval num_timesteps=21500, episode_reward=-141.57 +/- 124.87
Episode length: 142.47 +/- 124.59
Eval num_timesteps=22000, episode_reward=-130.86 +/- 109.43
Episode length: 131.79 +/- 109.19
Eval num_timesteps=22500, episode_reward=-129.15 +/- 103.61
Episode length: 130.09 +/- 103.40
Eval num_timesteps=23000, episode_reward=-113.03 +/- 82.65
Episode length: 114.00 +/- 82.51
New best mean reward!
Eval num_timesteps=23500, episode_reward=-106.18 +/- 65.74
Episode length: 107.16 +/- 65.62
New best mean reward!
Eval num_timesteps=24000, episode_reward=-95.99 +/- 45.36
Episode length: 96.98 +/- 45.27
New best mean reward!
FINISHED IN 500.86817351100035 s


starting seed  10170 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-498.46 +/- 9.76
Episode length: 498.49 +/- 9.61
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-197.85 +/- 54.46
Episode length: 198.84 +/- 54.40
New best mean reward!
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-453.81 +/- 78.21
Episode length: 454.12 +/- 77.80
Eval num_timesteps=7500, episode_reward=-192.40 +/- 75.50
Episode length: 193.36 +/- 75.34
New best mean reward!
Eval num_timesteps=8000, episode_reward=-291.93 +/- 70.32
Episode length: 292.89 +/- 70.20
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-177.82 +/- 147.93
Episode length: 178.65 +/- 147.56
New best mean reward!
Eval num_timesteps=10000, episode_reward=-125.32 +/- 60.01
Episode length: 126.30 +/- 59.89
New best mean reward!
Eval num_timesteps=10500, episode_reward=-107.68 +/- 46.65
Episode length: 108.68 +/- 46.65
New best mean reward!
Eval num_timesteps=11000, episode_reward=-118.61 +/- 60.12
Episode length: 119.59 +/- 59.99
Eval num_timesteps=11500, episode_reward=-144.86 +/- 34.09
Episode length: 145.86 +/- 34.09
Eval num_timesteps=12000, episode_reward=-160.00 +/- 67.15
Episode length: 160.98 +/- 67.05
Eval num_timesteps=12500, episode_reward=-133.31 +/- 58.79
Episode length: 134.30 +/- 58.73
Eval num_timesteps=13000, episode_reward=-88.31 +/- 20.42
Episode length: 89.31 +/- 20.42
New best mean reward!
FINISHED IN 245.90007462899666 s


starting seed  10171 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-348.47 +/- 188.72
Episode length: 348.87 +/- 188.23
New best mean reward!
Eval num_timesteps=12000, episode_reward=-193.63 +/- 161.42
Episode length: 194.43 +/- 161.04
New best mean reward!
Eval num_timesteps=12500, episode_reward=-190.76 +/- 150.68
Episode length: 191.58 +/- 150.31
New best mean reward!
Eval num_timesteps=13000, episode_reward=-129.22 +/- 95.86
Episode length: 130.17 +/- 95.67
New best mean reward!
Eval num_timesteps=13500, episode_reward=-197.63 +/- 161.91
Episode length: 198.43 +/- 161.53
Eval num_timesteps=14000, episode_reward=-209.89 +/- 178.17
Episode length: 210.62 +/- 177.73
Eval num_timesteps=14500, episode_reward=-184.48 +/- 158.86
Episode length: 185.29 +/- 158.48
Eval num_timesteps=15000, episode_reward=-184.02 +/- 153.18
Episode length: 184.84 +/- 152.81
Eval num_timesteps=15500, episode_reward=-263.65 +/- 188.02
Episode length: 264.27 +/- 187.54
Eval num_timesteps=16000, episode_reward=-350.49 +/- 186.64
Episode length: 350.90 +/- 186.17
Eval num_timesteps=16500, episode_reward=-356.51 +/- 188.21
Episode length: 356.88 +/- 187.73
Eval num_timesteps=17000, episode_reward=-165.09 +/- 137.94
Episode length: 165.96 +/- 137.62
Eval num_timesteps=17500, episode_reward=-140.18 +/- 119.70
Episode length: 141.09 +/- 119.43
Eval num_timesteps=18000, episode_reward=-167.14 +/- 145.61
Episode length: 168.00 +/- 145.29
Eval num_timesteps=18500, episode_reward=-138.57 +/- 122.97
Episode length: 139.47 +/- 122.67
Eval num_timesteps=19000, episode_reward=-120.69 +/- 92.52
Episode length: 121.64 +/- 92.32
New best mean reward!
Eval num_timesteps=19500, episode_reward=-114.63 +/- 91.18
Episode length: 115.58 +/- 90.96
New best mean reward!
Eval num_timesteps=20000, episode_reward=-119.09 +/- 92.47
Episode length: 120.04 +/- 92.26
Eval num_timesteps=20500, episode_reward=-111.40 +/- 78.07
Episode length: 112.37 +/- 77.92
New best mean reward!
Eval num_timesteps=21000, episode_reward=-131.82 +/- 107.34
Episode length: 132.75 +/- 107.10
Eval num_timesteps=21500, episode_reward=-130.35 +/- 114.50
Episode length: 131.27 +/- 114.24
Eval num_timesteps=22000, episode_reward=-113.42 +/- 75.79
Episode length: 114.39 +/- 75.64
Eval num_timesteps=22500, episode_reward=-99.46 +/- 44.48
Episode length: 100.46 +/- 44.48
New best mean reward!
FINISHED IN 433.06313720700564 s


starting seed  10172 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-279.56 +/- 197.43
Episode length: 280.13 +/- 196.95
New best mean reward!
Eval num_timesteps=10500, episode_reward=-292.91 +/- 203.45
Episode length: 293.42 +/- 202.95
Eval num_timesteps=11000, episode_reward=-309.48 +/- 200.53
Episode length: 309.96 +/- 200.04
Eval num_timesteps=11500, episode_reward=-134.15 +/- 125.62
Episode length: 135.05 +/- 125.33
New best mean reward!
Eval num_timesteps=12000, episode_reward=-129.33 +/- 120.63
Episode length: 130.25 +/- 120.38
New best mean reward!
Eval num_timesteps=12500, episode_reward=-153.87 +/- 145.45
Episode length: 154.73 +/- 145.12
Eval num_timesteps=13000, episode_reward=-131.56 +/- 121.25
Episode length: 132.47 +/- 120.97
Eval num_timesteps=13500, episode_reward=-134.35 +/- 120.85
Episode length: 135.26 +/- 120.58
Eval num_timesteps=14000, episode_reward=-150.41 +/- 139.42
Episode length: 151.29 +/- 139.12
Eval num_timesteps=14500, episode_reward=-134.59 +/- 122.59
Episode length: 135.50 +/- 122.32
Eval num_timesteps=15000, episode_reward=-111.36 +/- 88.20
Episode length: 112.33 +/- 88.07
New best mean reward!
Eval num_timesteps=15500, episode_reward=-120.53 +/- 102.25
Episode length: 121.47 +/- 102.03
Eval num_timesteps=16000, episode_reward=-101.30 +/- 61.80
Episode length: 102.29 +/- 61.73
New best mean reward!
Eval num_timesteps=16500, episode_reward=-105.42 +/- 59.07
Episode length: 106.41 +/- 59.00
Eval num_timesteps=17000, episode_reward=-101.51 +/- 57.15
Episode length: 102.50 +/- 57.08
Eval num_timesteps=17500, episode_reward=-104.35 +/- 51.04
Episode length: 105.35 +/- 51.04
Eval num_timesteps=18000, episode_reward=-100.73 +/- 64.65
Episode length: 101.71 +/- 64.53
New best mean reward!
Eval num_timesteps=18500, episode_reward=-103.82 +/- 71.21
Episode length: 104.80 +/- 71.10
Eval num_timesteps=19000, episode_reward=-119.33 +/- 95.08
Episode length: 120.28 +/- 94.88
Eval num_timesteps=19500, episode_reward=-96.04 +/- 44.12
Episode length: 97.04 +/- 44.12
New best mean reward!
FINISHED IN 345.1493823020137 s


starting seed  10173 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-480.68 +/- 76.63
Episode length: 480.74 +/- 76.39
New best mean reward!
Eval num_timesteps=1500, episode_reward=-476.05 +/- 87.35
Episode length: 476.12 +/- 87.09
New best mean reward!
Eval num_timesteps=2000, episode_reward=-326.37 +/- 168.43
Episode length: 326.89 +/- 167.94
New best mean reward!
Eval num_timesteps=2500, episode_reward=-483.48 +/- 72.05
Episode length: 483.53 +/- 71.83
Eval num_timesteps=3000, episode_reward=-409.70 +/- 146.90
Episode length: 409.98 +/- 146.46
Eval num_timesteps=3500, episode_reward=-350.60 +/- 166.47
Episode length: 351.05 +/- 165.98
Eval num_timesteps=4000, episode_reward=-304.62 +/- 168.37
Episode length: 305.20 +/- 167.88
New best mean reward!
Eval num_timesteps=4500, episode_reward=-418.51 +/- 139.32
Episode length: 418.78 +/- 138.89
Eval num_timesteps=5000, episode_reward=-278.31 +/- 162.68
Episode length: 278.97 +/- 162.22
New best mean reward!
Eval num_timesteps=5500, episode_reward=-211.74 +/- 118.75
Episode length: 212.60 +/- 118.41
New best mean reward!
Eval num_timesteps=6000, episode_reward=-250.84 +/- 149.58
Episode length: 251.58 +/- 149.14
Eval num_timesteps=6500, episode_reward=-393.60 +/- 104.92
Episode length: 394.15 +/- 104.47
Eval num_timesteps=7000, episode_reward=-381.13 +/- 151.23
Episode length: 381.52 +/- 150.75
Eval num_timesteps=7500, episode_reward=-322.00 +/- 156.08
Episode length: 322.57 +/- 155.59
Eval num_timesteps=8000, episode_reward=-362.07 +/- 156.87
Episode length: 362.51 +/- 156.38
Eval num_timesteps=8500, episode_reward=-312.77 +/- 151.82
Episode length: 313.40 +/- 151.36
Eval num_timesteps=9000, episode_reward=-367.78 +/- 145.15
Episode length: 368.24 +/- 144.65
Eval num_timesteps=9500, episode_reward=-321.20 +/- 157.69
Episode length: 321.77 +/- 157.20
Eval num_timesteps=10000, episode_reward=-319.64 +/- 148.47
Episode length: 320.25 +/- 147.99
Eval num_timesteps=10500, episode_reward=-278.08 +/- 150.70
Episode length: 278.77 +/- 150.24
Eval num_timesteps=11000, episode_reward=-277.17 +/- 156.46
Episode length: 277.85 +/- 156.00
Eval num_timesteps=11500, episode_reward=-387.01 +/- 151.91
Episode length: 387.37 +/- 151.44
Eval num_timesteps=12000, episode_reward=-264.02 +/- 156.42
Episode length: 264.72 +/- 155.97
Eval num_timesteps=12500, episode_reward=-391.11 +/- 140.59
Episode length: 391.49 +/- 140.11
Eval num_timesteps=13000, episode_reward=-435.76 +/- 100.82
Episode length: 436.06 +/- 100.38
Eval num_timesteps=13500, episode_reward=-284.86 +/- 146.08
Episode length: 285.55 +/- 145.62
Eval num_timesteps=14000, episode_reward=-354.26 +/- 153.53
Episode length: 354.74 +/- 153.03
Eval num_timesteps=14500, episode_reward=-257.56 +/- 132.80
Episode length: 258.34 +/- 132.40
Eval num_timesteps=15000, episode_reward=-278.59 +/- 139.97
Episode length: 279.32 +/- 139.54
Eval num_timesteps=15500, episode_reward=-178.78 +/- 76.80
Episode length: 179.73 +/- 76.59
New best mean reward!
Eval num_timesteps=16000, episode_reward=-257.03 +/- 146.23
Episode length: 257.78 +/- 145.81
Eval num_timesteps=16500, episode_reward=-231.37 +/- 137.97
Episode length: 232.17 +/- 137.58
Eval num_timesteps=17000, episode_reward=-330.98 +/- 158.18
Episode length: 331.52 +/- 157.69
Eval num_timesteps=17500, episode_reward=-262.45 +/- 158.26
Episode length: 263.15 +/- 157.81
Eval num_timesteps=18000, episode_reward=-229.11 +/- 131.09
Episode length: 229.94 +/- 130.74
Eval num_timesteps=18500, episode_reward=-235.52 +/- 140.78
Episode length: 236.31 +/- 140.39
Eval num_timesteps=19000, episode_reward=-194.90 +/- 104.34
Episode length: 195.80 +/- 104.05
Eval num_timesteps=19500, episode_reward=-174.27 +/- 58.54
Episode length: 175.25 +/- 58.43
New best mean reward!
Eval num_timesteps=20000, episode_reward=-165.56 +/- 42.24
Episode length: 166.55 +/- 42.16
New best mean reward!
Eval num_timesteps=20500, episode_reward=-169.66 +/- 64.45
Episode length: 170.63 +/- 64.29
Eval num_timesteps=21000, episode_reward=-158.20 +/- 33.26
Episode length: 159.20 +/- 33.26
New best mean reward!
Eval num_timesteps=21500, episode_reward=-155.89 +/- 41.90
Episode length: 156.88 +/- 41.82
New best mean reward!
Eval num_timesteps=22000, episode_reward=-161.24 +/- 29.69
Episode length: 162.24 +/- 29.69
Eval num_timesteps=22500, episode_reward=-156.29 +/- 32.68
Episode length: 157.29 +/- 32.68
Eval num_timesteps=23000, episode_reward=-159.75 +/- 41.38
Episode length: 160.75 +/- 41.38
Eval num_timesteps=23500, episode_reward=-159.97 +/- 39.67
Episode length: 160.97 +/- 39.67
Eval num_timesteps=24000, episode_reward=-168.43 +/- 46.97
Episode length: 169.42 +/- 46.90
Eval num_timesteps=24500, episode_reward=-158.48 +/- 28.83
Episode length: 159.48 +/- 28.83
Eval num_timesteps=25000, episode_reward=-154.71 +/- 24.79
Episode length: 155.71 +/- 24.79
New best mean reward!
Eval num_timesteps=25500, episode_reward=-157.26 +/- 25.57
Episode length: 158.26 +/- 25.57
Eval num_timesteps=26000, episode_reward=-159.33 +/- 36.52
Episode length: 160.33 +/- 36.52
Eval num_timesteps=26500, episode_reward=-185.22 +/- 84.89
Episode length: 186.16 +/- 84.66
Eval num_timesteps=27000, episode_reward=-164.36 +/- 64.25
Episode length: 165.33 +/- 64.09
Eval num_timesteps=27500, episode_reward=-166.24 +/- 53.46
Episode length: 167.22 +/- 53.33
Eval num_timesteps=28000, episode_reward=-182.48 +/- 78.30
Episode length: 183.44 +/- 78.14
Eval num_timesteps=28500, episode_reward=-199.27 +/- 99.71
Episode length: 200.19 +/- 99.47
Eval num_timesteps=29000, episode_reward=-201.79 +/- 102.84
Episode length: 202.70 +/- 102.58
Eval num_timesteps=29500, episode_reward=-207.70 +/- 111.83
Episode length: 208.58 +/- 111.52
Eval num_timesteps=30000, episode_reward=-199.74 +/- 103.57
Episode length: 200.64 +/- 103.28
Eval num_timesteps=30500, episode_reward=-201.90 +/- 104.49
Episode length: 202.80 +/- 104.20
Eval num_timesteps=31000, episode_reward=-193.43 +/- 99.41
Episode length: 194.34 +/- 99.14
Eval num_timesteps=31500, episode_reward=-181.26 +/- 83.67
Episode length: 182.20 +/- 83.44
Eval num_timesteps=32000, episode_reward=-185.12 +/- 84.49
Episode length: 186.06 +/- 84.26
Eval num_timesteps=32500, episode_reward=-178.84 +/- 63.71
Episode length: 179.81 +/- 63.56
Eval num_timesteps=33000, episode_reward=-162.60 +/- 28.78
Episode length: 163.60 +/- 28.78
Eval num_timesteps=33500, episode_reward=-158.26 +/- 25.63
Episode length: 159.26 +/- 25.63
Eval num_timesteps=34000, episode_reward=-161.20 +/- 30.79
Episode length: 162.20 +/- 30.79
Eval num_timesteps=34500, episode_reward=-158.49 +/- 27.97
Episode length: 159.49 +/- 27.97
Eval num_timesteps=35000, episode_reward=-172.93 +/- 58.99
Episode length: 173.91 +/- 58.88
Eval num_timesteps=35500, episode_reward=-160.09 +/- 33.73
Episode length: 161.09 +/- 33.73
Eval num_timesteps=36000, episode_reward=-160.90 +/- 35.99
Episode length: 161.90 +/- 35.99
Eval num_timesteps=36500, episode_reward=-160.78 +/- 31.46
Episode length: 161.78 +/- 31.46
Eval num_timesteps=37000, episode_reward=-152.96 +/- 21.52
Episode length: 153.96 +/- 21.52
New best mean reward!
Eval num_timesteps=37500, episode_reward=-158.30 +/- 43.13
Episode length: 159.29 +/- 43.05
Eval num_timesteps=38000, episode_reward=-160.41 +/- 41.36
Episode length: 161.41 +/- 41.36
Eval num_timesteps=38500, episode_reward=-153.25 +/- 21.70
Episode length: 154.25 +/- 21.70
Eval num_timesteps=39000, episode_reward=-161.00 +/- 44.34
Episode length: 161.99 +/- 44.26
Eval num_timesteps=39500, episode_reward=-155.18 +/- 23.39
Episode length: 156.18 +/- 23.39
Eval num_timesteps=40000, episode_reward=-154.81 +/- 24.50
Episode length: 155.81 +/- 24.50
Eval num_timesteps=40500, episode_reward=-149.94 +/- 20.93
Episode length: 150.94 +/- 20.93
New best mean reward!
Eval num_timesteps=41000, episode_reward=-153.54 +/- 24.25
Episode length: 154.54 +/- 24.25
Eval num_timesteps=41500, episode_reward=-153.98 +/- 26.60
Episode length: 154.98 +/- 26.60
Eval num_timesteps=42000, episode_reward=-159.98 +/- 29.99
Episode length: 160.98 +/- 29.99
Eval num_timesteps=42500, episode_reward=-152.87 +/- 30.76
Episode length: 153.87 +/- 30.76
Eval num_timesteps=43000, episode_reward=-155.25 +/- 48.96
Episode length: 156.25 +/- 48.96
Eval num_timesteps=43500, episode_reward=-145.27 +/- 32.26
Episode length: 146.27 +/- 32.26
New best mean reward!
Eval num_timesteps=44000, episode_reward=-151.75 +/- 46.59
Episode length: 152.75 +/- 46.59
Eval num_timesteps=44500, episode_reward=-143.85 +/- 36.17
Episode length: 144.85 +/- 36.17
New best mean reward!
Eval num_timesteps=45000, episode_reward=-134.43 +/- 41.80
Episode length: 135.43 +/- 41.80
New best mean reward!
Eval num_timesteps=45500, episode_reward=-149.85 +/- 48.87
Episode length: 150.85 +/- 48.87
Eval num_timesteps=46000, episode_reward=-131.97 +/- 34.30
Episode length: 132.97 +/- 34.30
New best mean reward!
Eval num_timesteps=46500, episode_reward=-122.80 +/- 31.03
Episode length: 123.80 +/- 31.03
New best mean reward!
Eval num_timesteps=47000, episode_reward=-136.96 +/- 63.18
Episode length: 137.94 +/- 63.07
Eval num_timesteps=47500, episode_reward=-129.81 +/- 29.07
Episode length: 130.81 +/- 29.07
Eval num_timesteps=48000, episode_reward=-134.19 +/- 50.48
Episode length: 135.18 +/- 50.41
Eval num_timesteps=48500, episode_reward=-129.31 +/- 26.60
Episode length: 130.31 +/- 26.60
Eval num_timesteps=49000, episode_reward=-130.74 +/- 29.09
Episode length: 131.74 +/- 29.09
Eval num_timesteps=49500, episode_reward=-130.47 +/- 34.24
Episode length: 131.47 +/- 34.24
Eval num_timesteps=50000, episode_reward=-127.57 +/- 25.77
Episode length: 128.57 +/- 25.77
FINISHED IN 799.0482185040019 s


starting seed  10174 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-486.75 +/- 65.05
Episode length: 486.79 +/- 64.85
New best mean reward!
Eval num_timesteps=14500, episode_reward=-441.17 +/- 126.19
Episode length: 441.35 +/- 125.81
New best mean reward!
Eval num_timesteps=15000, episode_reward=-422.76 +/- 138.97
Episode length: 423.00 +/- 138.54
New best mean reward!
Eval num_timesteps=15500, episode_reward=-433.17 +/- 134.25
Episode length: 433.37 +/- 133.85
Eval num_timesteps=16000, episode_reward=-398.89 +/- 167.57
Episode length: 399.16 +/- 167.13
New best mean reward!
Eval num_timesteps=16500, episode_reward=-366.73 +/- 179.73
Episode length: 367.09 +/- 179.26
New best mean reward!
Eval num_timesteps=17000, episode_reward=-398.55 +/- 152.60
Episode length: 398.86 +/- 152.14
Eval num_timesteps=17500, episode_reward=-409.67 +/- 149.22
Episode length: 409.94 +/- 148.78
Eval num_timesteps=18000, episode_reward=-383.86 +/- 158.14
Episode length: 384.22 +/- 157.67
Eval num_timesteps=18500, episode_reward=-367.08 +/- 166.54
Episode length: 367.49 +/- 166.07
Eval num_timesteps=19000, episode_reward=-357.43 +/- 166.18
Episode length: 357.86 +/- 165.69
New best mean reward!
Eval num_timesteps=19500, episode_reward=-347.38 +/- 166.99
Episode length: 347.84 +/- 166.49
New best mean reward!
Eval num_timesteps=20000, episode_reward=-385.71 +/- 162.58
Episode length: 386.05 +/- 162.12
Eval num_timesteps=20500, episode_reward=-332.67 +/- 165.17
Episode length: 333.19 +/- 164.69
New best mean reward!
Eval num_timesteps=21000, episode_reward=-154.43 +/- 148.22
Episode length: 155.28 +/- 147.87
New best mean reward!
Eval num_timesteps=21500, episode_reward=-154.81 +/- 149.24
Episode length: 155.66 +/- 148.89
Eval num_timesteps=22000, episode_reward=-152.55 +/- 150.05
Episode length: 153.40 +/- 149.70
New best mean reward!
Eval num_timesteps=22500, episode_reward=-109.54 +/- 93.15
Episode length: 110.49 +/- 92.95
New best mean reward!
Eval num_timesteps=23000, episode_reward=-97.35 +/- 64.96
Episode length: 98.33 +/- 64.84
New best mean reward!
FINISHED IN 647.0456245919922 s


starting seed  10175 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-86.12 +/- 46.04
Episode length: 87.11 +/- 45.95
New best mean reward!
FINISHED IN 255.92232976295054 s


starting seed  10176 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-263.79 +/- 197.80
Episode length: 264.38 +/- 197.31
New best mean reward!
Eval num_timesteps=8500, episode_reward=-497.88 +/- 21.09
Episode length: 497.89 +/- 20.99
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-307.34 +/- 199.59
Episode length: 307.84 +/- 199.11
Eval num_timesteps=10500, episode_reward=-185.37 +/- 176.08
Episode length: 186.14 +/- 175.67
New best mean reward!
Eval num_timesteps=11000, episode_reward=-184.50 +/- 171.05
Episode length: 185.28 +/- 170.65
New best mean reward!
Eval num_timesteps=11500, episode_reward=-172.21 +/- 165.65
Episode length: 173.01 +/- 165.26
New best mean reward!
Eval num_timesteps=12000, episode_reward=-144.91 +/- 136.93
Episode length: 145.79 +/- 136.62
New best mean reward!
Eval num_timesteps=12500, episode_reward=-173.19 +/- 165.51
Episode length: 174.00 +/- 165.14
Eval num_timesteps=13000, episode_reward=-164.25 +/- 159.05
Episode length: 165.08 +/- 158.69
Eval num_timesteps=13500, episode_reward=-140.55 +/- 135.99
Episode length: 141.43 +/- 135.68
New best mean reward!
Eval num_timesteps=14000, episode_reward=-134.71 +/- 122.48
Episode length: 135.62 +/- 122.21
New best mean reward!
Eval num_timesteps=14500, episode_reward=-174.49 +/- 167.74
Episode length: 175.29 +/- 167.35
Eval num_timesteps=15000, episode_reward=-132.53 +/- 130.93
Episode length: 133.42 +/- 130.62
New best mean reward!
Eval num_timesteps=15500, episode_reward=-173.96 +/- 167.20
Episode length: 174.76 +/- 166.81
Eval num_timesteps=16000, episode_reward=-141.91 +/- 134.45
Episode length: 142.80 +/- 134.16
Eval num_timesteps=16500, episode_reward=-147.56 +/- 138.77
Episode length: 148.45 +/- 138.49
Eval num_timesteps=17000, episode_reward=-131.26 +/- 120.35
Episode length: 132.17 +/- 120.07
New best mean reward!
Eval num_timesteps=17500, episode_reward=-108.31 +/- 75.78
Episode length: 109.29 +/- 75.68
New best mean reward!
Eval num_timesteps=18000, episode_reward=-107.71 +/- 81.97
Episode length: 108.68 +/- 81.83
New best mean reward!
Eval num_timesteps=18500, episode_reward=-104.31 +/- 64.29
Episode length: 105.29 +/- 64.17
New best mean reward!
Eval num_timesteps=19000, episode_reward=-105.99 +/- 81.55
Episode length: 106.96 +/- 81.41
Eval num_timesteps=19500, episode_reward=-144.40 +/- 141.08
Episode length: 145.27 +/- 140.75
Eval num_timesteps=20000, episode_reward=-148.63 +/- 140.84
Episode length: 149.50 +/- 140.51
Eval num_timesteps=20500, episode_reward=-148.73 +/- 148.50
Episode length: 149.59 +/- 148.17
Eval num_timesteps=21000, episode_reward=-138.15 +/- 138.82
Episode length: 139.03 +/- 138.50
Eval num_timesteps=21500, episode_reward=-143.06 +/- 139.07
Episode length: 143.94 +/- 138.76
Eval num_timesteps=22000, episode_reward=-184.05 +/- 170.97
Episode length: 184.84 +/- 170.58
Eval num_timesteps=22500, episode_reward=-122.52 +/- 115.56
Episode length: 123.44 +/- 115.30
Eval num_timesteps=23000, episode_reward=-103.23 +/- 61.13
Episode length: 104.22 +/- 61.07
New best mean reward!
Eval num_timesteps=23500, episode_reward=-97.70 +/- 54.90
Episode length: 98.69 +/- 54.83
New best mean reward!
FINISHED IN 469.1825067800237 s


starting seed  10177 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-480.82 +/- 83.63
Episode length: 480.87 +/- 83.42
New best mean reward!
Eval num_timesteps=9500, episode_reward=-225.98 +/- 172.67
Episode length: 226.70 +/- 172.22
New best mean reward!
Eval num_timesteps=10000, episode_reward=-109.25 +/- 41.71
Episode length: 110.24 +/- 41.62
New best mean reward!
Eval num_timesteps=10500, episode_reward=-116.65 +/- 30.19
Episode length: 117.65 +/- 30.19
Eval num_timesteps=11000, episode_reward=-109.87 +/- 29.33
Episode length: 110.87 +/- 29.33
Eval num_timesteps=11500, episode_reward=-115.79 +/- 31.91
Episode length: 116.79 +/- 31.91
Eval num_timesteps=12000, episode_reward=-136.29 +/- 36.30
Episode length: 137.29 +/- 36.30
Eval num_timesteps=12500, episode_reward=-126.58 +/- 33.50
Episode length: 127.58 +/- 33.50
Eval num_timesteps=13000, episode_reward=-109.39 +/- 31.65
Episode length: 110.39 +/- 31.65
Eval num_timesteps=13500, episode_reward=-112.21 +/- 26.27
Episode length: 113.21 +/- 26.27
Eval num_timesteps=14000, episode_reward=-114.76 +/- 34.81
Episode length: 115.76 +/- 34.81
Eval num_timesteps=14500, episode_reward=-108.15 +/- 25.34
Episode length: 109.15 +/- 25.34
New best mean reward!
Eval num_timesteps=15000, episode_reward=-110.39 +/- 26.53
Episode length: 111.39 +/- 26.53
Eval num_timesteps=15500, episode_reward=-114.92 +/- 47.73
Episode length: 115.91 +/- 47.65
Eval num_timesteps=16000, episode_reward=-119.61 +/- 55.62
Episode length: 120.60 +/- 55.55
Eval num_timesteps=16500, episode_reward=-110.52 +/- 38.53
Episode length: 111.52 +/- 38.53
Eval num_timesteps=17000, episode_reward=-97.40 +/- 26.74
Episode length: 98.40 +/- 26.74
New best mean reward!
FINISHED IN 287.1413108360139 s


starting seed  10178 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-100.80 +/- 46.90
Episode length: 101.79 +/- 46.81
New best mean reward!
Eval num_timesteps=8500, episode_reward=-100.18 +/- 48.16
Episode length: 101.17 +/- 48.08
New best mean reward!
Eval num_timesteps=9000, episode_reward=-103.24 +/- 59.61
Episode length: 104.23 +/- 59.54
Eval num_timesteps=9500, episode_reward=-85.17 +/- 14.27
Episode length: 86.17 +/- 14.27
New best mean reward!
FINISHED IN 202.86776849499438 s


starting seed  10179 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-290.04 +/- 188.64
Episode length: 290.60 +/- 188.15
New best mean reward!
Eval num_timesteps=10000, episode_reward=-259.91 +/- 185.18
Episode length: 260.56 +/- 184.72
New best mean reward!
Eval num_timesteps=10500, episode_reward=-214.16 +/- 164.60
Episode length: 214.93 +/- 164.20
New best mean reward!
Eval num_timesteps=11000, episode_reward=-128.48 +/- 94.75
Episode length: 129.43 +/- 94.55
New best mean reward!
Eval num_timesteps=11500, episode_reward=-122.45 +/- 79.20
Episode length: 123.42 +/- 79.06
New best mean reward!
Eval num_timesteps=12000, episode_reward=-132.12 +/- 94.69
Episode length: 133.07 +/- 94.49
Eval num_timesteps=12500, episode_reward=-151.40 +/- 128.31
Episode length: 152.29 +/- 128.02
Eval num_timesteps=13000, episode_reward=-157.40 +/- 116.07
Episode length: 158.31 +/- 115.81
Eval num_timesteps=13500, episode_reward=-98.30 +/- 46.03
Episode length: 99.29 +/- 45.94
New best mean reward!
FINISHED IN 268.4964464450022 s


starting seed  10180 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-88.29 +/- 52.34
Episode length: 89.28 +/- 52.26
New best mean reward!
FINISHED IN 220.6784090399742 s


starting seed  10181 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-348.78 +/- 163.56
Episode length: 349.25 +/- 163.07
New best mean reward!
Eval num_timesteps=2000, episode_reward=-219.00 +/- 31.57
Episode length: 220.00 +/- 31.57
New best mean reward!
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-329.33 +/- 82.13
Episode length: 330.20 +/- 81.86
Eval num_timesteps=5000, episode_reward=-181.46 +/- 65.21
Episode length: 182.43 +/- 65.06
New best mean reward!
Eval num_timesteps=5500, episode_reward=-163.80 +/- 42.88
Episode length: 164.79 +/- 42.80
New best mean reward!
Eval num_timesteps=6000, episode_reward=-415.05 +/- 144.23
Episode length: 415.31 +/- 143.80
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-103.80 +/- 60.98
Episode length: 104.78 +/- 60.85
New best mean reward!
Eval num_timesteps=9500, episode_reward=-284.65 +/- 175.10
Episode length: 285.26 +/- 174.62
Eval num_timesteps=10000, episode_reward=-90.19 +/- 20.13
Episode length: 91.19 +/- 20.13
New best mean reward!
FINISHED IN 208.20366708998336 s


starting seed  10182 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-295.89 +/- 176.95
Episode length: 296.47 +/- 176.47
New best mean reward!
Eval num_timesteps=10500, episode_reward=-292.27 +/- 154.21
Episode length: 292.94 +/- 153.77
New best mean reward!
Eval num_timesteps=11000, episode_reward=-220.73 +/- 102.34
Episode length: 221.63 +/- 102.07
New best mean reward!
Eval num_timesteps=11500, episode_reward=-181.35 +/- 62.41
Episode length: 182.32 +/- 62.26
New best mean reward!
Eval num_timesteps=12000, episode_reward=-245.77 +/- 132.57
Episode length: 246.58 +/- 132.21
Eval num_timesteps=12500, episode_reward=-211.83 +/- 102.87
Episode length: 212.74 +/- 102.61
Eval num_timesteps=13000, episode_reward=-175.24 +/- 109.34
Episode length: 176.15 +/- 109.07
New best mean reward!
Eval num_timesteps=13500, episode_reward=-182.06 +/- 57.72
Episode length: 183.04 +/- 57.61
Eval num_timesteps=14000, episode_reward=-138.39 +/- 40.78
Episode length: 139.39 +/- 40.78
New best mean reward!
Eval num_timesteps=14500, episode_reward=-92.55 +/- 23.56
Episode length: 93.55 +/- 23.56
New best mean reward!
FINISHED IN 305.2490199370077 s


starting seed  10183 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-327.02 +/- 173.66
Episode length: 327.52 +/- 173.16
New best mean reward!
Eval num_timesteps=9000, episode_reward=-123.94 +/- 25.60
Episode length: 124.94 +/- 25.60
New best mean reward!
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-123.69 +/- 83.04
Episode length: 124.65 +/- 82.86
New best mean reward!
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-313.15 +/- 98.07
Episode length: 314.05 +/- 97.88
Eval num_timesteps=12000, episode_reward=-310.39 +/- 94.89
Episode length: 311.25 +/- 94.61
Eval num_timesteps=12500, episode_reward=-262.34 +/- 94.13
Episode length: 263.25 +/- 93.90
Eval num_timesteps=13000, episode_reward=-386.74 +/- 89.00
Episode length: 387.48 +/- 88.67
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-114.06 +/- 55.13
Episode length: 115.05 +/- 55.06
New best mean reward!
Eval num_timesteps=14500, episode_reward=-96.71 +/- 46.96
Episode length: 97.70 +/- 46.87
New best mean reward!
FINISHED IN 318.14480672299396 s


starting seed  10184 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-188.42 +/- 87.70
Episode length: 189.36 +/- 87.48
New best mean reward!
Eval num_timesteps=1000, episode_reward=-223.75 +/- 123.41
Episode length: 224.60 +/- 123.07
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-489.97 +/- 58.99
Episode length: 490.00 +/- 58.83
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-328.43 +/- 198.12
Episode length: 328.87 +/- 197.64
Eval num_timesteps=11500, episode_reward=-263.54 +/- 194.34
Episode length: 264.15 +/- 193.86
Eval num_timesteps=12000, episode_reward=-233.19 +/- 195.26
Episode length: 233.86 +/- 194.81
Eval num_timesteps=12500, episode_reward=-149.69 +/- 145.65
Episode length: 150.55 +/- 145.31
New best mean reward!
Eval num_timesteps=13000, episode_reward=-113.58 +/- 92.78
Episode length: 114.54 +/- 92.61
New best mean reward!
Eval num_timesteps=13500, episode_reward=-110.45 +/- 90.57
Episode length: 111.41 +/- 90.39
New best mean reward!
Eval num_timesteps=14000, episode_reward=-106.58 +/- 72.41
Episode length: 107.56 +/- 72.30
New best mean reward!
Eval num_timesteps=14500, episode_reward=-139.98 +/- 128.52
Episode length: 140.88 +/- 128.24
Eval num_timesteps=15000, episode_reward=-107.13 +/- 77.77
Episode length: 108.10 +/- 77.62
Eval num_timesteps=15500, episode_reward=-91.49 +/- 35.72
Episode length: 92.49 +/- 35.72
New best mean reward!
FINISHED IN 310.5094074999797 s


starting seed  10185 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-256.33 +/- 142.04
Episode length: 257.10 +/- 141.65
New best mean reward!
Eval num_timesteps=1000, episode_reward=-206.22 +/- 105.93
Episode length: 207.12 +/- 105.65
New best mean reward!
Eval num_timesteps=1500, episode_reward=-260.19 +/- 149.55
Episode length: 260.94 +/- 149.14
Eval num_timesteps=2000, episode_reward=-205.05 +/- 106.40
Episode length: 205.96 +/- 106.15
New best mean reward!
Eval num_timesteps=2500, episode_reward=-165.64 +/- 31.31
Episode length: 166.64 +/- 31.31
New best mean reward!
Eval num_timesteps=3000, episode_reward=-174.49 +/- 43.98
Episode length: 175.49 +/- 43.98
Eval num_timesteps=3500, episode_reward=-182.77 +/- 52.34
Episode length: 183.76 +/- 52.28
Eval num_timesteps=4000, episode_reward=-174.71 +/- 45.17
Episode length: 175.71 +/- 45.17
Eval num_timesteps=4500, episode_reward=-177.73 +/- 44.25
Episode length: 178.73 +/- 44.25
Eval num_timesteps=5000, episode_reward=-171.02 +/- 31.84
Episode length: 172.02 +/- 31.84
Eval num_timesteps=5500, episode_reward=-170.32 +/- 36.47
Episode length: 171.32 +/- 36.47
Eval num_timesteps=6000, episode_reward=-178.31 +/- 58.84
Episode length: 179.30 +/- 58.79
Eval num_timesteps=6500, episode_reward=-168.79 +/- 36.79
Episode length: 169.79 +/- 36.79
Eval num_timesteps=7000, episode_reward=-340.82 +/- 82.29
Episode length: 341.67 +/- 82.00
Eval num_timesteps=7500, episode_reward=-316.89 +/- 91.65
Episode length: 317.74 +/- 91.35
Eval num_timesteps=8000, episode_reward=-262.79 +/- 65.17
Episode length: 263.77 +/- 65.10
Eval num_timesteps=8500, episode_reward=-397.01 +/- 72.23
Episode length: 397.83 +/- 71.98
Eval num_timesteps=9000, episode_reward=-483.78 +/- 37.29
Episode length: 484.04 +/- 36.96
Eval num_timesteps=9500, episode_reward=-496.02 +/- 18.78
Episode length: 496.07 +/- 18.58
Eval num_timesteps=10000, episode_reward=-487.16 +/- 31.62
Episode length: 487.37 +/- 31.30
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-489.39 +/- 30.86
Episode length: 489.54 +/- 30.57
Eval num_timesteps=13000, episode_reward=-498.80 +/- 11.94
Episode length: 498.81 +/- 11.84
Eval num_timesteps=13500, episode_reward=-498.12 +/- 14.74
Episode length: 498.14 +/- 14.62
Eval num_timesteps=14000, episode_reward=-487.95 +/- 37.36
Episode length: 488.13 +/- 37.09
Eval num_timesteps=14500, episode_reward=-497.18 +/- 14.36
Episode length: 497.23 +/- 14.18
Eval num_timesteps=15000, episode_reward=-173.56 +/- 26.43
Episode length: 174.56 +/- 26.43
Eval num_timesteps=15500, episode_reward=-215.84 +/- 57.39
Episode length: 216.82 +/- 57.29
Eval num_timesteps=16000, episode_reward=-186.74 +/- 59.35
Episode length: 187.72 +/- 59.25
Eval num_timesteps=16500, episode_reward=-177.84 +/- 32.34
Episode length: 178.84 +/- 32.34
Eval num_timesteps=17000, episode_reward=-188.30 +/- 43.89
Episode length: 189.29 +/- 43.82
Eval num_timesteps=17500, episode_reward=-174.55 +/- 49.55
Episode length: 175.55 +/- 49.55
Eval num_timesteps=18000, episode_reward=-290.48 +/- 72.07
Episode length: 291.46 +/- 72.01
Eval num_timesteps=18500, episode_reward=-193.06 +/- 49.26
Episode length: 194.05 +/- 49.20
Eval num_timesteps=19000, episode_reward=-202.82 +/- 43.99
Episode length: 203.82 +/- 43.99
Eval num_timesteps=19500, episode_reward=-300.13 +/- 55.53
Episode length: 301.13 +/- 55.53
Eval num_timesteps=20000, episode_reward=-348.33 +/- 79.20
Episode length: 349.27 +/- 79.09
Eval num_timesteps=20500, episode_reward=-202.59 +/- 59.69
Episode length: 203.57 +/- 59.59
Eval num_timesteps=21000, episode_reward=-176.16 +/- 33.94
Episode length: 177.16 +/- 33.94
Eval num_timesteps=21500, episode_reward=-189.30 +/- 35.46
Episode length: 190.30 +/- 35.46
Eval num_timesteps=22000, episode_reward=-416.24 +/- 86.80
Episode length: 416.88 +/- 86.46
Eval num_timesteps=22500, episode_reward=-223.54 +/- 49.23
Episode length: 224.53 +/- 49.17
Eval num_timesteps=23000, episode_reward=-280.33 +/- 35.44
Episode length: 281.33 +/- 35.44
Eval num_timesteps=23500, episode_reward=-460.82 +/- 65.62
Episode length: 461.13 +/- 65.21
Eval num_timesteps=24000, episode_reward=-459.03 +/- 80.82
Episode length: 459.26 +/- 80.43
Eval num_timesteps=24500, episode_reward=-486.59 +/- 27.16
Episode length: 486.90 +/- 26.82
Eval num_timesteps=25000, episode_reward=-452.13 +/- 85.83
Episode length: 452.39 +/- 85.41
Eval num_timesteps=25500, episode_reward=-419.45 +/- 82.87
Episode length: 420.21 +/- 82.64
Eval num_timesteps=26000, episode_reward=-457.88 +/- 82.34
Episode length: 458.11 +/- 81.95
Eval num_timesteps=26500, episode_reward=-467.31 +/- 68.65
Episode length: 467.52 +/- 68.28
Eval num_timesteps=27000, episode_reward=-468.39 +/- 43.03
Episode length: 468.86 +/- 42.64
Eval num_timesteps=27500, episode_reward=-299.24 +/- 53.50
Episode length: 300.22 +/- 53.42
Eval num_timesteps=28000, episode_reward=-474.72 +/- 38.71
Episode length: 475.13 +/- 38.33
Eval num_timesteps=28500, episode_reward=-304.82 +/- 39.82
Episode length: 305.81 +/- 39.78
Eval num_timesteps=29000, episode_reward=-220.30 +/- 41.35
Episode length: 221.30 +/- 41.35
Eval num_timesteps=29500, episode_reward=-211.23 +/- 31.26
Episode length: 212.23 +/- 31.26
Eval num_timesteps=30000, episode_reward=-202.79 +/- 79.40
Episode length: 203.75 +/- 79.25
Eval num_timesteps=30500, episode_reward=-184.96 +/- 33.73
Episode length: 185.96 +/- 33.73
Eval num_timesteps=31000, episode_reward=-210.51 +/- 109.50
Episode length: 211.39 +/- 109.18
Eval num_timesteps=31500, episode_reward=-187.34 +/- 76.00
Episode length: 188.32 +/- 75.92
Eval num_timesteps=32000, episode_reward=-200.96 +/- 91.28
Episode length: 201.91 +/- 91.11
Eval num_timesteps=32500, episode_reward=-188.20 +/- 74.22
Episode length: 189.17 +/- 74.09
Eval num_timesteps=33000, episode_reward=-182.91 +/- 62.57
Episode length: 183.90 +/- 62.52
Eval num_timesteps=33500, episode_reward=-195.27 +/- 81.50
Episode length: 196.21 +/- 81.28
Eval num_timesteps=34000, episode_reward=-218.38 +/- 39.37
Episode length: 219.37 +/- 39.30
Eval num_timesteps=34500, episode_reward=-265.00 +/- 42.63
Episode length: 266.00 +/- 42.63
Eval num_timesteps=35000, episode_reward=-463.99 +/- 75.92
Episode length: 464.23 +/- 75.56
Eval num_timesteps=35500, episode_reward=-490.62 +/- 30.34
Episode length: 490.73 +/- 30.07
Eval num_timesteps=36000, episode_reward=-469.27 +/- 54.71
Episode length: 469.60 +/- 54.33
Eval num_timesteps=36500, episode_reward=-496.54 +/- 21.47
Episode length: 496.57 +/- 21.31
Eval num_timesteps=37000, episode_reward=-480.80 +/- 57.38
Episode length: 480.91 +/- 57.08
Eval num_timesteps=37500, episode_reward=-490.96 +/- 38.60
Episode length: 491.03 +/- 38.38
Eval num_timesteps=38000, episode_reward=-460.14 +/- 54.24
Episode length: 460.65 +/- 53.88
Eval num_timesteps=38500, episode_reward=-363.09 +/- 47.62
Episode length: 364.07 +/- 47.56
Eval num_timesteps=39000, episode_reward=-479.61 +/- 49.22
Episode length: 479.86 +/- 48.91
Eval num_timesteps=39500, episode_reward=-474.20 +/- 51.16
Episode length: 474.51 +/- 50.81
Eval num_timesteps=40000, episode_reward=-372.86 +/- 46.66
Episode length: 373.83 +/- 46.58
Eval num_timesteps=40500, episode_reward=-489.77 +/- 40.50
Episode length: 489.84 +/- 40.27
Eval num_timesteps=41000, episode_reward=-478.54 +/- 61.85
Episode length: 478.66 +/- 61.54
Eval num_timesteps=41500, episode_reward=-487.47 +/- 39.07
Episode length: 487.58 +/- 38.79
Eval num_timesteps=42000, episode_reward=-482.12 +/- 40.23
Episode length: 482.38 +/- 39.90
Eval num_timesteps=42500, episode_reward=-402.00 +/- 56.72
Episode length: 402.93 +/- 56.60
Eval num_timesteps=43000, episode_reward=-365.38 +/- 51.05
Episode length: 366.35 +/- 50.97
Eval num_timesteps=43500, episode_reward=-478.73 +/- 41.13
Episode length: 479.03 +/- 40.77
Eval num_timesteps=44000, episode_reward=-451.04 +/- 53.15
Episode length: 451.66 +/- 52.80
Eval num_timesteps=44500, episode_reward=-483.26 +/- 39.42
Episode length: 483.49 +/- 39.09
Eval num_timesteps=45000, episode_reward=-488.72 +/- 48.28
Episode length: 488.79 +/- 48.06
Eval num_timesteps=45500, episode_reward=-490.68 +/- 30.88
Episode length: 490.80 +/- 30.61
Eval num_timesteps=46000, episode_reward=-456.88 +/- 49.40
Episode length: 457.50 +/- 49.07
Eval num_timesteps=46500, episode_reward=-340.13 +/- 46.26
Episode length: 341.12 +/- 46.23
Eval num_timesteps=47000, episode_reward=-339.21 +/- 44.96
Episode length: 340.21 +/- 44.96
Eval num_timesteps=47500, episode_reward=-484.70 +/- 38.60
Episode length: 484.89 +/- 38.28
Eval num_timesteps=48000, episode_reward=-492.25 +/- 27.75
Episode length: 492.36 +/- 27.50
Eval num_timesteps=48500, episode_reward=-488.57 +/- 43.10
Episode length: 488.67 +/- 42.86
Eval num_timesteps=49000, episode_reward=-493.69 +/- 33.53
Episode length: 493.75 +/- 33.36
Eval num_timesteps=49500, episode_reward=-487.68 +/- 30.98
Episode length: 487.88 +/- 30.66
Eval num_timesteps=50000, episode_reward=-485.97 +/- 44.44
Episode length: 486.11 +/- 44.17
FINISHED IN 912.9845647129696 s


starting seed  10186 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-492.71 +/- 28.41
Episode length: 492.79 +/- 28.17
New best mean reward!
Eval num_timesteps=6000, episode_reward=-456.49 +/- 68.80
Episode length: 456.83 +/- 68.39
New best mean reward!
Eval num_timesteps=6500, episode_reward=-380.65 +/- 128.87
Episode length: 381.13 +/- 128.39
New best mean reward!
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-161.82 +/- 32.35
Episode length: 162.82 +/- 32.35
New best mean reward!
Eval num_timesteps=9500, episode_reward=-163.59 +/- 50.35
Episode length: 164.58 +/- 50.28
Eval num_timesteps=10000, episode_reward=-194.73 +/- 88.59
Episode length: 195.66 +/- 88.35
Eval num_timesteps=10500, episode_reward=-156.06 +/- 45.27
Episode length: 157.05 +/- 45.19
New best mean reward!
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-168.98 +/- 34.80
Episode length: 169.98 +/- 34.80
Eval num_timesteps=12000, episode_reward=-213.13 +/- 57.64
Episode length: 214.12 +/- 57.59
Eval num_timesteps=12500, episode_reward=-179.96 +/- 44.17
Episode length: 180.96 +/- 44.17
Eval num_timesteps=13000, episode_reward=-284.96 +/- 155.55
Episode length: 285.63 +/- 155.09
Eval num_timesteps=13500, episode_reward=-304.14 +/- 174.69
Episode length: 304.72 +/- 174.22
Eval num_timesteps=14000, episode_reward=-469.02 +/- 102.28
Episode length: 469.11 +/- 102.00
Eval num_timesteps=14500, episode_reward=-455.22 +/- 127.43
Episode length: 455.33 +/- 127.12
Eval num_timesteps=15000, episode_reward=-457.62 +/- 120.74
Episode length: 457.73 +/- 120.43
Eval num_timesteps=15500, episode_reward=-496.17 +/- 38.11
Episode length: 496.18 +/- 38.01
Eval num_timesteps=16000, episode_reward=-488.49 +/- 65.46
Episode length: 488.52 +/- 65.29
Eval num_timesteps=16500, episode_reward=-410.04 +/- 165.20
Episode length: 410.28 +/- 164.78
Eval num_timesteps=17000, episode_reward=-316.63 +/- 198.37
Episode length: 317.10 +/- 197.88
Eval num_timesteps=17500, episode_reward=-294.41 +/- 202.25
Episode length: 294.92 +/- 201.76
Eval num_timesteps=18000, episode_reward=-242.58 +/- 191.78
Episode length: 243.24 +/- 191.33
Eval num_timesteps=18500, episode_reward=-228.67 +/- 188.98
Episode length: 229.36 +/- 188.53
Eval num_timesteps=19000, episode_reward=-223.77 +/- 183.81
Episode length: 224.47 +/- 183.35
Eval num_timesteps=19500, episode_reward=-206.84 +/- 182.01
Episode length: 207.58 +/- 181.59
Eval num_timesteps=20000, episode_reward=-241.27 +/- 192.24
Episode length: 241.93 +/- 191.78
Eval num_timesteps=20500, episode_reward=-206.74 +/- 179.92
Episode length: 207.48 +/- 179.50
Eval num_timesteps=21000, episode_reward=-205.19 +/- 176.67
Episode length: 205.93 +/- 176.24
Eval num_timesteps=21500, episode_reward=-126.88 +/- 101.33
Episode length: 127.82 +/- 101.11
New best mean reward!
Eval num_timesteps=22000, episode_reward=-145.90 +/- 122.93
Episode length: 146.80 +/- 122.64
Eval num_timesteps=22500, episode_reward=-131.59 +/- 107.69
Episode length: 132.52 +/- 107.45
Eval num_timesteps=23000, episode_reward=-137.71 +/- 119.71
Episode length: 138.62 +/- 119.44
Eval num_timesteps=23500, episode_reward=-147.69 +/- 133.22
Episode length: 148.57 +/- 132.91
Eval num_timesteps=24000, episode_reward=-119.07 +/- 94.69
Episode length: 120.02 +/- 94.49
New best mean reward!
Eval num_timesteps=24500, episode_reward=-125.90 +/- 104.92
Episode length: 126.83 +/- 104.67
Eval num_timesteps=25000, episode_reward=-135.84 +/- 120.86
Episode length: 136.75 +/- 120.58
Eval num_timesteps=25500, episode_reward=-113.00 +/- 79.03
Episode length: 113.97 +/- 78.88
New best mean reward!
Eval num_timesteps=26000, episode_reward=-124.44 +/- 93.35
Episode length: 125.39 +/- 93.15
Eval num_timesteps=26500, episode_reward=-123.70 +/- 98.72
Episode length: 124.65 +/- 98.53
Eval num_timesteps=27000, episode_reward=-104.15 +/- 51.57
Episode length: 105.14 +/- 51.49
New best mean reward!
Eval num_timesteps=27500, episode_reward=-106.39 +/- 63.81
Episode length: 107.37 +/- 63.68
Eval num_timesteps=28000, episode_reward=-131.97 +/- 112.33
Episode length: 132.90 +/- 112.10
Eval num_timesteps=28500, episode_reward=-130.22 +/- 107.96
Episode length: 131.15 +/- 107.72
Eval num_timesteps=29000, episode_reward=-143.82 +/- 121.94
Episode length: 144.73 +/- 121.68
Eval num_timesteps=29500, episode_reward=-127.45 +/- 107.12
Episode length: 128.38 +/- 106.87
Eval num_timesteps=30000, episode_reward=-153.11 +/- 140.97
Episode length: 153.98 +/- 140.65
Eval num_timesteps=30500, episode_reward=-158.10 +/- 136.46
Episode length: 158.98 +/- 136.16
Eval num_timesteps=31000, episode_reward=-133.14 +/- 111.71
Episode length: 134.07 +/- 111.48
Eval num_timesteps=31500, episode_reward=-161.44 +/- 145.32
Episode length: 162.30 +/- 144.99
Eval num_timesteps=32000, episode_reward=-123.85 +/- 112.95
Episode length: 124.77 +/- 112.68
Eval num_timesteps=32500, episode_reward=-136.07 +/- 108.78
Episode length: 137.00 +/- 108.55
Eval num_timesteps=33000, episode_reward=-169.04 +/- 144.03
Episode length: 169.90 +/- 143.71
Eval num_timesteps=33500, episode_reward=-118.49 +/- 89.22
Episode length: 119.45 +/- 89.05
Eval num_timesteps=34000, episode_reward=-126.16 +/- 92.58
Episode length: 127.11 +/- 92.38
Eval num_timesteps=34500, episode_reward=-125.31 +/- 95.01
Episode length: 126.26 +/- 94.81
Eval num_timesteps=35000, episode_reward=-132.58 +/- 104.92
Episode length: 133.52 +/- 104.71
Eval num_timesteps=35500, episode_reward=-136.83 +/- 114.11
Episode length: 137.75 +/- 113.85
Eval num_timesteps=36000, episode_reward=-125.55 +/- 106.84
Episode length: 126.48 +/- 106.59
Eval num_timesteps=36500, episode_reward=-150.64 +/- 139.13
Episode length: 151.51 +/- 138.81
Eval num_timesteps=37000, episode_reward=-179.29 +/- 166.34
Episode length: 180.10 +/- 165.97
Eval num_timesteps=37500, episode_reward=-156.72 +/- 149.10
Episode length: 157.57 +/- 148.76
Eval num_timesteps=38000, episode_reward=-142.56 +/- 131.12
Episode length: 143.45 +/- 130.82
Eval num_timesteps=38500, episode_reward=-167.58 +/- 159.13
Episode length: 168.40 +/- 158.76
Eval num_timesteps=39000, episode_reward=-157.01 +/- 146.11
Episode length: 157.86 +/- 145.76
Eval num_timesteps=39500, episode_reward=-164.34 +/- 155.05
Episode length: 165.18 +/- 154.70
Eval num_timesteps=40000, episode_reward=-156.08 +/- 149.78
Episode length: 156.94 +/- 149.46
Eval num_timesteps=40500, episode_reward=-143.13 +/- 140.05
Episode length: 144.00 +/- 139.72
Eval num_timesteps=41000, episode_reward=-150.05 +/- 139.51
Episode length: 150.93 +/- 139.21
Eval num_timesteps=41500, episode_reward=-135.58 +/- 120.91
Episode length: 136.50 +/- 120.67
Eval num_timesteps=42000, episode_reward=-160.38 +/- 153.44
Episode length: 161.22 +/- 153.09
Eval num_timesteps=42500, episode_reward=-137.61 +/- 128.61
Episode length: 138.51 +/- 128.33
Eval num_timesteps=43000, episode_reward=-120.52 +/- 101.18
Episode length: 121.47 +/- 100.99
Eval num_timesteps=43500, episode_reward=-145.19 +/- 137.40
Episode length: 146.07 +/- 137.09
Eval num_timesteps=44000, episode_reward=-125.20 +/- 108.72
Episode length: 126.13 +/- 108.48
Eval num_timesteps=44500, episode_reward=-137.40 +/- 124.28
Episode length: 138.30 +/- 123.99
Eval num_timesteps=45000, episode_reward=-155.17 +/- 149.83
Episode length: 156.02 +/- 149.49
Eval num_timesteps=45500, episode_reward=-145.61 +/- 144.55
Episode length: 146.47 +/- 144.21
Eval num_timesteps=46000, episode_reward=-145.20 +/- 130.92
Episode length: 146.09 +/- 130.62
Eval num_timesteps=46500, episode_reward=-145.36 +/- 139.60
Episode length: 146.23 +/- 139.27
Eval num_timesteps=47000, episode_reward=-147.72 +/- 139.13
Episode length: 148.59 +/- 138.80
Eval num_timesteps=47500, episode_reward=-150.84 +/- 143.07
Episode length: 151.71 +/- 142.75
Eval num_timesteps=48000, episode_reward=-123.54 +/- 107.37
Episode length: 124.47 +/- 107.13
Eval num_timesteps=48500, episode_reward=-149.79 +/- 144.11
Episode length: 150.65 +/- 143.77
Eval num_timesteps=49000, episode_reward=-134.81 +/- 125.73
Episode length: 135.71 +/- 125.44
Eval num_timesteps=49500, episode_reward=-123.42 +/- 113.48
Episode length: 124.34 +/- 113.22
Eval num_timesteps=50000, episode_reward=-133.42 +/- 119.11
Episode length: 134.34 +/- 118.86
FINISHED IN 850.4324318559957 s


starting seed  10187 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-450.13 +/- 100.45
Episode length: 450.35 +/- 100.06
New best mean reward!
Eval num_timesteps=8000, episode_reward=-422.57 +/- 132.69
Episode length: 422.83 +/- 132.26
New best mean reward!
Eval num_timesteps=8500, episode_reward=-273.88 +/- 95.04
Episode length: 274.75 +/- 94.73
New best mean reward!
Eval num_timesteps=9000, episode_reward=-315.03 +/- 47.32
Episode length: 316.00 +/- 47.20
Eval num_timesteps=9500, episode_reward=-325.14 +/- 55.26
Episode length: 326.09 +/- 55.10
Eval num_timesteps=10000, episode_reward=-378.43 +/- 53.95
Episode length: 379.36 +/- 53.79
Eval num_timesteps=10500, episode_reward=-242.74 +/- 95.62
Episode length: 243.63 +/- 95.32
New best mean reward!
Eval num_timesteps=11000, episode_reward=-490.84 +/- 52.17
Episode length: 490.87 +/- 52.00
Eval num_timesteps=11500, episode_reward=-205.10 +/- 50.21
Episode length: 206.08 +/- 50.09
New best mean reward!
Eval num_timesteps=12000, episode_reward=-245.97 +/- 129.67
Episode length: 246.78 +/- 129.30
Eval num_timesteps=12500, episode_reward=-180.81 +/- 51.95
Episode length: 181.79 +/- 51.82
New best mean reward!
Eval num_timesteps=13000, episode_reward=-196.48 +/- 62.65
Episode length: 197.45 +/- 62.50
Eval num_timesteps=13500, episode_reward=-185.34 +/- 48.47
Episode length: 186.34 +/- 48.47
Eval num_timesteps=14000, episode_reward=-179.22 +/- 30.42
Episode length: 180.22 +/- 30.42
New best mean reward!
Eval num_timesteps=14500, episode_reward=-187.47 +/- 39.87
Episode length: 188.47 +/- 39.87
Eval num_timesteps=15000, episode_reward=-186.79 +/- 36.25
Episode length: 187.79 +/- 36.25
Eval num_timesteps=15500, episode_reward=-242.24 +/- 98.76
Episode length: 243.13 +/- 98.48
Eval num_timesteps=16000, episode_reward=-212.03 +/- 78.63
Episode length: 212.97 +/- 78.41
Eval num_timesteps=16500, episode_reward=-235.45 +/- 98.08
Episode length: 236.34 +/- 97.78
Eval num_timesteps=17000, episode_reward=-214.64 +/- 70.56
Episode length: 215.61 +/- 70.44
Eval num_timesteps=17500, episode_reward=-184.26 +/- 48.19
Episode length: 185.26 +/- 48.19
Eval num_timesteps=18000, episode_reward=-186.93 +/- 47.45
Episode length: 187.92 +/- 47.38
Eval num_timesteps=18500, episode_reward=-184.58 +/- 51.32
Episode length: 185.58 +/- 51.32
Eval num_timesteps=19000, episode_reward=-171.70 +/- 27.78
Episode length: 172.70 +/- 27.78
New best mean reward!
Eval num_timesteps=19500, episode_reward=-169.50 +/- 34.95
Episode length: 170.50 +/- 34.95
New best mean reward!
Eval num_timesteps=20000, episode_reward=-180.31 +/- 55.03
Episode length: 181.30 +/- 54.97
Eval num_timesteps=20500, episode_reward=-184.08 +/- 55.76
Episode length: 185.06 +/- 55.64
Eval num_timesteps=21000, episode_reward=-196.50 +/- 87.33
Episode length: 197.44 +/- 87.12
Eval num_timesteps=21500, episode_reward=-213.01 +/- 109.86
Episode length: 213.89 +/- 109.55
Eval num_timesteps=22000, episode_reward=-220.62 +/- 107.86
Episode length: 221.51 +/- 107.58
Eval num_timesteps=22500, episode_reward=-183.64 +/- 63.73
Episode length: 184.61 +/- 63.58
Eval num_timesteps=23000, episode_reward=-185.38 +/- 51.60
Episode length: 186.37 +/- 51.54
Eval num_timesteps=23500, episode_reward=-178.12 +/- 47.76
Episode length: 179.11 +/- 47.69
Eval num_timesteps=24000, episode_reward=-178.01 +/- 55.30
Episode length: 178.99 +/- 55.18
Eval num_timesteps=24500, episode_reward=-180.80 +/- 46.37
Episode length: 181.80 +/- 46.37
Eval num_timesteps=25000, episode_reward=-176.65 +/- 47.19
Episode length: 177.65 +/- 47.19
Eval num_timesteps=25500, episode_reward=-182.65 +/- 60.19
Episode length: 183.63 +/- 60.09
Eval num_timesteps=26000, episode_reward=-176.45 +/- 44.27
Episode length: 177.44 +/- 44.20
Eval num_timesteps=26500, episode_reward=-188.58 +/- 64.19
Episode length: 189.56 +/- 64.09
Eval num_timesteps=27000, episode_reward=-202.35 +/- 84.09
Episode length: 203.29 +/- 83.88
Eval num_timesteps=27500, episode_reward=-209.89 +/- 98.88
Episode length: 210.80 +/- 98.62
Eval num_timesteps=28000, episode_reward=-192.50 +/- 65.88
Episode length: 193.47 +/- 65.74
Eval num_timesteps=28500, episode_reward=-196.18 +/- 72.80
Episode length: 197.14 +/- 72.64
Eval num_timesteps=29000, episode_reward=-206.09 +/- 90.82
Episode length: 207.02 +/- 90.60
Eval num_timesteps=29500, episode_reward=-216.84 +/- 101.90
Episode length: 217.74 +/- 101.62
Eval num_timesteps=30000, episode_reward=-211.42 +/- 92.93
Episode length: 212.34 +/- 92.68
Eval num_timesteps=30500, episode_reward=-205.62 +/- 93.15
Episode length: 206.54 +/- 92.90
Eval num_timesteps=31000, episode_reward=-194.36 +/- 72.10
Episode length: 195.32 +/- 71.93
Eval num_timesteps=31500, episode_reward=-188.67 +/- 65.98
Episode length: 189.64 +/- 65.84
Eval num_timesteps=32000, episode_reward=-172.73 +/- 50.88
Episode length: 173.72 +/- 50.82
Eval num_timesteps=32500, episode_reward=-183.91 +/- 54.13
Episode length: 184.90 +/- 54.08
Eval num_timesteps=33000, episode_reward=-181.17 +/- 47.31
Episode length: 182.16 +/- 47.24
Eval num_timesteps=33500, episode_reward=-176.87 +/- 48.28
Episode length: 177.86 +/- 48.21
Eval num_timesteps=34000, episode_reward=-174.15 +/- 42.51
Episode length: 175.14 +/- 42.43
Eval num_timesteps=34500, episode_reward=-196.50 +/- 89.80
Episode length: 197.43 +/- 89.56
Eval num_timesteps=35000, episode_reward=-196.71 +/- 88.02
Episode length: 197.65 +/- 87.81
Eval num_timesteps=35500, episode_reward=-188.09 +/- 81.69
Episode length: 189.04 +/- 81.50
Eval num_timesteps=36000, episode_reward=-185.37 +/- 59.49
Episode length: 186.36 +/- 59.44
Eval num_timesteps=36500, episode_reward=-197.41 +/- 72.80
Episode length: 198.38 +/- 72.68
Eval num_timesteps=37000, episode_reward=-185.71 +/- 60.13
Episode length: 186.69 +/- 60.03
Eval num_timesteps=37500, episode_reward=-193.46 +/- 63.57
Episode length: 194.44 +/- 63.47
Eval num_timesteps=38000, episode_reward=-192.70 +/- 71.92
Episode length: 193.66 +/- 71.75
Eval num_timesteps=38500, episode_reward=-189.23 +/- 73.50
Episode length: 190.19 +/- 73.33
Eval num_timesteps=39000, episode_reward=-186.54 +/- 61.44
Episode length: 187.52 +/- 61.34
Eval num_timesteps=39500, episode_reward=-189.77 +/- 65.80
Episode length: 190.75 +/- 65.71
Eval num_timesteps=40000, episode_reward=-178.07 +/- 56.70
Episode length: 179.06 +/- 56.64
Eval num_timesteps=40500, episode_reward=-182.31 +/- 50.05
Episode length: 183.30 +/- 49.98
Eval num_timesteps=41000, episode_reward=-184.49 +/- 66.65
Episode length: 185.46 +/- 66.51
Eval num_timesteps=41500, episode_reward=-186.79 +/- 71.64
Episode length: 187.76 +/- 71.51
Eval num_timesteps=42000, episode_reward=-175.39 +/- 41.06
Episode length: 176.39 +/- 41.06
Eval num_timesteps=42500, episode_reward=-181.01 +/- 60.42
Episode length: 181.99 +/- 60.32
Eval num_timesteps=43000, episode_reward=-185.88 +/- 68.85
Episode length: 186.87 +/- 68.80
Eval num_timesteps=43500, episode_reward=-180.60 +/- 49.81
Episode length: 181.60 +/- 49.81
Eval num_timesteps=44000, episode_reward=-179.71 +/- 51.34
Episode length: 180.70 +/- 51.27
Eval num_timesteps=44500, episode_reward=-184.98 +/- 65.33
Episode length: 185.96 +/- 65.23
Eval num_timesteps=45000, episode_reward=-169.93 +/- 52.47
Episode length: 170.92 +/- 52.41
Eval num_timesteps=45500, episode_reward=-178.13 +/- 47.04
Episode length: 179.13 +/- 47.04
Eval num_timesteps=46000, episode_reward=-176.49 +/- 50.55
Episode length: 177.48 +/- 50.49
Eval num_timesteps=46500, episode_reward=-170.08 +/- 44.74
Episode length: 171.08 +/- 44.74
Eval num_timesteps=47000, episode_reward=-164.12 +/- 28.20
Episode length: 165.12 +/- 28.20
New best mean reward!
Eval num_timesteps=47500, episode_reward=-176.16 +/- 54.69
Episode length: 177.15 +/- 54.63
Eval num_timesteps=48000, episode_reward=-184.49 +/- 62.44
Episode length: 185.48 +/- 62.39
Eval num_timesteps=48500, episode_reward=-178.05 +/- 57.36
Episode length: 179.03 +/- 57.25
Eval num_timesteps=49000, episode_reward=-172.12 +/- 38.34
Episode length: 173.12 +/- 38.34
Eval num_timesteps=49500, episode_reward=-175.86 +/- 42.82
Episode length: 176.86 +/- 42.82
Eval num_timesteps=50000, episode_reward=-177.43 +/- 52.74
Episode length: 178.42 +/- 52.68
FINISHED IN 760.6687840049854 s


starting seed  10188 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-496.63 +/- 33.53
Episode length: 496.64 +/- 33.43
New best mean reward!
Eval num_timesteps=10000, episode_reward=-483.74 +/- 79.73
Episode length: 483.78 +/- 79.54
New best mean reward!
Eval num_timesteps=10500, episode_reward=-369.31 +/- 187.03
Episode length: 369.64 +/- 186.57
New best mean reward!
Eval num_timesteps=11000, episode_reward=-368.48 +/- 187.96
Episode length: 368.81 +/- 187.50
New best mean reward!
Eval num_timesteps=11500, episode_reward=-398.68 +/- 175.93
Episode length: 398.93 +/- 175.50
Eval num_timesteps=12000, episode_reward=-364.64 +/- 189.36
Episode length: 364.98 +/- 188.89
New best mean reward!
Eval num_timesteps=12500, episode_reward=-331.35 +/- 195.25
Episode length: 331.78 +/- 194.75
New best mean reward!
Eval num_timesteps=13000, episode_reward=-266.53 +/- 191.23
Episode length: 267.15 +/- 190.77
New best mean reward!
Eval num_timesteps=13500, episode_reward=-189.86 +/- 168.22
Episode length: 190.64 +/- 167.81
New best mean reward!
Eval num_timesteps=14000, episode_reward=-202.17 +/- 182.34
Episode length: 202.90 +/- 181.90
Eval num_timesteps=14500, episode_reward=-156.57 +/- 151.55
Episode length: 157.41 +/- 151.19
New best mean reward!
Eval num_timesteps=15000, episode_reward=-126.62 +/- 107.69
Episode length: 127.55 +/- 107.45
New best mean reward!
Eval num_timesteps=15500, episode_reward=-143.16 +/- 132.48
Episode length: 144.05 +/- 132.19
Eval num_timesteps=16000, episode_reward=-127.96 +/- 113.56
Episode length: 128.88 +/- 113.29
Eval num_timesteps=16500, episode_reward=-114.89 +/- 87.13
Episode length: 115.85 +/- 86.95
New best mean reward!
Eval num_timesteps=17000, episode_reward=-133.33 +/- 123.75
Episode length: 134.24 +/- 123.49
Eval num_timesteps=17500, episode_reward=-114.37 +/- 100.67
Episode length: 115.31 +/- 100.44
New best mean reward!
Eval num_timesteps=18000, episode_reward=-103.09 +/- 64.31
Episode length: 104.07 +/- 64.18
New best mean reward!
Eval num_timesteps=18500, episode_reward=-103.40 +/- 67.65
Episode length: 104.38 +/- 67.53
Eval num_timesteps=19000, episode_reward=-107.98 +/- 67.51
Episode length: 108.96 +/- 67.39
Eval num_timesteps=19500, episode_reward=-97.85 +/- 50.31
Episode length: 98.84 +/- 50.23
New best mean reward!
FINISHED IN 417.385089065996 s


starting seed  10189 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-488.68 +/- 64.46
Episode length: 488.71 +/- 64.29
New best mean reward!
Eval num_timesteps=10000, episode_reward=-359.32 +/- 192.51
Episode length: 359.67 +/- 192.04
New best mean reward!
Eval num_timesteps=10500, episode_reward=-286.69 +/- 201.44
Episode length: 287.23 +/- 200.95
New best mean reward!
Eval num_timesteps=11000, episode_reward=-184.17 +/- 166.69
Episode length: 184.96 +/- 166.30
New best mean reward!
Eval num_timesteps=11500, episode_reward=-271.91 +/- 202.72
Episode length: 272.47 +/- 202.23
Eval num_timesteps=12000, episode_reward=-204.07 +/- 178.58
Episode length: 204.83 +/- 178.18
Eval num_timesteps=12500, episode_reward=-220.26 +/- 186.60
Episode length: 220.97 +/- 186.17
Eval num_timesteps=13000, episode_reward=-173.74 +/- 160.94
Episode length: 174.55 +/- 160.56
New best mean reward!
Eval num_timesteps=13500, episode_reward=-174.57 +/- 158.39
Episode length: 175.39 +/- 158.02
Eval num_timesteps=14000, episode_reward=-141.56 +/- 135.10
Episode length: 142.44 +/- 134.78
New best mean reward!
Eval num_timesteps=14500, episode_reward=-223.38 +/- 188.80
Episode length: 224.08 +/- 188.36
Eval num_timesteps=15000, episode_reward=-181.91 +/- 165.90
Episode length: 182.71 +/- 165.52
Eval num_timesteps=15500, episode_reward=-114.54 +/- 103.94
Episode length: 115.48 +/- 103.71
New best mean reward!
Eval num_timesteps=16000, episode_reward=-121.71 +/- 106.20
Episode length: 122.65 +/- 105.98
Eval num_timesteps=16500, episode_reward=-124.70 +/- 109.05
Episode length: 125.63 +/- 108.81
Eval num_timesteps=17000, episode_reward=-109.11 +/- 88.29
Episode length: 110.07 +/- 88.11
New best mean reward!
Eval num_timesteps=17500, episode_reward=-93.99 +/- 60.43
Episode length: 94.98 +/- 60.36
New best mean reward!
FINISHED IN 324.158359966008 s


starting seed  10190 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-498.06 +/- 19.30
Episode length: 498.07 +/- 19.20
New best mean reward!
Eval num_timesteps=5500, episode_reward=-179.37 +/- 48.58
Episode length: 180.36 +/- 48.52
New best mean reward!
Eval num_timesteps=6000, episode_reward=-142.64 +/- 51.56
Episode length: 143.63 +/- 51.49
New best mean reward!
Eval num_timesteps=6500, episode_reward=-166.12 +/- 54.34
Episode length: 167.11 +/- 54.28
Eval num_timesteps=7000, episode_reward=-186.46 +/- 82.73
Episode length: 187.44 +/- 82.66
Eval num_timesteps=7500, episode_reward=-217.85 +/- 74.60
Episode length: 218.84 +/- 74.56
Eval num_timesteps=8000, episode_reward=-197.05 +/- 71.57
Episode length: 198.02 +/- 71.44
Eval num_timesteps=8500, episode_reward=-178.49 +/- 91.05
Episode length: 179.45 +/- 90.91
Eval num_timesteps=9000, episode_reward=-240.09 +/- 108.57
Episode length: 241.04 +/- 108.45
Eval num_timesteps=9500, episode_reward=-188.83 +/- 112.74
Episode length: 189.73 +/- 112.46
Eval num_timesteps=10000, episode_reward=-129.69 +/- 51.04
Episode length: 130.69 +/- 51.04
New best mean reward!
Eval num_timesteps=10500, episode_reward=-90.73 +/- 27.15
Episode length: 91.73 +/- 27.15
New best mean reward!
FINISHED IN 160.5358162909979 s


starting seed  10191 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-82.32 +/- 14.51
Episode length: 83.32 +/- 14.51
New best mean reward!
FINISHED IN 2.2734770239912905 s


starting seed  10192 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-324.43 +/- 170.73
Episode length: 324.96 +/- 170.25
New best mean reward!
Eval num_timesteps=7500, episode_reward=-129.37 +/- 37.98
Episode length: 130.37 +/- 37.98
New best mean reward!
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-182.42 +/- 144.27
Episode length: 183.27 +/- 143.94
Eval num_timesteps=9000, episode_reward=-266.69 +/- 198.27
Episode length: 267.28 +/- 197.79
Eval num_timesteps=9500, episode_reward=-93.47 +/- 42.85
Episode length: 94.47 +/- 42.85
New best mean reward!
FINISHED IN 239.41442910197657 s


starting seed  10193 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-118.93 +/- 99.63
Episode length: 119.88 +/- 99.44
New best mean reward!
Eval num_timesteps=8500, episode_reward=-109.84 +/- 77.26
Episode length: 110.81 +/- 77.11
New best mean reward!
Eval num_timesteps=9000, episode_reward=-102.52 +/- 59.53
Episode length: 103.51 +/- 59.46
New best mean reward!
Eval num_timesteps=9500, episode_reward=-91.22 +/- 47.04
Episode length: 92.21 +/- 46.95
New best mean reward!
FINISHED IN 166.39307051402284 s


starting seed  10194 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-330.19 +/- 149.76
Episode length: 330.77 +/- 149.28
New best mean reward!
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-459.45 +/- 106.25
Episode length: 459.58 +/- 105.91
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-207.79 +/- 175.40
Episode length: 208.53 +/- 174.96
New best mean reward!
Eval num_timesteps=11500, episode_reward=-156.41 +/- 140.80
Episode length: 157.27 +/- 140.45
New best mean reward!
Eval num_timesteps=12000, episode_reward=-192.58 +/- 166.04
Episode length: 193.36 +/- 165.64
Eval num_timesteps=12500, episode_reward=-173.90 +/- 164.25
Episode length: 174.70 +/- 163.86
Eval num_timesteps=13000, episode_reward=-99.36 +/- 39.23
Episode length: 100.36 +/- 39.23
New best mean reward!
FINISHED IN 229.59881353395758 s


starting seed  10195 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-468.01 +/- 67.88
Episode length: 468.21 +/- 67.50
New best mean reward!
Eval num_timesteps=3500, episode_reward=-280.14 +/- 136.59
Episode length: 280.90 +/- 136.20
New best mean reward!
Eval num_timesteps=4000, episode_reward=-447.68 +/- 95.73
Episode length: 447.92 +/- 95.32
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-492.56 +/- 52.11
Episode length: 492.58 +/- 51.97
Eval num_timesteps=8500, episode_reward=-477.19 +/- 90.42
Episode length: 477.25 +/- 90.18
Eval num_timesteps=9000, episode_reward=-324.47 +/- 192.02
Episode length: 324.93 +/- 191.53
Eval num_timesteps=9500, episode_reward=-150.73 +/- 134.93
Episode length: 151.61 +/- 134.62
New best mean reward!
Eval num_timesteps=10000, episode_reward=-139.94 +/- 123.99
Episode length: 140.85 +/- 123.73
New best mean reward!
Eval num_timesteps=10500, episode_reward=-106.27 +/- 86.81
Episode length: 107.24 +/- 86.67
New best mean reward!
Eval num_timesteps=11000, episode_reward=-108.76 +/- 82.56
Episode length: 109.73 +/- 82.42
Eval num_timesteps=11500, episode_reward=-96.44 +/- 64.32
Episode length: 97.42 +/- 64.19
New best mean reward!
FINISHED IN 201.03222364996327 s


starting seed  10196 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-152.76 +/- 54.04
Episode length: 153.76 +/- 54.04
New best mean reward!
Eval num_timesteps=10000, episode_reward=-283.28 +/- 172.68
Episode length: 283.91 +/- 172.22
Eval num_timesteps=10500, episode_reward=-213.27 +/- 133.01
Episode length: 214.10 +/- 132.65
Eval num_timesteps=11000, episode_reward=-139.37 +/- 48.77
Episode length: 140.36 +/- 48.69
New best mean reward!
Eval num_timesteps=11500, episode_reward=-182.72 +/- 111.04
Episode length: 183.62 +/- 110.76
Eval num_timesteps=12000, episode_reward=-349.70 +/- 189.97
Episode length: 350.09 +/- 189.48
Eval num_timesteps=12500, episode_reward=-130.73 +/- 112.03
Episode length: 131.65 +/- 111.77
New best mean reward!
Eval num_timesteps=13000, episode_reward=-83.80 +/- 16.48
Episode length: 84.80 +/- 16.48
New best mean reward!
FINISHED IN 210.68851505598286 s


starting seed  10197 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-427.55 +/- 137.79
Episode length: 427.77 +/- 137.38
New best mean reward!
Eval num_timesteps=7000, episode_reward=-496.79 +/- 31.94
Episode length: 496.80 +/- 31.84
Eval num_timesteps=7500, episode_reward=-393.21 +/- 157.99
Episode length: 393.53 +/- 157.53
New best mean reward!
Eval num_timesteps=8000, episode_reward=-133.37 +/- 28.04
Episode length: 134.37 +/- 28.04
New best mean reward!
Eval num_timesteps=8500, episode_reward=-151.57 +/- 90.66
Episode length: 152.51 +/- 90.43
Eval num_timesteps=9000, episode_reward=-181.94 +/- 121.68
Episode length: 182.82 +/- 121.37
Eval num_timesteps=9500, episode_reward=-142.27 +/- 48.22
Episode length: 143.26 +/- 48.14
Eval num_timesteps=10000, episode_reward=-186.42 +/- 128.59
Episode length: 187.28 +/- 128.25
Eval num_timesteps=10500, episode_reward=-331.94 +/- 176.17
Episode length: 332.42 +/- 175.68
Eval num_timesteps=11000, episode_reward=-459.22 +/- 116.30
Episode length: 459.33 +/- 115.99
Eval num_timesteps=11500, episode_reward=-485.31 +/- 70.64
Episode length: 485.36 +/- 70.44
Eval num_timesteps=12000, episode_reward=-362.71 +/- 187.22
Episode length: 363.08 +/- 186.76
Eval num_timesteps=12500, episode_reward=-302.26 +/- 197.06
Episode length: 302.77 +/- 196.57
Eval num_timesteps=13000, episode_reward=-316.65 +/- 196.87
Episode length: 317.12 +/- 196.38
Eval num_timesteps=13500, episode_reward=-274.87 +/- 192.83
Episode length: 275.46 +/- 192.35
Eval num_timesteps=14000, episode_reward=-183.05 +/- 172.14
Episode length: 183.83 +/- 171.74
Eval num_timesteps=14500, episode_reward=-153.28 +/- 152.51
Episode length: 154.12 +/- 152.14
Eval num_timesteps=15000, episode_reward=-182.42 +/- 174.31
Episode length: 183.19 +/- 173.89
Eval num_timesteps=15500, episode_reward=-237.38 +/- 193.96
Episode length: 238.03 +/- 193.49
Eval num_timesteps=16000, episode_reward=-222.12 +/- 183.04
Episode length: 222.82 +/- 182.59
Eval num_timesteps=16500, episode_reward=-116.89 +/- 101.66
Episode length: 117.83 +/- 101.43
New best mean reward!
Eval num_timesteps=17000, episode_reward=-85.32 +/- 45.37
Episode length: 86.31 +/- 45.28
New best mean reward!
FINISHED IN 252.0009278160287 s


starting seed  10198 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-495.88 +/- 40.99
Episode length: 495.89 +/- 40.89
New best mean reward!
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-471.48 +/- 104.12
Episode length: 471.55 +/- 103.86
New best mean reward!
Eval num_timesteps=10500, episode_reward=-431.66 +/- 151.91
Episode length: 431.83 +/- 151.54
New best mean reward!
Eval num_timesteps=11000, episode_reward=-488.13 +/- 67.57
Episode length: 488.16 +/- 67.40
Eval num_timesteps=11500, episode_reward=-374.39 +/- 188.05
Episode length: 374.70 +/- 187.59
New best mean reward!
Eval num_timesteps=12000, episode_reward=-352.55 +/- 193.89
Episode length: 352.92 +/- 193.41
New best mean reward!
Eval num_timesteps=12500, episode_reward=-303.03 +/- 199.85
Episode length: 303.54 +/- 199.36
New best mean reward!
Eval num_timesteps=13000, episode_reward=-192.72 +/- 173.91
Episode length: 193.49 +/- 173.50
New best mean reward!
Eval num_timesteps=13500, episode_reward=-237.62 +/- 191.05
Episode length: 238.29 +/- 190.60
Eval num_timesteps=14000, episode_reward=-217.93 +/- 186.46
Episode length: 218.65 +/- 186.04
Eval num_timesteps=14500, episode_reward=-227.68 +/- 188.37
Episode length: 228.37 +/- 187.93
Eval num_timesteps=15000, episode_reward=-174.70 +/- 164.01
Episode length: 175.51 +/- 163.64
New best mean reward!
Eval num_timesteps=15500, episode_reward=-139.30 +/- 133.65
Episode length: 140.19 +/- 133.36
New best mean reward!
Eval num_timesteps=16000, episode_reward=-114.65 +/- 104.56
Episode length: 115.59 +/- 104.34
New best mean reward!
Eval num_timesteps=16500, episode_reward=-135.15 +/- 128.96
Episode length: 136.05 +/- 128.68
Eval num_timesteps=17000, episode_reward=-119.98 +/- 105.93
Episode length: 120.92 +/- 105.71
Eval num_timesteps=17500, episode_reward=-140.13 +/- 128.01
Episode length: 141.04 +/- 127.76
Eval num_timesteps=18000, episode_reward=-142.03 +/- 141.45
Episode length: 142.90 +/- 141.12
Eval num_timesteps=18500, episode_reward=-168.86 +/- 160.81
Episode length: 169.68 +/- 160.44
Eval num_timesteps=19000, episode_reward=-117.45 +/- 107.83
Episode length: 118.39 +/- 107.62
Eval num_timesteps=19500, episode_reward=-107.97 +/- 85.96
Episode length: 108.94 +/- 85.82
New best mean reward!
Eval num_timesteps=20000, episode_reward=-104.30 +/- 83.98
Episode length: 105.28 +/- 83.89
New best mean reward!
Eval num_timesteps=20500, episode_reward=-119.57 +/- 108.04
Episode length: 120.51 +/- 107.83
Eval num_timesteps=21000, episode_reward=-91.59 +/- 46.13
Episode length: 92.58 +/- 46.04
New best mean reward!
FINISHED IN 290.7807293679798 s


starting seed  10199 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-97.48 +/- 26.23
Episode length: 98.48 +/- 26.23
New best mean reward!
FINISHED IN 124.39112899301108 s
AVG TIME: 351.74172542788847
