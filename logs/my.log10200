nohup: ignoring input


starting seed  10200 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-145.96 +/- 46.61
Episode length: 146.95 +/- 46.54
New best mean reward!
Eval num_timesteps=7500, episode_reward=-109.93 +/- 32.87
Episode length: 110.93 +/- 32.87
New best mean reward!
Eval num_timesteps=8000, episode_reward=-111.91 +/- 52.24
Episode length: 112.90 +/- 52.17
Eval num_timesteps=8500, episode_reward=-97.89 +/- 25.25
Episode length: 98.89 +/- 25.25
New best mean reward!
FINISHED IN 126.9334445919958 s


starting seed  10201 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-468.70 +/- 106.17
Episode length: 468.78 +/- 105.90
New best mean reward!
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-91.86 +/- 25.09
Episode length: 92.86 +/- 25.09
New best mean reward!
FINISHED IN 95.06539026304381 s


starting seed  10202 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-158.57 +/- 72.98
Episode length: 159.54 +/- 72.84
New best mean reward!
Eval num_timesteps=9000, episode_reward=-143.83 +/- 30.96
Episode length: 144.83 +/- 30.96
New best mean reward!
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-490.29 +/- 56.14
Episode length: 490.32 +/- 55.97
Eval num_timesteps=10500, episode_reward=-489.20 +/- 61.42
Episode length: 489.23 +/- 61.25
Eval num_timesteps=11000, episode_reward=-478.63 +/- 84.67
Episode length: 478.69 +/- 84.43
Eval num_timesteps=11500, episode_reward=-260.88 +/- 204.14
Episode length: 261.46 +/- 203.65
Eval num_timesteps=12000, episode_reward=-185.71 +/- 178.14
Episode length: 186.47 +/- 177.71
Eval num_timesteps=12500, episode_reward=-409.32 +/- 159.53
Episode length: 409.57 +/- 159.10
Eval num_timesteps=13000, episode_reward=-350.69 +/- 179.96
Episode length: 351.10 +/- 179.47
Eval num_timesteps=13500, episode_reward=-311.33 +/- 205.14
Episode length: 311.79 +/- 204.64
Eval num_timesteps=14000, episode_reward=-298.29 +/- 198.34
Episode length: 298.80 +/- 197.84
Eval num_timesteps=14500, episode_reward=-206.09 +/- 182.73
Episode length: 206.83 +/- 182.31
Eval num_timesteps=15000, episode_reward=-150.53 +/- 142.91
Episode length: 151.40 +/- 142.60
Eval num_timesteps=15500, episode_reward=-131.90 +/- 122.97
Episode length: 132.81 +/- 122.70
New best mean reward!
Eval num_timesteps=16000, episode_reward=-143.73 +/- 130.46
Episode length: 144.62 +/- 130.16
Eval num_timesteps=16500, episode_reward=-131.00 +/- 122.88
Episode length: 131.91 +/- 122.61
New best mean reward!
Eval num_timesteps=17000, episode_reward=-136.74 +/- 130.36
Episode length: 137.63 +/- 130.06
Eval num_timesteps=17500, episode_reward=-166.12 +/- 160.21
Episode length: 166.94 +/- 159.83
Eval num_timesteps=18000, episode_reward=-197.17 +/- 178.80
Episode length: 197.92 +/- 178.37
Eval num_timesteps=18500, episode_reward=-142.42 +/- 140.00
Episode length: 143.29 +/- 139.67
Eval num_timesteps=19000, episode_reward=-152.71 +/- 150.10
Episode length: 153.56 +/- 149.75
Eval num_timesteps=19500, episode_reward=-168.40 +/- 158.48
Episode length: 169.22 +/- 158.10
Eval num_timesteps=20000, episode_reward=-150.21 +/- 144.44
Episode length: 151.09 +/- 144.15
Eval num_timesteps=20500, episode_reward=-112.66 +/- 92.90
Episode length: 113.61 +/- 92.70
New best mean reward!
Eval num_timesteps=21000, episode_reward=-117.65 +/- 101.84
Episode length: 118.59 +/- 101.61
Eval num_timesteps=21500, episode_reward=-133.72 +/- 122.35
Episode length: 134.63 +/- 122.08
Eval num_timesteps=22000, episode_reward=-112.36 +/- 100.68
Episode length: 113.30 +/- 100.45
New best mean reward!
Eval num_timesteps=22500, episode_reward=-124.83 +/- 112.81
Episode length: 125.76 +/- 112.57
Eval num_timesteps=23000, episode_reward=-110.74 +/- 92.40
Episode length: 111.69 +/- 92.19
New best mean reward!
Eval num_timesteps=23500, episode_reward=-117.81 +/- 107.60
Episode length: 118.74 +/- 107.35
Eval num_timesteps=24000, episode_reward=-109.76 +/- 86.26
Episode length: 110.72 +/- 86.08
New best mean reward!
Eval num_timesteps=24500, episode_reward=-100.25 +/- 62.86
Episode length: 101.23 +/- 62.73
New best mean reward!
Eval num_timesteps=25000, episode_reward=-103.05 +/- 79.26
Episode length: 104.02 +/- 79.11
Eval num_timesteps=25500, episode_reward=-92.09 +/- 46.36
Episode length: 93.08 +/- 46.27
New best mean reward!
FINISHED IN 384.2922006979934 s


starting seed  10203 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-431.59 +/- 151.41
Episode length: 431.76 +/- 151.04
New best mean reward!
Eval num_timesteps=16000, episode_reward=-305.98 +/- 202.98
Episode length: 306.46 +/- 202.48
New best mean reward!
Eval num_timesteps=16500, episode_reward=-253.18 +/- 195.13
Episode length: 253.80 +/- 194.65
New best mean reward!
Eval num_timesteps=17000, episode_reward=-290.12 +/- 204.53
Episode length: 290.64 +/- 204.03
Eval num_timesteps=17500, episode_reward=-300.62 +/- 203.76
Episode length: 301.11 +/- 203.26
Eval num_timesteps=18000, episode_reward=-258.66 +/- 201.47
Episode length: 259.25 +/- 200.98
Eval num_timesteps=18500, episode_reward=-169.28 +/- 157.73
Episode length: 170.10 +/- 157.35
New best mean reward!
Eval num_timesteps=19000, episode_reward=-130.40 +/- 124.34
Episode length: 131.30 +/- 124.04
New best mean reward!
Eval num_timesteps=19500, episode_reward=-145.64 +/- 139.11
Episode length: 146.51 +/- 138.78
Eval num_timesteps=20000, episode_reward=-108.03 +/- 82.54
Episode length: 108.99 +/- 82.35
New best mean reward!
Eval num_timesteps=20500, episode_reward=-103.65 +/- 66.67
Episode length: 104.63 +/- 66.55
New best mean reward!
Eval num_timesteps=21000, episode_reward=-89.82 +/- 27.03
Episode length: 90.82 +/- 27.03
New best mean reward!
FINISHED IN 475.4343461439712 s


starting seed  10204 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-464.79 +/- 99.06
Episode length: 464.91 +/- 98.74
New best mean reward!
Eval num_timesteps=11000, episode_reward=-493.14 +/- 48.04
Episode length: 493.16 +/- 47.90
Eval num_timesteps=11500, episode_reward=-397.50 +/- 150.68
Episode length: 397.82 +/- 150.22
New best mean reward!
Eval num_timesteps=12000, episode_reward=-358.14 +/- 164.53
Episode length: 358.57 +/- 164.04
New best mean reward!
Eval num_timesteps=12500, episode_reward=-471.77 +/- 90.77
Episode length: 471.86 +/- 90.49
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-492.80 +/- 50.40
Episode length: 492.82 +/- 50.26
Eval num_timesteps=14000, episode_reward=-488.66 +/- 64.50
Episode length: 488.69 +/- 64.33
Eval num_timesteps=14500, episode_reward=-492.82 +/- 50.26
Episode length: 492.84 +/- 50.12
Eval num_timesteps=15000, episode_reward=-444.85 +/- 136.83
Episode length: 444.99 +/- 136.48
Eval num_timesteps=15500, episode_reward=-496.29 +/- 36.91
Episode length: 496.30 +/- 36.81
Eval num_timesteps=16000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16500, episode_reward=-447.50 +/- 135.96
Episode length: 447.63 +/- 135.63
Eval num_timesteps=17000, episode_reward=-413.58 +/- 163.53
Episode length: 413.80 +/- 163.12
Eval num_timesteps=17500, episode_reward=-440.19 +/- 142.44
Episode length: 440.34 +/- 142.08
Eval num_timesteps=18000, episode_reward=-424.11 +/- 153.57
Episode length: 424.31 +/- 153.18
Eval num_timesteps=18500, episode_reward=-449.24 +/- 131.55
Episode length: 449.37 +/- 131.22
Eval num_timesteps=19000, episode_reward=-403.60 +/- 172.00
Episode length: 403.84 +/- 171.58
Eval num_timesteps=19500, episode_reward=-365.00 +/- 187.00
Episode length: 365.35 +/- 186.53
Eval num_timesteps=20000, episode_reward=-386.27 +/- 179.18
Episode length: 386.56 +/- 178.73
Eval num_timesteps=20500, episode_reward=-312.20 +/- 200.88
Episode length: 312.67 +/- 200.38
New best mean reward!
Eval num_timesteps=21000, episode_reward=-345.48 +/- 194.53
Episode length: 345.87 +/- 194.05
Eval num_timesteps=21500, episode_reward=-308.00 +/- 198.26
Episode length: 308.50 +/- 197.78
New best mean reward!
Eval num_timesteps=22000, episode_reward=-298.14 +/- 195.14
Episode length: 298.68 +/- 194.67
New best mean reward!
Eval num_timesteps=22500, episode_reward=-337.83 +/- 196.56
Episode length: 338.24 +/- 196.08
Eval num_timesteps=23000, episode_reward=-401.82 +/- 169.07
Episode length: 402.08 +/- 168.64
Eval num_timesteps=23500, episode_reward=-323.76 +/- 198.93
Episode length: 324.21 +/- 198.44
Eval num_timesteps=24000, episode_reward=-276.60 +/- 199.59
Episode length: 277.16 +/- 199.10
New best mean reward!
Eval num_timesteps=24500, episode_reward=-237.41 +/- 193.75
Episode length: 238.08 +/- 193.30
New best mean reward!
Eval num_timesteps=25000, episode_reward=-294.34 +/- 201.05
Episode length: 294.86 +/- 200.56
Eval num_timesteps=25500, episode_reward=-249.36 +/- 190.84
Episode length: 250.01 +/- 190.38
Eval num_timesteps=26000, episode_reward=-269.83 +/- 198.96
Episode length: 270.41 +/- 198.47
Eval num_timesteps=26500, episode_reward=-232.52 +/- 193.64
Episode length: 233.18 +/- 193.17
New best mean reward!
Eval num_timesteps=27000, episode_reward=-239.53 +/- 194.76
Episode length: 240.20 +/- 194.31
Eval num_timesteps=27500, episode_reward=-303.80 +/- 199.47
Episode length: 304.30 +/- 198.97
Eval num_timesteps=28000, episode_reward=-336.22 +/- 197.28
Episode length: 336.63 +/- 196.79
Eval num_timesteps=28500, episode_reward=-263.92 +/- 199.97
Episode length: 264.51 +/- 199.49
Eval num_timesteps=29000, episode_reward=-286.89 +/- 201.15
Episode length: 287.42 +/- 200.65
Eval num_timesteps=29500, episode_reward=-274.63 +/- 201.46
Episode length: 275.19 +/- 200.96
Eval num_timesteps=30000, episode_reward=-234.05 +/- 193.96
Episode length: 234.71 +/- 193.49
Eval num_timesteps=30500, episode_reward=-316.96 +/- 203.40
Episode length: 317.41 +/- 202.90
Eval num_timesteps=31000, episode_reward=-288.97 +/- 203.54
Episode length: 289.49 +/- 203.04
Eval num_timesteps=31500, episode_reward=-253.67 +/- 201.95
Episode length: 254.29 +/- 201.48
Eval num_timesteps=32000, episode_reward=-279.08 +/- 204.07
Episode length: 279.63 +/- 203.58
Eval num_timesteps=32500, episode_reward=-287.84 +/- 202.25
Episode length: 288.37 +/- 201.76
Eval num_timesteps=33000, episode_reward=-271.05 +/- 201.30
Episode length: 271.64 +/- 200.83
Eval num_timesteps=33500, episode_reward=-298.22 +/- 203.61
Episode length: 298.74 +/- 203.13
Eval num_timesteps=34000, episode_reward=-246.70 +/- 198.58
Episode length: 247.33 +/- 198.11
Eval num_timesteps=34500, episode_reward=-291.65 +/- 203.47
Episode length: 292.18 +/- 202.99
Eval num_timesteps=35000, episode_reward=-310.43 +/- 201.04
Episode length: 310.91 +/- 200.55
Eval num_timesteps=35500, episode_reward=-270.81 +/- 204.10
Episode length: 271.37 +/- 203.61
Eval num_timesteps=36000, episode_reward=-202.92 +/- 183.55
Episode length: 203.65 +/- 183.11
New best mean reward!
Eval num_timesteps=36500, episode_reward=-264.37 +/- 204.85
Episode length: 264.95 +/- 204.37
Eval num_timesteps=37000, episode_reward=-283.69 +/- 202.83
Episode length: 284.23 +/- 202.34
Eval num_timesteps=37500, episode_reward=-283.46 +/- 201.74
Episode length: 284.00 +/- 201.25
Eval num_timesteps=38000, episode_reward=-281.64 +/- 206.17
Episode length: 282.17 +/- 205.67
Eval num_timesteps=38500, episode_reward=-266.02 +/- 199.56
Episode length: 266.62 +/- 199.09
Eval num_timesteps=39000, episode_reward=-233.01 +/- 197.27
Episode length: 233.66 +/- 196.80
Eval num_timesteps=39500, episode_reward=-278.37 +/- 205.17
Episode length: 278.91 +/- 204.67
Eval num_timesteps=40000, episode_reward=-290.44 +/- 204.92
Episode length: 290.96 +/- 204.43
Eval num_timesteps=40500, episode_reward=-241.84 +/- 199.03
Episode length: 242.47 +/- 198.55
Eval num_timesteps=41000, episode_reward=-295.03 +/- 197.44
Episode length: 295.56 +/- 196.95
Eval num_timesteps=41500, episode_reward=-308.52 +/- 200.37
Episode length: 309.01 +/- 199.89
Eval num_timesteps=42000, episode_reward=-276.49 +/- 203.26
Episode length: 277.05 +/- 202.77
Eval num_timesteps=42500, episode_reward=-270.08 +/- 202.70
Episode length: 270.65 +/- 202.22
Eval num_timesteps=43000, episode_reward=-304.87 +/- 203.62
Episode length: 305.36 +/- 203.13
Eval num_timesteps=43500, episode_reward=-242.96 +/- 201.71
Episode length: 243.58 +/- 201.23
Eval num_timesteps=44000, episode_reward=-311.13 +/- 202.35
Episode length: 311.60 +/- 201.85
Eval num_timesteps=44500, episode_reward=-277.31 +/- 202.55
Episode length: 277.86 +/- 202.06
Eval num_timesteps=45000, episode_reward=-291.04 +/- 205.74
Episode length: 291.55 +/- 205.24
Eval num_timesteps=45500, episode_reward=-274.79 +/- 201.58
Episode length: 275.35 +/- 201.09
Eval num_timesteps=46000, episode_reward=-273.29 +/- 202.66
Episode length: 273.86 +/- 202.18
Eval num_timesteps=46500, episode_reward=-288.22 +/- 203.86
Episode length: 288.75 +/- 203.37
Eval num_timesteps=47000, episode_reward=-289.61 +/- 204.50
Episode length: 290.13 +/- 204.00
Eval num_timesteps=47500, episode_reward=-284.20 +/- 202.04
Episode length: 284.74 +/- 201.55
Eval num_timesteps=48000, episode_reward=-308.20 +/- 204.49
Episode length: 308.67 +/- 203.99
Eval num_timesteps=48500, episode_reward=-262.65 +/- 199.39
Episode length: 263.24 +/- 198.90
Eval num_timesteps=49000, episode_reward=-278.38 +/- 200.32
Episode length: 278.94 +/- 199.83
Eval num_timesteps=49500, episode_reward=-288.73 +/- 199.46
Episode length: 289.27 +/- 198.98
Eval num_timesteps=50000, episode_reward=-282.80 +/- 205.01
Episode length: 283.34 +/- 204.52
FINISHED IN 1096.1139453090145 s


starting seed  10205 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-479.57 +/- 81.18
Episode length: 479.63 +/- 80.95
New best mean reward!
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-396.42 +/- 149.25
Episode length: 396.75 +/- 148.79
New best mean reward!
Eval num_timesteps=10500, episode_reward=-284.86 +/- 137.66
Episode length: 285.59 +/- 137.23
New best mean reward!
Eval num_timesteps=11000, episode_reward=-348.12 +/- 156.64
Episode length: 348.61 +/- 156.14
Eval num_timesteps=11500, episode_reward=-379.97 +/- 150.33
Episode length: 380.37 +/- 149.85
Eval num_timesteps=12000, episode_reward=-247.60 +/- 83.86
Episode length: 248.53 +/- 83.64
New best mean reward!
Eval num_timesteps=12500, episode_reward=-497.23 +/- 11.18
Episode length: 497.34 +/- 10.96
Eval num_timesteps=13000, episode_reward=-174.48 +/- 46.71
Episode length: 175.48 +/- 46.71
New best mean reward!
Eval num_timesteps=13500, episode_reward=-180.92 +/- 55.97
Episode length: 181.91 +/- 55.91
Eval num_timesteps=14000, episode_reward=-178.81 +/- 60.99
Episode length: 179.80 +/- 60.93
Eval num_timesteps=14500, episode_reward=-198.82 +/- 80.09
Episode length: 199.78 +/- 79.94
Eval num_timesteps=15000, episode_reward=-201.16 +/- 88.93
Episode length: 202.10 +/- 88.73
Eval num_timesteps=15500, episode_reward=-203.46 +/- 87.00
Episode length: 204.41 +/- 86.83
Eval num_timesteps=16000, episode_reward=-152.97 +/- 38.73
Episode length: 153.97 +/- 38.73
New best mean reward!
Eval num_timesteps=16500, episode_reward=-139.82 +/- 49.97
Episode length: 140.81 +/- 49.90
New best mean reward!
Eval num_timesteps=17000, episode_reward=-137.36 +/- 41.55
Episode length: 138.36 +/- 41.55
New best mean reward!
Eval num_timesteps=17500, episode_reward=-177.98 +/- 39.89
Episode length: 178.98 +/- 39.89
Eval num_timesteps=18000, episode_reward=-158.62 +/- 29.25
Episode length: 159.62 +/- 29.25
Eval num_timesteps=18500, episode_reward=-135.61 +/- 28.68
Episode length: 136.61 +/- 28.68
New best mean reward!
Eval num_timesteps=19000, episode_reward=-131.20 +/- 23.18
Episode length: 132.20 +/- 23.18
New best mean reward!
Eval num_timesteps=19500, episode_reward=-136.81 +/- 33.60
Episode length: 137.81 +/- 33.60
Eval num_timesteps=20000, episode_reward=-138.96 +/- 34.44
Episode length: 139.96 +/- 34.44
Eval num_timesteps=20500, episode_reward=-146.43 +/- 38.80
Episode length: 147.43 +/- 38.80
Eval num_timesteps=21000, episode_reward=-111.60 +/- 36.97
Episode length: 112.60 +/- 36.97
New best mean reward!
Eval num_timesteps=21500, episode_reward=-114.20 +/- 42.12
Episode length: 115.20 +/- 42.12
Eval num_timesteps=22000, episode_reward=-133.86 +/- 46.43
Episode length: 134.85 +/- 46.35
Eval num_timesteps=22500, episode_reward=-143.73 +/- 108.53
Episode length: 144.65 +/- 108.27
Eval num_timesteps=23000, episode_reward=-120.88 +/- 52.89
Episode length: 121.87 +/- 52.82
Eval num_timesteps=23500, episode_reward=-151.91 +/- 115.97
Episode length: 152.82 +/- 115.70
Eval num_timesteps=24000, episode_reward=-132.06 +/- 106.77
Episode length: 132.99 +/- 106.53
Eval num_timesteps=24500, episode_reward=-187.29 +/- 162.77
Episode length: 188.09 +/- 162.39
Eval num_timesteps=25000, episode_reward=-155.43 +/- 135.99
Episode length: 156.31 +/- 135.69
Eval num_timesteps=25500, episode_reward=-147.54 +/- 127.33
Episode length: 148.44 +/- 127.05
Eval num_timesteps=26000, episode_reward=-143.34 +/- 125.71
Episode length: 144.24 +/- 125.42
Eval num_timesteps=26500, episode_reward=-118.26 +/- 84.11
Episode length: 119.22 +/- 83.92
Eval num_timesteps=27000, episode_reward=-149.51 +/- 130.74
Episode length: 150.40 +/- 130.44
Eval num_timesteps=27500, episode_reward=-149.20 +/- 137.99
Episode length: 150.07 +/- 137.66
Eval num_timesteps=28000, episode_reward=-116.45 +/- 99.89
Episode length: 117.39 +/- 99.66
Eval num_timesteps=28500, episode_reward=-126.22 +/- 107.77
Episode length: 127.15 +/- 107.53
Eval num_timesteps=29000, episode_reward=-115.85 +/- 93.91
Episode length: 116.80 +/- 93.71
Eval num_timesteps=29500, episode_reward=-103.91 +/- 83.02
Episode length: 104.87 +/- 82.83
New best mean reward!
Eval num_timesteps=30000, episode_reward=-152.89 +/- 150.23
Episode length: 153.74 +/- 149.88
Eval num_timesteps=30500, episode_reward=-117.28 +/- 100.14
Episode length: 118.22 +/- 99.91
Eval num_timesteps=31000, episode_reward=-123.25 +/- 112.97
Episode length: 124.17 +/- 112.70
Eval num_timesteps=31500, episode_reward=-165.35 +/- 158.13
Episode length: 166.17 +/- 157.75
Eval num_timesteps=32000, episode_reward=-167.89 +/- 160.43
Episode length: 168.71 +/- 160.06
Eval num_timesteps=32500, episode_reward=-135.53 +/- 128.60
Episode length: 136.43 +/- 128.31
Eval num_timesteps=33000, episode_reward=-145.90 +/- 141.67
Episode length: 146.77 +/- 141.35
Eval num_timesteps=33500, episode_reward=-124.38 +/- 115.27
Episode length: 125.30 +/- 115.01
Eval num_timesteps=34000, episode_reward=-127.86 +/- 122.26
Episode length: 128.77 +/- 121.99
Eval num_timesteps=34500, episode_reward=-114.20 +/- 107.46
Episode length: 115.14 +/- 107.25
Eval num_timesteps=35000, episode_reward=-126.38 +/- 117.02
Episode length: 127.30 +/- 116.76
Eval num_timesteps=35500, episode_reward=-119.94 +/- 114.01
Episode length: 120.86 +/- 113.74
Eval num_timesteps=36000, episode_reward=-123.38 +/- 121.06
Episode length: 124.29 +/- 120.78
Eval num_timesteps=36500, episode_reward=-118.81 +/- 110.25
Episode length: 119.74 +/- 110.01
Eval num_timesteps=37000, episode_reward=-100.68 +/- 65.67
Episode length: 101.67 +/- 65.61
New best mean reward!
Eval num_timesteps=37500, episode_reward=-113.42 +/- 102.20
Episode length: 114.36 +/- 101.97
Eval num_timesteps=38000, episode_reward=-103.75 +/- 83.84
Episode length: 104.71 +/- 83.65
Eval num_timesteps=38500, episode_reward=-100.48 +/- 84.24
Episode length: 101.44 +/- 84.05
New best mean reward!
Eval num_timesteps=39000, episode_reward=-100.26 +/- 80.10
Episode length: 101.23 +/- 79.95
New best mean reward!
Eval num_timesteps=39500, episode_reward=-102.94 +/- 83.66
Episode length: 103.91 +/- 83.52
Eval num_timesteps=40000, episode_reward=-83.41 +/- 20.07
Episode length: 84.41 +/- 20.07
New best mean reward!
FINISHED IN 632.9123852839693 s


starting seed  10206 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-306.23 +/- 169.30
Episode length: 306.80 +/- 168.81
New best mean reward!
Eval num_timesteps=9500, episode_reward=-106.77 +/- 69.60
Episode length: 107.75 +/- 69.49
New best mean reward!
Eval num_timesteps=10000, episode_reward=-109.98 +/- 70.04
Episode length: 110.96 +/- 69.93
Eval num_timesteps=10500, episode_reward=-96.77 +/- 29.00
Episode length: 97.77 +/- 29.00
New best mean reward!
FINISHED IN 268.3674286970054 s


starting seed  10207 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-381.83 +/- 162.27
Episode length: 382.18 +/- 161.80
New best mean reward!
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-190.67 +/- 102.14
Episode length: 191.58 +/- 101.87
New best mean reward!
Eval num_timesteps=11000, episode_reward=-156.36 +/- 54.10
Episode length: 157.34 +/- 53.97
New best mean reward!
Eval num_timesteps=11500, episode_reward=-156.61 +/- 30.63
Episode length: 157.61 +/- 30.63
Eval num_timesteps=12000, episode_reward=-93.20 +/- 32.77
Episode length: 94.20 +/- 32.77
New best mean reward!
FINISHED IN 284.49656989698997 s


starting seed  10208 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-362.44 +/- 163.62
Episode length: 362.86 +/- 163.13
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-477.50 +/- 89.36
Episode length: 477.56 +/- 89.13
Eval num_timesteps=5000, episode_reward=-148.40 +/- 62.79
Episode length: 149.38 +/- 62.68
New best mean reward!
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-202.16 +/- 58.89
Episode length: 203.13 +/- 58.74
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-495.66 +/- 43.18
Episode length: 495.67 +/- 43.08
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-475.51 +/- 97.01
Episode length: 475.57 +/- 96.77
Eval num_timesteps=9000, episode_reward=-458.96 +/- 123.19
Episode length: 459.06 +/- 122.89
Eval num_timesteps=9500, episode_reward=-487.76 +/- 69.68
Episode length: 487.79 +/- 69.51
Eval num_timesteps=10000, episode_reward=-297.33 +/- 204.37
Episode length: 297.83 +/- 203.88
Eval num_timesteps=10500, episode_reward=-310.33 +/- 204.56
Episode length: 310.80 +/- 204.07
Eval num_timesteps=11000, episode_reward=-248.31 +/- 202.19
Episode length: 248.92 +/- 201.70
Eval num_timesteps=11500, episode_reward=-255.48 +/- 202.67
Episode length: 256.08 +/- 202.19
Eval num_timesteps=12000, episode_reward=-194.56 +/- 178.57
Episode length: 195.32 +/- 178.16
Eval num_timesteps=12500, episode_reward=-136.52 +/- 136.08
Episode length: 137.40 +/- 135.76
New best mean reward!
Eval num_timesteps=13000, episode_reward=-188.55 +/- 176.47
Episode length: 189.31 +/- 176.05
Eval num_timesteps=13500, episode_reward=-170.31 +/- 167.39
Episode length: 171.11 +/- 166.99
Eval num_timesteps=14000, episode_reward=-181.27 +/- 171.30
Episode length: 182.05 +/- 170.89
Eval num_timesteps=14500, episode_reward=-136.40 +/- 136.98
Episode length: 137.28 +/- 136.66
New best mean reward!
Eval num_timesteps=15000, episode_reward=-168.19 +/- 161.86
Episode length: 169.01 +/- 161.49
Eval num_timesteps=15500, episode_reward=-139.86 +/- 135.12
Episode length: 140.74 +/- 134.80
Eval num_timesteps=16000, episode_reward=-115.13 +/- 100.72
Episode length: 116.07 +/- 100.49
New best mean reward!
Eval num_timesteps=16500, episode_reward=-94.35 +/- 61.48
Episode length: 95.33 +/- 61.35
New best mean reward!
FINISHED IN 317.78033550595865 s


starting seed  10209 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-451.64 +/- 120.26
Episode length: 451.78 +/- 119.92
New best mean reward!
Eval num_timesteps=9000, episode_reward=-447.77 +/- 135.14
Episode length: 447.90 +/- 134.80
New best mean reward!
Eval num_timesteps=9500, episode_reward=-145.89 +/- 128.88
Episode length: 146.79 +/- 128.61
New best mean reward!
Eval num_timesteps=10000, episode_reward=-164.99 +/- 158.96
Episode length: 165.81 +/- 158.58
Eval num_timesteps=10500, episode_reward=-123.23 +/- 106.01
Episode length: 124.16 +/- 105.77
New best mean reward!
Eval num_timesteps=11000, episode_reward=-94.91 +/- 39.94
Episode length: 95.91 +/- 39.94
New best mean reward!
FINISHED IN 266.7307658889913 s


starting seed  10210 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-479.70 +/- 88.56
Episode length: 479.75 +/- 88.34
New best mean reward!
Eval num_timesteps=9000, episode_reward=-484.25 +/- 77.26
Episode length: 484.29 +/- 77.06
Eval num_timesteps=9500, episode_reward=-418.97 +/- 162.24
Episode length: 419.17 +/- 161.84
New best mean reward!
Eval num_timesteps=10000, episode_reward=-280.54 +/- 199.40
Episode length: 281.09 +/- 198.90
New best mean reward!
Eval num_timesteps=10500, episode_reward=-265.61 +/- 199.71
Episode length: 266.19 +/- 199.22
New best mean reward!
Eval num_timesteps=11000, episode_reward=-187.95 +/- 165.09
Episode length: 188.74 +/- 164.69
New best mean reward!
Eval num_timesteps=11500, episode_reward=-219.23 +/- 185.35
Episode length: 219.94 +/- 184.91
Eval num_timesteps=12000, episode_reward=-183.46 +/- 163.68
Episode length: 184.27 +/- 163.31
New best mean reward!
Eval num_timesteps=12500, episode_reward=-129.16 +/- 107.69
Episode length: 130.09 +/- 107.45
New best mean reward!
Eval num_timesteps=13000, episode_reward=-140.70 +/- 125.60
Episode length: 141.60 +/- 125.32
Eval num_timesteps=13500, episode_reward=-135.87 +/- 123.85
Episode length: 136.77 +/- 123.55
Eval num_timesteps=14000, episode_reward=-138.84 +/- 129.57
Episode length: 139.73 +/- 129.26
Eval num_timesteps=14500, episode_reward=-107.44 +/- 84.45
Episode length: 108.40 +/- 84.27
New best mean reward!
Eval num_timesteps=15000, episode_reward=-100.05 +/- 62.81
Episode length: 101.03 +/- 62.68
New best mean reward!
Eval num_timesteps=15500, episode_reward=-86.23 +/- 25.09
Episode length: 87.23 +/- 25.09
New best mean reward!
FINISHED IN 320.99145080096787 s


starting seed  10211 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-128.64 +/- 81.70
Episode length: 129.60 +/- 81.51
New best mean reward!
Eval num_timesteps=10000, episode_reward=-122.15 +/- 82.12
Episode length: 123.11 +/- 81.93
New best mean reward!
Eval num_timesteps=10500, episode_reward=-105.22 +/- 36.07
Episode length: 106.22 +/- 36.07
New best mean reward!
Eval num_timesteps=11000, episode_reward=-99.60 +/- 28.26
Episode length: 100.60 +/- 28.26
New best mean reward!
FINISHED IN 259.0922423010343 s


starting seed  10212 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-380.60 +/- 163.39
Episode length: 380.95 +/- 162.92
New best mean reward!
Eval num_timesteps=8000, episode_reward=-200.90 +/- 142.13
Episode length: 201.72 +/- 141.75
New best mean reward!
Eval num_timesteps=8500, episode_reward=-168.64 +/- 104.17
Episode length: 169.56 +/- 103.91
New best mean reward!
Eval num_timesteps=9000, episode_reward=-451.17 +/- 132.34
Episode length: 451.29 +/- 132.02
Eval num_timesteps=9500, episode_reward=-199.68 +/- 183.83
Episode length: 200.41 +/- 183.39
Eval num_timesteps=10000, episode_reward=-144.19 +/- 145.64
Episode length: 145.05 +/- 145.30
New best mean reward!
Eval num_timesteps=10500, episode_reward=-86.19 +/- 22.24
Episode length: 87.19 +/- 22.24
New best mean reward!
FINISHED IN 230.46831685001962 s


starting seed  10213 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-100.57 +/- 25.45
Episode length: 101.57 +/- 25.45
New best mean reward!
Eval num_timesteps=7500, episode_reward=-204.27 +/- 180.68
Episode length: 205.03 +/- 180.28
Eval num_timesteps=8000, episode_reward=-208.08 +/- 170.32
Episode length: 208.85 +/- 169.92
Eval num_timesteps=8500, episode_reward=-316.59 +/- 200.49
Episode length: 317.05 +/- 199.99
Eval num_timesteps=9000, episode_reward=-108.89 +/- 78.23
Episode length: 109.86 +/- 78.08
Eval num_timesteps=9500, episode_reward=-97.73 +/- 51.45
Episode length: 98.72 +/- 51.37
New best mean reward!
FINISHED IN 223.76433040603297 s


starting seed  10214 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-492.79 +/- 50.48
Episode length: 492.81 +/- 50.34
New best mean reward!
Eval num_timesteps=7000, episode_reward=-120.45 +/- 49.82
Episode length: 121.44 +/- 49.74
New best mean reward!
Eval num_timesteps=7500, episode_reward=-120.95 +/- 53.31
Episode length: 121.94 +/- 53.24
Eval num_timesteps=8000, episode_reward=-124.89 +/- 69.47
Episode length: 125.87 +/- 69.36
Eval num_timesteps=8500, episode_reward=-136.11 +/- 74.91
Episode length: 137.08 +/- 74.77
Eval num_timesteps=9000, episode_reward=-136.63 +/- 90.17
Episode length: 137.58 +/- 89.97
Eval num_timesteps=9500, episode_reward=-113.84 +/- 50.97
Episode length: 114.83 +/- 50.89
New best mean reward!
Eval num_timesteps=10000, episode_reward=-113.76 +/- 32.26
Episode length: 114.76 +/- 32.26
New best mean reward!
Eval num_timesteps=10500, episode_reward=-120.90 +/- 62.44
Episode length: 121.88 +/- 62.31
Eval num_timesteps=11000, episode_reward=-116.61 +/- 59.56
Episode length: 117.60 +/- 59.49
Eval num_timesteps=11500, episode_reward=-136.21 +/- 89.68
Episode length: 137.17 +/- 89.52
Eval num_timesteps=12000, episode_reward=-145.56 +/- 102.47
Episode length: 146.50 +/- 102.27
Eval num_timesteps=12500, episode_reward=-180.54 +/- 147.60
Episode length: 181.37 +/- 147.23
Eval num_timesteps=13000, episode_reward=-121.22 +/- 56.97
Episode length: 122.21 +/- 56.90
Eval num_timesteps=13500, episode_reward=-124.45 +/- 76.34
Episode length: 125.43 +/- 76.25
Eval num_timesteps=14000, episode_reward=-110.93 +/- 33.01
Episode length: 111.93 +/- 33.01
New best mean reward!
Eval num_timesteps=14500, episode_reward=-118.62 +/- 39.85
Episode length: 119.62 +/- 39.85
Eval num_timesteps=15000, episode_reward=-106.42 +/- 32.42
Episode length: 107.42 +/- 32.42
New best mean reward!
Eval num_timesteps=15500, episode_reward=-108.42 +/- 39.57
Episode length: 109.42 +/- 39.57
Eval num_timesteps=16000, episode_reward=-100.53 +/- 30.72
Episode length: 101.53 +/- 30.72
New best mean reward!
Eval num_timesteps=16500, episode_reward=-113.42 +/- 55.38
Episode length: 114.41 +/- 55.31
Eval num_timesteps=17000, episode_reward=-104.17 +/- 46.17
Episode length: 105.16 +/- 46.08
Eval num_timesteps=17500, episode_reward=-102.58 +/- 34.05
Episode length: 103.58 +/- 34.05
Eval num_timesteps=18000, episode_reward=-100.91 +/- 24.96
Episode length: 101.91 +/- 24.96
Eval num_timesteps=18500, episode_reward=-103.85 +/- 65.46
Episode length: 104.83 +/- 65.34
Eval num_timesteps=19000, episode_reward=-97.70 +/- 37.06
Episode length: 98.70 +/- 37.06
New best mean reward!
FINISHED IN 280.9167724599829 s


starting seed  10215 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-496.39 +/- 22.32
Episode length: 496.42 +/- 22.16
New best mean reward!
Eval num_timesteps=6500, episode_reward=-419.24 +/- 116.16
Episode length: 419.63 +/- 115.74
New best mean reward!
Eval num_timesteps=7000, episode_reward=-230.57 +/- 134.51
Episode length: 231.38 +/- 134.13
New best mean reward!
Eval num_timesteps=7500, episode_reward=-480.11 +/- 75.09
Episode length: 480.19 +/- 74.85
Eval num_timesteps=8000, episode_reward=-167.55 +/- 41.44
Episode length: 168.55 +/- 41.44
New best mean reward!
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-182.58 +/- 54.84
Episode length: 183.56 +/- 54.72
Eval num_timesteps=9500, episode_reward=-172.50 +/- 58.76
Episode length: 173.48 +/- 58.64
Eval num_timesteps=10000, episode_reward=-365.51 +/- 163.20
Episode length: 365.92 +/- 162.72
Eval num_timesteps=10500, episode_reward=-379.74 +/- 157.16
Episode length: 380.12 +/- 156.69
Eval num_timesteps=11000, episode_reward=-166.76 +/- 42.92
Episode length: 167.75 +/- 42.84
New best mean reward!
Eval num_timesteps=11500, episode_reward=-163.87 +/- 36.32
Episode length: 164.87 +/- 36.32
New best mean reward!
Eval num_timesteps=12000, episode_reward=-177.35 +/- 33.98
Episode length: 178.35 +/- 33.98
Eval num_timesteps=12500, episode_reward=-235.50 +/- 104.94
Episode length: 236.39 +/- 104.66
Eval num_timesteps=13000, episode_reward=-449.10 +/- 109.69
Episode length: 449.29 +/- 109.32
Eval num_timesteps=13500, episode_reward=-434.37 +/- 142.23
Episode length: 434.55 +/- 141.85
Eval num_timesteps=14000, episode_reward=-246.19 +/- 192.98
Episode length: 246.83 +/- 192.50
Eval num_timesteps=14500, episode_reward=-166.60 +/- 151.58
Episode length: 167.45 +/- 151.25
Eval num_timesteps=15000, episode_reward=-136.61 +/- 125.56
Episode length: 137.51 +/- 125.27
New best mean reward!
Eval num_timesteps=15500, episode_reward=-110.95 +/- 79.12
Episode length: 111.92 +/- 78.97
New best mean reward!
Eval num_timesteps=16000, episode_reward=-86.84 +/- 24.09
Episode length: 87.84 +/- 24.09
New best mean reward!
FINISHED IN 311.9405488410266 s


starting seed  10216 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-154.96 +/- 141.05
Episode length: 155.82 +/- 140.71
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-471.97 +/- 102.28
Episode length: 472.04 +/- 102.03
Eval num_timesteps=7500, episode_reward=-335.28 +/- 191.50
Episode length: 335.71 +/- 191.01
Eval num_timesteps=8000, episode_reward=-468.94 +/- 87.75
Episode length: 469.08 +/- 87.45
Eval num_timesteps=8500, episode_reward=-492.68 +/- 36.94
Episode length: 492.72 +/- 36.75
Eval num_timesteps=9000, episode_reward=-348.74 +/- 146.80
Episode length: 349.38 +/- 146.43
Eval num_timesteps=9500, episode_reward=-126.25 +/- 55.27
Episode length: 127.24 +/- 55.20
New best mean reward!
Eval num_timesteps=10000, episode_reward=-139.35 +/- 45.38
Episode length: 140.35 +/- 45.38
Eval num_timesteps=10500, episode_reward=-144.05 +/- 34.13
Episode length: 145.05 +/- 34.13
Eval num_timesteps=11000, episode_reward=-105.58 +/- 44.23
Episode length: 106.57 +/- 44.15
New best mean reward!
Eval num_timesteps=11500, episode_reward=-123.67 +/- 105.21
Episode length: 124.60 +/- 104.96
Eval num_timesteps=12000, episode_reward=-114.35 +/- 83.55
Episode length: 115.31 +/- 83.37
Eval num_timesteps=12500, episode_reward=-117.39 +/- 96.67
Episode length: 118.34 +/- 96.48
Eval num_timesteps=13000, episode_reward=-130.02 +/- 118.79
Episode length: 130.93 +/- 118.51
Eval num_timesteps=13500, episode_reward=-111.96 +/- 93.00
Episode length: 112.91 +/- 92.79
Eval num_timesteps=14000, episode_reward=-110.66 +/- 91.32
Episode length: 111.61 +/- 91.10
Eval num_timesteps=14500, episode_reward=-113.45 +/- 90.76
Episode length: 114.41 +/- 90.59
Eval num_timesteps=15000, episode_reward=-104.75 +/- 58.16
Episode length: 105.74 +/- 58.10
New best mean reward!
Eval num_timesteps=15500, episode_reward=-99.43 +/- 62.16
Episode length: 100.41 +/- 62.03
New best mean reward!
FINISHED IN 278.88951254897984 s


starting seed  10217 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-462.41 +/- 38.33
Episode length: 463.09 +/- 38.02
New best mean reward!
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-98.12 +/- 47.24
Episode length: 99.11 +/- 47.16
New best mean reward!
FINISHED IN 215.4395720619941 s


starting seed  10218 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-455.88 +/- 125.71
Episode length: 455.99 +/- 125.40
New best mean reward!
Eval num_timesteps=12000, episode_reward=-434.03 +/- 146.04
Episode length: 434.20 +/- 145.66
New best mean reward!
Eval num_timesteps=12500, episode_reward=-382.34 +/- 180.36
Episode length: 382.64 +/- 179.90
New best mean reward!
Eval num_timesteps=13000, episode_reward=-360.64 +/- 187.05
Episode length: 361.01 +/- 186.58
New best mean reward!
Eval num_timesteps=13500, episode_reward=-352.61 +/- 192.68
Episode length: 352.98 +/- 192.20
New best mean reward!
Eval num_timesteps=14000, episode_reward=-279.86 +/- 196.90
Episode length: 280.42 +/- 196.41
New best mean reward!
Eval num_timesteps=14500, episode_reward=-246.99 +/- 193.25
Episode length: 247.64 +/- 192.79
New best mean reward!
Eval num_timesteps=15000, episode_reward=-233.92 +/- 191.44
Episode length: 234.59 +/- 190.98
New best mean reward!
Eval num_timesteps=15500, episode_reward=-250.74 +/- 194.50
Episode length: 251.37 +/- 194.03
Eval num_timesteps=16000, episode_reward=-244.08 +/- 189.98
Episode length: 244.73 +/- 189.51
Eval num_timesteps=16500, episode_reward=-278.81 +/- 199.92
Episode length: 279.38 +/- 199.45
Eval num_timesteps=17000, episode_reward=-214.75 +/- 184.54
Episode length: 215.46 +/- 184.09
New best mean reward!
Eval num_timesteps=17500, episode_reward=-193.17 +/- 171.07
Episode length: 193.95 +/- 170.68
New best mean reward!
Eval num_timesteps=18000, episode_reward=-192.02 +/- 178.92
Episode length: 192.77 +/- 178.49
New best mean reward!
Eval num_timesteps=18500, episode_reward=-143.11 +/- 132.28
Episode length: 144.00 +/- 131.99
New best mean reward!
Eval num_timesteps=19000, episode_reward=-160.51 +/- 152.62
Episode length: 161.36 +/- 152.29
Eval num_timesteps=19500, episode_reward=-143.70 +/- 144.80
Episode length: 144.56 +/- 144.46
Eval num_timesteps=20000, episode_reward=-142.30 +/- 136.76
Episode length: 143.18 +/- 136.44
New best mean reward!
Eval num_timesteps=20500, episode_reward=-134.11 +/- 125.47
Episode length: 135.01 +/- 125.18
New best mean reward!
Eval num_timesteps=21000, episode_reward=-115.21 +/- 101.15
Episode length: 116.15 +/- 100.92
New best mean reward!
Eval num_timesteps=21500, episode_reward=-139.99 +/- 131.38
Episode length: 140.88 +/- 131.08
Eval num_timesteps=22000, episode_reward=-125.39 +/- 113.61
Episode length: 126.31 +/- 113.35
Eval num_timesteps=22500, episode_reward=-140.68 +/- 135.59
Episode length: 141.57 +/- 135.30
Eval num_timesteps=23000, episode_reward=-155.32 +/- 149.66
Episode length: 156.17 +/- 149.32
Eval num_timesteps=23500, episode_reward=-130.73 +/- 125.97
Episode length: 131.63 +/- 125.68
Eval num_timesteps=24000, episode_reward=-152.59 +/- 144.63
Episode length: 153.46 +/- 144.32
Eval num_timesteps=24500, episode_reward=-127.44 +/- 121.93
Episode length: 128.35 +/- 121.66
Eval num_timesteps=25000, episode_reward=-109.06 +/- 79.44
Episode length: 110.04 +/- 79.35
New best mean reward!
Eval num_timesteps=25500, episode_reward=-111.68 +/- 84.06
Episode length: 112.65 +/- 83.92
Eval num_timesteps=26000, episode_reward=-132.89 +/- 124.03
Episode length: 133.80 +/- 123.77
Eval num_timesteps=26500, episode_reward=-119.91 +/- 110.43
Episode length: 120.85 +/- 110.23
Eval num_timesteps=27000, episode_reward=-110.96 +/- 87.07
Episode length: 111.92 +/- 86.89
Eval num_timesteps=27500, episode_reward=-106.76 +/- 68.88
Episode length: 107.74 +/- 68.76
New best mean reward!
Eval num_timesteps=28000, episode_reward=-110.37 +/- 87.97
Episode length: 111.33 +/- 87.79
Eval num_timesteps=28500, episode_reward=-109.28 +/- 83.41
Episode length: 110.24 +/- 83.23
Eval num_timesteps=29000, episode_reward=-95.16 +/- 39.99
Episode length: 96.16 +/- 39.99
New best mean reward!
FINISHED IN 511.0929442549823 s


starting seed  10219 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-372.04 +/- 171.33
Episode length: 372.40 +/- 170.85
New best mean reward!
Eval num_timesteps=7000, episode_reward=-120.35 +/- 100.32
Episode length: 121.29 +/- 100.09
New best mean reward!
Eval num_timesteps=7500, episode_reward=-89.25 +/- 49.33
Episode length: 90.24 +/- 49.24
New best mean reward!
FINISHED IN 173.06000087002758 s


starting seed  10220 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-266.52 +/- 160.59
Episode length: 267.21 +/- 160.14
New best mean reward!
Eval num_timesteps=7000, episode_reward=-185.33 +/- 117.58
Episode length: 186.21 +/- 117.26
New best mean reward!
Eval num_timesteps=7500, episode_reward=-375.25 +/- 178.48
Episode length: 375.58 +/- 178.01
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-140.11 +/- 121.69
Episode length: 141.01 +/- 121.40
New best mean reward!
Eval num_timesteps=10000, episode_reward=-187.57 +/- 135.94
Episode length: 188.42 +/- 135.60
Eval num_timesteps=10500, episode_reward=-98.56 +/- 47.29
Episode length: 99.55 +/- 47.21
New best mean reward!
FINISHED IN 308.6425809059874 s


starting seed  10221 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-130.57 +/- 47.40
Episode length: 131.56 +/- 47.33
New best mean reward!
Eval num_timesteps=5500, episode_reward=-90.10 +/- 22.51
Episode length: 91.10 +/- 22.51
New best mean reward!
FINISHED IN 157.4691907609813 s


starting seed  10222 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-189.78 +/- 66.34
Episode length: 190.75 +/- 66.20
New best mean reward!
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-487.23 +/- 62.59
Episode length: 487.27 +/- 62.39
Eval num_timesteps=8500, episode_reward=-108.24 +/- 20.71
Episode length: 109.24 +/- 20.71
New best mean reward!
Eval num_timesteps=9000, episode_reward=-168.57 +/- 44.31
Episode length: 169.57 +/- 44.31
Eval num_timesteps=9500, episode_reward=-162.28 +/- 45.99
Episode length: 163.28 +/- 45.99
Eval num_timesteps=10000, episode_reward=-169.50 +/- 59.02
Episode length: 170.49 +/- 58.96
Eval num_timesteps=10500, episode_reward=-161.99 +/- 43.34
Episode length: 162.99 +/- 43.34
Eval num_timesteps=11000, episode_reward=-163.66 +/- 33.85
Episode length: 164.66 +/- 33.85
Eval num_timesteps=11500, episode_reward=-160.17 +/- 35.11
Episode length: 161.17 +/- 35.11
Eval num_timesteps=12000, episode_reward=-124.14 +/- 22.17
Episode length: 125.14 +/- 22.17
Eval num_timesteps=12500, episode_reward=-147.96 +/- 33.44
Episode length: 148.96 +/- 33.44
Eval num_timesteps=13000, episode_reward=-119.63 +/- 48.67
Episode length: 120.62 +/- 48.59
Eval num_timesteps=13500, episode_reward=-104.31 +/- 45.46
Episode length: 105.30 +/- 45.38
New best mean reward!
Eval num_timesteps=14000, episode_reward=-92.47 +/- 19.13
Episode length: 93.47 +/- 19.13
New best mean reward!
FINISHED IN 244.50056294701062 s


starting seed  10223 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-394.29 +/- 175.72
Episode length: 394.56 +/- 175.28
New best mean reward!
Eval num_timesteps=4500, episode_reward=-101.33 +/- 32.91
Episode length: 102.33 +/- 32.91
New best mean reward!
Eval num_timesteps=5000, episode_reward=-159.46 +/- 141.29
Episode length: 160.32 +/- 140.95
Eval num_timesteps=5500, episode_reward=-376.28 +/- 185.88
Episode length: 376.59 +/- 185.42
Eval num_timesteps=6000, episode_reward=-101.35 +/- 63.35
Episode length: 102.33 +/- 63.22
Eval num_timesteps=6500, episode_reward=-91.15 +/- 37.12
Episode length: 92.15 +/- 37.12
New best mean reward!
FINISHED IN 170.68310552102048 s


starting seed  10224 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-324.41 +/- 195.62
Episode length: 324.87 +/- 195.13
New best mean reward!
Eval num_timesteps=9000, episode_reward=-235.06 +/- 189.55
Episode length: 235.73 +/- 189.09
New best mean reward!
Eval num_timesteps=9500, episode_reward=-107.83 +/- 72.73
Episode length: 108.81 +/- 72.63
New best mean reward!
Eval num_timesteps=10000, episode_reward=-89.66 +/- 33.59
Episode length: 90.66 +/- 33.59
New best mean reward!
FINISHED IN 222.12455142999534 s


starting seed  10225 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-353.40 +/- 195.92
Episode length: 353.76 +/- 195.44
New best mean reward!
Eval num_timesteps=1000, episode_reward=-481.36 +/- 74.24
Episode length: 481.42 +/- 74.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-464.49 +/- 101.07
Episode length: 464.60 +/- 100.76
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-416.35 +/- 143.13
Episode length: 416.61 +/- 142.69
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-467.53 +/- 97.77
Episode length: 467.63 +/- 97.47
Eval num_timesteps=6000, episode_reward=-367.54 +/- 161.51
Episode length: 367.95 +/- 161.02
Eval num_timesteps=6500, episode_reward=-374.62 +/- 161.67
Episode length: 375.00 +/- 161.19
Eval num_timesteps=7000, episode_reward=-313.40 +/- 164.21
Episode length: 313.97 +/- 163.72
New best mean reward!
Eval num_timesteps=7500, episode_reward=-258.48 +/- 176.46
Episode length: 259.14 +/- 176.00
New best mean reward!
Eval num_timesteps=8000, episode_reward=-366.89 +/- 166.93
Episode length: 367.28 +/- 166.44
Eval num_timesteps=8500, episode_reward=-318.01 +/- 198.02
Episode length: 318.47 +/- 197.52
Eval num_timesteps=9000, episode_reward=-287.12 +/- 201.05
Episode length: 287.65 +/- 200.55
Eval num_timesteps=9500, episode_reward=-229.69 +/- 190.27
Episode length: 230.36 +/- 189.80
New best mean reward!
Eval num_timesteps=10000, episode_reward=-307.65 +/- 201.14
Episode length: 308.13 +/- 200.64
Eval num_timesteps=10500, episode_reward=-150.27 +/- 137.94
Episode length: 151.15 +/- 137.64
New best mean reward!
Eval num_timesteps=11000, episode_reward=-210.14 +/- 186.90
Episode length: 210.85 +/- 186.45
Eval num_timesteps=11500, episode_reward=-138.49 +/- 137.51
Episode length: 139.37 +/- 137.19
New best mean reward!
Eval num_timesteps=12000, episode_reward=-144.24 +/- 141.00
Episode length: 145.11 +/- 140.67
Eval num_timesteps=12500, episode_reward=-145.67 +/- 148.38
Episode length: 146.53 +/- 148.05
Eval num_timesteps=13000, episode_reward=-174.78 +/- 169.53
Episode length: 175.57 +/- 169.13
Eval num_timesteps=13500, episode_reward=-116.83 +/- 108.25
Episode length: 117.76 +/- 108.01
New best mean reward!
Eval num_timesteps=14000, episode_reward=-117.78 +/- 115.40
Episode length: 118.70 +/- 115.14
Eval num_timesteps=14500, episode_reward=-89.55 +/- 62.00
Episode length: 90.53 +/- 61.87
New best mean reward!
FINISHED IN 277.39600990497274 s


starting seed  10226 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-107.37 +/- 29.33
Episode length: 108.37 +/- 29.33
New best mean reward!
Eval num_timesteps=7000, episode_reward=-117.60 +/- 31.54
Episode length: 118.60 +/- 31.54
Eval num_timesteps=7500, episode_reward=-108.00 +/- 31.97
Episode length: 109.00 +/- 31.97
Eval num_timesteps=8000, episode_reward=-113.61 +/- 23.72
Episode length: 114.61 +/- 23.72
Eval num_timesteps=8500, episode_reward=-182.96 +/- 38.65
Episode length: 183.96 +/- 38.65
Eval num_timesteps=9000, episode_reward=-159.17 +/- 36.48
Episode length: 160.17 +/- 36.48
Eval num_timesteps=9500, episode_reward=-148.73 +/- 24.27
Episode length: 149.73 +/- 24.27
Eval num_timesteps=10000, episode_reward=-101.42 +/- 33.03
Episode length: 102.42 +/- 33.03
New best mean reward!
Eval num_timesteps=10500, episode_reward=-85.86 +/- 20.67
Episode length: 86.86 +/- 20.67
New best mean reward!
FINISHED IN 183.86737898900174 s


starting seed  10227 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-393.91 +/- 157.76
Episode length: 394.24 +/- 157.31
New best mean reward!
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-495.96 +/- 40.20
Episode length: 495.97 +/- 40.10
Eval num_timesteps=14500, episode_reward=-483.23 +/- 82.22
Episode length: 483.27 +/- 82.02
Eval num_timesteps=15000, episode_reward=-496.04 +/- 39.40
Episode length: 496.05 +/- 39.30
Eval num_timesteps=15500, episode_reward=-396.26 +/- 179.80
Episode length: 396.51 +/- 179.36
Eval num_timesteps=16000, episode_reward=-348.73 +/- 202.05
Episode length: 349.09 +/- 201.58
New best mean reward!
Eval num_timesteps=16500, episode_reward=-356.70 +/- 199.85
Episode length: 357.04 +/- 199.37
Eval num_timesteps=17000, episode_reward=-258.81 +/- 205.92
Episode length: 259.39 +/- 205.43
New best mean reward!
Eval num_timesteps=17500, episode_reward=-278.67 +/- 208.80
Episode length: 279.20 +/- 208.31
Eval num_timesteps=18000, episode_reward=-280.35 +/- 207.77
Episode length: 280.88 +/- 207.27
Eval num_timesteps=18500, episode_reward=-383.58 +/- 186.92
Episode length: 383.86 +/- 186.48
Eval num_timesteps=19000, episode_reward=-365.31 +/- 193.04
Episode length: 365.64 +/- 192.58
Eval num_timesteps=19500, episode_reward=-305.51 +/- 206.96
Episode length: 305.98 +/- 206.46
Eval num_timesteps=20000, episode_reward=-346.01 +/- 201.27
Episode length: 346.38 +/- 200.79
Eval num_timesteps=20500, episode_reward=-268.72 +/- 202.88
Episode length: 269.29 +/- 202.39
Eval num_timesteps=21000, episode_reward=-218.17 +/- 188.39
Episode length: 218.87 +/- 187.94
New best mean reward!
Eval num_timesteps=21500, episode_reward=-172.96 +/- 165.42
Episode length: 173.76 +/- 165.02
New best mean reward!
Eval num_timesteps=22000, episode_reward=-155.05 +/- 151.00
Episode length: 155.90 +/- 150.66
New best mean reward!
Eval num_timesteps=22500, episode_reward=-109.41 +/- 96.28
Episode length: 110.36 +/- 96.08
New best mean reward!
Eval num_timesteps=23000, episode_reward=-116.29 +/- 102.38
Episode length: 117.23 +/- 102.16
Eval num_timesteps=23500, episode_reward=-144.04 +/- 139.73
Episode length: 144.91 +/- 139.40
Eval num_timesteps=24000, episode_reward=-110.70 +/- 93.72
Episode length: 111.65 +/- 93.52
Eval num_timesteps=24500, episode_reward=-156.38 +/- 152.08
Episode length: 157.24 +/- 151.77
Eval num_timesteps=25000, episode_reward=-106.12 +/- 84.26
Episode length: 107.08 +/- 84.07
New best mean reward!
Eval num_timesteps=25500, episode_reward=-112.71 +/- 93.43
Episode length: 113.66 +/- 93.22
Eval num_timesteps=26000, episode_reward=-96.72 +/- 73.36
Episode length: 97.69 +/- 73.19
New best mean reward!
FINISHED IN 626.6674064989784 s


starting seed  10228 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-452.69 +/- 124.26
Episode length: 452.82 +/- 123.93
New best mean reward!
Eval num_timesteps=14500, episode_reward=-314.55 +/- 196.52
Episode length: 315.03 +/- 196.03
New best mean reward!
Eval num_timesteps=15000, episode_reward=-302.62 +/- 201.40
Episode length: 303.12 +/- 200.91
New best mean reward!
Eval num_timesteps=15500, episode_reward=-289.66 +/- 198.33
Episode length: 290.20 +/- 197.84
New best mean reward!
Eval num_timesteps=16000, episode_reward=-251.18 +/- 192.58
Episode length: 251.82 +/- 192.11
New best mean reward!
Eval num_timesteps=16500, episode_reward=-207.47 +/- 179.88
Episode length: 208.21 +/- 179.45
New best mean reward!
Eval num_timesteps=17000, episode_reward=-164.91 +/- 158.38
Episode length: 165.73 +/- 158.00
New best mean reward!
Eval num_timesteps=17500, episode_reward=-194.24 +/- 175.42
Episode length: 195.00 +/- 175.00
Eval num_timesteps=18000, episode_reward=-185.96 +/- 166.01
Episode length: 186.75 +/- 165.61
Eval num_timesteps=18500, episode_reward=-183.11 +/- 169.85
Episode length: 183.90 +/- 169.46
Eval num_timesteps=19000, episode_reward=-185.16 +/- 173.88
Episode length: 185.93 +/- 173.46
Eval num_timesteps=19500, episode_reward=-138.08 +/- 130.54
Episode length: 138.98 +/- 130.26
New best mean reward!
Eval num_timesteps=20000, episode_reward=-129.92 +/- 122.79
Episode length: 130.83 +/- 122.52
New best mean reward!
Eval num_timesteps=20500, episode_reward=-128.28 +/- 116.45
Episode length: 129.21 +/- 116.23
New best mean reward!
Eval num_timesteps=21000, episode_reward=-117.79 +/- 101.41
Episode length: 118.73 +/- 101.18
New best mean reward!
Eval num_timesteps=21500, episode_reward=-92.58 +/- 30.42
Episode length: 93.58 +/- 30.42
New best mean reward!
FINISHED IN 591.3381702720071 s


starting seed  10229 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-224.87 +/- 193.58
Episode length: 225.55 +/- 193.13
New best mean reward!
Eval num_timesteps=7500, episode_reward=-205.52 +/- 188.93
Episode length: 206.23 +/- 188.48
New best mean reward!
Eval num_timesteps=8000, episode_reward=-101.31 +/- 54.89
Episode length: 102.30 +/- 54.82
New best mean reward!
Eval num_timesteps=8500, episode_reward=-89.76 +/- 30.26
Episode length: 90.76 +/- 30.26
New best mean reward!
FINISHED IN 196.656958276988 s


starting seed  10230 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-256.71 +/- 143.06
Episode length: 257.47 +/- 142.66
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-472.80 +/- 88.43
Episode length: 472.91 +/- 88.16
Eval num_timesteps=5000, episode_reward=-178.03 +/- 73.21
Episode length: 179.01 +/- 73.12
New best mean reward!
Eval num_timesteps=5500, episode_reward=-360.33 +/- 131.57
Episode length: 360.91 +/- 131.13
Eval num_timesteps=6000, episode_reward=-451.24 +/- 106.65
Episode length: 451.43 +/- 106.28
Eval num_timesteps=6500, episode_reward=-356.65 +/- 171.53
Episode length: 357.07 +/- 171.05
Eval num_timesteps=7000, episode_reward=-293.48 +/- 164.19
Episode length: 294.10 +/- 163.71
Eval num_timesteps=7500, episode_reward=-313.77 +/- 177.41
Episode length: 314.30 +/- 176.92
Eval num_timesteps=8000, episode_reward=-291.38 +/- 185.45
Episode length: 291.96 +/- 184.98
Eval num_timesteps=8500, episode_reward=-424.49 +/- 147.84
Episode length: 424.70 +/- 147.44
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-451.68 +/- 126.08
Episode length: 451.81 +/- 125.75
Eval num_timesteps=10000, episode_reward=-460.82 +/- 117.64
Episode length: 460.92 +/- 117.34
Eval num_timesteps=10500, episode_reward=-465.45 +/- 109.95
Episode length: 465.54 +/- 109.66
Eval num_timesteps=11000, episode_reward=-469.75 +/- 103.11
Episode length: 469.83 +/- 102.84
Eval num_timesteps=11500, episode_reward=-354.01 +/- 187.29
Episode length: 354.39 +/- 186.81
Eval num_timesteps=12000, episode_reward=-233.81 +/- 183.87
Episode length: 234.50 +/- 183.42
Eval num_timesteps=12500, episode_reward=-294.43 +/- 195.48
Episode length: 294.97 +/- 195.00
Eval num_timesteps=13000, episode_reward=-145.74 +/- 133.08
Episode length: 146.63 +/- 132.78
New best mean reward!
Eval num_timesteps=13500, episode_reward=-130.44 +/- 104.55
Episode length: 131.38 +/- 104.34
New best mean reward!
Eval num_timesteps=14000, episode_reward=-160.20 +/- 141.31
Episode length: 161.06 +/- 140.98
Eval num_timesteps=14500, episode_reward=-108.15 +/- 67.96
Episode length: 109.13 +/- 67.85
New best mean reward!
Eval num_timesteps=15000, episode_reward=-92.64 +/- 29.62
Episode length: 93.64 +/- 29.62
New best mean reward!
FINISHED IN 301.5943669929984 s


starting seed  10231 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-174.74 +/- 54.46
Episode length: 175.72 +/- 54.34
New best mean reward!
Eval num_timesteps=9000, episode_reward=-245.48 +/- 118.00
Episode length: 246.31 +/- 117.63
Eval num_timesteps=9500, episode_reward=-200.27 +/- 77.83
Episode length: 201.22 +/- 77.64
Eval num_timesteps=10000, episode_reward=-397.30 +/- 103.54
Episode length: 397.82 +/- 103.06
Eval num_timesteps=10500, episode_reward=-464.76 +/- 63.34
Episode length: 465.05 +/- 62.95
Eval num_timesteps=11000, episode_reward=-311.91 +/- 153.39
Episode length: 312.53 +/- 152.93
Eval num_timesteps=11500, episode_reward=-499.16 +/- 8.36
Episode length: 499.17 +/- 8.26
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-498.52 +/- 14.73
Episode length: 498.53 +/- 14.63
Eval num_timesteps=13000, episode_reward=-499.40 +/- 5.97
Episode length: 499.41 +/- 5.87
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-464.24 +/- 42.08
Episode length: 464.77 +/- 41.68
Eval num_timesteps=14500, episode_reward=-203.61 +/- 51.27
Episode length: 204.60 +/- 51.21
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-497.39 +/- 18.60
Episode length: 497.41 +/- 18.46
Eval num_timesteps=16000, episode_reward=-188.97 +/- 54.83
Episode length: 189.96 +/- 54.77
Eval num_timesteps=16500, episode_reward=-151.43 +/- 55.07
Episode length: 152.42 +/- 55.00
New best mean reward!
Eval num_timesteps=17000, episode_reward=-137.14 +/- 69.63
Episode length: 138.11 +/- 69.48
New best mean reward!
Eval num_timesteps=17500, episode_reward=-126.42 +/- 51.60
Episode length: 127.41 +/- 51.53
New best mean reward!
Eval num_timesteps=18000, episode_reward=-135.56 +/- 62.22
Episode length: 136.54 +/- 62.10
Eval num_timesteps=18500, episode_reward=-146.61 +/- 87.29
Episode length: 147.56 +/- 87.09
Eval num_timesteps=19000, episode_reward=-203.68 +/- 155.17
Episode length: 204.48 +/- 154.79
Eval num_timesteps=19500, episode_reward=-194.55 +/- 151.80
Episode length: 195.36 +/- 151.42
Eval num_timesteps=20000, episode_reward=-188.86 +/- 143.28
Episode length: 189.69 +/- 142.91
Eval num_timesteps=20500, episode_reward=-179.97 +/- 151.19
Episode length: 180.79 +/- 150.81
Eval num_timesteps=21000, episode_reward=-188.00 +/- 152.55
Episode length: 188.81 +/- 152.16
Eval num_timesteps=21500, episode_reward=-237.60 +/- 174.01
Episode length: 238.32 +/- 173.59
Eval num_timesteps=22000, episode_reward=-177.19 +/- 153.25
Episode length: 178.01 +/- 152.87
Eval num_timesteps=22500, episode_reward=-191.86 +/- 161.04
Episode length: 192.65 +/- 160.64
Eval num_timesteps=23000, episode_reward=-168.49 +/- 147.69
Episode length: 169.33 +/- 147.33
Eval num_timesteps=23500, episode_reward=-162.11 +/- 140.05
Episode length: 162.97 +/- 139.72
Eval num_timesteps=24000, episode_reward=-163.76 +/- 142.81
Episode length: 164.64 +/- 142.53
Eval num_timesteps=24500, episode_reward=-171.71 +/- 139.52
Episode length: 172.57 +/- 139.19
Eval num_timesteps=25000, episode_reward=-146.95 +/- 127.39
Episode length: 147.85 +/- 127.11
Eval num_timesteps=25500, episode_reward=-143.79 +/- 121.22
Episode length: 144.70 +/- 120.96
Eval num_timesteps=26000, episode_reward=-169.09 +/- 150.07
Episode length: 169.93 +/- 149.71
Eval num_timesteps=26500, episode_reward=-179.03 +/- 158.83
Episode length: 179.84 +/- 158.45
Eval num_timesteps=27000, episode_reward=-167.65 +/- 147.33
Episode length: 168.50 +/- 146.99
Eval num_timesteps=27500, episode_reward=-192.32 +/- 169.49
Episode length: 193.09 +/- 169.07
Eval num_timesteps=28000, episode_reward=-161.14 +/- 142.73
Episode length: 162.00 +/- 142.40
Eval num_timesteps=28500, episode_reward=-153.83 +/- 134.63
Episode length: 154.71 +/- 134.32
Eval num_timesteps=29000, episode_reward=-168.16 +/- 143.65
Episode length: 169.02 +/- 143.33
Eval num_timesteps=29500, episode_reward=-107.98 +/- 51.98
Episode length: 108.97 +/- 51.91
New best mean reward!
Eval num_timesteps=30000, episode_reward=-141.32 +/- 118.02
Episode length: 142.23 +/- 117.74
Eval num_timesteps=30500, episode_reward=-140.83 +/- 122.76
Episode length: 141.73 +/- 122.46
Eval num_timesteps=31000, episode_reward=-148.56 +/- 131.64
Episode length: 149.45 +/- 131.35
Eval num_timesteps=31500, episode_reward=-139.91 +/- 117.78
Episode length: 140.82 +/- 117.51
Eval num_timesteps=32000, episode_reward=-127.64 +/- 98.04
Episode length: 128.58 +/- 97.82
Eval num_timesteps=32500, episode_reward=-143.83 +/- 108.02
Episode length: 144.78 +/- 107.85
Eval num_timesteps=33000, episode_reward=-112.83 +/- 69.23
Episode length: 113.81 +/- 69.11
Eval num_timesteps=33500, episode_reward=-125.22 +/- 93.76
Episode length: 126.17 +/- 93.56
Eval num_timesteps=34000, episode_reward=-149.14 +/- 121.09
Episode length: 150.05 +/- 120.82
Eval num_timesteps=34500, episode_reward=-139.11 +/- 113.11
Episode length: 140.03 +/- 112.86
Eval num_timesteps=35000, episode_reward=-138.11 +/- 112.51
Episode length: 139.03 +/- 112.25
Eval num_timesteps=35500, episode_reward=-136.26 +/- 111.79
Episode length: 137.18 +/- 111.53
Eval num_timesteps=36000, episode_reward=-152.37 +/- 128.57
Episode length: 153.27 +/- 128.30
Eval num_timesteps=36500, episode_reward=-122.97 +/- 96.17
Episode length: 123.92 +/- 95.97
Eval num_timesteps=37000, episode_reward=-125.87 +/- 92.38
Episode length: 126.82 +/- 92.17
Eval num_timesteps=37500, episode_reward=-133.44 +/- 106.38
Episode length: 134.38 +/- 106.17
Eval num_timesteps=38000, episode_reward=-136.89 +/- 113.76
Episode length: 137.81 +/- 113.51
Eval num_timesteps=38500, episode_reward=-155.78 +/- 142.06
Episode length: 156.65 +/- 141.75
Eval num_timesteps=39000, episode_reward=-147.12 +/- 126.59
Episode length: 148.02 +/- 126.31
Eval num_timesteps=39500, episode_reward=-143.79 +/- 121.67
Episode length: 144.69 +/- 121.37
Eval num_timesteps=40000, episode_reward=-136.20 +/- 111.00
Episode length: 137.12 +/- 110.73
Eval num_timesteps=40500, episode_reward=-144.82 +/- 119.80
Episode length: 145.73 +/- 119.54
Eval num_timesteps=41000, episode_reward=-132.88 +/- 110.63
Episode length: 133.81 +/- 110.40
Eval num_timesteps=41500, episode_reward=-134.46 +/- 112.48
Episode length: 135.38 +/- 112.22
Eval num_timesteps=42000, episode_reward=-124.91 +/- 109.69
Episode length: 125.84 +/- 109.45
Eval num_timesteps=42500, episode_reward=-163.72 +/- 142.58
Episode length: 164.58 +/- 142.25
Eval num_timesteps=43000, episode_reward=-129.48 +/- 110.88
Episode length: 130.40 +/- 110.61
Eval num_timesteps=43500, episode_reward=-123.09 +/- 93.30
Episode length: 124.04 +/- 93.10
Eval num_timesteps=44000, episode_reward=-118.03 +/- 83.07
Episode length: 118.99 +/- 82.89
Eval num_timesteps=44500, episode_reward=-129.46 +/- 101.66
Episode length: 130.41 +/- 101.48
Eval num_timesteps=45000, episode_reward=-125.48 +/- 95.66
Episode length: 126.43 +/- 95.46
Eval num_timesteps=45500, episode_reward=-136.07 +/- 109.88
Episode length: 137.00 +/- 109.65
Eval num_timesteps=46000, episode_reward=-142.59 +/- 123.98
Episode length: 143.49 +/- 123.69
Eval num_timesteps=46500, episode_reward=-125.54 +/- 89.54
Episode length: 126.50 +/- 89.38
Eval num_timesteps=47000, episode_reward=-127.43 +/- 99.86
Episode length: 128.37 +/- 99.64
Eval num_timesteps=47500, episode_reward=-124.71 +/- 101.87
Episode length: 125.66 +/- 101.69
Eval num_timesteps=48000, episode_reward=-131.53 +/- 112.48
Episode length: 132.45 +/- 112.22
Eval num_timesteps=48500, episode_reward=-137.46 +/- 123.40
Episode length: 138.36 +/- 123.10
Eval num_timesteps=49000, episode_reward=-126.57 +/- 98.22
Episode length: 127.52 +/- 98.03
Eval num_timesteps=49500, episode_reward=-144.85 +/- 127.49
Episode length: 145.75 +/- 127.22
Eval num_timesteps=50000, episode_reward=-142.47 +/- 123.47
Episode length: 143.37 +/- 123.18
FINISHED IN 752.8871666440391 s


starting seed  10232 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-165.57 +/- 64.38
Episode length: 166.54 +/- 64.23
New best mean reward!
Eval num_timesteps=14500, episode_reward=-341.15 +/- 184.14
Episode length: 341.58 +/- 183.65
Eval num_timesteps=15000, episode_reward=-393.62 +/- 165.97
Episode length: 393.92 +/- 165.52
Eval num_timesteps=15500, episode_reward=-338.40 +/- 187.88
Episode length: 338.83 +/- 187.39
Eval num_timesteps=16000, episode_reward=-287.12 +/- 197.10
Episode length: 287.66 +/- 196.60
Eval num_timesteps=16500, episode_reward=-450.53 +/- 128.54
Episode length: 450.66 +/- 128.20
Eval num_timesteps=17000, episode_reward=-89.91 +/- 35.24
Episode length: 90.91 +/- 35.24
New best mean reward!
FINISHED IN 398.1070684270235 s


starting seed  10233 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-383.95 +/- 152.16
Episode length: 384.32 +/- 151.68
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-191.11 +/- 51.48
Episode length: 192.11 +/- 51.48
New best mean reward!
Eval num_timesteps=5000, episode_reward=-98.16 +/- 26.56
Episode length: 99.16 +/- 26.56
New best mean reward!
FINISHED IN 104.55669660604326 s


starting seed  10234 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-171.78 +/- 43.12
Episode length: 172.78 +/- 43.12
New best mean reward!
Eval num_timesteps=9000, episode_reward=-405.06 +/- 147.30
Episode length: 405.36 +/- 146.85
Eval num_timesteps=9500, episode_reward=-406.69 +/- 145.26
Episode length: 406.99 +/- 144.81
Eval num_timesteps=10000, episode_reward=-469.54 +/- 92.81
Episode length: 469.64 +/- 92.52
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-496.63 +/- 33.53
Episode length: 496.64 +/- 33.43
Eval num_timesteps=15000, episode_reward=-382.87 +/- 160.40
Episode length: 383.22 +/- 159.93
Eval num_timesteps=15500, episode_reward=-260.26 +/- 148.08
Episode length: 261.01 +/- 147.67
Eval num_timesteps=16000, episode_reward=-225.34 +/- 131.82
Episode length: 226.16 +/- 131.44
Eval num_timesteps=16500, episode_reward=-240.60 +/- 138.74
Episode length: 241.39 +/- 138.34
Eval num_timesteps=17000, episode_reward=-216.15 +/- 116.39
Episode length: 217.02 +/- 116.07
Eval num_timesteps=17500, episode_reward=-298.44 +/- 162.09
Episode length: 299.06 +/- 161.62
Eval num_timesteps=18000, episode_reward=-356.89 +/- 159.03
Episode length: 357.35 +/- 158.55
Eval num_timesteps=18500, episode_reward=-354.74 +/- 163.00
Episode length: 355.19 +/- 162.51
Eval num_timesteps=19000, episode_reward=-265.18 +/- 150.10
Episode length: 265.90 +/- 149.66
Eval num_timesteps=19500, episode_reward=-242.53 +/- 139.16
Episode length: 243.31 +/- 138.75
Eval num_timesteps=20000, episode_reward=-297.96 +/- 160.82
Episode length: 298.58 +/- 160.34
Eval num_timesteps=20500, episode_reward=-263.88 +/- 152.77
Episode length: 264.59 +/- 152.32
Eval num_timesteps=21000, episode_reward=-283.88 +/- 155.50
Episode length: 284.56 +/- 155.06
Eval num_timesteps=21500, episode_reward=-217.96 +/- 121.72
Episode length: 218.82 +/- 121.39
Eval num_timesteps=22000, episode_reward=-265.42 +/- 149.90
Episode length: 266.14 +/- 149.46
Eval num_timesteps=22500, episode_reward=-237.38 +/- 137.68
Episode length: 238.18 +/- 137.30
Eval num_timesteps=23000, episode_reward=-282.37 +/- 155.34
Episode length: 283.05 +/- 154.89
Eval num_timesteps=23500, episode_reward=-310.88 +/- 163.23
Episode length: 311.46 +/- 162.74
Eval num_timesteps=24000, episode_reward=-226.23 +/- 128.17
Episode length: 227.06 +/- 127.81
Eval num_timesteps=24500, episode_reward=-232.75 +/- 130.07
Episode length: 233.57 +/- 129.70
Eval num_timesteps=25000, episode_reward=-282.56 +/- 150.32
Episode length: 283.26 +/- 149.89
Eval num_timesteps=25500, episode_reward=-225.69 +/- 127.53
Episode length: 226.54 +/- 127.21
Eval num_timesteps=26000, episode_reward=-261.54 +/- 147.44
Episode length: 262.27 +/- 147.01
Eval num_timesteps=26500, episode_reward=-230.07 +/- 130.63
Episode length: 230.89 +/- 130.26
Eval num_timesteps=27000, episode_reward=-244.08 +/- 139.11
Episode length: 244.86 +/- 138.71
Eval num_timesteps=27500, episode_reward=-208.07 +/- 104.50
Episode length: 208.97 +/- 104.22
Eval num_timesteps=28000, episode_reward=-196.80 +/- 88.95
Episode length: 197.74 +/- 88.75
Eval num_timesteps=28500, episode_reward=-192.49 +/- 80.05
Episode length: 193.46 +/- 79.93
Eval num_timesteps=29000, episode_reward=-212.59 +/- 102.88
Episode length: 213.50 +/- 102.62
Eval num_timesteps=29500, episode_reward=-191.73 +/- 87.46
Episode length: 192.67 +/- 87.25
Eval num_timesteps=30000, episode_reward=-186.76 +/- 80.61
Episode length: 187.71 +/- 80.41
Eval num_timesteps=30500, episode_reward=-217.71 +/- 118.88
Episode length: 218.57 +/- 118.54
Eval num_timesteps=31000, episode_reward=-201.22 +/- 98.67
Episode length: 202.13 +/- 98.40
Eval num_timesteps=31500, episode_reward=-207.57 +/- 109.11
Episode length: 208.46 +/- 108.82
Eval num_timesteps=32000, episode_reward=-187.00 +/- 79.39
Episode length: 187.96 +/- 79.23
Eval num_timesteps=32500, episode_reward=-199.78 +/- 79.89
Episode length: 200.74 +/- 79.74
Eval num_timesteps=33000, episode_reward=-190.17 +/- 77.20
Episode length: 191.13 +/- 77.04
Eval num_timesteps=33500, episode_reward=-199.78 +/- 90.09
Episode length: 200.71 +/- 89.86
Eval num_timesteps=34000, episode_reward=-192.51 +/- 80.81
Episode length: 193.47 +/- 80.66
Eval num_timesteps=34500, episode_reward=-187.08 +/- 68.49
Episode length: 188.05 +/- 68.35
Eval num_timesteps=35000, episode_reward=-186.71 +/- 61.26
Episode length: 187.69 +/- 61.16
Eval num_timesteps=35500, episode_reward=-175.83 +/- 60.30
Episode length: 176.83 +/- 60.30
Eval num_timesteps=36000, episode_reward=-170.66 +/- 41.19
Episode length: 171.66 +/- 41.19
New best mean reward!
Eval num_timesteps=36500, episode_reward=-169.02 +/- 52.19
Episode length: 170.01 +/- 52.13
New best mean reward!
Eval num_timesteps=37000, episode_reward=-167.24 +/- 61.11
Episode length: 168.22 +/- 61.00
New best mean reward!
Eval num_timesteps=37500, episode_reward=-173.79 +/- 58.77
Episode length: 174.78 +/- 58.71
Eval num_timesteps=38000, episode_reward=-165.77 +/- 36.82
Episode length: 166.77 +/- 36.82
New best mean reward!
Eval num_timesteps=38500, episode_reward=-172.53 +/- 44.31
Episode length: 173.53 +/- 44.31
Eval num_timesteps=39000, episode_reward=-182.44 +/- 70.77
Episode length: 183.42 +/- 70.68
Eval num_timesteps=39500, episode_reward=-172.22 +/- 54.86
Episode length: 173.21 +/- 54.80
Eval num_timesteps=40000, episode_reward=-165.80 +/- 51.22
Episode length: 166.80 +/- 51.22
Eval num_timesteps=40500, episode_reward=-164.17 +/- 40.94
Episode length: 165.17 +/- 40.94
New best mean reward!
Eval num_timesteps=41000, episode_reward=-163.05 +/- 40.61
Episode length: 164.05 +/- 40.61
New best mean reward!
Eval num_timesteps=41500, episode_reward=-173.45 +/- 48.46
Episode length: 174.45 +/- 48.46
Eval num_timesteps=42000, episode_reward=-165.87 +/- 43.83
Episode length: 166.87 +/- 43.83
Eval num_timesteps=42500, episode_reward=-167.08 +/- 43.41
Episode length: 168.08 +/- 43.41
Eval num_timesteps=43000, episode_reward=-162.43 +/- 30.20
Episode length: 163.43 +/- 30.20
New best mean reward!
Eval num_timesteps=43500, episode_reward=-166.19 +/- 41.40
Episode length: 167.19 +/- 41.40
Eval num_timesteps=44000, episode_reward=-171.43 +/- 39.82
Episode length: 172.43 +/- 39.82
Eval num_timesteps=44500, episode_reward=-171.01 +/- 50.44
Episode length: 172.00 +/- 50.38
Eval num_timesteps=45000, episode_reward=-170.46 +/- 50.24
Episode length: 171.46 +/- 50.24
Eval num_timesteps=45500, episode_reward=-176.57 +/- 56.21
Episode length: 177.56 +/- 56.15
Eval num_timesteps=46000, episode_reward=-164.82 +/- 37.88
Episode length: 165.82 +/- 37.88
Eval num_timesteps=46500, episode_reward=-173.68 +/- 52.11
Episode length: 174.68 +/- 52.11
Eval num_timesteps=47000, episode_reward=-167.27 +/- 51.99
Episode length: 168.26 +/- 51.92
Eval num_timesteps=47500, episode_reward=-170.36 +/- 52.24
Episode length: 171.35 +/- 52.18
Eval num_timesteps=48000, episode_reward=-162.47 +/- 35.61
Episode length: 163.47 +/- 35.61
Eval num_timesteps=48500, episode_reward=-168.49 +/- 44.06
Episode length: 169.49 +/- 44.06
Eval num_timesteps=49000, episode_reward=-167.08 +/- 45.80
Episode length: 168.08 +/- 45.80
Eval num_timesteps=49500, episode_reward=-168.76 +/- 41.37
Episode length: 169.76 +/- 41.37
Eval num_timesteps=50000, episode_reward=-169.38 +/- 42.54
Episode length: 170.38 +/- 42.54
FINISHED IN 957.2326283220318 s


starting seed  10235 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-110.72 +/- 53.19
Episode length: 111.71 +/- 53.12
New best mean reward!
Eval num_timesteps=3500, episode_reward=-184.63 +/- 150.64
Episode length: 185.46 +/- 150.29
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-496.23 +/- 37.51
Episode length: 496.24 +/- 37.41
Eval num_timesteps=10000, episode_reward=-488.17 +/- 67.34
Episode length: 488.20 +/- 67.17
Eval num_timesteps=10500, episode_reward=-406.16 +/- 167.49
Episode length: 406.40 +/- 167.07
Eval num_timesteps=11000, episode_reward=-149.00 +/- 132.46
Episode length: 149.89 +/- 132.17
Eval num_timesteps=11500, episode_reward=-152.77 +/- 142.12
Episode length: 153.63 +/- 141.78
Eval num_timesteps=12000, episode_reward=-98.06 +/- 49.03
Episode length: 99.05 +/- 48.95
New best mean reward!
FINISHED IN 248.12471909302985 s


starting seed  10236 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-486.01 +/- 68.80
Episode length: 486.05 +/- 68.61
New best mean reward!
Eval num_timesteps=6500, episode_reward=-137.79 +/- 131.72
Episode length: 138.68 +/- 131.42
New best mean reward!
Eval num_timesteps=7000, episode_reward=-142.50 +/- 140.51
Episode length: 143.38 +/- 140.20
Eval num_timesteps=7500, episode_reward=-106.97 +/- 80.24
Episode length: 107.94 +/- 80.09
New best mean reward!
Eval num_timesteps=8000, episode_reward=-87.68 +/- 25.06
Episode length: 88.68 +/- 25.06
New best mean reward!
FINISHED IN 166.5158600069699 s


starting seed  10237 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-144.94 +/- 27.94
Episode length: 145.94 +/- 27.94
New best mean reward!
Eval num_timesteps=7500, episode_reward=-151.98 +/- 50.33
Episode length: 152.97 +/- 50.26
Eval num_timesteps=8000, episode_reward=-141.76 +/- 49.32
Episode length: 142.75 +/- 49.25
New best mean reward!
Eval num_timesteps=8500, episode_reward=-104.66 +/- 31.83
Episode length: 105.66 +/- 31.83
New best mean reward!
Eval num_timesteps=9000, episode_reward=-115.26 +/- 33.35
Episode length: 116.26 +/- 33.35
Eval num_timesteps=9500, episode_reward=-116.67 +/- 37.01
Episode length: 117.67 +/- 37.01
Eval num_timesteps=10000, episode_reward=-100.42 +/- 51.23
Episode length: 101.42 +/- 51.23
New best mean reward!
Eval num_timesteps=10500, episode_reward=-98.19 +/- 62.13
Episode length: 99.17 +/- 62.00
New best mean reward!
FINISHED IN 241.51987405400723 s


starting seed  10238 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-496.02 +/- 39.60
Episode length: 496.03 +/- 39.50
New best mean reward!
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-414.99 +/- 164.56
Episode length: 415.21 +/- 164.16
New best mean reward!
Eval num_timesteps=9500, episode_reward=-254.86 +/- 201.26
Episode length: 255.46 +/- 200.77
New best mean reward!
Eval num_timesteps=10000, episode_reward=-287.44 +/- 202.24
Episode length: 287.97 +/- 201.74
Eval num_timesteps=10500, episode_reward=-154.44 +/- 152.72
Episode length: 155.28 +/- 152.36
New best mean reward!
Eval num_timesteps=11000, episode_reward=-130.55 +/- 113.58
Episode length: 131.47 +/- 113.32
New best mean reward!
Eval num_timesteps=11500, episode_reward=-103.23 +/- 70.83
Episode length: 104.21 +/- 70.72
New best mean reward!
Eval num_timesteps=12000, episode_reward=-98.93 +/- 60.65
Episode length: 99.92 +/- 60.59
New best mean reward!
FINISHED IN 270.72972237697104 s


starting seed  10239 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-439.13 +/- 118.94
Episode length: 439.34 +/- 118.54
New best mean reward!
Eval num_timesteps=6500, episode_reward=-281.97 +/- 144.95
Episode length: 282.68 +/- 144.51
New best mean reward!
Eval num_timesteps=7000, episode_reward=-189.81 +/- 102.31
Episode length: 190.72 +/- 102.04
New best mean reward!
Eval num_timesteps=7500, episode_reward=-200.10 +/- 127.18
Episode length: 200.97 +/- 126.87
Eval num_timesteps=8000, episode_reward=-331.21 +/- 152.82
Episode length: 331.78 +/- 152.34
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-274.46 +/- 166.39
Episode length: 275.14 +/- 165.95
Eval num_timesteps=9500, episode_reward=-320.97 +/- 165.97
Episode length: 321.52 +/- 165.48
Eval num_timesteps=10000, episode_reward=-205.58 +/- 121.05
Episode length: 206.47 +/- 120.78
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-438.67 +/- 128.55
Episode length: 438.86 +/- 128.16
Eval num_timesteps=11500, episode_reward=-301.02 +/- 183.01
Episode length: 301.59 +/- 182.54
Eval num_timesteps=12000, episode_reward=-238.77 +/- 165.98
Episode length: 239.51 +/- 165.57
Eval num_timesteps=12500, episode_reward=-275.07 +/- 179.31
Episode length: 275.71 +/- 178.86
Eval num_timesteps=13000, episode_reward=-256.29 +/- 180.37
Episode length: 256.94 +/- 179.90
Eval num_timesteps=13500, episode_reward=-139.06 +/- 73.22
Episode length: 140.04 +/- 73.12
New best mean reward!
Eval num_timesteps=14000, episode_reward=-134.31 +/- 98.83
Episode length: 135.25 +/- 98.61
New best mean reward!
Eval num_timesteps=14500, episode_reward=-136.38 +/- 106.11
Episode length: 137.32 +/- 105.91
Eval num_timesteps=15000, episode_reward=-112.14 +/- 75.85
Episode length: 113.11 +/- 75.69
New best mean reward!
Eval num_timesteps=15500, episode_reward=-112.00 +/- 72.95
Episode length: 112.97 +/- 72.79
New best mean reward!
Eval num_timesteps=16000, episode_reward=-107.57 +/- 55.33
Episode length: 108.56 +/- 55.26
New best mean reward!
Eval num_timesteps=16500, episode_reward=-95.12 +/- 20.83
Episode length: 96.12 +/- 20.83
New best mean reward!
FINISHED IN 329.94599483697675 s


starting seed  10240 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-489.39 +/- 40.65
Episode length: 489.46 +/- 40.41
New best mean reward!
Eval num_timesteps=11000, episode_reward=-440.94 +/- 98.74
Episode length: 441.21 +/- 98.31
New best mean reward!
Eval num_timesteps=11500, episode_reward=-444.57 +/- 111.70
Episode length: 444.78 +/- 111.31
Eval num_timesteps=12000, episode_reward=-420.82 +/- 146.10
Episode length: 421.05 +/- 145.68
New best mean reward!
Eval num_timesteps=12500, episode_reward=-357.81 +/- 173.08
Episode length: 358.22 +/- 172.59
New best mean reward!
Eval num_timesteps=13000, episode_reward=-381.97 +/- 168.73
Episode length: 382.30 +/- 168.26
Eval num_timesteps=13500, episode_reward=-388.27 +/- 171.44
Episode length: 388.57 +/- 170.99
Eval num_timesteps=14000, episode_reward=-440.21 +/- 137.21
Episode length: 440.37 +/- 136.84
Eval num_timesteps=14500, episode_reward=-328.88 +/- 194.65
Episode length: 329.32 +/- 194.16
New best mean reward!
Eval num_timesteps=15000, episode_reward=-376.73 +/- 180.34
Episode length: 377.05 +/- 179.88
Eval num_timesteps=15500, episode_reward=-297.42 +/- 196.31
Episode length: 297.94 +/- 195.81
New best mean reward!
Eval num_timesteps=16000, episode_reward=-274.92 +/- 193.23
Episode length: 275.50 +/- 192.74
New best mean reward!
Eval num_timesteps=16500, episode_reward=-309.58 +/- 200.94
Episode length: 310.06 +/- 200.45
Eval num_timesteps=17000, episode_reward=-302.83 +/- 198.07
Episode length: 303.33 +/- 197.57
Eval num_timesteps=17500, episode_reward=-252.65 +/- 191.07
Episode length: 253.28 +/- 190.59
New best mean reward!
Eval num_timesteps=18000, episode_reward=-268.65 +/- 197.35
Episode length: 269.23 +/- 196.86
Eval num_timesteps=18500, episode_reward=-251.35 +/- 192.13
Episode length: 251.98 +/- 191.66
New best mean reward!
Eval num_timesteps=19000, episode_reward=-190.63 +/- 162.26
Episode length: 191.42 +/- 161.86
New best mean reward!
Eval num_timesteps=19500, episode_reward=-189.59 +/- 163.71
Episode length: 190.38 +/- 163.31
New best mean reward!
Eval num_timesteps=20000, episode_reward=-188.68 +/- 162.57
Episode length: 189.47 +/- 162.17
New best mean reward!
Eval num_timesteps=20500, episode_reward=-172.59 +/- 155.05
Episode length: 173.41 +/- 154.67
New best mean reward!
Eval num_timesteps=21000, episode_reward=-186.07 +/- 157.43
Episode length: 186.89 +/- 157.07
Eval num_timesteps=21500, episode_reward=-188.67 +/- 163.33
Episode length: 189.46 +/- 162.93
Eval num_timesteps=22000, episode_reward=-189.87 +/- 165.88
Episode length: 190.66 +/- 165.49
Eval num_timesteps=22500, episode_reward=-168.41 +/- 150.29
Episode length: 169.25 +/- 149.94
New best mean reward!
Eval num_timesteps=23000, episode_reward=-173.50 +/- 154.34
Episode length: 174.32 +/- 153.95
Eval num_timesteps=23500, episode_reward=-173.36 +/- 146.33
Episode length: 174.20 +/- 145.97
Eval num_timesteps=24000, episode_reward=-179.46 +/- 152.84
Episode length: 180.29 +/- 152.49
Eval num_timesteps=24500, episode_reward=-115.74 +/- 73.45
Episode length: 116.71 +/- 73.29
New best mean reward!
Eval num_timesteps=25000, episode_reward=-123.14 +/- 106.34
Episode length: 124.08 +/- 106.12
Eval num_timesteps=25500, episode_reward=-127.54 +/- 106.90
Episode length: 128.47 +/- 106.66
Eval num_timesteps=26000, episode_reward=-102.92 +/- 34.19
Episode length: 103.92 +/- 34.19
New best mean reward!
Eval num_timesteps=26500, episode_reward=-123.07 +/- 96.37
Episode length: 124.03 +/- 96.22
Eval num_timesteps=27000, episode_reward=-103.64 +/- 61.08
Episode length: 104.63 +/- 61.02
Eval num_timesteps=27500, episode_reward=-113.12 +/- 71.89
Episode length: 114.09 +/- 71.73
Eval num_timesteps=28000, episode_reward=-103.54 +/- 50.24
Episode length: 104.53 +/- 50.16
Eval num_timesteps=28500, episode_reward=-95.95 +/- 28.06
Episode length: 96.95 +/- 28.06
New best mean reward!
FINISHED IN 558.3300739100087 s


starting seed  10241 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-336.90 +/- 196.81
Episode length: 337.31 +/- 196.32
New best mean reward!
Eval num_timesteps=8000, episode_reward=-454.32 +/- 129.97
Episode length: 454.43 +/- 129.65
Eval num_timesteps=8500, episode_reward=-305.07 +/- 196.48
Episode length: 305.58 +/- 195.99
New best mean reward!
Eval num_timesteps=9000, episode_reward=-102.87 +/- 74.27
Episode length: 103.84 +/- 74.11
New best mean reward!
Eval num_timesteps=9500, episode_reward=-80.61 +/- 16.12
Episode length: 81.61 +/- 16.12
New best mean reward!
FINISHED IN 287.1411849600263 s


starting seed  10242 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-250.12 +/- 198.53
Episode length: 250.74 +/- 198.05
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-278.75 +/- 151.47
Episode length: 279.46 +/- 151.05
Eval num_timesteps=9500, episode_reward=-216.94 +/- 131.85
Episode length: 217.77 +/- 131.49
New best mean reward!
Eval num_timesteps=10000, episode_reward=-429.93 +/- 127.86
Episode length: 430.17 +/- 127.44
Eval num_timesteps=10500, episode_reward=-216.70 +/- 163.93
Episode length: 217.46 +/- 163.51
New best mean reward!
Eval num_timesteps=11000, episode_reward=-213.04 +/- 176.73
Episode length: 213.77 +/- 176.29
New best mean reward!
Eval num_timesteps=11500, episode_reward=-135.10 +/- 45.20
Episode length: 136.09 +/- 45.12
New best mean reward!
Eval num_timesteps=12000, episode_reward=-171.90 +/- 72.03
Episode length: 172.87 +/- 71.89
Eval num_timesteps=12500, episode_reward=-151.02 +/- 47.67
Episode length: 152.01 +/- 47.60
Eval num_timesteps=13000, episode_reward=-170.40 +/- 62.46
Episode length: 171.38 +/- 62.36
Eval num_timesteps=13500, episode_reward=-159.37 +/- 45.82
Episode length: 160.36 +/- 45.75
Eval num_timesteps=14000, episode_reward=-166.86 +/- 64.15
Episode length: 167.84 +/- 64.05
Eval num_timesteps=14500, episode_reward=-150.39 +/- 21.99
Episode length: 151.39 +/- 21.99
Eval num_timesteps=15000, episode_reward=-195.62 +/- 117.23
Episode length: 196.50 +/- 116.91
Eval num_timesteps=15500, episode_reward=-131.22 +/- 53.21
Episode length: 132.21 +/- 53.14
New best mean reward!
Eval num_timesteps=16000, episode_reward=-190.98 +/- 113.15
Episode length: 191.88 +/- 112.88
Eval num_timesteps=16500, episode_reward=-185.36 +/- 133.20
Episode length: 186.22 +/- 132.87
Eval num_timesteps=17000, episode_reward=-161.76 +/- 92.68
Episode length: 162.70 +/- 92.46
Eval num_timesteps=17500, episode_reward=-173.16 +/- 89.58
Episode length: 174.10 +/- 89.36
Eval num_timesteps=18000, episode_reward=-126.60 +/- 26.93
Episode length: 127.60 +/- 26.93
New best mean reward!
Eval num_timesteps=18500, episode_reward=-127.71 +/- 25.71
Episode length: 128.71 +/- 25.71
Eval num_timesteps=19000, episode_reward=-133.17 +/- 31.17
Episode length: 134.17 +/- 31.17
Eval num_timesteps=19500, episode_reward=-137.00 +/- 32.28
Episode length: 138.00 +/- 32.28
Eval num_timesteps=20000, episode_reward=-130.67 +/- 43.96
Episode length: 131.66 +/- 43.88
Eval num_timesteps=20500, episode_reward=-114.71 +/- 48.57
Episode length: 115.70 +/- 48.49
New best mean reward!
Eval num_timesteps=21000, episode_reward=-107.44 +/- 22.99
Episode length: 108.44 +/- 22.99
New best mean reward!
Eval num_timesteps=21500, episode_reward=-104.69 +/- 47.25
Episode length: 105.68 +/- 47.16
New best mean reward!
Eval num_timesteps=22000, episode_reward=-83.62 +/- 22.40
Episode length: 84.62 +/- 22.40
New best mean reward!
FINISHED IN 373.3971299609984 s


starting seed  10243 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-496.25 +/- 37.31
Episode length: 496.26 +/- 37.21
New best mean reward!
Eval num_timesteps=13000, episode_reward=-431.61 +/- 151.36
Episode length: 431.78 +/- 150.99
New best mean reward!
Eval num_timesteps=13500, episode_reward=-413.78 +/- 162.85
Episode length: 414.00 +/- 162.44
New best mean reward!
Eval num_timesteps=14000, episode_reward=-318.69 +/- 205.28
Episode length: 319.13 +/- 204.79
New best mean reward!
Eval num_timesteps=14500, episode_reward=-278.80 +/- 201.45
Episode length: 279.36 +/- 200.96
New best mean reward!
Eval num_timesteps=15000, episode_reward=-266.77 +/- 204.37
Episode length: 267.34 +/- 203.87
New best mean reward!
Eval num_timesteps=15500, episode_reward=-236.03 +/- 197.32
Episode length: 236.68 +/- 196.85
New best mean reward!
Eval num_timesteps=16000, episode_reward=-234.39 +/- 200.14
Episode length: 235.03 +/- 199.66
New best mean reward!
Eval num_timesteps=16500, episode_reward=-173.35 +/- 159.05
Episode length: 174.17 +/- 158.68
New best mean reward!
Eval num_timesteps=17000, episode_reward=-174.32 +/- 158.49
Episode length: 175.14 +/- 158.12
Eval num_timesteps=17500, episode_reward=-190.61 +/- 172.33
Episode length: 191.38 +/- 171.92
Eval num_timesteps=18000, episode_reward=-160.16 +/- 142.58
Episode length: 161.03 +/- 142.27
New best mean reward!
Eval num_timesteps=18500, episode_reward=-135.83 +/- 126.43
Episode length: 136.73 +/- 126.14
New best mean reward!
Eval num_timesteps=19000, episode_reward=-133.91 +/- 120.19
Episode length: 134.82 +/- 119.92
New best mean reward!
Eval num_timesteps=19500, episode_reward=-131.28 +/- 120.31
Episode length: 132.19 +/- 120.03
New best mean reward!
Eval num_timesteps=20000, episode_reward=-143.57 +/- 131.63
Episode length: 144.46 +/- 131.34
Eval num_timesteps=20500, episode_reward=-128.09 +/- 111.49
Episode length: 129.02 +/- 111.26
New best mean reward!
Eval num_timesteps=21000, episode_reward=-143.68 +/- 139.46
Episode length: 144.55 +/- 139.12
Eval num_timesteps=21500, episode_reward=-132.91 +/- 120.16
Episode length: 133.82 +/- 119.88
Eval num_timesteps=22000, episode_reward=-114.41 +/- 89.99
Episode length: 115.37 +/- 89.81
New best mean reward!
Eval num_timesteps=22500, episode_reward=-111.25 +/- 86.57
Episode length: 112.21 +/- 86.39
New best mean reward!
Eval num_timesteps=23000, episode_reward=-97.80 +/- 62.53
Episode length: 98.78 +/- 62.40
New best mean reward!
FINISHED IN 483.18124580202857 s


starting seed  10244 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-121.52 +/- 85.22
Episode length: 122.48 +/- 85.04
New best mean reward!
Eval num_timesteps=9500, episode_reward=-121.89 +/- 96.19
Episode length: 122.84 +/- 95.99
Eval num_timesteps=10000, episode_reward=-90.50 +/- 36.69
Episode length: 91.50 +/- 36.69
New best mean reward!
FINISHED IN 222.69653947098413 s


starting seed  10245 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-85.05 +/- 16.31
Episode length: 86.05 +/- 16.31
New best mean reward!
FINISHED IN 91.84005961596267 s


starting seed  10246 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-114.77 +/- 97.62
Episode length: 115.72 +/- 97.42
New best mean reward!
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-308.69 +/- 197.22
Episode length: 309.18 +/- 196.72
Eval num_timesteps=2500, episode_reward=-99.09 +/- 51.09
Episode length: 100.08 +/- 51.01
New best mean reward!
FINISHED IN 46.10097497300012 s


starting seed  10247 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-319.66 +/- 142.09
Episode length: 320.32 +/- 141.66
New best mean reward!
Eval num_timesteps=8500, episode_reward=-392.25 +/- 138.92
Episode length: 392.67 +/- 138.47
Eval num_timesteps=9000, episode_reward=-444.06 +/- 100.96
Episode length: 444.38 +/- 100.59
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-148.31 +/- 119.42
Episode length: 149.22 +/- 119.15
New best mean reward!
Eval num_timesteps=11000, episode_reward=-131.87 +/- 96.96
Episode length: 132.81 +/- 96.73
New best mean reward!
Eval num_timesteps=11500, episode_reward=-123.76 +/- 95.13
Episode length: 124.72 +/- 94.97
New best mean reward!
Eval num_timesteps=12000, episode_reward=-114.03 +/- 67.06
Episode length: 115.01 +/- 66.94
New best mean reward!
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-409.57 +/- 155.75
Episode length: 409.83 +/- 155.32
Eval num_timesteps=13500, episode_reward=-261.81 +/- 172.09
Episode length: 262.48 +/- 171.64
Eval num_timesteps=14000, episode_reward=-116.70 +/- 74.26
Episode length: 117.67 +/- 74.10
Eval num_timesteps=14500, episode_reward=-102.72 +/- 41.48
Episode length: 103.72 +/- 41.48
New best mean reward!
Eval num_timesteps=15000, episode_reward=-88.83 +/- 19.05
Episode length: 89.83 +/- 19.05
New best mean reward!
FINISHED IN 348.7128690680256 s


starting seed  10248 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-403.40 +/- 148.60
Episode length: 403.70 +/- 148.14
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-261.57 +/- 130.54
Episode length: 262.42 +/- 130.27
New best mean reward!
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-498.88 +/- 6.43
Episode length: 498.92 +/- 6.27
Eval num_timesteps=10500, episode_reward=-186.63 +/- 52.84
Episode length: 187.62 +/- 52.78
New best mean reward!
Eval num_timesteps=11000, episode_reward=-251.37 +/- 144.01
Episode length: 252.13 +/- 143.59
Eval num_timesteps=11500, episode_reward=-224.09 +/- 125.06
Episode length: 224.93 +/- 124.71
Eval num_timesteps=12000, episode_reward=-214.52 +/- 114.74
Episode length: 215.40 +/- 114.44
Eval num_timesteps=12500, episode_reward=-299.40 +/- 165.96
Episode length: 300.00 +/- 165.48
Eval num_timesteps=13000, episode_reward=-405.67 +/- 151.85
Episode length: 405.95 +/- 151.40
Eval num_timesteps=13500, episode_reward=-483.33 +/- 72.80
Episode length: 483.38 +/- 72.59
Eval num_timesteps=14000, episode_reward=-414.34 +/- 149.14
Episode length: 414.59 +/- 148.71
Eval num_timesteps=14500, episode_reward=-107.61 +/- 32.92
Episode length: 108.61 +/- 32.92
New best mean reward!
Eval num_timesteps=15000, episode_reward=-85.39 +/- 24.55
Episode length: 86.39 +/- 24.55
New best mean reward!
FINISHED IN 325.10515696700895 s


starting seed  10249 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-318.70 +/- 171.51
Episode length: 319.23 +/- 171.02
New best mean reward!
Eval num_timesteps=9000, episode_reward=-341.45 +/- 173.07
Episode length: 341.91 +/- 172.58
Eval num_timesteps=9500, episode_reward=-213.27 +/- 129.10
Episode length: 214.11 +/- 128.75
New best mean reward!
Eval num_timesteps=10000, episode_reward=-158.60 +/- 28.97
Episode length: 159.60 +/- 28.97
New best mean reward!
Eval num_timesteps=10500, episode_reward=-155.63 +/- 28.27
Episode length: 156.63 +/- 28.27
New best mean reward!
Eval num_timesteps=11000, episode_reward=-158.01 +/- 43.03
Episode length: 159.01 +/- 43.03
Eval num_timesteps=11500, episode_reward=-164.72 +/- 40.13
Episode length: 165.72 +/- 40.13
Eval num_timesteps=12000, episode_reward=-165.32 +/- 49.80
Episode length: 166.31 +/- 49.73
Eval num_timesteps=12500, episode_reward=-153.47 +/- 30.92
Episode length: 154.47 +/- 30.92
New best mean reward!
Eval num_timesteps=13000, episode_reward=-159.95 +/- 31.86
Episode length: 160.95 +/- 31.86
Eval num_timesteps=13500, episode_reward=-159.40 +/- 30.32
Episode length: 160.40 +/- 30.32
Eval num_timesteps=14000, episode_reward=-163.30 +/- 42.77
Episode length: 164.30 +/- 42.77
Eval num_timesteps=14500, episode_reward=-104.48 +/- 19.94
Episode length: 105.48 +/- 19.94
New best mean reward!
Eval num_timesteps=15000, episode_reward=-113.34 +/- 59.48
Episode length: 114.32 +/- 59.35
Eval num_timesteps=15500, episode_reward=-100.18 +/- 31.82
Episode length: 101.18 +/- 31.82
New best mean reward!
Eval num_timesteps=16000, episode_reward=-86.43 +/- 44.36
Episode length: 87.42 +/- 44.27
New best mean reward!
FINISHED IN 294.1273461970268 s


starting seed  10250 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-492.27 +/- 54.11
Episode length: 492.29 +/- 53.97
New best mean reward!
Eval num_timesteps=10000, episode_reward=-160.10 +/- 140.88
Episode length: 160.96 +/- 140.54
New best mean reward!
Eval num_timesteps=10500, episode_reward=-121.01 +/- 96.94
Episode length: 121.96 +/- 96.75
New best mean reward!
Eval num_timesteps=11000, episode_reward=-118.39 +/- 95.45
Episode length: 119.34 +/- 95.25
New best mean reward!
Eval num_timesteps=11500, episode_reward=-100.38 +/- 48.27
Episode length: 101.37 +/- 48.18
New best mean reward!
Eval num_timesteps=12000, episode_reward=-96.05 +/- 45.12
Episode length: 97.04 +/- 45.03
New best mean reward!
FINISHED IN 261.00480907200836 s


starting seed  10251 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-478.73 +/- 77.67
Episode length: 478.80 +/- 77.42
New best mean reward!
Eval num_timesteps=9000, episode_reward=-347.32 +/- 189.64
Episode length: 347.72 +/- 189.16
New best mean reward!
Eval num_timesteps=9500, episode_reward=-86.34 +/- 25.59
Episode length: 87.34 +/- 25.59
New best mean reward!
FINISHED IN 225.8144778549904 s


starting seed  10252 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-479.93 +/- 87.58
Episode length: 479.98 +/- 87.36
New best mean reward!
Eval num_timesteps=14500, episode_reward=-492.06 +/- 55.59
Episode length: 492.08 +/- 55.45
Eval num_timesteps=15000, episode_reward=-421.79 +/- 161.80
Episode length: 421.98 +/- 161.41
New best mean reward!
Eval num_timesteps=15500, episode_reward=-425.49 +/- 159.23
Episode length: 425.67 +/- 158.85
Eval num_timesteps=16000, episode_reward=-332.80 +/- 201.71
Episode length: 333.21 +/- 201.22
New best mean reward!
Eval num_timesteps=16500, episode_reward=-275.45 +/- 202.60
Episode length: 276.01 +/- 202.11
New best mean reward!
Eval num_timesteps=17000, episode_reward=-369.71 +/- 190.17
Episode length: 370.03 +/- 189.70
Eval num_timesteps=17500, episode_reward=-314.92 +/- 204.91
Episode length: 315.37 +/- 204.42
Eval num_timesteps=18000, episode_reward=-329.42 +/- 202.01
Episode length: 329.84 +/- 201.52
Eval num_timesteps=18500, episode_reward=-153.01 +/- 147.04
Episode length: 153.86 +/- 146.68
New best mean reward!
Eval num_timesteps=19000, episode_reward=-186.33 +/- 172.77
Episode length: 187.10 +/- 172.35
Eval num_timesteps=19500, episode_reward=-166.72 +/- 162.37
Episode length: 167.53 +/- 161.98
Eval num_timesteps=20000, episode_reward=-178.13 +/- 167.02
Episode length: 178.92 +/- 166.62
Eval num_timesteps=20500, episode_reward=-195.19 +/- 179.87
Episode length: 195.95 +/- 179.47
Eval num_timesteps=21000, episode_reward=-226.00 +/- 193.73
Episode length: 226.67 +/- 193.26
Eval num_timesteps=21500, episode_reward=-199.00 +/- 177.04
Episode length: 199.76 +/- 176.63
Eval num_timesteps=22000, episode_reward=-163.49 +/- 151.90
Episode length: 164.34 +/- 151.57
Eval num_timesteps=22500, episode_reward=-151.89 +/- 143.60
Episode length: 152.75 +/- 143.26
New best mean reward!
Eval num_timesteps=23000, episode_reward=-188.31 +/- 173.98
Episode length: 189.08 +/- 173.57
Eval num_timesteps=23500, episode_reward=-138.87 +/- 133.14
Episode length: 139.76 +/- 132.84
New best mean reward!
Eval num_timesteps=24000, episode_reward=-140.87 +/- 127.64
Episode length: 141.77 +/- 127.36
Eval num_timesteps=24500, episode_reward=-163.28 +/- 156.12
Episode length: 164.11 +/- 155.75
Eval num_timesteps=25000, episode_reward=-134.43 +/- 121.98
Episode length: 135.34 +/- 121.71
New best mean reward!
Eval num_timesteps=25500, episode_reward=-142.94 +/- 129.19
Episode length: 143.83 +/- 128.88
Eval num_timesteps=26000, episode_reward=-102.94 +/- 77.44
Episode length: 103.91 +/- 77.28
New best mean reward!
Eval num_timesteps=26500, episode_reward=-93.53 +/- 50.76
Episode length: 94.52 +/- 50.68
New best mean reward!
FINISHED IN 527.1520978990011 s


starting seed  10253 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-139.29 +/- 107.24
Episode length: 140.22 +/- 107.01
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-99.32 +/- 23.48
Episode length: 100.32 +/- 23.48
New best mean reward!
FINISHED IN 156.40718986699358 s


starting seed  10254 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-251.33 +/- 189.01
Episode length: 251.97 +/- 188.54
New best mean reward!
Eval num_timesteps=13000, episode_reward=-153.16 +/- 120.55
Episode length: 154.06 +/- 120.27
New best mean reward!
Eval num_timesteps=13500, episode_reward=-308.81 +/- 191.05
Episode length: 309.32 +/- 190.56
Eval num_timesteps=14000, episode_reward=-101.86 +/- 34.13
Episode length: 102.86 +/- 34.13
New best mean reward!
Eval num_timesteps=14500, episode_reward=-96.79 +/- 22.85
Episode length: 97.79 +/- 22.85
New best mean reward!
FINISHED IN 333.94765342300525 s


starting seed  10255 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-123.92 +/- 106.64
Episode length: 124.85 +/- 106.39
New best mean reward!
Eval num_timesteps=9500, episode_reward=-113.71 +/- 99.15
Episode length: 114.66 +/- 98.96
New best mean reward!
Eval num_timesteps=10000, episode_reward=-92.10 +/- 21.82
Episode length: 93.10 +/- 21.82
New best mean reward!
FINISHED IN 249.23023644898785 s


starting seed  10256 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-398.92 +/- 148.57
Episode length: 399.24 +/- 148.10
New best mean reward!
Eval num_timesteps=1500, episode_reward=-184.56 +/- 62.22
Episode length: 185.53 +/- 62.06
New best mean reward!
Eval num_timesteps=2000, episode_reward=-306.48 +/- 167.94
Episode length: 307.06 +/- 167.45
Eval num_timesteps=2500, episode_reward=-438.07 +/- 128.56
Episode length: 438.26 +/- 128.17
Eval num_timesteps=3000, episode_reward=-434.63 +/- 125.24
Episode length: 434.85 +/- 124.83
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-493.61 +/- 45.06
Episode length: 493.63 +/- 44.92
Eval num_timesteps=9500, episode_reward=-410.58 +/- 145.45
Episode length: 410.86 +/- 145.00
Eval num_timesteps=10000, episode_reward=-341.31 +/- 167.72
Episode length: 341.79 +/- 167.23
Eval num_timesteps=10500, episode_reward=-289.65 +/- 155.95
Episode length: 290.31 +/- 155.50
Eval num_timesteps=11000, episode_reward=-238.69 +/- 141.80
Episode length: 239.47 +/- 141.40
Eval num_timesteps=11500, episode_reward=-211.97 +/- 119.56
Episode length: 212.83 +/- 119.23
Eval num_timesteps=12000, episode_reward=-201.31 +/- 95.71
Episode length: 202.24 +/- 95.49
Eval num_timesteps=12500, episode_reward=-217.04 +/- 124.48
Episode length: 217.89 +/- 124.14
Eval num_timesteps=13000, episode_reward=-196.83 +/- 96.03
Episode length: 197.75 +/- 95.78
Eval num_timesteps=13500, episode_reward=-226.88 +/- 128.31
Episode length: 227.71 +/- 127.94
Eval num_timesteps=14000, episode_reward=-216.42 +/- 122.67
Episode length: 217.27 +/- 122.32
Eval num_timesteps=14500, episode_reward=-182.91 +/- 78.82
Episode length: 183.86 +/- 78.62
New best mean reward!
Eval num_timesteps=15000, episode_reward=-197.56 +/- 99.07
Episode length: 198.47 +/- 98.80
Eval num_timesteps=15500, episode_reward=-195.40 +/- 100.44
Episode length: 196.31 +/- 100.17
Eval num_timesteps=16000, episode_reward=-201.61 +/- 110.06
Episode length: 202.50 +/- 109.76
Eval num_timesteps=16500, episode_reward=-255.37 +/- 149.93
Episode length: 256.11 +/- 149.51
Eval num_timesteps=17000, episode_reward=-301.52 +/- 166.89
Episode length: 302.12 +/- 166.42
Eval num_timesteps=17500, episode_reward=-330.61 +/- 166.31
Episode length: 331.13 +/- 165.82
Eval num_timesteps=18000, episode_reward=-177.30 +/- 57.52
Episode length: 178.28 +/- 57.41
New best mean reward!
Eval num_timesteps=18500, episode_reward=-180.12 +/- 72.28
Episode length: 181.08 +/- 72.10
Eval num_timesteps=19000, episode_reward=-205.30 +/- 101.68
Episode length: 206.21 +/- 101.42
Eval num_timesteps=19500, episode_reward=-212.61 +/- 122.84
Episode length: 213.46 +/- 122.49
Eval num_timesteps=20000, episode_reward=-252.78 +/- 148.24
Episode length: 253.54 +/- 147.84
Eval num_timesteps=20500, episode_reward=-222.10 +/- 123.78
Episode length: 222.95 +/- 123.45
Eval num_timesteps=21000, episode_reward=-202.83 +/- 104.43
Episode length: 203.74 +/- 104.18
Eval num_timesteps=21500, episode_reward=-178.22 +/- 64.85
Episode length: 179.19 +/- 64.70
Eval num_timesteps=22000, episode_reward=-127.12 +/- 33.40
Episode length: 128.12 +/- 33.40
New best mean reward!
Eval num_timesteps=22500, episode_reward=-168.62 +/- 40.63
Episode length: 169.62 +/- 40.63
Eval num_timesteps=23000, episode_reward=-163.91 +/- 37.13
Episode length: 164.91 +/- 37.13
Eval num_timesteps=23500, episode_reward=-119.73 +/- 40.99
Episode length: 120.73 +/- 40.99
New best mean reward!
Eval num_timesteps=24000, episode_reward=-167.14 +/- 43.15
Episode length: 168.14 +/- 43.15
Eval num_timesteps=24500, episode_reward=-166.18 +/- 33.29
Episode length: 167.18 +/- 33.29
Eval num_timesteps=25000, episode_reward=-122.17 +/- 28.09
Episode length: 123.17 +/- 28.09
Eval num_timesteps=25500, episode_reward=-119.93 +/- 37.24
Episode length: 120.93 +/- 37.24
Eval num_timesteps=26000, episode_reward=-117.93 +/- 16.47
Episode length: 118.93 +/- 16.47
New best mean reward!
Eval num_timesteps=26500, episode_reward=-118.40 +/- 44.42
Episode length: 119.40 +/- 44.42
Eval num_timesteps=27000, episode_reward=-116.01 +/- 19.82
Episode length: 117.01 +/- 19.82
New best mean reward!
Eval num_timesteps=27500, episode_reward=-122.00 +/- 23.45
Episode length: 123.00 +/- 23.45
Eval num_timesteps=28000, episode_reward=-148.81 +/- 35.66
Episode length: 149.81 +/- 35.66
Eval num_timesteps=28500, episode_reward=-120.40 +/- 25.09
Episode length: 121.40 +/- 25.09
Eval num_timesteps=29000, episode_reward=-124.52 +/- 28.86
Episode length: 125.52 +/- 28.86
Eval num_timesteps=29500, episode_reward=-105.58 +/- 24.94
Episode length: 106.58 +/- 24.94
New best mean reward!
Eval num_timesteps=30000, episode_reward=-101.85 +/- 41.51
Episode length: 102.85 +/- 41.51
New best mean reward!
Eval num_timesteps=30500, episode_reward=-90.50 +/- 25.46
Episode length: 91.50 +/- 25.46
New best mean reward!
FINISHED IN 497.3316748199868 s


starting seed  10257 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-429.33 +/- 151.53
Episode length: 429.51 +/- 151.15
New best mean reward!
Eval num_timesteps=7500, episode_reward=-362.45 +/- 185.30
Episode length: 362.81 +/- 184.83
New best mean reward!
Eval num_timesteps=8000, episode_reward=-98.50 +/- 28.05
Episode length: 99.50 +/- 28.05
New best mean reward!
FINISHED IN 224.13385841896525 s


starting seed  10258 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-300.86 +/- 151.78
Episode length: 301.51 +/- 151.32
New best mean reward!
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-81.91 +/- 19.95
Episode length: 82.91 +/- 19.95
New best mean reward!
FINISHED IN 250.13005360099487 s


starting seed  10259 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-239.28 +/- 152.77
Episode length: 240.04 +/- 152.36
New best mean reward!
Eval num_timesteps=8000, episode_reward=-117.74 +/- 80.13
Episode length: 118.70 +/- 79.94
New best mean reward!
Eval num_timesteps=8500, episode_reward=-103.94 +/- 63.30
Episode length: 104.92 +/- 63.17
New best mean reward!
Eval num_timesteps=9000, episode_reward=-278.35 +/- 185.34
Episode length: 278.95 +/- 184.86
Eval num_timesteps=9500, episode_reward=-107.48 +/- 63.34
Episode length: 108.46 +/- 63.21
Eval num_timesteps=10000, episode_reward=-96.51 +/- 36.36
Episode length: 97.51 +/- 36.36
New best mean reward!
FINISHED IN 193.3864825119963 s


starting seed  10260 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-244.26 +/- 125.54
Episode length: 245.08 +/- 125.17
New best mean reward!
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-425.16 +/- 146.15
Episode length: 425.37 +/- 145.74
Eval num_timesteps=7500, episode_reward=-91.51 +/- 22.34
Episode length: 92.51 +/- 22.34
New best mean reward!
FINISHED IN 179.88271246297518 s


starting seed  10261 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-102.05 +/- 57.46
Episode length: 103.04 +/- 57.40
New best mean reward!
Eval num_timesteps=7500, episode_reward=-108.71 +/- 56.31
Episode length: 109.70 +/- 56.24
Eval num_timesteps=8000, episode_reward=-132.05 +/- 89.34
Episode length: 133.01 +/- 89.17
Eval num_timesteps=8500, episode_reward=-421.57 +/- 150.58
Episode length: 421.79 +/- 150.18
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-291.79 +/- 197.02
Episode length: 292.32 +/- 196.52
Eval num_timesteps=12000, episode_reward=-322.10 +/- 194.55
Episode length: 322.56 +/- 194.06
Eval num_timesteps=12500, episode_reward=-227.25 +/- 184.53
Episode length: 227.94 +/- 184.07
Eval num_timesteps=13000, episode_reward=-214.18 +/- 177.43
Episode length: 214.91 +/- 177.00
Eval num_timesteps=13500, episode_reward=-158.63 +/- 147.37
Episode length: 159.48 +/- 147.02
Eval num_timesteps=14000, episode_reward=-138.09 +/- 129.12
Episode length: 138.98 +/- 128.81
Eval num_timesteps=14500, episode_reward=-139.00 +/- 127.89
Episode length: 139.89 +/- 127.58
Eval num_timesteps=15000, episode_reward=-137.85 +/- 129.57
Episode length: 138.74 +/- 129.26
Eval num_timesteps=15500, episode_reward=-117.75 +/- 92.41
Episode length: 118.71 +/- 92.25
Eval num_timesteps=16000, episode_reward=-100.00 +/- 49.98
Episode length: 100.99 +/- 49.90
New best mean reward!
FINISHED IN 349.8906648270204 s


starting seed  10262 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-397.26 +/- 169.23
Episode length: 397.53 +/- 168.79
New best mean reward!
Eval num_timesteps=4000, episode_reward=-112.31 +/- 99.82
Episode length: 113.25 +/- 99.58
New best mean reward!
Eval num_timesteps=4500, episode_reward=-289.76 +/- 206.57
Episode length: 290.27 +/- 206.07
Eval num_timesteps=5000, episode_reward=-217.45 +/- 186.59
Episode length: 218.15 +/- 186.14
Eval num_timesteps=5500, episode_reward=-93.52 +/- 46.26
Episode length: 94.52 +/- 46.26
New best mean reward!
FINISHED IN 127.02748345700093 s


starting seed  10263 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-120.32 +/- 52.96
Episode length: 121.31 +/- 52.88
New best mean reward!
Eval num_timesteps=6500, episode_reward=-142.65 +/- 66.07
Episode length: 143.63 +/- 65.96
Eval num_timesteps=7000, episode_reward=-104.95 +/- 49.72
Episode length: 105.95 +/- 49.72
New best mean reward!
Eval num_timesteps=7500, episode_reward=-98.03 +/- 36.41
Episode length: 99.03 +/- 36.41
New best mean reward!
FINISHED IN 158.02620247297455 s


starting seed  10264 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-396.63 +/- 170.30
Episode length: 396.90 +/- 169.86
New best mean reward!
Eval num_timesteps=14000, episode_reward=-204.10 +/- 179.95
Episode length: 204.84 +/- 179.52
New best mean reward!
Eval num_timesteps=14500, episode_reward=-198.34 +/- 182.60
Episode length: 199.09 +/- 182.19
New best mean reward!
Eval num_timesteps=15000, episode_reward=-190.39 +/- 171.96
Episode length: 191.16 +/- 171.55
New best mean reward!
Eval num_timesteps=15500, episode_reward=-134.97 +/- 132.93
Episode length: 135.86 +/- 132.62
New best mean reward!
Eval num_timesteps=16000, episode_reward=-145.48 +/- 139.01
Episode length: 146.35 +/- 138.68
Eval num_timesteps=16500, episode_reward=-124.41 +/- 109.24
Episode length: 125.34 +/- 109.00
New best mean reward!
Eval num_timesteps=17000, episode_reward=-127.26 +/- 114.44
Episode length: 128.18 +/- 114.18
Eval num_timesteps=17500, episode_reward=-119.74 +/- 109.66
Episode length: 120.67 +/- 109.42
New best mean reward!
Eval num_timesteps=18000, episode_reward=-131.42 +/- 126.89
Episode length: 132.32 +/- 126.60
Eval num_timesteps=18500, episode_reward=-97.29 +/- 62.58
Episode length: 98.27 +/- 62.45
New best mean reward!
FINISHED IN 404.6162413510028 s


starting seed  10265 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-90.40 +/- 24.06
Episode length: 91.40 +/- 24.06
New best mean reward!
FINISHED IN 257.5151382050244 s


starting seed  10266 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-481.67 +/- 72.84
Episode length: 481.73 +/- 72.60
New best mean reward!
Eval num_timesteps=2500, episode_reward=-265.25 +/- 139.42
Episode length: 266.00 +/- 138.99
New best mean reward!
Eval num_timesteps=3000, episode_reward=-199.53 +/- 89.71
Episode length: 200.47 +/- 89.51
New best mean reward!
Eval num_timesteps=3500, episode_reward=-447.20 +/- 117.51
Episode length: 447.38 +/- 117.14
Eval num_timesteps=4000, episode_reward=-271.52 +/- 195.34
Episode length: 272.10 +/- 194.84
Eval num_timesteps=4500, episode_reward=-117.00 +/- 94.28
Episode length: 117.95 +/- 94.07
New best mean reward!
Eval num_timesteps=5000, episode_reward=-85.27 +/- 20.97
Episode length: 86.27 +/- 20.97
New best mean reward!
FINISHED IN 105.71232793602394 s


starting seed  10267 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-495.87 +/- 41.09
Episode length: 495.88 +/- 40.99
New best mean reward!
Eval num_timesteps=11000, episode_reward=-122.40 +/- 100.92
Episode length: 123.34 +/- 100.69
New best mean reward!
Eval num_timesteps=11500, episode_reward=-176.64 +/- 154.51
Episode length: 177.47 +/- 154.16
Eval num_timesteps=12000, episode_reward=-372.09 +/- 184.56
Episode length: 372.42 +/- 184.10
Eval num_timesteps=12500, episode_reward=-83.39 +/- 19.20
Episode length: 84.39 +/- 19.20
New best mean reward!
FINISHED IN 312.49897234799573 s


starting seed  10268 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-499.22 +/- 7.76
Episode length: 499.23 +/- 7.66
New best mean reward!
Eval num_timesteps=7500, episode_reward=-448.43 +/- 88.72
Episode length: 448.73 +/- 88.32
New best mean reward!
Eval num_timesteps=8000, episode_reward=-142.45 +/- 49.34
Episode length: 143.44 +/- 49.27
New best mean reward!
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-284.89 +/- 70.70
Episode length: 285.84 +/- 70.55
Eval num_timesteps=9500, episode_reward=-137.12 +/- 34.81
Episode length: 138.12 +/- 34.81
New best mean reward!
Eval num_timesteps=10000, episode_reward=-107.41 +/- 28.07
Episode length: 108.41 +/- 28.07
New best mean reward!
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-328.83 +/- 67.77
Episode length: 329.83 +/- 67.77
Eval num_timesteps=11500, episode_reward=-169.71 +/- 59.63
Episode length: 170.70 +/- 59.58
Eval num_timesteps=12000, episode_reward=-218.71 +/- 67.14
Episode length: 219.70 +/- 67.10
Eval num_timesteps=12500, episode_reward=-92.27 +/- 26.09
Episode length: 93.27 +/- 26.09
New best mean reward!
FINISHED IN 263.1523376979749 s


starting seed  10269 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-210.73 +/- 116.94
Episode length: 211.63 +/- 116.70
New best mean reward!
Eval num_timesteps=1000, episode_reward=-469.30 +/- 53.43
Episode length: 469.68 +/- 53.08
Eval num_timesteps=1500, episode_reward=-156.72 +/- 24.92
Episode length: 157.72 +/- 24.92
New best mean reward!
Eval num_timesteps=2000, episode_reward=-333.82 +/- 88.91
Episode length: 334.74 +/- 88.76
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-499.37 +/- 6.27
Episode length: 499.38 +/- 6.17
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-434.42 +/- 73.98
Episode length: 435.04 +/- 73.65
Eval num_timesteps=12000, episode_reward=-311.40 +/- 80.73
Episode length: 312.36 +/- 80.64
Eval num_timesteps=12500, episode_reward=-218.95 +/- 76.99
Episode length: 219.89 +/- 76.77
Eval num_timesteps=13000, episode_reward=-137.21 +/- 64.87
Episode length: 138.19 +/- 64.76
New best mean reward!
Eval num_timesteps=13500, episode_reward=-127.04 +/- 45.50
Episode length: 128.03 +/- 45.42
New best mean reward!
Eval num_timesteps=14000, episode_reward=-133.90 +/- 70.54
Episode length: 134.87 +/- 70.38
Eval num_timesteps=14500, episode_reward=-118.20 +/- 50.16
Episode length: 119.20 +/- 50.16
New best mean reward!
Eval num_timesteps=15000, episode_reward=-126.87 +/- 64.27
Episode length: 127.85 +/- 64.15
Eval num_timesteps=15500, episode_reward=-144.05 +/- 29.74
Episode length: 145.05 +/- 29.74
Eval num_timesteps=16000, episode_reward=-137.41 +/- 44.89
Episode length: 138.40 +/- 44.81
Eval num_timesteps=16500, episode_reward=-134.77 +/- 28.72
Episode length: 135.77 +/- 28.72
Eval num_timesteps=17000, episode_reward=-141.46 +/- 47.82
Episode length: 142.45 +/- 47.75
Eval num_timesteps=17500, episode_reward=-134.52 +/- 60.36
Episode length: 135.50 +/- 60.24
Eval num_timesteps=18000, episode_reward=-186.40 +/- 35.60
Episode length: 187.40 +/- 35.60
Eval num_timesteps=18500, episode_reward=-188.86 +/- 41.57
Episode length: 189.86 +/- 41.57
Eval num_timesteps=19000, episode_reward=-199.09 +/- 53.90
Episode length: 200.07 +/- 53.79
Eval num_timesteps=19500, episode_reward=-325.02 +/- 50.44
Episode length: 326.00 +/- 50.37
Eval num_timesteps=20000, episode_reward=-432.37 +/- 42.65
Episode length: 433.27 +/- 42.50
Eval num_timesteps=20500, episode_reward=-290.84 +/- 44.20
Episode length: 291.82 +/- 44.10
Eval num_timesteps=21000, episode_reward=-258.22 +/- 31.23
Episode length: 259.22 +/- 31.23
Eval num_timesteps=21500, episode_reward=-275.59 +/- 55.33
Episode length: 276.55 +/- 55.16
Eval num_timesteps=22000, episode_reward=-454.87 +/- 38.83
Episode length: 455.60 +/- 38.51
Eval num_timesteps=22500, episode_reward=-483.62 +/- 24.52
Episode length: 484.01 +/- 24.12
Eval num_timesteps=23000, episode_reward=-357.78 +/- 62.11
Episode length: 358.70 +/- 61.93
Eval num_timesteps=23500, episode_reward=-267.25 +/- 34.99
Episode length: 268.25 +/- 34.99
Eval num_timesteps=24000, episode_reward=-246.32 +/- 40.70
Episode length: 247.31 +/- 40.63
Eval num_timesteps=24500, episode_reward=-244.81 +/- 55.65
Episode length: 245.78 +/- 55.51
Eval num_timesteps=25000, episode_reward=-172.75 +/- 26.08
Episode length: 173.75 +/- 26.08
Eval num_timesteps=25500, episode_reward=-184.73 +/- 39.97
Episode length: 185.72 +/- 39.89
Eval num_timesteps=26000, episode_reward=-179.28 +/- 41.16
Episode length: 180.27 +/- 41.09
Eval num_timesteps=26500, episode_reward=-201.39 +/- 31.20
Episode length: 202.39 +/- 31.20
Eval num_timesteps=27000, episode_reward=-176.83 +/- 42.63
Episode length: 177.82 +/- 42.56
Eval num_timesteps=27500, episode_reward=-176.34 +/- 25.13
Episode length: 177.34 +/- 25.13
Eval num_timesteps=28000, episode_reward=-194.47 +/- 25.95
Episode length: 195.47 +/- 25.95
Eval num_timesteps=28500, episode_reward=-174.94 +/- 55.62
Episode length: 175.92 +/- 55.50
Eval num_timesteps=29000, episode_reward=-152.26 +/- 45.89
Episode length: 153.25 +/- 45.81
Eval num_timesteps=29500, episode_reward=-164.43 +/- 38.37
Episode length: 165.42 +/- 38.28
Eval num_timesteps=30000, episode_reward=-169.99 +/- 38.93
Episode length: 170.98 +/- 38.84
Eval num_timesteps=30500, episode_reward=-170.99 +/- 23.06
Episode length: 171.99 +/- 23.06
Eval num_timesteps=31000, episode_reward=-162.97 +/- 73.24
Episode length: 163.93 +/- 73.06
Eval num_timesteps=31500, episode_reward=-155.01 +/- 57.60
Episode length: 155.99 +/- 57.48
Eval num_timesteps=32000, episode_reward=-173.80 +/- 41.16
Episode length: 174.79 +/- 41.08
Eval num_timesteps=32500, episode_reward=-159.47 +/- 30.01
Episode length: 160.47 +/- 30.01
Eval num_timesteps=33000, episode_reward=-189.59 +/- 32.49
Episode length: 190.59 +/- 32.49
Eval num_timesteps=33500, episode_reward=-156.02 +/- 21.55
Episode length: 157.02 +/- 21.55
Eval num_timesteps=34000, episode_reward=-158.99 +/- 27.60
Episode length: 159.99 +/- 27.60
Eval num_timesteps=34500, episode_reward=-157.62 +/- 17.42
Episode length: 158.62 +/- 17.42
Eval num_timesteps=35000, episode_reward=-155.30 +/- 45.58
Episode length: 156.29 +/- 45.50
Eval num_timesteps=35500, episode_reward=-146.33 +/- 22.05
Episode length: 147.33 +/- 22.05
Eval num_timesteps=36000, episode_reward=-161.13 +/- 29.17
Episode length: 162.13 +/- 29.17
Eval num_timesteps=36500, episode_reward=-151.15 +/- 28.47
Episode length: 152.15 +/- 28.47
Eval num_timesteps=37000, episode_reward=-155.37 +/- 24.70
Episode length: 156.37 +/- 24.70
Eval num_timesteps=37500, episode_reward=-153.56 +/- 30.78
Episode length: 154.56 +/- 30.78
Eval num_timesteps=38000, episode_reward=-157.56 +/- 26.48
Episode length: 158.56 +/- 26.48
Eval num_timesteps=38500, episode_reward=-154.80 +/- 42.73
Episode length: 155.79 +/- 42.65
Eval num_timesteps=39000, episode_reward=-178.98 +/- 42.61
Episode length: 179.97 +/- 42.53
Eval num_timesteps=39500, episode_reward=-167.43 +/- 38.29
Episode length: 168.42 +/- 38.20
Eval num_timesteps=40000, episode_reward=-158.36 +/- 21.26
Episode length: 159.36 +/- 21.26
Eval num_timesteps=40500, episode_reward=-145.25 +/- 49.56
Episode length: 146.24 +/- 49.49
Eval num_timesteps=41000, episode_reward=-141.35 +/- 44.83
Episode length: 142.34 +/- 44.75
Eval num_timesteps=41500, episode_reward=-142.48 +/- 27.26
Episode length: 143.48 +/- 27.26
Eval num_timesteps=42000, episode_reward=-157.61 +/- 25.29
Episode length: 158.61 +/- 25.29
Eval num_timesteps=42500, episode_reward=-159.76 +/- 29.64
Episode length: 160.76 +/- 29.64
Eval num_timesteps=43000, episode_reward=-164.11 +/- 25.92
Episode length: 165.11 +/- 25.92
Eval num_timesteps=43500, episode_reward=-179.07 +/- 48.06
Episode length: 180.06 +/- 48.00
Eval num_timesteps=44000, episode_reward=-174.56 +/- 27.98
Episode length: 175.56 +/- 27.98
Eval num_timesteps=44500, episode_reward=-183.39 +/- 46.88
Episode length: 184.38 +/- 46.81
Eval num_timesteps=45000, episode_reward=-180.98 +/- 28.65
Episode length: 181.98 +/- 28.65
Eval num_timesteps=45500, episode_reward=-190.94 +/- 44.65
Episode length: 191.93 +/- 44.58
Eval num_timesteps=46000, episode_reward=-181.49 +/- 48.07
Episode length: 182.48 +/- 48.00
Eval num_timesteps=46500, episode_reward=-184.04 +/- 41.35
Episode length: 185.03 +/- 41.28
Eval num_timesteps=47000, episode_reward=-188.17 +/- 41.50
Episode length: 189.16 +/- 41.42
Eval num_timesteps=47500, episode_reward=-191.67 +/- 38.26
Episode length: 192.67 +/- 38.26
Eval num_timesteps=48000, episode_reward=-193.65 +/- 43.64
Episode length: 194.64 +/- 43.57
Eval num_timesteps=48500, episode_reward=-189.30 +/- 32.53
Episode length: 190.30 +/- 32.53
Eval num_timesteps=49000, episode_reward=-189.69 +/- 51.13
Episode length: 190.67 +/- 51.01
Eval num_timesteps=49500, episode_reward=-194.11 +/- 40.95
Episode length: 195.10 +/- 40.88
Eval num_timesteps=50000, episode_reward=-190.62 +/- 29.63
Episode length: 191.62 +/- 29.63
FINISHED IN 858.6314877460245 s


starting seed  10270 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16500, episode_reward=-279.88 +/- 205.06
Episode length: 280.42 +/- 204.57
New best mean reward!
Eval num_timesteps=17000, episode_reward=-271.48 +/- 208.05
Episode length: 272.03 +/- 207.55
New best mean reward!
Eval num_timesteps=17500, episode_reward=-190.83 +/- 181.88
Episode length: 191.58 +/- 181.45
New best mean reward!
Eval num_timesteps=18000, episode_reward=-203.56 +/- 183.04
Episode length: 204.29 +/- 182.60
Eval num_timesteps=18500, episode_reward=-203.65 +/- 185.67
Episode length: 204.37 +/- 185.22
Eval num_timesteps=19000, episode_reward=-129.16 +/- 122.37
Episode length: 130.07 +/- 122.10
New best mean reward!
Eval num_timesteps=19500, episode_reward=-135.15 +/- 123.27
Episode length: 136.06 +/- 123.00
Eval num_timesteps=20000, episode_reward=-131.31 +/- 127.92
Episode length: 132.21 +/- 127.63
Eval num_timesteps=20500, episode_reward=-125.42 +/- 112.25
Episode length: 126.35 +/- 112.02
New best mean reward!
Eval num_timesteps=21000, episode_reward=-137.45 +/- 133.07
Episode length: 138.34 +/- 132.77
Eval num_timesteps=21500, episode_reward=-111.65 +/- 94.50
Episode length: 112.60 +/- 94.30
New best mean reward!
Eval num_timesteps=22000, episode_reward=-115.22 +/- 107.68
Episode length: 116.15 +/- 107.43
Eval num_timesteps=22500, episode_reward=-111.57 +/- 94.08
Episode length: 112.52 +/- 93.87
New best mean reward!
Eval num_timesteps=23000, episode_reward=-122.01 +/- 113.83
Episode length: 122.94 +/- 113.60
Eval num_timesteps=23500, episode_reward=-133.55 +/- 128.61
Episode length: 134.45 +/- 128.33
Eval num_timesteps=24000, episode_reward=-160.82 +/- 149.55
Episode length: 161.67 +/- 149.21
Eval num_timesteps=24500, episode_reward=-116.81 +/- 115.67
Episode length: 117.73 +/- 115.41
Eval num_timesteps=25000, episode_reward=-129.90 +/- 120.65
Episode length: 130.82 +/- 120.40
Eval num_timesteps=25500, episode_reward=-148.01 +/- 145.76
Episode length: 148.87 +/- 145.42
Eval num_timesteps=26000, episode_reward=-185.68 +/- 173.20
Episode length: 186.45 +/- 172.78
Eval num_timesteps=26500, episode_reward=-108.92 +/- 85.50
Episode length: 109.88 +/- 85.32
New best mean reward!
Eval num_timesteps=27000, episode_reward=-161.76 +/- 155.91
Episode length: 162.59 +/- 155.55
Eval num_timesteps=27500, episode_reward=-119.88 +/- 114.84
Episode length: 120.80 +/- 114.58
Eval num_timesteps=28000, episode_reward=-144.56 +/- 140.60
Episode length: 145.43 +/- 140.27
Eval num_timesteps=28500, episode_reward=-123.92 +/- 112.09
Episode length: 124.85 +/- 111.86
Eval num_timesteps=29000, episode_reward=-98.61 +/- 67.06
Episode length: 99.59 +/- 66.94
New best mean reward!
FINISHED IN 667.4013163719792 s


starting seed  10271 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-496.82 +/- 31.64
Episode length: 496.83 +/- 31.54
New best mean reward!
Eval num_timesteps=11000, episode_reward=-94.45 +/- 28.31
Episode length: 95.45 +/- 28.31
New best mean reward!
FINISHED IN 287.85140853602206 s


starting seed  10272 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-303.00 +/- 205.99
Episode length: 303.48 +/- 205.49
New best mean reward!
Eval num_timesteps=11500, episode_reward=-404.25 +/- 170.99
Episode length: 404.49 +/- 170.56
Eval num_timesteps=12000, episode_reward=-119.54 +/- 107.51
Episode length: 120.48 +/- 107.30
New best mean reward!
Eval num_timesteps=12500, episode_reward=-117.94 +/- 107.47
Episode length: 118.87 +/- 107.22
New best mean reward!
Eval num_timesteps=13000, episode_reward=-95.95 +/- 33.83
Episode length: 96.95 +/- 33.83
New best mean reward!
FINISHED IN 341.32323977700435 s


starting seed  10273 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-415.44 +/- 143.35
Episode length: 415.70 +/- 142.91
New best mean reward!
Eval num_timesteps=1500, episode_reward=-171.07 +/- 35.45
Episode length: 172.07 +/- 35.45
New best mean reward!
Eval num_timesteps=2000, episode_reward=-171.38 +/- 36.50
Episode length: 172.38 +/- 36.50
Eval num_timesteps=2500, episode_reward=-157.35 +/- 38.89
Episode length: 158.35 +/- 38.89
New best mean reward!
Eval num_timesteps=3000, episode_reward=-493.19 +/- 47.67
Episode length: 493.21 +/- 47.53
Eval num_timesteps=3500, episode_reward=-480.21 +/- 78.36
Episode length: 480.27 +/- 78.12
Eval num_timesteps=4000, episode_reward=-343.99 +/- 160.92
Episode length: 344.48 +/- 160.42
Eval num_timesteps=4500, episode_reward=-275.60 +/- 143.62
Episode length: 276.32 +/- 143.18
Eval num_timesteps=5000, episode_reward=-394.09 +/- 136.30
Episode length: 394.48 +/- 135.83
Eval num_timesteps=5500, episode_reward=-309.60 +/- 133.11
Episode length: 310.30 +/- 132.68
Eval num_timesteps=6000, episode_reward=-207.42 +/- 76.81
Episode length: 208.38 +/- 76.66
Eval num_timesteps=6500, episode_reward=-390.09 +/- 139.43
Episode length: 390.48 +/- 138.95
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-174.36 +/- 30.53
Episode length: 175.36 +/- 30.53
Eval num_timesteps=10500, episode_reward=-158.81 +/- 31.99
Episode length: 159.81 +/- 31.99
Eval num_timesteps=11000, episode_reward=-158.63 +/- 29.46
Episode length: 159.63 +/- 29.46
Eval num_timesteps=11500, episode_reward=-170.84 +/- 51.78
Episode length: 171.83 +/- 51.72
Eval num_timesteps=12000, episode_reward=-148.97 +/- 28.53
Episode length: 149.97 +/- 28.53
New best mean reward!
Eval num_timesteps=12500, episode_reward=-143.56 +/- 25.41
Episode length: 144.56 +/- 25.41
New best mean reward!
Eval num_timesteps=13000, episode_reward=-160.94 +/- 46.68
Episode length: 161.93 +/- 46.61
Eval num_timesteps=13500, episode_reward=-163.42 +/- 53.18
Episode length: 164.41 +/- 53.11
Eval num_timesteps=14000, episode_reward=-166.23 +/- 58.52
Episode length: 167.21 +/- 58.41
Eval num_timesteps=14500, episode_reward=-148.29 +/- 30.74
Episode length: 149.29 +/- 30.74
Eval num_timesteps=15000, episode_reward=-139.90 +/- 46.83
Episode length: 140.89 +/- 46.75
New best mean reward!
Eval num_timesteps=15500, episode_reward=-140.09 +/- 41.50
Episode length: 141.09 +/- 41.50
Eval num_timesteps=16000, episode_reward=-137.61 +/- 28.14
Episode length: 138.61 +/- 28.14
New best mean reward!
Eval num_timesteps=16500, episode_reward=-140.89 +/- 27.79
Episode length: 141.89 +/- 27.79
Eval num_timesteps=17000, episode_reward=-98.00 +/- 23.97
Episode length: 99.00 +/- 23.97
New best mean reward!
FINISHED IN 347.66610210901126 s


starting seed  10274 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-477.74 +/- 81.55
Episode length: 477.81 +/- 81.29
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-394.55 +/- 85.22
Episode length: 395.31 +/- 84.93
New best mean reward!
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-462.48 +/- 102.96
Episode length: 462.60 +/- 102.64
Eval num_timesteps=7500, episode_reward=-428.51 +/- 145.42
Episode length: 428.71 +/- 145.03
Eval num_timesteps=8000, episode_reward=-404.17 +/- 167.12
Episode length: 404.42 +/- 166.69
Eval num_timesteps=8500, episode_reward=-193.25 +/- 173.83
Episode length: 194.03 +/- 173.44
New best mean reward!
Eval num_timesteps=9000, episode_reward=-169.66 +/- 161.46
Episode length: 170.47 +/- 161.07
New best mean reward!
Eval num_timesteps=9500, episode_reward=-107.97 +/- 66.50
Episode length: 108.95 +/- 66.38
New best mean reward!
Eval num_timesteps=10000, episode_reward=-115.51 +/- 101.72
Episode length: 116.45 +/- 101.50
Eval num_timesteps=10500, episode_reward=-103.83 +/- 73.88
Episode length: 104.80 +/- 73.72
New best mean reward!
Eval num_timesteps=11000, episode_reward=-90.77 +/- 21.27
Episode length: 91.77 +/- 21.27
New best mean reward!
FINISHED IN 226.49649350502295 s


starting seed  10275 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-257.54 +/- 143.77
Episode length: 258.29 +/- 143.35
New best mean reward!
Eval num_timesteps=10500, episode_reward=-186.79 +/- 107.68
Episode length: 187.70 +/- 107.42
New best mean reward!
Eval num_timesteps=11000, episode_reward=-135.72 +/- 54.18
Episode length: 136.70 +/- 54.04
New best mean reward!
Eval num_timesteps=11500, episode_reward=-220.22 +/- 160.79
Episode length: 220.98 +/- 160.37
Eval num_timesteps=12000, episode_reward=-104.83 +/- 26.51
Episode length: 105.83 +/- 26.51
New best mean reward!
Eval num_timesteps=12500, episode_reward=-96.18 +/- 15.28
Episode length: 97.18 +/- 15.28
New best mean reward!
FINISHED IN 280.226541502052 s


starting seed  10276 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-495.84 +/- 41.39
Episode length: 495.85 +/- 41.29
New best mean reward!
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-269.15 +/- 204.88
Episode length: 269.71 +/- 204.39
New best mean reward!
Eval num_timesteps=10000, episode_reward=-279.04 +/- 208.38
Episode length: 279.57 +/- 207.88
Eval num_timesteps=10500, episode_reward=-247.76 +/- 201.38
Episode length: 248.38 +/- 200.91
New best mean reward!
Eval num_timesteps=11000, episode_reward=-221.42 +/- 192.63
Episode length: 222.10 +/- 192.17
New best mean reward!
Eval num_timesteps=11500, episode_reward=-169.30 +/- 168.15
Episode length: 170.10 +/- 167.75
New best mean reward!
Eval num_timesteps=12000, episode_reward=-206.26 +/- 188.62
Episode length: 206.97 +/- 188.17
Eval num_timesteps=12500, episode_reward=-160.78 +/- 161.14
Episode length: 161.60 +/- 160.76
New best mean reward!
Eval num_timesteps=13000, episode_reward=-209.81 +/- 190.56
Episode length: 210.51 +/- 190.10
Eval num_timesteps=13500, episode_reward=-261.11 +/- 203.81
Episode length: 261.69 +/- 203.32
Eval num_timesteps=14000, episode_reward=-393.95 +/- 179.35
Episode length: 394.21 +/- 178.91
Eval num_timesteps=14500, episode_reward=-422.75 +/- 159.88
Episode length: 422.94 +/- 159.49
Eval num_timesteps=15000, episode_reward=-355.38 +/- 197.37
Episode length: 355.73 +/- 196.90
Eval num_timesteps=15500, episode_reward=-251.88 +/- 200.26
Episode length: 252.49 +/- 199.78
Eval num_timesteps=16000, episode_reward=-201.56 +/- 185.49
Episode length: 202.29 +/- 185.05
Eval num_timesteps=16500, episode_reward=-142.09 +/- 140.68
Episode length: 142.97 +/- 140.37
New best mean reward!
Eval num_timesteps=17000, episode_reward=-147.34 +/- 139.99
Episode length: 148.21 +/- 139.66
Eval num_timesteps=17500, episode_reward=-116.85 +/- 98.58
Episode length: 117.80 +/- 98.39
New best mean reward!
Eval num_timesteps=18000, episode_reward=-137.20 +/- 136.39
Episode length: 138.08 +/- 136.07
Eval num_timesteps=18500, episode_reward=-87.65 +/- 44.91
Episode length: 88.64 +/- 44.82
New best mean reward!
FINISHED IN 384.0039735029568 s


starting seed  10277 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-448.87 +/- 132.40
Episode length: 449.00 +/- 132.07
New best mean reward!
Eval num_timesteps=9000, episode_reward=-487.36 +/- 71.91
Episode length: 487.39 +/- 71.74
Eval num_timesteps=9500, episode_reward=-347.00 +/- 188.83
Episode length: 347.42 +/- 188.36
New best mean reward!
Eval num_timesteps=10000, episode_reward=-319.91 +/- 191.53
Episode length: 320.41 +/- 191.06
New best mean reward!
Eval num_timesteps=10500, episode_reward=-467.72 +/- 109.52
Episode length: 467.80 +/- 109.25
Eval num_timesteps=11000, episode_reward=-382.98 +/- 179.78
Episode length: 383.29 +/- 179.33
Eval num_timesteps=11500, episode_reward=-106.28 +/- 78.32
Episode length: 107.25 +/- 78.17
New best mean reward!
Eval num_timesteps=12000, episode_reward=-107.28 +/- 74.64
Episode length: 108.25 +/- 74.48
Eval num_timesteps=12500, episode_reward=-90.56 +/- 20.69
Episode length: 91.56 +/- 20.69
New best mean reward!
FINISHED IN 286.10312932898523 s


starting seed  10278 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-486.22 +/- 57.86
Episode length: 486.28 +/- 57.64
New best mean reward!
Eval num_timesteps=10500, episode_reward=-485.31 +/- 60.74
Episode length: 485.37 +/- 60.51
New best mean reward!
Eval num_timesteps=11000, episode_reward=-173.60 +/- 64.28
Episode length: 174.58 +/- 64.18
New best mean reward!
Eval num_timesteps=11500, episode_reward=-125.95 +/- 26.91
Episode length: 126.95 +/- 26.91
New best mean reward!
Eval num_timesteps=12000, episode_reward=-103.94 +/- 28.89
Episode length: 104.94 +/- 28.89
New best mean reward!
Eval num_timesteps=12500, episode_reward=-98.41 +/- 17.10
Episode length: 99.41 +/- 17.10
New best mean reward!
FINISHED IN 281.5240721090231 s


starting seed  10279 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-468.40 +/- 96.94
Episode length: 468.50 +/- 96.65
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-496.59 +/- 33.93
Episode length: 496.60 +/- 33.83
Eval num_timesteps=9000, episode_reward=-401.38 +/- 167.53
Episode length: 401.64 +/- 167.10
New best mean reward!
Eval num_timesteps=9500, episode_reward=-250.60 +/- 194.35
Episode length: 251.23 +/- 193.88
New best mean reward!
Eval num_timesteps=10000, episode_reward=-157.77 +/- 155.83
Episode length: 158.60 +/- 155.46
New best mean reward!
Eval num_timesteps=10500, episode_reward=-119.20 +/- 114.47
Episode length: 120.12 +/- 114.20
New best mean reward!
Eval num_timesteps=11000, episode_reward=-122.62 +/- 114.21
Episode length: 123.54 +/- 113.95
Eval num_timesteps=11500, episode_reward=-110.87 +/- 92.74
Episode length: 111.83 +/- 92.57
New best mean reward!
Eval num_timesteps=12000, episode_reward=-99.66 +/- 74.68
Episode length: 100.63 +/- 74.52
New best mean reward!
FINISHED IN 255.0421118939994 s


starting seed  10280 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-178.49 +/- 153.60
Episode length: 179.32 +/- 153.24
New best mean reward!
Eval num_timesteps=1000, episode_reward=-87.25 +/- 25.11
Episode length: 88.25 +/- 25.11
New best mean reward!
FINISHED IN 11.136345825972967 s


starting seed  10281 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-175.19 +/- 173.59
Episode length: 175.98 +/- 173.19
New best mean reward!
Eval num_timesteps=7500, episode_reward=-150.82 +/- 148.69
Episode length: 151.67 +/- 148.34
New best mean reward!
Eval num_timesteps=8000, episode_reward=-128.46 +/- 124.83
Episode length: 129.37 +/- 124.56
New best mean reward!
Eval num_timesteps=8500, episode_reward=-349.65 +/- 197.73
Episode length: 350.02 +/- 197.25
Eval num_timesteps=9000, episode_reward=-120.39 +/- 120.58
Episode length: 121.30 +/- 120.30
New best mean reward!
Eval num_timesteps=9500, episode_reward=-91.41 +/- 47.34
Episode length: 92.40 +/- 47.25
New best mean reward!
FINISHED IN 206.10644501802744 s


starting seed  10282 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-366.64 +/- 178.83
Episode length: 367.00 +/- 178.35
New best mean reward!
Eval num_timesteps=11500, episode_reward=-412.46 +/- 157.17
Episode length: 412.70 +/- 156.74
Eval num_timesteps=12000, episode_reward=-262.64 +/- 189.69
Episode length: 263.27 +/- 189.23
New best mean reward!
Eval num_timesteps=12500, episode_reward=-171.82 +/- 147.78
Episode length: 172.66 +/- 147.43
New best mean reward!
Eval num_timesteps=13000, episode_reward=-186.93 +/- 156.62
Episode length: 187.74 +/- 156.24
Eval num_timesteps=13500, episode_reward=-139.67 +/- 112.38
Episode length: 140.59 +/- 112.12
New best mean reward!
Eval num_timesteps=14000, episode_reward=-131.31 +/- 99.17
Episode length: 132.25 +/- 98.95
New best mean reward!
Eval num_timesteps=14500, episode_reward=-120.70 +/- 76.96
Episode length: 121.67 +/- 76.81
New best mean reward!
Eval num_timesteps=15000, episode_reward=-115.04 +/- 70.42
Episode length: 116.02 +/- 70.31
New best mean reward!
Eval num_timesteps=15500, episode_reward=-108.99 +/- 57.64
Episode length: 109.98 +/- 57.57
New best mean reward!
Eval num_timesteps=16000, episode_reward=-119.44 +/- 66.07
Episode length: 120.43 +/- 66.01
Eval num_timesteps=16500, episode_reward=-109.10 +/- 52.97
Episode length: 110.09 +/- 52.89
Eval num_timesteps=17000, episode_reward=-117.94 +/- 71.29
Episode length: 118.92 +/- 71.19
Eval num_timesteps=17500, episode_reward=-119.98 +/- 77.19
Episode length: 120.96 +/- 77.09
Eval num_timesteps=18000, episode_reward=-127.65 +/- 86.78
Episode length: 128.61 +/- 86.61
Eval num_timesteps=18500, episode_reward=-112.88 +/- 57.55
Episode length: 113.87 +/- 57.49
Eval num_timesteps=19000, episode_reward=-103.36 +/- 35.04
Episode length: 104.36 +/- 35.04
New best mean reward!
Eval num_timesteps=19500, episode_reward=-99.73 +/- 31.41
Episode length: 100.73 +/- 31.41
New best mean reward!
FINISHED IN 368.9735601840075 s


starting seed  10283 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-471.52 +/- 86.07
Episode length: 471.62 +/- 85.77
New best mean reward!
Eval num_timesteps=1000, episode_reward=-170.38 +/- 37.93
Episode length: 171.38 +/- 37.93
New best mean reward!
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-495.66 +/- 30.39
Episode length: 495.68 +/- 30.25
Eval num_timesteps=3000, episode_reward=-332.00 +/- 160.03
Episode length: 332.53 +/- 159.54
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-420.32 +/- 143.05
Episode length: 420.56 +/- 142.63
Eval num_timesteps=5000, episode_reward=-305.76 +/- 167.23
Episode length: 306.35 +/- 166.75
Eval num_timesteps=5500, episode_reward=-236.92 +/- 146.08
Episode length: 237.69 +/- 145.67
Eval num_timesteps=6000, episode_reward=-383.73 +/- 135.12
Episode length: 384.26 +/- 134.72
Eval num_timesteps=6500, episode_reward=-146.68 +/- 37.03
Episode length: 147.68 +/- 37.03
New best mean reward!
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-428.34 +/- 73.79
Episode length: 428.96 +/- 73.42
Eval num_timesteps=8000, episode_reward=-163.38 +/- 57.40
Episode length: 164.37 +/- 57.34
Eval num_timesteps=8500, episode_reward=-153.51 +/- 57.51
Episode length: 154.51 +/- 57.51
Eval num_timesteps=9000, episode_reward=-172.51 +/- 83.76
Episode length: 173.46 +/- 83.57
Eval num_timesteps=9500, episode_reward=-193.12 +/- 79.85
Episode length: 194.07 +/- 79.66
Eval num_timesteps=10000, episode_reward=-159.28 +/- 38.19
Episode length: 160.28 +/- 38.19
Eval num_timesteps=10500, episode_reward=-207.70 +/- 88.94
Episode length: 208.65 +/- 88.77
Eval num_timesteps=11000, episode_reward=-214.33 +/- 109.47
Episode length: 215.22 +/- 109.18
Eval num_timesteps=11500, episode_reward=-167.59 +/- 37.69
Episode length: 168.59 +/- 37.69
Eval num_timesteps=12000, episode_reward=-169.19 +/- 45.68
Episode length: 170.19 +/- 45.68
Eval num_timesteps=12500, episode_reward=-165.33 +/- 42.55
Episode length: 166.33 +/- 42.55
Eval num_timesteps=13000, episode_reward=-180.14 +/- 52.15
Episode length: 181.14 +/- 52.15
Eval num_timesteps=13500, episode_reward=-180.52 +/- 40.53
Episode length: 181.52 +/- 40.53
Eval num_timesteps=14000, episode_reward=-169.78 +/- 37.34
Episode length: 170.78 +/- 37.34
Eval num_timesteps=14500, episode_reward=-167.61 +/- 38.48
Episode length: 168.61 +/- 38.48
Eval num_timesteps=15000, episode_reward=-168.32 +/- 57.81
Episode length: 169.32 +/- 57.81
Eval num_timesteps=15500, episode_reward=-160.61 +/- 54.35
Episode length: 161.60 +/- 54.28
Eval num_timesteps=16000, episode_reward=-163.93 +/- 42.47
Episode length: 164.93 +/- 42.47
Eval num_timesteps=16500, episode_reward=-163.96 +/- 63.80
Episode length: 164.94 +/- 63.69
Eval num_timesteps=17000, episode_reward=-155.44 +/- 31.70
Episode length: 156.44 +/- 31.70
Eval num_timesteps=17500, episode_reward=-140.00 +/- 28.71
Episode length: 141.00 +/- 28.71
New best mean reward!
Eval num_timesteps=18000, episode_reward=-135.17 +/- 39.05
Episode length: 136.17 +/- 39.05
New best mean reward!
Eval num_timesteps=18500, episode_reward=-138.37 +/- 35.13
Episode length: 139.37 +/- 35.13
Eval num_timesteps=19000, episode_reward=-142.51 +/- 44.10
Episode length: 143.51 +/- 44.10
Eval num_timesteps=19500, episode_reward=-135.28 +/- 54.75
Episode length: 136.27 +/- 54.68
Eval num_timesteps=20000, episode_reward=-120.12 +/- 18.95
Episode length: 121.12 +/- 18.95
New best mean reward!
Eval num_timesteps=20500, episode_reward=-114.54 +/- 37.74
Episode length: 115.54 +/- 37.74
New best mean reward!
Eval num_timesteps=21000, episode_reward=-121.86 +/- 56.13
Episode length: 122.85 +/- 56.06
Eval num_timesteps=21500, episode_reward=-102.15 +/- 26.95
Episode length: 103.15 +/- 26.95
New best mean reward!
Eval num_timesteps=22000, episode_reward=-108.65 +/- 34.82
Episode length: 109.65 +/- 34.82
Eval num_timesteps=22500, episode_reward=-130.52 +/- 63.45
Episode length: 131.50 +/- 63.34
Eval num_timesteps=23000, episode_reward=-106.51 +/- 32.07
Episode length: 107.51 +/- 32.07
Eval num_timesteps=23500, episode_reward=-121.04 +/- 31.30
Episode length: 122.04 +/- 31.30
Eval num_timesteps=24000, episode_reward=-120.07 +/- 34.24
Episode length: 121.07 +/- 34.24
Eval num_timesteps=24500, episode_reward=-122.41 +/- 44.16
Episode length: 123.40 +/- 44.08
Eval num_timesteps=25000, episode_reward=-122.40 +/- 33.00
Episode length: 123.40 +/- 33.00
Eval num_timesteps=25500, episode_reward=-129.13 +/- 59.36
Episode length: 130.12 +/- 59.30
Eval num_timesteps=26000, episode_reward=-131.34 +/- 41.05
Episode length: 132.34 +/- 41.05
Eval num_timesteps=26500, episode_reward=-142.64 +/- 59.60
Episode length: 143.63 +/- 59.54
Eval num_timesteps=27000, episode_reward=-128.59 +/- 28.77
Episode length: 129.59 +/- 28.77
Eval num_timesteps=27500, episode_reward=-113.92 +/- 40.16
Episode length: 114.92 +/- 40.16
Eval num_timesteps=28000, episode_reward=-100.43 +/- 39.52
Episode length: 101.43 +/- 39.52
New best mean reward!
Eval num_timesteps=28500, episode_reward=-106.76 +/- 30.20
Episode length: 107.76 +/- 30.20
Eval num_timesteps=29000, episode_reward=-95.76 +/- 29.05
Episode length: 96.76 +/- 29.05
New best mean reward!
FINISHED IN 462.1409350030008 s


starting seed  10284 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-459.98 +/- 120.26
Episode length: 460.08 +/- 119.96
New best mean reward!
Eval num_timesteps=11000, episode_reward=-496.04 +/- 39.40
Episode length: 496.05 +/- 39.30
Eval num_timesteps=11500, episode_reward=-284.41 +/- 196.67
Episode length: 284.96 +/- 196.17
New best mean reward!
Eval num_timesteps=12000, episode_reward=-150.19 +/- 145.08
Episode length: 151.05 +/- 144.75
New best mean reward!
Eval num_timesteps=12500, episode_reward=-208.03 +/- 179.08
Episode length: 208.77 +/- 178.66
Eval num_timesteps=13000, episode_reward=-238.93 +/- 193.33
Episode length: 239.58 +/- 192.85
Eval num_timesteps=13500, episode_reward=-176.99 +/- 167.88
Episode length: 177.78 +/- 167.48
Eval num_timesteps=14000, episode_reward=-170.65 +/- 158.18
Episode length: 171.48 +/- 157.82
Eval num_timesteps=14500, episode_reward=-144.78 +/- 136.22
Episode length: 145.66 +/- 135.91
New best mean reward!
Eval num_timesteps=15000, episode_reward=-123.33 +/- 113.23
Episode length: 124.25 +/- 112.96
New best mean reward!
Eval num_timesteps=15500, episode_reward=-133.17 +/- 124.93
Episode length: 134.08 +/- 124.67
Eval num_timesteps=16000, episode_reward=-141.62 +/- 135.78
Episode length: 142.50 +/- 135.46
Eval num_timesteps=16500, episode_reward=-159.45 +/- 153.16
Episode length: 160.29 +/- 152.81
Eval num_timesteps=17000, episode_reward=-101.99 +/- 73.54
Episode length: 102.96 +/- 73.37
New best mean reward!
Eval num_timesteps=17500, episode_reward=-105.29 +/- 80.02
Episode length: 106.26 +/- 79.88
Eval num_timesteps=18000, episode_reward=-91.26 +/- 22.62
Episode length: 92.26 +/- 22.62
New best mean reward!
FINISHED IN 380.36715672101127 s


starting seed  10285 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-487.83 +/- 69.22
Episode length: 487.86 +/- 69.05
New best mean reward!
Eval num_timesteps=4500, episode_reward=-133.60 +/- 126.73
Episode length: 134.51 +/- 126.47
New best mean reward!
Eval num_timesteps=5000, episode_reward=-128.02 +/- 104.58
Episode length: 128.96 +/- 104.37
New best mean reward!
Eval num_timesteps=5500, episode_reward=-87.37 +/- 22.59
Episode length: 88.37 +/- 22.59
New best mean reward!
FINISHED IN 117.05905875301687 s


starting seed  10286 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-496.02 +/- 39.60
Episode length: 496.03 +/- 39.50
New best mean reward!
Eval num_timesteps=11000, episode_reward=-496.13 +/- 38.51
Episode length: 496.14 +/- 38.41
Eval num_timesteps=11500, episode_reward=-488.78 +/- 64.12
Episode length: 488.81 +/- 63.95
New best mean reward!
Eval num_timesteps=12000, episode_reward=-224.94 +/- 187.47
Episode length: 225.64 +/- 187.03
New best mean reward!
Eval num_timesteps=12500, episode_reward=-134.32 +/- 125.99
Episode length: 135.22 +/- 125.70
New best mean reward!
Eval num_timesteps=13000, episode_reward=-153.72 +/- 148.46
Episode length: 154.57 +/- 148.11
Eval num_timesteps=13500, episode_reward=-176.32 +/- 158.78
Episode length: 177.14 +/- 158.41
Eval num_timesteps=14000, episode_reward=-146.63 +/- 140.35
Episode length: 147.50 +/- 140.02
Eval num_timesteps=14500, episode_reward=-153.96 +/- 144.62
Episode length: 154.82 +/- 144.29
Eval num_timesteps=15000, episode_reward=-128.58 +/- 104.84
Episode length: 129.52 +/- 104.63
New best mean reward!
Eval num_timesteps=15500, episode_reward=-119.96 +/- 86.37
Episode length: 120.93 +/- 86.24
New best mean reward!
Eval num_timesteps=16000, episode_reward=-96.42 +/- 35.04
Episode length: 97.42 +/- 35.04
New best mean reward!
FINISHED IN 360.57285708299605 s


starting seed  10287 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-442.18 +/- 127.97
Episode length: 442.35 +/- 127.60
New best mean reward!
Eval num_timesteps=9000, episode_reward=-469.00 +/- 93.40
Episode length: 469.10 +/- 93.11
Eval num_timesteps=9500, episode_reward=-394.52 +/- 153.34
Episode length: 394.85 +/- 152.87
New best mean reward!
Eval num_timesteps=10000, episode_reward=-192.90 +/- 166.56
Episode length: 193.68 +/- 166.16
New best mean reward!
Eval num_timesteps=10500, episode_reward=-283.67 +/- 194.67
Episode length: 284.23 +/- 194.18
Eval num_timesteps=11000, episode_reward=-215.75 +/- 182.79
Episode length: 216.47 +/- 182.36
Eval num_timesteps=11500, episode_reward=-190.18 +/- 164.51
Episode length: 190.97 +/- 164.11
New best mean reward!
Eval num_timesteps=12000, episode_reward=-139.21 +/- 118.29
Episode length: 140.12 +/- 118.02
New best mean reward!
Eval num_timesteps=12500, episode_reward=-110.93 +/- 75.83
Episode length: 111.90 +/- 75.68
New best mean reward!
Eval num_timesteps=13000, episode_reward=-98.87 +/- 40.95
Episode length: 99.87 +/- 40.95
New best mean reward!
FINISHED IN 376.6214995980263 s


starting seed  10288 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-484.86 +/- 74.19
Episode length: 484.90 +/- 73.99
New best mean reward!
Eval num_timesteps=6500, episode_reward=-343.10 +/- 174.16
Episode length: 343.56 +/- 173.67
New best mean reward!
Eval num_timesteps=7000, episode_reward=-334.58 +/- 190.94
Episode length: 335.03 +/- 190.46
New best mean reward!
Eval num_timesteps=7500, episode_reward=-232.04 +/- 186.64
Episode length: 232.72 +/- 186.18
New best mean reward!
Eval num_timesteps=8000, episode_reward=-172.82 +/- 150.53
Episode length: 173.65 +/- 150.16
New best mean reward!
Eval num_timesteps=8500, episode_reward=-180.69 +/- 127.19
Episode length: 181.57 +/- 126.89
Eval num_timesteps=9000, episode_reward=-168.67 +/- 108.76
Episode length: 169.59 +/- 108.51
New best mean reward!
Eval num_timesteps=9500, episode_reward=-224.53 +/- 140.64
Episode length: 225.34 +/- 140.27
Eval num_timesteps=10000, episode_reward=-180.80 +/- 112.42
Episode length: 181.70 +/- 112.14
Eval num_timesteps=10500, episode_reward=-166.81 +/- 115.43
Episode length: 167.71 +/- 115.14
New best mean reward!
Eval num_timesteps=11000, episode_reward=-154.24 +/- 106.55
Episode length: 155.17 +/- 106.32
New best mean reward!
Eval num_timesteps=11500, episode_reward=-151.84 +/- 106.23
Episode length: 152.77 +/- 106.00
New best mean reward!
Eval num_timesteps=12000, episode_reward=-135.54 +/- 95.92
Episode length: 136.49 +/- 95.73
New best mean reward!
Eval num_timesteps=12500, episode_reward=-136.41 +/- 109.24
Episode length: 137.34 +/- 109.01
Eval num_timesteps=13000, episode_reward=-115.20 +/- 81.38
Episode length: 116.17 +/- 81.24
New best mean reward!
Eval num_timesteps=13500, episode_reward=-97.15 +/- 47.97
Episode length: 98.14 +/- 47.88
New best mean reward!
FINISHED IN 270.3694132230012 s


starting seed  10289 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-90.77 +/- 42.75
Episode length: 91.77 +/- 42.75
New best mean reward!
FINISHED IN 115.37955186399631 s


starting seed  10290 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-287.41 +/- 167.50
Episode length: 288.03 +/- 167.02
New best mean reward!
Eval num_timesteps=8500, episode_reward=-189.95 +/- 111.11
Episode length: 190.84 +/- 110.80
New best mean reward!
Eval num_timesteps=9000, episode_reward=-234.37 +/- 137.32
Episode length: 235.18 +/- 136.95
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-471.46 +/- 92.50
Episode length: 471.55 +/- 92.22
Eval num_timesteps=10500, episode_reward=-325.04 +/- 193.91
Episode length: 325.49 +/- 193.42
Eval num_timesteps=11000, episode_reward=-371.10 +/- 174.01
Episode length: 371.46 +/- 173.53
Eval num_timesteps=11500, episode_reward=-260.64 +/- 177.56
Episode length: 261.29 +/- 177.09
Eval num_timesteps=12000, episode_reward=-182.57 +/- 148.95
Episode length: 183.40 +/- 148.59
New best mean reward!
Eval num_timesteps=12500, episode_reward=-140.21 +/- 116.57
Episode length: 141.12 +/- 116.29
New best mean reward!
Eval num_timesteps=13000, episode_reward=-211.02 +/- 178.42
Episode length: 211.75 +/- 177.98
Eval num_timesteps=13500, episode_reward=-205.63 +/- 172.26
Episode length: 206.39 +/- 171.85
Eval num_timesteps=14000, episode_reward=-184.76 +/- 157.86
Episode length: 185.57 +/- 157.48
Eval num_timesteps=14500, episode_reward=-155.80 +/- 142.81
Episode length: 156.66 +/- 142.48
Eval num_timesteps=15000, episode_reward=-140.68 +/- 93.33
Episode length: 141.63 +/- 93.14
Eval num_timesteps=15500, episode_reward=-103.09 +/- 54.80
Episode length: 104.08 +/- 54.73
New best mean reward!
Eval num_timesteps=16000, episode_reward=-101.98 +/- 55.61
Episode length: 102.97 +/- 55.54
New best mean reward!
Eval num_timesteps=16500, episode_reward=-104.45 +/- 40.21
Episode length: 105.45 +/- 40.21
Eval num_timesteps=17000, episode_reward=-96.83 +/- 40.21
Episode length: 97.83 +/- 40.21
New best mean reward!
FINISHED IN 324.39359985495685 s


starting seed  10291 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-350.44 +/- 80.77
Episode length: 351.34 +/- 80.58
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-317.49 +/- 69.51
Episode length: 318.46 +/- 69.43
New best mean reward!
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-420.03 +/- 116.72
Episode length: 420.39 +/- 116.28
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-386.03 +/- 85.26
Episode length: 386.86 +/- 85.03
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-496.82 +/- 31.64
Episode length: 496.83 +/- 31.54
Eval num_timesteps=10500, episode_reward=-368.10 +/- 95.24
Episode length: 368.89 +/- 94.95
Eval num_timesteps=11000, episode_reward=-325.97 +/- 147.11
Episode length: 326.57 +/- 146.63
Eval num_timesteps=11500, episode_reward=-408.82 +/- 113.06
Episode length: 409.25 +/- 112.60
Eval num_timesteps=12000, episode_reward=-297.42 +/- 78.01
Episode length: 298.41 +/- 77.98
New best mean reward!
Eval num_timesteps=12500, episode_reward=-343.14 +/- 100.42
Episode length: 344.01 +/- 100.22
Eval num_timesteps=13000, episode_reward=-334.01 +/- 149.08
Episode length: 334.58 +/- 148.61
Eval num_timesteps=13500, episode_reward=-440.47 +/- 92.10
Episode length: 440.81 +/- 91.68
Eval num_timesteps=14000, episode_reward=-432.09 +/- 89.64
Episode length: 432.52 +/- 89.21
Eval num_timesteps=14500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16000, episode_reward=-472.93 +/- 71.49
Episode length: 473.07 +/- 71.16
Eval num_timesteps=16500, episode_reward=-347.11 +/- 116.81
Episode length: 347.83 +/- 116.45
Eval num_timesteps=17000, episode_reward=-367.25 +/- 115.69
Episode length: 367.95 +/- 115.35
Eval num_timesteps=17500, episode_reward=-341.97 +/- 105.29
Episode length: 342.75 +/- 104.96
Eval num_timesteps=18000, episode_reward=-302.94 +/- 112.91
Episode length: 303.73 +/- 112.54
Eval num_timesteps=18500, episode_reward=-395.92 +/- 111.11
Episode length: 396.50 +/- 110.72
Eval num_timesteps=19000, episode_reward=-407.34 +/- 113.86
Episode length: 407.82 +/- 113.44
Eval num_timesteps=19500, episode_reward=-400.15 +/- 119.12
Episode length: 400.63 +/- 118.69
Eval num_timesteps=20000, episode_reward=-399.51 +/- 110.44
Episode length: 400.05 +/- 110.03
Eval num_timesteps=20500, episode_reward=-274.95 +/- 168.94
Episode length: 275.62 +/- 168.50
New best mean reward!
Eval num_timesteps=21000, episode_reward=-371.29 +/- 115.26
Episode length: 371.89 +/- 114.81
Eval num_timesteps=21500, episode_reward=-359.12 +/- 117.26
Episode length: 359.78 +/- 116.85
Eval num_timesteps=22000, episode_reward=-222.84 +/- 109.53
Episode length: 223.73 +/- 109.25
New best mean reward!
Eval num_timesteps=22500, episode_reward=-261.37 +/- 125.78
Episode length: 262.19 +/- 125.44
Eval num_timesteps=23000, episode_reward=-206.70 +/- 93.41
Episode length: 207.63 +/- 93.19
New best mean reward!
Eval num_timesteps=23500, episode_reward=-174.76 +/- 49.89
Episode length: 175.76 +/- 49.89
New best mean reward!
Eval num_timesteps=24000, episode_reward=-262.54 +/- 127.93
Episode length: 263.33 +/- 127.54
Eval num_timesteps=24500, episode_reward=-228.09 +/- 114.66
Episode length: 228.97 +/- 114.37
Eval num_timesteps=25000, episode_reward=-237.61 +/- 137.16
Episode length: 238.41 +/- 136.78
Eval num_timesteps=25500, episode_reward=-251.10 +/- 131.82
Episode length: 251.91 +/- 131.46
Eval num_timesteps=26000, episode_reward=-269.74 +/- 147.56
Episode length: 270.46 +/- 147.12
Eval num_timesteps=26500, episode_reward=-442.77 +/- 89.75
Episode length: 443.11 +/- 89.33
Eval num_timesteps=27000, episode_reward=-314.10 +/- 157.82
Episode length: 314.71 +/- 157.36
Eval num_timesteps=27500, episode_reward=-386.00 +/- 147.87
Episode length: 386.39 +/- 147.40
Eval num_timesteps=28000, episode_reward=-457.99 +/- 99.42
Episode length: 458.15 +/- 99.06
Eval num_timesteps=28500, episode_reward=-393.97 +/- 138.08
Episode length: 394.36 +/- 137.61
Eval num_timesteps=29000, episode_reward=-328.75 +/- 159.32
Episode length: 329.30 +/- 158.83
Eval num_timesteps=29500, episode_reward=-324.83 +/- 149.25
Episode length: 325.44 +/- 148.79
Eval num_timesteps=30000, episode_reward=-287.12 +/- 148.15
Episode length: 287.83 +/- 147.73
Eval num_timesteps=30500, episode_reward=-258.46 +/- 152.66
Episode length: 259.18 +/- 152.22
Eval num_timesteps=31000, episode_reward=-269.99 +/- 154.48
Episode length: 270.70 +/- 154.05
Eval num_timesteps=31500, episode_reward=-241.75 +/- 130.78
Episode length: 242.59 +/- 130.46
Eval num_timesteps=32000, episode_reward=-260.44 +/- 142.41
Episode length: 261.20 +/- 142.01
Eval num_timesteps=32500, episode_reward=-232.77 +/- 132.32
Episode length: 233.60 +/- 131.98
Eval num_timesteps=33000, episode_reward=-220.61 +/- 131.99
Episode length: 221.44 +/- 131.63
Eval num_timesteps=33500, episode_reward=-169.46 +/- 75.29
Episode length: 170.43 +/- 75.16
New best mean reward!
Eval num_timesteps=34000, episode_reward=-157.67 +/- 57.25
Episode length: 158.65 +/- 57.13
New best mean reward!
Eval num_timesteps=34500, episode_reward=-145.57 +/- 68.47
Episode length: 146.54 +/- 68.32
New best mean reward!
Eval num_timesteps=35000, episode_reward=-136.58 +/- 41.87
Episode length: 137.58 +/- 41.87
New best mean reward!
Eval num_timesteps=35500, episode_reward=-148.48 +/- 51.57
Episode length: 149.47 +/- 51.50
Eval num_timesteps=36000, episode_reward=-148.08 +/- 32.43
Episode length: 149.08 +/- 32.43
Eval num_timesteps=36500, episode_reward=-169.08 +/- 68.08
Episode length: 170.06 +/- 67.98
Eval num_timesteps=37000, episode_reward=-163.79 +/- 58.52
Episode length: 164.78 +/- 58.46
Eval num_timesteps=37500, episode_reward=-214.49 +/- 125.31
Episode length: 215.35 +/- 124.99
Eval num_timesteps=38000, episode_reward=-241.67 +/- 137.59
Episode length: 242.47 +/- 137.22
Eval num_timesteps=38500, episode_reward=-253.92 +/- 135.54
Episode length: 254.71 +/- 135.16
Eval num_timesteps=39000, episode_reward=-221.81 +/- 110.46
Episode length: 222.71 +/- 110.21
Eval num_timesteps=39500, episode_reward=-190.69 +/- 86.48
Episode length: 191.63 +/- 86.26
Eval num_timesteps=40000, episode_reward=-226.42 +/- 123.55
Episode length: 227.28 +/- 123.24
Eval num_timesteps=40500, episode_reward=-227.82 +/- 129.22
Episode length: 228.67 +/- 128.90
Eval num_timesteps=41000, episode_reward=-233.79 +/- 132.69
Episode length: 234.62 +/- 132.35
Eval num_timesteps=41500, episode_reward=-243.19 +/- 137.53
Episode length: 243.98 +/- 137.14
Eval num_timesteps=42000, episode_reward=-240.04 +/- 128.31
Episode length: 240.87 +/- 127.96
Eval num_timesteps=42500, episode_reward=-217.82 +/- 118.85
Episode length: 218.69 +/- 118.54
Eval num_timesteps=43000, episode_reward=-204.96 +/- 109.15
Episode length: 205.85 +/- 108.85
Eval num_timesteps=43500, episode_reward=-214.15 +/- 122.88
Episode length: 215.01 +/- 122.56
Eval num_timesteps=44000, episode_reward=-190.97 +/- 97.10
Episode length: 191.90 +/- 96.88
Eval num_timesteps=44500, episode_reward=-207.23 +/- 114.21
Episode length: 208.13 +/- 113.95
Eval num_timesteps=45000, episode_reward=-208.80 +/- 115.13
Episode length: 209.68 +/- 114.82
Eval num_timesteps=45500, episode_reward=-211.35 +/- 120.01
Episode length: 212.22 +/- 119.70
Eval num_timesteps=46000, episode_reward=-206.89 +/- 118.02
Episode length: 207.76 +/- 117.70
Eval num_timesteps=46500, episode_reward=-206.50 +/- 119.46
Episode length: 207.37 +/- 119.14
Eval num_timesteps=47000, episode_reward=-193.97 +/- 94.78
Episode length: 194.91 +/- 94.58
Eval num_timesteps=47500, episode_reward=-212.49 +/- 120.41
Episode length: 213.38 +/- 120.14
Eval num_timesteps=48000, episode_reward=-185.44 +/- 92.84
Episode length: 186.37 +/- 92.61
Eval num_timesteps=48500, episode_reward=-201.99 +/- 110.49
Episode length: 202.88 +/- 110.20
Eval num_timesteps=49000, episode_reward=-197.11 +/- 106.37
Episode length: 198.02 +/- 106.12
Eval num_timesteps=49500, episode_reward=-195.16 +/- 99.29
Episode length: 196.08 +/- 99.04
Eval num_timesteps=50000, episode_reward=-213.66 +/- 126.40
Episode length: 214.52 +/- 126.08
FINISHED IN 1014.9657311689807 s


starting seed  10292 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-172.40 +/- 72.18
Episode length: 173.36 +/- 72.00
New best mean reward!
Eval num_timesteps=8000, episode_reward=-256.09 +/- 154.66
Episode length: 256.81 +/- 154.22
Eval num_timesteps=8500, episode_reward=-370.78 +/- 163.66
Episode length: 371.17 +/- 163.18
Eval num_timesteps=9000, episode_reward=-321.31 +/- 167.14
Episode length: 321.85 +/- 166.65
Eval num_timesteps=9500, episode_reward=-271.20 +/- 153.78
Episode length: 271.90 +/- 153.33
Eval num_timesteps=10000, episode_reward=-223.98 +/- 135.98
Episode length: 224.79 +/- 135.59
Eval num_timesteps=10500, episode_reward=-201.47 +/- 114.18
Episode length: 202.35 +/- 113.87
Eval num_timesteps=11000, episode_reward=-273.84 +/- 157.08
Episode length: 274.52 +/- 156.62
Eval num_timesteps=11500, episode_reward=-255.82 +/- 152.64
Episode length: 256.55 +/- 152.21
Eval num_timesteps=12000, episode_reward=-251.26 +/- 144.16
Episode length: 252.02 +/- 143.74
Eval num_timesteps=12500, episode_reward=-171.38 +/- 72.97
Episode length: 172.34 +/- 72.79
New best mean reward!
Eval num_timesteps=13000, episode_reward=-311.10 +/- 164.50
Episode length: 311.68 +/- 164.01
Eval num_timesteps=13500, episode_reward=-186.39 +/- 92.39
Episode length: 187.32 +/- 92.15
Eval num_timesteps=14000, episode_reward=-268.75 +/- 157.64
Episode length: 269.44 +/- 157.18
Eval num_timesteps=14500, episode_reward=-282.78 +/- 156.03
Episode length: 283.46 +/- 155.58
Eval num_timesteps=15000, episode_reward=-284.06 +/- 163.44
Episode length: 284.70 +/- 162.97
Eval num_timesteps=15500, episode_reward=-271.17 +/- 157.74
Episode length: 271.86 +/- 157.30
Eval num_timesteps=16000, episode_reward=-348.47 +/- 162.92
Episode length: 348.95 +/- 162.44
Eval num_timesteps=16500, episode_reward=-211.69 +/- 123.09
Episode length: 212.54 +/- 122.74
Eval num_timesteps=17000, episode_reward=-161.17 +/- 45.30
Episode length: 162.16 +/- 45.23
New best mean reward!
Eval num_timesteps=17500, episode_reward=-165.93 +/- 39.87
Episode length: 166.93 +/- 39.87
Eval num_timesteps=18000, episode_reward=-162.25 +/- 44.29
Episode length: 163.24 +/- 44.22
Eval num_timesteps=18500, episode_reward=-163.08 +/- 34.88
Episode length: 164.08 +/- 34.88
Eval num_timesteps=19000, episode_reward=-166.09 +/- 41.54
Episode length: 167.09 +/- 41.54
Eval num_timesteps=19500, episode_reward=-166.10 +/- 41.39
Episode length: 167.10 +/- 41.39
Eval num_timesteps=20000, episode_reward=-163.89 +/- 35.85
Episode length: 164.89 +/- 35.85
Eval num_timesteps=20500, episode_reward=-167.14 +/- 37.27
Episode length: 168.14 +/- 37.27
Eval num_timesteps=21000, episode_reward=-157.72 +/- 40.02
Episode length: 158.72 +/- 40.02
New best mean reward!
Eval num_timesteps=21500, episode_reward=-157.29 +/- 33.33
Episode length: 158.29 +/- 33.33
New best mean reward!
Eval num_timesteps=22000, episode_reward=-170.52 +/- 51.00
Episode length: 171.51 +/- 50.93
Eval num_timesteps=22500, episode_reward=-164.95 +/- 55.60
Episode length: 165.94 +/- 55.54
Eval num_timesteps=23000, episode_reward=-167.76 +/- 37.69
Episode length: 168.76 +/- 37.69
Eval num_timesteps=23500, episode_reward=-157.91 +/- 32.67
Episode length: 158.91 +/- 32.67
Eval num_timesteps=24000, episode_reward=-164.34 +/- 54.60
Episode length: 165.33 +/- 54.54
Eval num_timesteps=24500, episode_reward=-159.12 +/- 38.57
Episode length: 160.12 +/- 38.57
Eval num_timesteps=25000, episode_reward=-159.77 +/- 43.91
Episode length: 160.76 +/- 43.83
Eval num_timesteps=25500, episode_reward=-162.42 +/- 38.76
Episode length: 163.42 +/- 38.76
Eval num_timesteps=26000, episode_reward=-162.75 +/- 40.32
Episode length: 163.75 +/- 40.32
Eval num_timesteps=26500, episode_reward=-170.11 +/- 59.49
Episode length: 171.10 +/- 59.43
Eval num_timesteps=27000, episode_reward=-159.27 +/- 33.39
Episode length: 160.27 +/- 33.39
Eval num_timesteps=27500, episode_reward=-162.26 +/- 39.46
Episode length: 163.26 +/- 39.46
Eval num_timesteps=28000, episode_reward=-171.86 +/- 61.57
Episode length: 172.84 +/- 61.46
Eval num_timesteps=28500, episode_reward=-165.11 +/- 46.03
Episode length: 166.10 +/- 45.96
Eval num_timesteps=29000, episode_reward=-165.60 +/- 37.58
Episode length: 166.60 +/- 37.58
Eval num_timesteps=29500, episode_reward=-168.84 +/- 42.33
Episode length: 169.84 +/- 42.33
Eval num_timesteps=30000, episode_reward=-157.65 +/- 28.19
Episode length: 158.65 +/- 28.19
Eval num_timesteps=30500, episode_reward=-162.28 +/- 41.27
Episode length: 163.28 +/- 41.27
Eval num_timesteps=31000, episode_reward=-160.95 +/- 31.44
Episode length: 161.95 +/- 31.44
Eval num_timesteps=31500, episode_reward=-169.00 +/- 55.93
Episode length: 169.99 +/- 55.87
Eval num_timesteps=32000, episode_reward=-161.57 +/- 36.14
Episode length: 162.57 +/- 36.14
Eval num_timesteps=32500, episode_reward=-167.16 +/- 39.63
Episode length: 168.16 +/- 39.63
Eval num_timesteps=33000, episode_reward=-165.75 +/- 41.81
Episode length: 166.75 +/- 41.81
Eval num_timesteps=33500, episode_reward=-165.93 +/- 48.95
Episode length: 166.92 +/- 48.88
Eval num_timesteps=34000, episode_reward=-167.24 +/- 50.30
Episode length: 168.23 +/- 50.23
Eval num_timesteps=34500, episode_reward=-168.55 +/- 47.59
Episode length: 169.54 +/- 47.52
Eval num_timesteps=35000, episode_reward=-159.98 +/- 28.82
Episode length: 160.98 +/- 28.82
Eval num_timesteps=35500, episode_reward=-165.20 +/- 37.87
Episode length: 166.20 +/- 37.87
Eval num_timesteps=36000, episode_reward=-161.45 +/- 42.32
Episode length: 162.45 +/- 42.32
Eval num_timesteps=36500, episode_reward=-163.41 +/- 41.69
Episode length: 164.41 +/- 41.69
Eval num_timesteps=37000, episode_reward=-166.21 +/- 40.88
Episode length: 167.21 +/- 40.88
Eval num_timesteps=37500, episode_reward=-170.08 +/- 50.75
Episode length: 171.07 +/- 50.69
Eval num_timesteps=38000, episode_reward=-165.44 +/- 42.80
Episode length: 166.44 +/- 42.80
Eval num_timesteps=38500, episode_reward=-165.17 +/- 36.69
Episode length: 166.17 +/- 36.69
Eval num_timesteps=39000, episode_reward=-163.31 +/- 35.00
Episode length: 164.31 +/- 35.00
Eval num_timesteps=39500, episode_reward=-168.30 +/- 48.40
Episode length: 169.30 +/- 48.40
Eval num_timesteps=40000, episode_reward=-168.21 +/- 34.06
Episode length: 169.21 +/- 34.06
Eval num_timesteps=40500, episode_reward=-160.26 +/- 37.00
Episode length: 161.26 +/- 37.00
Eval num_timesteps=41000, episode_reward=-165.09 +/- 46.13
Episode length: 166.09 +/- 46.13
Eval num_timesteps=41500, episode_reward=-165.39 +/- 51.35
Episode length: 166.38 +/- 51.29
Eval num_timesteps=42000, episode_reward=-166.88 +/- 38.64
Episode length: 167.88 +/- 38.64
Eval num_timesteps=42500, episode_reward=-164.09 +/- 37.25
Episode length: 165.09 +/- 37.25
Eval num_timesteps=43000, episode_reward=-162.65 +/- 41.72
Episode length: 163.65 +/- 41.72
Eval num_timesteps=43500, episode_reward=-159.58 +/- 31.40
Episode length: 160.58 +/- 31.40
Eval num_timesteps=44000, episode_reward=-164.51 +/- 42.37
Episode length: 165.51 +/- 42.37
Eval num_timesteps=44500, episode_reward=-168.48 +/- 51.48
Episode length: 169.47 +/- 51.41
Eval num_timesteps=45000, episode_reward=-161.24 +/- 47.78
Episode length: 162.23 +/- 47.71
Eval num_timesteps=45500, episode_reward=-159.21 +/- 34.87
Episode length: 160.21 +/- 34.87
Eval num_timesteps=46000, episode_reward=-155.26 +/- 30.95
Episode length: 156.26 +/- 30.95
New best mean reward!
Eval num_timesteps=46500, episode_reward=-155.97 +/- 26.30
Episode length: 156.97 +/- 26.30
Eval num_timesteps=47000, episode_reward=-166.78 +/- 41.35
Episode length: 167.78 +/- 41.35
Eval num_timesteps=47500, episode_reward=-164.70 +/- 48.38
Episode length: 165.70 +/- 48.38
Eval num_timesteps=48000, episode_reward=-165.42 +/- 37.37
Episode length: 166.42 +/- 37.37
Eval num_timesteps=48500, episode_reward=-162.37 +/- 34.62
Episode length: 163.37 +/- 34.62
Eval num_timesteps=49000, episode_reward=-164.96 +/- 54.45
Episode length: 165.95 +/- 54.39
Eval num_timesteps=49500, episode_reward=-158.32 +/- 31.02
Episode length: 159.32 +/- 31.02
Eval num_timesteps=50000, episode_reward=-162.89 +/- 44.45
Episode length: 163.89 +/- 44.45
FINISHED IN 820.532699347008 s


starting seed  10293 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-130.92 +/- 121.86
Episode length: 131.83 +/- 121.59
New best mean reward!
Eval num_timesteps=9500, episode_reward=-366.80 +/- 181.29
Episode length: 367.16 +/- 180.82
Eval num_timesteps=10000, episode_reward=-339.59 +/- 192.67
Episode length: 340.00 +/- 192.18
Eval num_timesteps=10500, episode_reward=-85.99 +/- 22.45
Episode length: 86.99 +/- 22.45
New best mean reward!
FINISHED IN 242.09090367000317 s


starting seed  10294 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-135.84 +/- 85.55
Episode length: 136.80 +/- 85.38
New best mean reward!
Eval num_timesteps=14000, episode_reward=-138.38 +/- 97.59
Episode length: 139.32 +/- 97.37
Eval num_timesteps=14500, episode_reward=-143.61 +/- 99.05
Episode length: 144.55 +/- 98.83
Eval num_timesteps=15000, episode_reward=-123.41 +/- 69.40
Episode length: 124.39 +/- 69.29
New best mean reward!
Eval num_timesteps=15500, episode_reward=-120.19 +/- 73.76
Episode length: 121.16 +/- 73.60
New best mean reward!
Eval num_timesteps=16000, episode_reward=-102.14 +/- 40.31
Episode length: 103.14 +/- 40.31
New best mean reward!
Eval num_timesteps=16500, episode_reward=-103.56 +/- 53.44
Episode length: 104.55 +/- 53.36
Eval num_timesteps=17000, episode_reward=-114.39 +/- 53.36
Episode length: 115.38 +/- 53.28
Eval num_timesteps=17500, episode_reward=-100.43 +/- 31.89
Episode length: 101.43 +/- 31.89
New best mean reward!
Eval num_timesteps=18000, episode_reward=-121.26 +/- 86.23
Episode length: 122.23 +/- 86.10
Eval num_timesteps=18500, episode_reward=-107.90 +/- 53.25
Episode length: 108.89 +/- 53.18
Eval num_timesteps=19000, episode_reward=-89.29 +/- 16.74
Episode length: 90.29 +/- 16.74
New best mean reward!
FINISHED IN 370.1861166720046 s


starting seed  10295 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-186.72 +/- 44.35
Episode length: 187.72 +/- 44.35
New best mean reward!
Eval num_timesteps=11000, episode_reward=-175.77 +/- 52.03
Episode length: 176.75 +/- 51.90
New best mean reward!
Eval num_timesteps=11500, episode_reward=-126.09 +/- 24.41
Episode length: 127.09 +/- 24.41
New best mean reward!
Eval num_timesteps=12000, episode_reward=-155.31 +/- 28.98
Episode length: 156.31 +/- 28.98
Eval num_timesteps=12500, episode_reward=-179.82 +/- 32.44
Episode length: 180.82 +/- 32.44
Eval num_timesteps=13000, episode_reward=-190.26 +/- 51.40
Episode length: 191.25 +/- 51.34
Eval num_timesteps=13500, episode_reward=-175.74 +/- 62.59
Episode length: 176.71 +/- 62.43
Eval num_timesteps=14000, episode_reward=-168.78 +/- 28.89
Episode length: 169.78 +/- 28.89
Eval num_timesteps=14500, episode_reward=-163.15 +/- 22.92
Episode length: 164.15 +/- 22.92
Eval num_timesteps=15000, episode_reward=-177.32 +/- 47.12
Episode length: 178.32 +/- 47.12
Eval num_timesteps=15500, episode_reward=-175.32 +/- 36.73
Episode length: 176.32 +/- 36.73
Eval num_timesteps=16000, episode_reward=-137.21 +/- 42.94
Episode length: 138.20 +/- 42.85
Eval num_timesteps=16500, episode_reward=-114.99 +/- 29.25
Episode length: 115.99 +/- 29.25
New best mean reward!
Eval num_timesteps=17000, episode_reward=-117.84 +/- 56.23
Episode length: 118.83 +/- 56.17
Eval num_timesteps=17500, episode_reward=-112.53 +/- 60.26
Episode length: 113.51 +/- 60.14
New best mean reward!
Eval num_timesteps=18000, episode_reward=-148.15 +/- 116.65
Episode length: 149.07 +/- 116.41
Eval num_timesteps=18500, episode_reward=-115.56 +/- 60.38
Episode length: 116.55 +/- 60.32
Eval num_timesteps=19000, episode_reward=-106.10 +/- 41.49
Episode length: 107.10 +/- 41.49
New best mean reward!
Eval num_timesteps=19500, episode_reward=-97.62 +/- 45.26
Episode length: 98.61 +/- 45.17
New best mean reward!
FINISHED IN 345.7746746200137 s


starting seed  10296 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-344.61 +/- 168.64
Episode length: 345.08 +/- 168.15
New best mean reward!
Eval num_timesteps=10000, episode_reward=-439.98 +/- 131.90
Episode length: 440.16 +/- 131.53
Eval num_timesteps=10500, episode_reward=-187.04 +/- 132.22
Episode length: 187.92 +/- 131.94
New best mean reward!
Eval num_timesteps=11000, episode_reward=-120.60 +/- 48.57
Episode length: 121.59 +/- 48.49
New best mean reward!
Eval num_timesteps=11500, episode_reward=-131.47 +/- 76.43
Episode length: 132.44 +/- 76.29
Eval num_timesteps=12000, episode_reward=-133.79 +/- 48.79
Episode length: 134.79 +/- 48.79
Eval num_timesteps=12500, episode_reward=-253.64 +/- 158.73
Episode length: 254.36 +/- 158.29
Eval num_timesteps=13000, episode_reward=-166.57 +/- 83.29
Episode length: 167.52 +/- 83.09
Eval num_timesteps=13500, episode_reward=-144.45 +/- 46.03
Episode length: 145.44 +/- 45.95
Eval num_timesteps=14000, episode_reward=-143.41 +/- 65.75
Episode length: 144.39 +/- 65.64
Eval num_timesteps=14500, episode_reward=-175.13 +/- 92.15
Episode length: 176.08 +/- 91.97
Eval num_timesteps=15000, episode_reward=-132.42 +/- 32.98
Episode length: 133.42 +/- 32.98
Eval num_timesteps=15500, episode_reward=-128.81 +/- 65.53
Episode length: 129.79 +/- 65.42
Eval num_timesteps=16000, episode_reward=-121.29 +/- 47.52
Episode length: 122.28 +/- 47.44
Eval num_timesteps=16500, episode_reward=-116.84 +/- 21.83
Episode length: 117.84 +/- 21.83
New best mean reward!
Eval num_timesteps=17000, episode_reward=-118.43 +/- 48.03
Episode length: 119.42 +/- 47.95
Eval num_timesteps=17500, episode_reward=-105.41 +/- 22.16
Episode length: 106.41 +/- 22.16
New best mean reward!
Eval num_timesteps=18000, episode_reward=-101.63 +/- 52.85
Episode length: 102.62 +/- 52.77
New best mean reward!
Eval num_timesteps=18500, episode_reward=-93.37 +/- 28.34
Episode length: 94.37 +/- 28.34
New best mean reward!
FINISHED IN 397.07233734900365 s


starting seed  10297 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-468.19 +/- 108.10
Episode length: 468.27 +/- 107.83
New best mean reward!
Eval num_timesteps=14000, episode_reward=-397.10 +/- 174.16
Episode length: 397.36 +/- 173.72
New best mean reward!
Eval num_timesteps=14500, episode_reward=-308.39 +/- 203.83
Episode length: 308.86 +/- 203.33
New best mean reward!
Eval num_timesteps=15000, episode_reward=-332.91 +/- 200.77
Episode length: 333.32 +/- 200.28
Eval num_timesteps=15500, episode_reward=-292.17 +/- 201.44
Episode length: 292.69 +/- 200.95
New best mean reward!
Eval num_timesteps=16000, episode_reward=-223.11 +/- 193.31
Episode length: 223.80 +/- 192.87
New best mean reward!
Eval num_timesteps=16500, episode_reward=-213.65 +/- 188.44
Episode length: 214.35 +/- 187.99
New best mean reward!
Eval num_timesteps=17000, episode_reward=-219.36 +/- 189.44
Episode length: 220.05 +/- 188.99
Eval num_timesteps=17500, episode_reward=-212.62 +/- 189.46
Episode length: 213.32 +/- 189.01
New best mean reward!
Eval num_timesteps=18000, episode_reward=-214.42 +/- 192.22
Episode length: 215.11 +/- 191.76
Eval num_timesteps=18500, episode_reward=-185.89 +/- 177.33
Episode length: 186.66 +/- 176.92
New best mean reward!
Eval num_timesteps=19000, episode_reward=-134.57 +/- 130.40
Episode length: 135.46 +/- 130.09
New best mean reward!
Eval num_timesteps=19500, episode_reward=-117.73 +/- 114.58
Episode length: 118.65 +/- 114.31
New best mean reward!
Eval num_timesteps=20000, episode_reward=-116.02 +/- 101.58
Episode length: 116.96 +/- 101.35
New best mean reward!
Eval num_timesteps=20500, episode_reward=-106.44 +/- 92.42
Episode length: 107.39 +/- 92.21
New best mean reward!
Eval num_timesteps=21000, episode_reward=-100.45 +/- 77.90
Episode length: 101.42 +/- 77.75
New best mean reward!
Eval num_timesteps=21500, episode_reward=-97.44 +/- 55.18
Episode length: 98.43 +/- 55.11
New best mean reward!
FINISHED IN 463.20636702200864 s


starting seed  10298 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-492.22 +/- 54.46
Episode length: 492.24 +/- 54.32
New best mean reward!
Eval num_timesteps=2500, episode_reward=-473.57 +/- 96.66
Episode length: 473.64 +/- 96.40
New best mean reward!
Eval num_timesteps=3000, episode_reward=-414.29 +/- 149.73
Episode length: 414.54 +/- 149.30
New best mean reward!
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-425.96 +/- 136.69
Episode length: 426.19 +/- 136.27
Eval num_timesteps=4500, episode_reward=-499.52 +/- 4.78
Episode length: 499.53 +/- 4.68
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-245.17 +/- 126.20
Episode length: 246.00 +/- 125.85
New best mean reward!
Eval num_timesteps=7500, episode_reward=-497.69 +/- 22.98
Episode length: 497.70 +/- 22.88
Eval num_timesteps=8000, episode_reward=-284.87 +/- 94.37
Episode length: 285.77 +/- 94.15
Eval num_timesteps=8500, episode_reward=-222.52 +/- 100.67
Episode length: 223.44 +/- 100.45
New best mean reward!
Eval num_timesteps=9000, episode_reward=-165.72 +/- 95.78
Episode length: 166.66 +/- 95.57
New best mean reward!
Eval num_timesteps=9500, episode_reward=-167.84 +/- 52.32
Episode length: 168.83 +/- 52.26
Eval num_timesteps=10000, episode_reward=-223.40 +/- 52.43
Episode length: 224.40 +/- 52.43
Eval num_timesteps=10500, episode_reward=-184.16 +/- 106.41
Episode length: 185.07 +/- 106.14
Eval num_timesteps=11000, episode_reward=-200.85 +/- 97.45
Episode length: 201.80 +/- 97.30
Eval num_timesteps=11500, episode_reward=-167.12 +/- 112.99
Episode length: 168.03 +/- 112.73
Eval num_timesteps=12000, episode_reward=-171.58 +/- 103.87
Episode length: 172.52 +/- 103.68
Eval num_timesteps=12500, episode_reward=-135.13 +/- 37.21
Episode length: 136.13 +/- 37.21
New best mean reward!
Eval num_timesteps=13000, episode_reward=-132.86 +/- 31.24
Episode length: 133.86 +/- 31.24
New best mean reward!
Eval num_timesteps=13500, episode_reward=-137.12 +/- 96.24
Episode length: 138.06 +/- 96.01
Eval num_timesteps=14000, episode_reward=-175.97 +/- 67.10
Episode length: 176.95 +/- 67.00
Eval num_timesteps=14500, episode_reward=-181.37 +/- 59.44
Episode length: 182.35 +/- 59.34
Eval num_timesteps=15000, episode_reward=-191.56 +/- 74.28
Episode length: 192.52 +/- 74.11
Eval num_timesteps=15500, episode_reward=-174.32 +/- 31.27
Episode length: 175.32 +/- 31.27
Eval num_timesteps=16000, episode_reward=-169.62 +/- 28.35
Episode length: 170.62 +/- 28.35
Eval num_timesteps=16500, episode_reward=-177.17 +/- 65.30
Episode length: 178.14 +/- 65.15
Eval num_timesteps=17000, episode_reward=-178.26 +/- 77.50
Episode length: 179.21 +/- 77.30
Eval num_timesteps=17500, episode_reward=-182.72 +/- 82.63
Episode length: 183.67 +/- 82.44
Eval num_timesteps=18000, episode_reward=-170.39 +/- 58.89
Episode length: 171.37 +/- 58.78
Eval num_timesteps=18500, episode_reward=-163.60 +/- 64.07
Episode length: 164.57 +/- 63.91
Eval num_timesteps=19000, episode_reward=-191.53 +/- 105.44
Episode length: 192.43 +/- 105.15
Eval num_timesteps=19500, episode_reward=-131.51 +/- 87.68
Episode length: 132.46 +/- 87.47
New best mean reward!
Eval num_timesteps=20000, episode_reward=-116.50 +/- 57.36
Episode length: 117.49 +/- 57.29
New best mean reward!
Eval num_timesteps=20500, episode_reward=-96.70 +/- 28.91
Episode length: 97.70 +/- 28.91
New best mean reward!
FINISHED IN 386.3352755610249 s


starting seed  10299 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-389.70 +/- 155.06
Episode length: 390.04 +/- 154.59
New best mean reward!
Eval num_timesteps=1000, episode_reward=-429.25 +/- 137.75
Episode length: 429.47 +/- 137.35
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-353.54 +/- 191.44
Episode length: 353.91 +/- 190.96
New best mean reward!
Eval num_timesteps=8500, episode_reward=-274.29 +/- 197.65
Episode length: 274.86 +/- 197.16
New best mean reward!
Eval num_timesteps=9000, episode_reward=-320.68 +/- 201.22
Episode length: 321.13 +/- 200.73
Eval num_timesteps=9500, episode_reward=-350.22 +/- 196.24
Episode length: 350.59 +/- 195.76
Eval num_timesteps=10000, episode_reward=-264.22 +/- 201.76
Episode length: 264.80 +/- 201.27
New best mean reward!
Eval num_timesteps=10500, episode_reward=-180.19 +/- 171.82
Episode length: 180.97 +/- 171.41
New best mean reward!
Eval num_timesteps=11000, episode_reward=-232.60 +/- 196.85
Episode length: 233.25 +/- 196.37
Eval num_timesteps=11500, episode_reward=-229.33 +/- 192.45
Episode length: 230.01 +/- 192.00
Eval num_timesteps=12000, episode_reward=-208.74 +/- 182.49
Episode length: 209.46 +/- 182.04
Eval num_timesteps=12500, episode_reward=-105.73 +/- 83.46
Episode length: 106.69 +/- 83.27
New best mean reward!
Eval num_timesteps=13000, episode_reward=-92.58 +/- 50.46
Episode length: 93.57 +/- 50.38
New best mean reward!
FINISHED IN 285.8327913369867 s
AVG TIME: 331.47222908398135
