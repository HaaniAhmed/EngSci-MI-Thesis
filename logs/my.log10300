nohup: ignoring input


starting seed  10300 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-395.85 +/- 176.08
Episode length: 396.11 +/- 175.64
New best mean reward!
Eval num_timesteps=10000, episode_reward=-88.46 +/- 26.08
Episode length: 89.46 +/- 26.08
New best mean reward!
FINISHED IN 209.1602951940149 s


starting seed  10301 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-157.99 +/- 151.73
Episode length: 158.83 +/- 151.37
New best mean reward!
Eval num_timesteps=11000, episode_reward=-128.46 +/- 119.72
Episode length: 129.38 +/- 119.47
New best mean reward!
Eval num_timesteps=11500, episode_reward=-133.24 +/- 125.15
Episode length: 134.14 +/- 124.85
Eval num_timesteps=12000, episode_reward=-188.23 +/- 172.76
Episode length: 189.00 +/- 172.34
Eval num_timesteps=12500, episode_reward=-223.21 +/- 191.44
Episode length: 223.89 +/- 190.97
Eval num_timesteps=13000, episode_reward=-277.52 +/- 198.67
Episode length: 278.08 +/- 198.17
Eval num_timesteps=13500, episode_reward=-270.06 +/- 201.22
Episode length: 270.63 +/- 200.72
Eval num_timesteps=14000, episode_reward=-367.40 +/- 189.94
Episode length: 367.73 +/- 189.47
Eval num_timesteps=14500, episode_reward=-312.88 +/- 202.56
Episode length: 313.35 +/- 202.07
Eval num_timesteps=15000, episode_reward=-246.78 +/- 200.21
Episode length: 247.40 +/- 199.73
Eval num_timesteps=15500, episode_reward=-199.61 +/- 183.48
Episode length: 200.34 +/- 183.04
Eval num_timesteps=16000, episode_reward=-222.91 +/- 196.00
Episode length: 223.58 +/- 195.54
Eval num_timesteps=16500, episode_reward=-224.08 +/- 187.30
Episode length: 224.77 +/- 186.85
Eval num_timesteps=17000, episode_reward=-185.23 +/- 173.67
Episode length: 186.00 +/- 173.26
Eval num_timesteps=17500, episode_reward=-154.89 +/- 152.18
Episode length: 155.73 +/- 151.82
Eval num_timesteps=18000, episode_reward=-220.07 +/- 189.76
Episode length: 220.76 +/- 189.30
Eval num_timesteps=18500, episode_reward=-180.99 +/- 168.33
Episode length: 181.79 +/- 167.95
Eval num_timesteps=19000, episode_reward=-165.29 +/- 154.30
Episode length: 166.12 +/- 153.93
Eval num_timesteps=19500, episode_reward=-165.74 +/- 155.13
Episode length: 166.57 +/- 154.76
Eval num_timesteps=20000, episode_reward=-141.80 +/- 129.99
Episode length: 142.70 +/- 129.72
Eval num_timesteps=20500, episode_reward=-151.65 +/- 144.43
Episode length: 152.51 +/- 144.09
Eval num_timesteps=21000, episode_reward=-129.72 +/- 116.90
Episode length: 130.64 +/- 116.65
Eval num_timesteps=21500, episode_reward=-122.85 +/- 108.23
Episode length: 123.78 +/- 107.98
New best mean reward!
Eval num_timesteps=22000, episode_reward=-98.13 +/- 47.31
Episode length: 99.12 +/- 47.23
New best mean reward!
FINISHED IN 442.77693437295966 s


starting seed  10302 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-463.60 +/- 103.66
Episode length: 463.71 +/- 103.34
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-462.21 +/- 107.60
Episode length: 462.32 +/- 107.29
New best mean reward!
Eval num_timesteps=6500, episode_reward=-437.47 +/- 131.08
Episode length: 437.66 +/- 130.69
New best mean reward!
Eval num_timesteps=7000, episode_reward=-243.24 +/- 145.96
Episode length: 244.00 +/- 145.54
New best mean reward!
Eval num_timesteps=7500, episode_reward=-321.55 +/- 165.68
Episode length: 322.10 +/- 165.20
Eval num_timesteps=8000, episode_reward=-313.96 +/- 172.97
Episode length: 314.51 +/- 172.49
Eval num_timesteps=8500, episode_reward=-179.22 +/- 100.07
Episode length: 180.15 +/- 99.85
New best mean reward!
Eval num_timesteps=9000, episode_reward=-233.91 +/- 139.80
Episode length: 234.70 +/- 139.40
Eval num_timesteps=9500, episode_reward=-193.56 +/- 88.19
Episode length: 194.50 +/- 87.98
Eval num_timesteps=10000, episode_reward=-174.15 +/- 53.69
Episode length: 175.14 +/- 53.63
New best mean reward!
Eval num_timesteps=10500, episode_reward=-173.93 +/- 72.96
Episode length: 174.90 +/- 72.82
New best mean reward!
Eval num_timesteps=11000, episode_reward=-165.94 +/- 35.00
Episode length: 166.94 +/- 35.00
New best mean reward!
Eval num_timesteps=11500, episode_reward=-150.76 +/- 22.21
Episode length: 151.76 +/- 22.21
New best mean reward!
Eval num_timesteps=12000, episode_reward=-151.86 +/- 29.36
Episode length: 152.86 +/- 29.36
Eval num_timesteps=12500, episode_reward=-156.45 +/- 33.53
Episode length: 157.45 +/- 33.53
Eval num_timesteps=13000, episode_reward=-151.98 +/- 35.08
Episode length: 152.98 +/- 35.08
Eval num_timesteps=13500, episode_reward=-150.40 +/- 26.99
Episode length: 151.40 +/- 26.99
New best mean reward!
Eval num_timesteps=14000, episode_reward=-154.94 +/- 32.77
Episode length: 155.94 +/- 32.77
Eval num_timesteps=14500, episode_reward=-154.93 +/- 43.88
Episode length: 155.92 +/- 43.80
Eval num_timesteps=15000, episode_reward=-89.46 +/- 46.70
Episode length: 90.45 +/- 46.62
New best mean reward!
FINISHED IN 276.0987028919626 s


starting seed  10303 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-406.82 +/- 126.89
Episode length: 407.20 +/- 126.44
New best mean reward!
Eval num_timesteps=9000, episode_reward=-339.40 +/- 154.18
Episode length: 339.96 +/- 153.73
New best mean reward!
Eval num_timesteps=9500, episode_reward=-129.79 +/- 73.00
Episode length: 130.76 +/- 72.85
New best mean reward!
Eval num_timesteps=10000, episode_reward=-300.03 +/- 151.20
Episode length: 300.70 +/- 150.76
Eval num_timesteps=10500, episode_reward=-210.00 +/- 136.37
Episode length: 210.83 +/- 136.01
Eval num_timesteps=11000, episode_reward=-139.72 +/- 82.87
Episode length: 140.69 +/- 82.74
Eval num_timesteps=11500, episode_reward=-117.75 +/- 45.88
Episode length: 118.75 +/- 45.88
New best mean reward!
Eval num_timesteps=12000, episode_reward=-116.16 +/- 35.31
Episode length: 117.16 +/- 35.31
New best mean reward!
Eval num_timesteps=12500, episode_reward=-117.59 +/- 46.47
Episode length: 118.59 +/- 46.47
Eval num_timesteps=13000, episode_reward=-125.58 +/- 63.90
Episode length: 126.56 +/- 63.78
Eval num_timesteps=13500, episode_reward=-178.63 +/- 148.71
Episode length: 179.46 +/- 148.35
Eval num_timesteps=14000, episode_reward=-238.89 +/- 173.41
Episode length: 239.59 +/- 172.96
Eval num_timesteps=14500, episode_reward=-201.79 +/- 147.79
Episode length: 202.61 +/- 147.43
Eval num_timesteps=15000, episode_reward=-263.22 +/- 164.27
Episode length: 263.92 +/- 163.84
Eval num_timesteps=15500, episode_reward=-266.77 +/- 179.61
Episode length: 267.41 +/- 179.14
Eval num_timesteps=16000, episode_reward=-232.38 +/- 167.37
Episode length: 233.11 +/- 166.94
Eval num_timesteps=16500, episode_reward=-165.33 +/- 122.83
Episode length: 166.22 +/- 122.53
Eval num_timesteps=17000, episode_reward=-140.59 +/- 98.44
Episode length: 141.53 +/- 98.22
Eval num_timesteps=17500, episode_reward=-105.96 +/- 36.26
Episode length: 106.96 +/- 36.26
New best mean reward!
Eval num_timesteps=18000, episode_reward=-119.37 +/- 53.86
Episode length: 120.36 +/- 53.79
Eval num_timesteps=18500, episode_reward=-136.35 +/- 98.00
Episode length: 137.29 +/- 97.78
Eval num_timesteps=19000, episode_reward=-141.55 +/- 99.27
Episode length: 142.49 +/- 99.05
Eval num_timesteps=19500, episode_reward=-123.82 +/- 65.66
Episode length: 124.80 +/- 65.55
Eval num_timesteps=20000, episode_reward=-143.12 +/- 107.36
Episode length: 144.05 +/- 107.13
Eval num_timesteps=20500, episode_reward=-149.08 +/- 116.22
Episode length: 149.99 +/- 115.95
Eval num_timesteps=21000, episode_reward=-169.21 +/- 134.30
Episode length: 170.08 +/- 133.98
Eval num_timesteps=21500, episode_reward=-120.71 +/- 73.84
Episode length: 121.68 +/- 73.68
Eval num_timesteps=22000, episode_reward=-96.98 +/- 27.97
Episode length: 97.98 +/- 27.97
New best mean reward!
FINISHED IN 507.5561582240043 s


starting seed  10304 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-411.00 +/- 137.97
Episode length: 411.30 +/- 137.52
New best mean reward!
Eval num_timesteps=11500, episode_reward=-426.82 +/- 136.38
Episode length: 427.05 +/- 135.97
Eval num_timesteps=12000, episode_reward=-399.70 +/- 157.77
Episode length: 399.99 +/- 157.32
New best mean reward!
Eval num_timesteps=12500, episode_reward=-342.63 +/- 165.22
Episode length: 343.12 +/- 164.73
New best mean reward!
Eval num_timesteps=13000, episode_reward=-374.55 +/- 139.33
Episode length: 375.02 +/- 138.85
Eval num_timesteps=13500, episode_reward=-314.09 +/- 150.99
Episode length: 314.71 +/- 150.52
New best mean reward!
Eval num_timesteps=14000, episode_reward=-329.46 +/- 164.58
Episode length: 329.99 +/- 164.10
Eval num_timesteps=14500, episode_reward=-181.30 +/- 167.68
Episode length: 182.10 +/- 167.30
New best mean reward!
Eval num_timesteps=15000, episode_reward=-168.58 +/- 158.01
Episode length: 169.40 +/- 157.64
New best mean reward!
Eval num_timesteps=15500, episode_reward=-163.26 +/- 155.30
Episode length: 164.10 +/- 154.95
New best mean reward!
Eval num_timesteps=16000, episode_reward=-200.72 +/- 177.88
Episode length: 201.47 +/- 177.46
Eval num_timesteps=16500, episode_reward=-110.53 +/- 74.47
Episode length: 111.50 +/- 74.32
New best mean reward!
Eval num_timesteps=17000, episode_reward=-167.71 +/- 157.79
Episode length: 168.53 +/- 157.41
Eval num_timesteps=17500, episode_reward=-98.65 +/- 65.27
Episode length: 99.64 +/- 65.21
New best mean reward!
FINISHED IN 371.14434452401474 s


starting seed  10305 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=17000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=17500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=18000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=18500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=19000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=19500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=20000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=20500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=21000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=21500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=22000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=22500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=23000, episode_reward=-324.07 +/- 203.68
Episode length: 324.51 +/- 203.20
New best mean reward!
Eval num_timesteps=23500, episode_reward=-218.18 +/- 191.18
Episode length: 218.87 +/- 190.73
New best mean reward!
Eval num_timesteps=24000, episode_reward=-214.41 +/- 188.97
Episode length: 215.11 +/- 188.52
New best mean reward!
Eval num_timesteps=24500, episode_reward=-226.45 +/- 194.28
Episode length: 227.13 +/- 193.83
Eval num_timesteps=25000, episode_reward=-170.58 +/- 164.16
Episode length: 171.39 +/- 163.78
New best mean reward!
Eval num_timesteps=25500, episode_reward=-169.56 +/- 166.73
Episode length: 170.36 +/- 166.34
New best mean reward!
Eval num_timesteps=26000, episode_reward=-153.67 +/- 151.57
Episode length: 154.52 +/- 151.22
New best mean reward!
Eval num_timesteps=26500, episode_reward=-113.55 +/- 103.98
Episode length: 114.49 +/- 103.76
New best mean reward!
Eval num_timesteps=27000, episode_reward=-105.16 +/- 81.16
Episode length: 106.13 +/- 81.01
New best mean reward!
Eval num_timesteps=27500, episode_reward=-93.28 +/- 60.14
Episode length: 94.27 +/- 60.07
New best mean reward!
FINISHED IN 773.3104407060309 s


starting seed  10306 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-496.00 +/- 39.80
Episode length: 496.01 +/- 39.70
New best mean reward!
Eval num_timesteps=2500, episode_reward=-265.02 +/- 204.72
Episode length: 265.59 +/- 204.22
New best mean reward!
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-484.48 +/- 67.94
Episode length: 484.53 +/- 67.73
Eval num_timesteps=7500, episode_reward=-263.74 +/- 149.28
Episode length: 264.47 +/- 148.86
New best mean reward!
Eval num_timesteps=8000, episode_reward=-247.26 +/- 74.95
Episode length: 248.19 +/- 74.71
New best mean reward!
Eval num_timesteps=8500, episode_reward=-484.27 +/- 43.50
Episode length: 484.43 +/- 43.20
Eval num_timesteps=9000, episode_reward=-324.59 +/- 51.98
Episode length: 325.59 +/- 51.98
Eval num_timesteps=9500, episode_reward=-332.09 +/- 86.61
Episode length: 332.98 +/- 86.39
Eval num_timesteps=10000, episode_reward=-484.89 +/- 36.56
Episode length: 485.11 +/- 36.24
Eval num_timesteps=10500, episode_reward=-267.15 +/- 47.70
Episode length: 268.15 +/- 47.70
Eval num_timesteps=11000, episode_reward=-494.69 +/- 33.26
Episode length: 494.72 +/- 33.11
Eval num_timesteps=11500, episode_reward=-495.34 +/- 19.78
Episode length: 495.41 +/- 19.56
Eval num_timesteps=12000, episode_reward=-408.48 +/- 67.02
Episode length: 409.28 +/- 66.75
Eval num_timesteps=12500, episode_reward=-417.28 +/- 86.02
Episode length: 417.91 +/- 85.67
Eval num_timesteps=13000, episode_reward=-307.45 +/- 77.86
Episode length: 308.37 +/- 77.66
Eval num_timesteps=13500, episode_reward=-206.79 +/- 25.91
Episode length: 207.79 +/- 25.91
New best mean reward!
Eval num_timesteps=14000, episode_reward=-211.60 +/- 75.50
Episode length: 212.55 +/- 75.31
Eval num_timesteps=14500, episode_reward=-140.61 +/- 39.26
Episode length: 141.61 +/- 39.26
New best mean reward!
Eval num_timesteps=15000, episode_reward=-194.57 +/- 60.79
Episode length: 195.55 +/- 60.69
Eval num_timesteps=15500, episode_reward=-171.93 +/- 29.86
Episode length: 172.93 +/- 29.86
Eval num_timesteps=16000, episode_reward=-223.32 +/- 105.99
Episode length: 224.20 +/- 105.68
Eval num_timesteps=16500, episode_reward=-203.23 +/- 61.07
Episode length: 204.20 +/- 60.93
Eval num_timesteps=17000, episode_reward=-209.32 +/- 40.34
Episode length: 210.32 +/- 40.34
Eval num_timesteps=17500, episode_reward=-129.65 +/- 21.71
Episode length: 130.65 +/- 21.71
New best mean reward!
Eval num_timesteps=18000, episode_reward=-141.05 +/- 42.08
Episode length: 142.05 +/- 42.08
Eval num_timesteps=18500, episode_reward=-114.80 +/- 17.13
Episode length: 115.80 +/- 17.13
New best mean reward!
Eval num_timesteps=19000, episode_reward=-112.27 +/- 29.01
Episode length: 113.27 +/- 29.01
New best mean reward!
Eval num_timesteps=19500, episode_reward=-111.42 +/- 20.31
Episode length: 112.42 +/- 20.31
New best mean reward!
Eval num_timesteps=20000, episode_reward=-125.21 +/- 70.28
Episode length: 126.18 +/- 70.12
Eval num_timesteps=20500, episode_reward=-101.34 +/- 27.15
Episode length: 102.34 +/- 27.15
New best mean reward!
Eval num_timesteps=21000, episode_reward=-105.61 +/- 62.64
Episode length: 106.59 +/- 62.52
Eval num_timesteps=21500, episode_reward=-100.47 +/- 30.21
Episode length: 101.47 +/- 30.21
New best mean reward!
Eval num_timesteps=22000, episode_reward=-100.46 +/- 31.16
Episode length: 101.46 +/- 31.16
New best mean reward!
Eval num_timesteps=22500, episode_reward=-100.67 +/- 29.11
Episode length: 101.67 +/- 29.11
Eval num_timesteps=23000, episode_reward=-96.37 +/- 35.71
Episode length: 97.37 +/- 35.71
New best mean reward!
FINISHED IN 378.00187476497376 s


starting seed  10307 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-310.43 +/- 168.68
Episode length: 310.99 +/- 168.19
New best mean reward!
Eval num_timesteps=1000, episode_reward=-339.97 +/- 166.34
Episode length: 340.46 +/- 165.85
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-310.31 +/- 180.55
Episode length: 310.84 +/- 180.06
New best mean reward!
Eval num_timesteps=2500, episode_reward=-164.09 +/- 64.98
Episode length: 165.07 +/- 64.88
New best mean reward!
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-472.83 +/- 99.08
Episode length: 472.90 +/- 98.83
Eval num_timesteps=8000, episode_reward=-295.81 +/- 200.47
Episode length: 296.32 +/- 199.97
Eval num_timesteps=8500, episode_reward=-171.52 +/- 165.23
Episode length: 172.32 +/- 164.84
Eval num_timesteps=9000, episode_reward=-194.70 +/- 177.26
Episode length: 195.45 +/- 176.83
Eval num_timesteps=9500, episode_reward=-170.70 +/- 165.95
Episode length: 171.50 +/- 165.55
Eval num_timesteps=10000, episode_reward=-177.96 +/- 167.78
Episode length: 178.75 +/- 167.38
Eval num_timesteps=10500, episode_reward=-183.39 +/- 176.37
Episode length: 184.16 +/- 175.96
Eval num_timesteps=11000, episode_reward=-245.18 +/- 196.50
Episode length: 245.81 +/- 196.02
Eval num_timesteps=11500, episode_reward=-178.76 +/- 171.42
Episode length: 179.54 +/- 171.01
Eval num_timesteps=12000, episode_reward=-137.97 +/- 127.77
Episode length: 138.87 +/- 127.49
New best mean reward!
Eval num_timesteps=12500, episode_reward=-151.57 +/- 148.05
Episode length: 152.42 +/- 147.69
Eval num_timesteps=13000, episode_reward=-184.10 +/- 171.55
Episode length: 184.88 +/- 171.14
Eval num_timesteps=13500, episode_reward=-173.41 +/- 160.43
Episode length: 174.23 +/- 160.07
Eval num_timesteps=14000, episode_reward=-119.63 +/- 110.98
Episode length: 120.56 +/- 110.74
New best mean reward!
Eval num_timesteps=14500, episode_reward=-89.41 +/- 46.07
Episode length: 90.40 +/- 45.98
New best mean reward!
FINISHED IN 273.5664024790167 s


starting seed  10308 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-269.99 +/- 182.99
Episode length: 270.61 +/- 182.51
New best mean reward!
Eval num_timesteps=9500, episode_reward=-90.13 +/- 32.89
Episode length: 91.13 +/- 32.89
New best mean reward!
FINISHED IN 291.85957966098795 s


starting seed  10309 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-438.52 +/- 141.36
Episode length: 438.68 +/- 141.00
New best mean reward!
Eval num_timesteps=6000, episode_reward=-110.20 +/- 91.43
Episode length: 111.17 +/- 91.31
New best mean reward!
Eval num_timesteps=6500, episode_reward=-176.64 +/- 159.45
Episode length: 177.45 +/- 159.07
Eval num_timesteps=7000, episode_reward=-90.94 +/- 31.64
Episode length: 91.94 +/- 31.64
New best mean reward!
FINISHED IN 163.2303103149752 s


starting seed  10310 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-401.61 +/- 171.94
Episode length: 401.86 +/- 171.51
New best mean reward!
Eval num_timesteps=8000, episode_reward=-237.29 +/- 193.69
Episode length: 237.94 +/- 193.22
New best mean reward!
Eval num_timesteps=8500, episode_reward=-238.74 +/- 198.00
Episode length: 239.38 +/- 197.53
Eval num_timesteps=9000, episode_reward=-292.71 +/- 207.83
Episode length: 293.21 +/- 207.33
Eval num_timesteps=9500, episode_reward=-476.51 +/- 93.20
Episode length: 476.57 +/- 92.97
Eval num_timesteps=10000, episode_reward=-439.48 +/- 145.04
Episode length: 439.63 +/- 144.68
Eval num_timesteps=10500, episode_reward=-407.75 +/- 169.12
Episode length: 407.98 +/- 168.70
Eval num_timesteps=11000, episode_reward=-436.08 +/- 146.82
Episode length: 436.24 +/- 146.46
Eval num_timesteps=11500, episode_reward=-414.42 +/- 166.13
Episode length: 414.63 +/- 165.72
Eval num_timesteps=12000, episode_reward=-436.60 +/- 146.03
Episode length: 436.76 +/- 145.67
Eval num_timesteps=12500, episode_reward=-389.59 +/- 181.78
Episode length: 389.86 +/- 181.34
Eval num_timesteps=13000, episode_reward=-297.33 +/- 203.76
Episode length: 297.83 +/- 203.26
Eval num_timesteps=13500, episode_reward=-163.24 +/- 158.83
Episode length: 164.06 +/- 158.45
New best mean reward!
Eval num_timesteps=14000, episode_reward=-136.28 +/- 129.91
Episode length: 137.17 +/- 129.60
New best mean reward!
Eval num_timesteps=14500, episode_reward=-136.21 +/- 135.51
Episode length: 137.09 +/- 135.19
New best mean reward!
Eval num_timesteps=15000, episode_reward=-110.58 +/- 93.18
Episode length: 111.53 +/- 92.97
New best mean reward!
Eval num_timesteps=15500, episode_reward=-94.28 +/- 72.82
Episode length: 95.25 +/- 72.65
New best mean reward!
FINISHED IN 418.4897305580089 s


starting seed  10311 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-355.86 +/- 190.47
Episode length: 356.23 +/- 189.99
New best mean reward!
Eval num_timesteps=4500, episode_reward=-130.09 +/- 26.93
Episode length: 131.09 +/- 26.93
New best mean reward!
Eval num_timesteps=5000, episode_reward=-141.73 +/- 48.45
Episode length: 142.73 +/- 48.45
Eval num_timesteps=5500, episode_reward=-146.31 +/- 32.17
Episode length: 147.31 +/- 32.17
Eval num_timesteps=6000, episode_reward=-153.54 +/- 34.32
Episode length: 154.54 +/- 34.32
Eval num_timesteps=6500, episode_reward=-147.44 +/- 39.33
Episode length: 148.44 +/- 39.33
Eval num_timesteps=7000, episode_reward=-153.87 +/- 25.00
Episode length: 154.87 +/- 25.00
Eval num_timesteps=7500, episode_reward=-483.73 +/- 70.99
Episode length: 483.78 +/- 70.77
Eval num_timesteps=8000, episode_reward=-157.09 +/- 28.59
Episode length: 158.09 +/- 28.59
Eval num_timesteps=8500, episode_reward=-135.58 +/- 22.24
Episode length: 136.58 +/- 22.24
Eval num_timesteps=9000, episode_reward=-127.51 +/- 23.19
Episode length: 128.51 +/- 23.19
New best mean reward!
Eval num_timesteps=9500, episode_reward=-131.77 +/- 20.72
Episode length: 132.77 +/- 20.72
Eval num_timesteps=10000, episode_reward=-85.22 +/- 28.76
Episode length: 86.22 +/- 28.76
New best mean reward!
FINISHED IN 210.56182077800622 s


starting seed  10312 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-483.82 +/- 71.68
Episode length: 483.87 +/- 71.47
New best mean reward!
Eval num_timesteps=11000, episode_reward=-449.62 +/- 131.04
Episode length: 449.75 +/- 130.70
New best mean reward!
Eval num_timesteps=11500, episode_reward=-145.37 +/- 132.47
Episode length: 146.26 +/- 132.18
New best mean reward!
Eval num_timesteps=12000, episode_reward=-126.35 +/- 119.98
Episode length: 127.26 +/- 119.70
New best mean reward!
Eval num_timesteps=12500, episode_reward=-85.89 +/- 30.49
Episode length: 86.89 +/- 30.49
New best mean reward!
FINISHED IN 349.7596400430193 s


starting seed  10313 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-161.92 +/- 83.77
Episode length: 162.88 +/- 83.60
New best mean reward!
Eval num_timesteps=9000, episode_reward=-115.50 +/- 50.18
Episode length: 116.49 +/- 50.11
New best mean reward!
Eval num_timesteps=9500, episode_reward=-144.56 +/- 99.25
Episode length: 145.51 +/- 99.07
Eval num_timesteps=10000, episode_reward=-246.20 +/- 149.82
Episode length: 246.96 +/- 149.41
Eval num_timesteps=10500, episode_reward=-157.91 +/- 113.88
Episode length: 158.82 +/- 113.61
Eval num_timesteps=11000, episode_reward=-148.91 +/- 118.07
Episode length: 149.82 +/- 117.80
Eval num_timesteps=11500, episode_reward=-97.09 +/- 46.71
Episode length: 98.08 +/- 46.63
New best mean reward!
FINISHED IN 262.97924028499983 s


starting seed  10314 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-177.28 +/- 37.43
Episode length: 178.28 +/- 37.43
New best mean reward!
Eval num_timesteps=1000, episode_reward=-421.24 +/- 131.15
Episode length: 421.55 +/- 130.73
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-360.00 +/- 102.87
Episode length: 360.75 +/- 102.53
Eval num_timesteps=3000, episode_reward=-181.26 +/- 37.79
Episode length: 182.26 +/- 37.79
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-327.88 +/- 154.79
Episode length: 328.45 +/- 154.31
Eval num_timesteps=8500, episode_reward=-226.37 +/- 121.17
Episode length: 227.23 +/- 120.85
Eval num_timesteps=9000, episode_reward=-305.67 +/- 162.18
Episode length: 306.27 +/- 161.70
Eval num_timesteps=9500, episode_reward=-393.06 +/- 149.22
Episode length: 393.41 +/- 148.76
Eval num_timesteps=10000, episode_reward=-230.02 +/- 129.91
Episode length: 230.85 +/- 129.55
Eval num_timesteps=10500, episode_reward=-237.31 +/- 133.46
Episode length: 238.13 +/- 133.11
Eval num_timesteps=11000, episode_reward=-273.60 +/- 157.86
Episode length: 274.30 +/- 157.43
Eval num_timesteps=11500, episode_reward=-259.76 +/- 152.54
Episode length: 260.48 +/- 152.09
Eval num_timesteps=12000, episode_reward=-312.97 +/- 168.03
Episode length: 313.54 +/- 167.55
Eval num_timesteps=12500, episode_reward=-257.65 +/- 151.77
Episode length: 258.38 +/- 151.34
Eval num_timesteps=13000, episode_reward=-240.10 +/- 140.97
Episode length: 240.88 +/- 140.56
Eval num_timesteps=13500, episode_reward=-242.84 +/- 143.50
Episode length: 243.61 +/- 143.08
Eval num_timesteps=14000, episode_reward=-294.74 +/- 165.16
Episode length: 295.36 +/- 164.68
Eval num_timesteps=14500, episode_reward=-222.21 +/- 117.65
Episode length: 223.11 +/- 117.42
Eval num_timesteps=15000, episode_reward=-230.50 +/- 138.26
Episode length: 231.32 +/- 137.91
Eval num_timesteps=15500, episode_reward=-188.29 +/- 98.01
Episode length: 189.22 +/- 97.79
Eval num_timesteps=16000, episode_reward=-229.40 +/- 121.37
Episode length: 230.28 +/- 121.10
Eval num_timesteps=16500, episode_reward=-194.48 +/- 92.54
Episode length: 195.42 +/- 92.34
Eval num_timesteps=17000, episode_reward=-182.12 +/- 83.16
Episode length: 183.08 +/- 83.01
Eval num_timesteps=17500, episode_reward=-186.73 +/- 82.55
Episode length: 187.69 +/- 82.40
Eval num_timesteps=18000, episode_reward=-155.95 +/- 56.27
Episode length: 156.93 +/- 56.15
New best mean reward!
Eval num_timesteps=18500, episode_reward=-151.54 +/- 44.68
Episode length: 152.54 +/- 44.68
New best mean reward!
Eval num_timesteps=19000, episode_reward=-155.90 +/- 38.86
Episode length: 156.90 +/- 38.86
Eval num_timesteps=19500, episode_reward=-150.71 +/- 41.09
Episode length: 151.71 +/- 41.09
New best mean reward!
Eval num_timesteps=20000, episode_reward=-154.96 +/- 50.74
Episode length: 155.95 +/- 50.67
Eval num_timesteps=20500, episode_reward=-170.31 +/- 68.23
Episode length: 171.29 +/- 68.13
Eval num_timesteps=21000, episode_reward=-124.33 +/- 38.33
Episode length: 125.33 +/- 38.33
New best mean reward!
Eval num_timesteps=21500, episode_reward=-126.36 +/- 32.58
Episode length: 127.36 +/- 32.58
Eval num_timesteps=22000, episode_reward=-124.70 +/- 29.42
Episode length: 125.70 +/- 29.42
Eval num_timesteps=22500, episode_reward=-127.11 +/- 34.34
Episode length: 128.11 +/- 34.34
Eval num_timesteps=23000, episode_reward=-123.25 +/- 27.91
Episode length: 124.25 +/- 27.91
New best mean reward!
Eval num_timesteps=23500, episode_reward=-128.74 +/- 52.39
Episode length: 129.73 +/- 52.32
Eval num_timesteps=24000, episode_reward=-112.14 +/- 24.51
Episode length: 113.14 +/- 24.51
New best mean reward!
Eval num_timesteps=24500, episode_reward=-101.42 +/- 38.34
Episode length: 102.42 +/- 38.34
New best mean reward!
Eval num_timesteps=25000, episode_reward=-93.96 +/- 23.92
Episode length: 94.96 +/- 23.92
New best mean reward!
FINISHED IN 410.0750316969934 s


starting seed  10315 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-492.25 +/- 54.25
Episode length: 492.27 +/- 54.11
New best mean reward!
Eval num_timesteps=9500, episode_reward=-381.66 +/- 181.51
Episode length: 381.96 +/- 181.05
New best mean reward!
Eval num_timesteps=10000, episode_reward=-276.01 +/- 203.32
Episode length: 276.57 +/- 202.84
New best mean reward!
Eval num_timesteps=10500, episode_reward=-303.96 +/- 205.37
Episode length: 304.44 +/- 204.88
Eval num_timesteps=11000, episode_reward=-162.96 +/- 156.84
Episode length: 163.79 +/- 156.47
New best mean reward!
Eval num_timesteps=11500, episode_reward=-175.74 +/- 162.78
Episode length: 176.55 +/- 162.40
Eval num_timesteps=12000, episode_reward=-138.22 +/- 136.42
Episode length: 139.10 +/- 136.10
New best mean reward!
Eval num_timesteps=12500, episode_reward=-107.82 +/- 83.81
Episode length: 108.79 +/- 83.67
New best mean reward!
Eval num_timesteps=13000, episode_reward=-88.01 +/- 45.23
Episode length: 89.00 +/- 45.14
New best mean reward!
FINISHED IN 282.1772163390415 s


starting seed  10316 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-102.94 +/- 32.26
Episode length: 103.94 +/- 32.26
New best mean reward!
Eval num_timesteps=6000, episode_reward=-166.35 +/- 87.46
Episode length: 167.30 +/- 87.27
Eval num_timesteps=6500, episode_reward=-493.26 +/- 21.86
Episode length: 493.42 +/- 21.60
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-120.17 +/- 28.69
Episode length: 121.17 +/- 28.69
Eval num_timesteps=8000, episode_reward=-123.00 +/- 26.02
Episode length: 124.00 +/- 26.02
Eval num_timesteps=8500, episode_reward=-209.58 +/- 109.24
Episode length: 210.48 +/- 108.97
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-367.62 +/- 119.28
Episode length: 368.21 +/- 118.82
Eval num_timesteps=10000, episode_reward=-463.07 +/- 62.20
Episode length: 463.39 +/- 61.80
Eval num_timesteps=10500, episode_reward=-391.42 +/- 98.08
Episode length: 392.03 +/- 97.65
Eval num_timesteps=11000, episode_reward=-129.07 +/- 33.30
Episode length: 130.07 +/- 33.30
Eval num_timesteps=11500, episode_reward=-149.47 +/- 56.25
Episode length: 150.46 +/- 56.19
Eval num_timesteps=12000, episode_reward=-138.09 +/- 39.21
Episode length: 139.09 +/- 39.21
Eval num_timesteps=12500, episode_reward=-127.49 +/- 57.78
Episode length: 128.48 +/- 57.72
Eval num_timesteps=13000, episode_reward=-144.52 +/- 46.11
Episode length: 145.52 +/- 46.11
Eval num_timesteps=13500, episode_reward=-85.37 +/- 24.88
Episode length: 86.37 +/- 24.88
New best mean reward!
FINISHED IN 244.7147549400106 s


starting seed  10317 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-436.39 +/- 141.50
Episode length: 436.56 +/- 141.13
New best mean reward!
Eval num_timesteps=8500, episode_reward=-120.81 +/- 84.81
Episode length: 121.77 +/- 84.63
New best mean reward!
Eval num_timesteps=9000, episode_reward=-146.60 +/- 127.31
Episode length: 147.49 +/- 127.00
Eval num_timesteps=9500, episode_reward=-95.93 +/- 31.57
Episode length: 96.93 +/- 31.57
New best mean reward!
FINISHED IN 211.56810242001666 s


starting seed  10318 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-258.43 +/- 153.95
Episode length: 259.15 +/- 153.51
New best mean reward!
Eval num_timesteps=1500, episode_reward=-464.93 +/- 99.99
Episode length: 465.04 +/- 99.68
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-467.93 +/- 108.85
Episode length: 468.01 +/- 108.58
Eval num_timesteps=13000, episode_reward=-361.48 +/- 183.36
Episode length: 361.86 +/- 182.89
Eval num_timesteps=13500, episode_reward=-144.77 +/- 126.55
Episode length: 145.66 +/- 126.24
New best mean reward!
Eval num_timesteps=14000, episode_reward=-150.18 +/- 132.34
Episode length: 151.06 +/- 132.02
Eval num_timesteps=14500, episode_reward=-125.11 +/- 106.31
Episode length: 126.04 +/- 106.07
New best mean reward!
Eval num_timesteps=15000, episode_reward=-123.01 +/- 99.25
Episode length: 123.95 +/- 99.02
New best mean reward!
Eval num_timesteps=15500, episode_reward=-114.41 +/- 83.18
Episode length: 115.37 +/- 82.99
New best mean reward!
Eval num_timesteps=16000, episode_reward=-125.53 +/- 111.08
Episode length: 126.47 +/- 110.88
Eval num_timesteps=16500, episode_reward=-94.73 +/- 37.59
Episode length: 95.73 +/- 37.59
New best mean reward!
FINISHED IN 373.3581454830128 s


starting seed  10319 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-440.73 +/- 141.22
Episode length: 440.88 +/- 140.87
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-429.71 +/- 150.99
Episode length: 429.89 +/- 150.61
New best mean reward!
Eval num_timesteps=8500, episode_reward=-209.69 +/- 183.23
Episode length: 210.43 +/- 182.82
New best mean reward!
Eval num_timesteps=9000, episode_reward=-252.83 +/- 202.38
Episode length: 253.43 +/- 201.89
Eval num_timesteps=9500, episode_reward=-169.01 +/- 160.89
Episode length: 169.83 +/- 160.52
New best mean reward!
Eval num_timesteps=10000, episode_reward=-255.63 +/- 194.10
Episode length: 256.26 +/- 193.64
Eval num_timesteps=10500, episode_reward=-243.91 +/- 191.63
Episode length: 244.56 +/- 191.16
Eval num_timesteps=11000, episode_reward=-436.37 +/- 141.68
Episode length: 436.54 +/- 141.31
Eval num_timesteps=11500, episode_reward=-290.33 +/- 193.80
Episode length: 290.88 +/- 193.31
Eval num_timesteps=12000, episode_reward=-373.89 +/- 184.05
Episode length: 374.21 +/- 183.58
Eval num_timesteps=12500, episode_reward=-496.42 +/- 35.62
Episode length: 496.43 +/- 35.52
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-496.02 +/- 39.60
Episode length: 496.03 +/- 39.50
Eval num_timesteps=16000, episode_reward=-357.24 +/- 190.79
Episode length: 357.60 +/- 190.31
Eval num_timesteps=16500, episode_reward=-290.66 +/- 202.42
Episode length: 291.18 +/- 201.93
Eval num_timesteps=17000, episode_reward=-206.83 +/- 181.10
Episode length: 207.56 +/- 180.66
Eval num_timesteps=17500, episode_reward=-168.39 +/- 154.73
Episode length: 169.22 +/- 154.37
New best mean reward!
Eval num_timesteps=18000, episode_reward=-128.61 +/- 125.10
Episode length: 129.51 +/- 124.80
New best mean reward!
Eval num_timesteps=18500, episode_reward=-184.77 +/- 162.25
Episode length: 185.58 +/- 161.88
Eval num_timesteps=19000, episode_reward=-101.83 +/- 54.30
Episode length: 102.82 +/- 54.22
New best mean reward!
Eval num_timesteps=19500, episode_reward=-91.50 +/- 29.52
Episode length: 92.50 +/- 29.52
New best mean reward!
FINISHED IN 417.6025138410041 s


starting seed  10320 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-91.70 +/- 40.22
Episode length: 92.70 +/- 40.22
New best mean reward!
FINISHED IN 263.3422016840195 s


starting seed  10321 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-124.53 +/- 81.46
Episode length: 125.49 +/- 81.28
New best mean reward!
Eval num_timesteps=12500, episode_reward=-168.56 +/- 141.98
Episode length: 169.41 +/- 141.63
Eval num_timesteps=13000, episode_reward=-185.38 +/- 158.59
Episode length: 186.19 +/- 158.21
Eval num_timesteps=13500, episode_reward=-105.82 +/- 48.83
Episode length: 106.81 +/- 48.75
New best mean reward!
Eval num_timesteps=14000, episode_reward=-103.57 +/- 25.71
Episode length: 104.57 +/- 25.71
New best mean reward!
Eval num_timesteps=14500, episode_reward=-95.71 +/- 20.31
Episode length: 96.71 +/- 20.31
New best mean reward!
FINISHED IN 332.6339843070018 s


starting seed  10322 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-429.63 +/- 137.06
Episode length: 429.84 +/- 136.66
New best mean reward!
Eval num_timesteps=13000, episode_reward=-238.28 +/- 134.45
Episode length: 239.09 +/- 134.08
New best mean reward!
Eval num_timesteps=13500, episode_reward=-308.43 +/- 161.53
Episode length: 309.02 +/- 161.04
Eval num_timesteps=14000, episode_reward=-284.66 +/- 152.57
Episode length: 285.36 +/- 152.14
Eval num_timesteps=14500, episode_reward=-408.52 +/- 147.33
Episode length: 408.80 +/- 146.88
Eval num_timesteps=15000, episode_reward=-368.25 +/- 161.06
Episode length: 368.68 +/- 160.59
Eval num_timesteps=15500, episode_reward=-338.66 +/- 164.53
Episode length: 339.16 +/- 164.04
Eval num_timesteps=16000, episode_reward=-313.35 +/- 162.70
Episode length: 313.94 +/- 162.23
Eval num_timesteps=16500, episode_reward=-264.50 +/- 147.33
Episode length: 265.23 +/- 146.90
Eval num_timesteps=17000, episode_reward=-307.80 +/- 164.75
Episode length: 308.38 +/- 164.26
Eval num_timesteps=17500, episode_reward=-395.35 +/- 153.58
Episode length: 395.67 +/- 153.11
Eval num_timesteps=18000, episode_reward=-301.76 +/- 158.57
Episode length: 302.39 +/- 158.10
Eval num_timesteps=18500, episode_reward=-220.98 +/- 110.79
Episode length: 221.87 +/- 110.51
New best mean reward!
Eval num_timesteps=19000, episode_reward=-225.58 +/- 122.66
Episode length: 226.43 +/- 122.33
Eval num_timesteps=19500, episode_reward=-191.58 +/- 71.40
Episode length: 192.55 +/- 71.27
New best mean reward!
Eval num_timesteps=20000, episode_reward=-225.62 +/- 125.98
Episode length: 226.47 +/- 125.65
Eval num_timesteps=20500, episode_reward=-215.75 +/- 115.65
Episode length: 216.62 +/- 115.33
Eval num_timesteps=21000, episode_reward=-245.32 +/- 134.67
Episode length: 246.12 +/- 134.29
Eval num_timesteps=21500, episode_reward=-286.70 +/- 155.18
Episode length: 287.37 +/- 154.72
Eval num_timesteps=22000, episode_reward=-197.41 +/- 88.05
Episode length: 198.35 +/- 87.84
Eval num_timesteps=22500, episode_reward=-207.85 +/- 103.92
Episode length: 208.76 +/- 103.67
Eval num_timesteps=23000, episode_reward=-178.55 +/- 50.09
Episode length: 179.54 +/- 50.03
New best mean reward!
Eval num_timesteps=23500, episode_reward=-195.10 +/- 73.93
Episode length: 196.08 +/- 73.84
Eval num_timesteps=24000, episode_reward=-203.67 +/- 100.53
Episode length: 204.59 +/- 100.30
Eval num_timesteps=24500, episode_reward=-200.64 +/- 105.23
Episode length: 201.54 +/- 104.95
Eval num_timesteps=25000, episode_reward=-196.64 +/- 100.68
Episode length: 197.56 +/- 100.44
Eval num_timesteps=25500, episode_reward=-159.34 +/- 119.72
Episode length: 160.24 +/- 119.43
New best mean reward!
Eval num_timesteps=26000, episode_reward=-219.49 +/- 173.95
Episode length: 220.23 +/- 173.53
Eval num_timesteps=26500, episode_reward=-239.73 +/- 183.42
Episode length: 240.41 +/- 182.97
Eval num_timesteps=27000, episode_reward=-314.65 +/- 200.64
Episode length: 315.12 +/- 200.15
Eval num_timesteps=27500, episode_reward=-378.66 +/- 186.05
Episode length: 378.96 +/- 185.59
Eval num_timesteps=28000, episode_reward=-391.35 +/- 180.40
Episode length: 391.62 +/- 179.96
Eval num_timesteps=28500, episode_reward=-235.28 +/- 193.57
Episode length: 235.94 +/- 193.11
Eval num_timesteps=29000, episode_reward=-283.69 +/- 201.96
Episode length: 284.23 +/- 201.47
Eval num_timesteps=29500, episode_reward=-223.22 +/- 183.89
Episode length: 223.92 +/- 183.44
Eval num_timesteps=30000, episode_reward=-169.64 +/- 161.18
Episode length: 170.46 +/- 160.81
Eval num_timesteps=30500, episode_reward=-187.66 +/- 172.35
Episode length: 188.44 +/- 171.95
Eval num_timesteps=31000, episode_reward=-173.77 +/- 166.53
Episode length: 174.58 +/- 166.16
Eval num_timesteps=31500, episode_reward=-138.15 +/- 131.09
Episode length: 139.04 +/- 130.79
New best mean reward!
Eval num_timesteps=32000, episode_reward=-139.05 +/- 124.98
Episode length: 139.96 +/- 124.72
Eval num_timesteps=32500, episode_reward=-132.52 +/- 125.26
Episode length: 133.42 +/- 124.96
New best mean reward!
Eval num_timesteps=33000, episode_reward=-155.33 +/- 147.19
Episode length: 156.20 +/- 146.88
Eval num_timesteps=33500, episode_reward=-124.98 +/- 113.71
Episode length: 125.90 +/- 113.45
New best mean reward!
Eval num_timesteps=34000, episode_reward=-145.11 +/- 137.03
Episode length: 145.99 +/- 136.72
Eval num_timesteps=34500, episode_reward=-116.82 +/- 100.16
Episode length: 117.77 +/- 99.97
New best mean reward!
Eval num_timesteps=35000, episode_reward=-148.10 +/- 140.70
Episode length: 148.99 +/- 140.43
Eval num_timesteps=35500, episode_reward=-136.33 +/- 128.82
Episode length: 137.23 +/- 128.54
Eval num_timesteps=36000, episode_reward=-137.93 +/- 131.29
Episode length: 138.82 +/- 130.99
Eval num_timesteps=36500, episode_reward=-106.25 +/- 77.19
Episode length: 107.22 +/- 77.03
New best mean reward!
Eval num_timesteps=37000, episode_reward=-122.72 +/- 116.12
Episode length: 123.64 +/- 115.86
Eval num_timesteps=37500, episode_reward=-102.16 +/- 67.96
Episode length: 103.14 +/- 67.84
New best mean reward!
Eval num_timesteps=38000, episode_reward=-122.51 +/- 106.60
Episode length: 123.45 +/- 106.39
Eval num_timesteps=38500, episode_reward=-104.65 +/- 78.77
Episode length: 105.62 +/- 78.61
Eval num_timesteps=39000, episode_reward=-101.38 +/- 75.00
Episode length: 102.35 +/- 74.84
New best mean reward!
Eval num_timesteps=39500, episode_reward=-96.64 +/- 62.56
Episode length: 97.62 +/- 62.43
New best mean reward!
FINISHED IN 794.2302038310445 s


starting seed  10323 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-305.05 +/- 187.96
Episode length: 305.57 +/- 187.47
New best mean reward!
Eval num_timesteps=7000, episode_reward=-91.69 +/- 49.06
Episode length: 92.68 +/- 48.98
New best mean reward!
FINISHED IN 179.3092029490508 s


starting seed  10324 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-351.42 +/- 173.23
Episode length: 351.85 +/- 172.74
New best mean reward!
Eval num_timesteps=9000, episode_reward=-92.45 +/- 30.37
Episode length: 93.45 +/- 30.37
New best mean reward!
FINISHED IN 228.08756188501138 s


starting seed  10325 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-164.60 +/- 58.80
Episode length: 165.59 +/- 58.74
New best mean reward!
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-409.85 +/- 114.25
Episode length: 410.29 +/- 113.81
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-411.23 +/- 167.65
Episode length: 411.45 +/- 167.24
Eval num_timesteps=9500, episode_reward=-288.51 +/- 203.10
Episode length: 289.05 +/- 202.62
Eval num_timesteps=10000, episode_reward=-318.94 +/- 200.20
Episode length: 319.40 +/- 199.71
Eval num_timesteps=10500, episode_reward=-262.99 +/- 199.71
Episode length: 263.59 +/- 199.24
Eval num_timesteps=11000, episode_reward=-178.91 +/- 169.27
Episode length: 179.70 +/- 168.87
Eval num_timesteps=11500, episode_reward=-243.97 +/- 202.10
Episode length: 244.59 +/- 201.62
Eval num_timesteps=12000, episode_reward=-210.79 +/- 189.74
Episode length: 211.51 +/- 189.31
Eval num_timesteps=12500, episode_reward=-219.71 +/- 190.84
Episode length: 220.40 +/- 190.39
Eval num_timesteps=13000, episode_reward=-179.88 +/- 170.30
Episode length: 180.67 +/- 169.90
Eval num_timesteps=13500, episode_reward=-177.85 +/- 167.90
Episode length: 178.64 +/- 167.50
Eval num_timesteps=14000, episode_reward=-127.93 +/- 114.02
Episode length: 128.85 +/- 113.76
New best mean reward!
Eval num_timesteps=14500, episode_reward=-105.80 +/- 83.50
Episode length: 106.76 +/- 83.32
New best mean reward!
Eval num_timesteps=15000, episode_reward=-93.13 +/- 44.98
Episode length: 94.12 +/- 44.89
New best mean reward!
FINISHED IN 304.1941498310189 s


starting seed  10326 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-241.25 +/- 147.74
Episode length: 242.01 +/- 147.32
New best mean reward!
Eval num_timesteps=8000, episode_reward=-251.38 +/- 139.76
Episode length: 252.16 +/- 139.36
Eval num_timesteps=8500, episode_reward=-166.53 +/- 41.34
Episode length: 167.53 +/- 41.34
New best mean reward!
Eval num_timesteps=9000, episode_reward=-167.97 +/- 45.38
Episode length: 168.97 +/- 45.38
Eval num_timesteps=9500, episode_reward=-427.95 +/- 134.32
Episode length: 428.18 +/- 133.91
Eval num_timesteps=10000, episode_reward=-172.20 +/- 41.40
Episode length: 173.20 +/- 41.40
Eval num_timesteps=10500, episode_reward=-159.46 +/- 38.04
Episode length: 160.46 +/- 38.04
New best mean reward!
Eval num_timesteps=11000, episode_reward=-172.75 +/- 41.64
Episode length: 173.75 +/- 41.64
Eval num_timesteps=11500, episode_reward=-396.68 +/- 148.42
Episode length: 397.01 +/- 147.96
Eval num_timesteps=12000, episode_reward=-176.93 +/- 36.24
Episode length: 177.93 +/- 36.24
Eval num_timesteps=12500, episode_reward=-165.96 +/- 34.70
Episode length: 166.96 +/- 34.70
Eval num_timesteps=13000, episode_reward=-172.63 +/- 40.69
Episode length: 173.63 +/- 40.69
Eval num_timesteps=13500, episode_reward=-116.06 +/- 36.58
Episode length: 117.06 +/- 36.58
New best mean reward!
Eval num_timesteps=14000, episode_reward=-105.04 +/- 23.64
Episode length: 106.04 +/- 23.64
New best mean reward!
Eval num_timesteps=14500, episode_reward=-118.26 +/- 55.50
Episode length: 119.25 +/- 55.43
Eval num_timesteps=15000, episode_reward=-112.04 +/- 30.82
Episode length: 113.04 +/- 30.82
Eval num_timesteps=15500, episode_reward=-102.64 +/- 18.22
Episode length: 103.64 +/- 18.22
New best mean reward!
Eval num_timesteps=16000, episode_reward=-128.15 +/- 99.66
Episode length: 129.09 +/- 99.44
Eval num_timesteps=16500, episode_reward=-122.50 +/- 84.61
Episode length: 123.46 +/- 84.43
Eval num_timesteps=17000, episode_reward=-185.24 +/- 164.38
Episode length: 186.03 +/- 163.98
Eval num_timesteps=17500, episode_reward=-200.33 +/- 169.39
Episode length: 201.10 +/- 168.98
Eval num_timesteps=18000, episode_reward=-116.32 +/- 86.13
Episode length: 117.28 +/- 85.95
Eval num_timesteps=18500, episode_reward=-138.13 +/- 112.87
Episode length: 139.06 +/- 112.65
Eval num_timesteps=19000, episode_reward=-122.85 +/- 90.61
Episode length: 123.80 +/- 90.40
Eval num_timesteps=19500, episode_reward=-147.69 +/- 129.92
Episode length: 148.58 +/- 129.62
Eval num_timesteps=20000, episode_reward=-174.48 +/- 156.31
Episode length: 175.30 +/- 155.94
Eval num_timesteps=20500, episode_reward=-147.48 +/- 124.18
Episode length: 148.39 +/- 123.92
Eval num_timesteps=21000, episode_reward=-162.74 +/- 144.36
Episode length: 163.60 +/- 144.03
Eval num_timesteps=21500, episode_reward=-161.85 +/- 146.71
Episode length: 162.70 +/- 146.36
Eval num_timesteps=22000, episode_reward=-204.23 +/- 174.24
Episode length: 204.99 +/- 173.83
Eval num_timesteps=22500, episode_reward=-149.80 +/- 128.54
Episode length: 150.70 +/- 128.27
Eval num_timesteps=23000, episode_reward=-188.44 +/- 161.31
Episode length: 189.24 +/- 160.93
Eval num_timesteps=23500, episode_reward=-196.76 +/- 168.88
Episode length: 197.54 +/- 168.49
Eval num_timesteps=24000, episode_reward=-169.18 +/- 152.03
Episode length: 170.02 +/- 151.68
Eval num_timesteps=24500, episode_reward=-179.47 +/- 167.96
Episode length: 180.26 +/- 167.55
Eval num_timesteps=25000, episode_reward=-196.19 +/- 174.97
Episode length: 196.95 +/- 174.55
Eval num_timesteps=25500, episode_reward=-203.97 +/- 179.70
Episode length: 204.71 +/- 179.27
Eval num_timesteps=26000, episode_reward=-184.91 +/- 168.14
Episode length: 185.70 +/- 167.75
Eval num_timesteps=26500, episode_reward=-137.71 +/- 125.09
Episode length: 138.62 +/- 124.83
Eval num_timesteps=27000, episode_reward=-119.84 +/- 102.56
Episode length: 120.78 +/- 102.34
Eval num_timesteps=27500, episode_reward=-148.17 +/- 140.75
Episode length: 149.04 +/- 140.42
Eval num_timesteps=28000, episode_reward=-139.88 +/- 131.10
Episode length: 140.77 +/- 130.80
Eval num_timesteps=28500, episode_reward=-125.43 +/- 114.70
Episode length: 126.35 +/- 114.44
Eval num_timesteps=29000, episode_reward=-112.71 +/- 95.73
Episode length: 113.67 +/- 95.57
Eval num_timesteps=29500, episode_reward=-112.99 +/- 99.44
Episode length: 113.94 +/- 99.25
Eval num_timesteps=30000, episode_reward=-117.05 +/- 109.47
Episode length: 117.98 +/- 109.23
Eval num_timesteps=30500, episode_reward=-117.00 +/- 101.25
Episode length: 117.94 +/- 101.02
Eval num_timesteps=31000, episode_reward=-102.72 +/- 84.07
Episode length: 103.68 +/- 83.88
Eval num_timesteps=31500, episode_reward=-104.31 +/- 76.64
Episode length: 105.28 +/- 76.48
Eval num_timesteps=32000, episode_reward=-105.18 +/- 84.60
Episode length: 106.14 +/- 84.41
Eval num_timesteps=32500, episode_reward=-95.04 +/- 57.38
Episode length: 96.03 +/- 57.31
New best mean reward!
FINISHED IN 560.150467458996 s


starting seed  10327 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-244.22 +/- 81.92
Episode length: 245.14 +/- 81.67
New best mean reward!
Eval num_timesteps=6500, episode_reward=-269.36 +/- 104.28
Episode length: 270.20 +/- 103.92
Eval num_timesteps=7000, episode_reward=-188.55 +/- 74.48
Episode length: 189.53 +/- 74.40
New best mean reward!
Eval num_timesteps=7500, episode_reward=-168.85 +/- 38.37
Episode length: 169.85 +/- 38.37
New best mean reward!
Eval num_timesteps=8000, episode_reward=-171.51 +/- 40.50
Episode length: 172.51 +/- 40.50
Eval num_timesteps=8500, episode_reward=-171.27 +/- 43.00
Episode length: 172.27 +/- 43.00
Eval num_timesteps=9000, episode_reward=-160.70 +/- 47.58
Episode length: 161.69 +/- 47.51
New best mean reward!
Eval num_timesteps=9500, episode_reward=-171.17 +/- 59.90
Episode length: 172.16 +/- 59.84
Eval num_timesteps=10000, episode_reward=-156.42 +/- 59.52
Episode length: 157.41 +/- 59.46
New best mean reward!
Eval num_timesteps=10500, episode_reward=-145.41 +/- 23.59
Episode length: 146.41 +/- 23.59
New best mean reward!
Eval num_timesteps=11000, episode_reward=-178.01 +/- 78.36
Episode length: 178.97 +/- 78.20
Eval num_timesteps=11500, episode_reward=-94.08 +/- 27.47
Episode length: 95.08 +/- 27.47
New best mean reward!
FINISHED IN 263.5911027410184 s


starting seed  10328 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-167.71 +/- 100.88
Episode length: 168.64 +/- 100.64
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-495.77 +/- 42.09
Episode length: 495.78 +/- 41.99
Eval num_timesteps=9000, episode_reward=-83.85 +/- 20.24
Episode length: 84.85 +/- 20.24
New best mean reward!
FINISHED IN 200.10105398396263 s


starting seed  10329 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-440.15 +/- 72.61
Episode length: 440.73 +/- 72.26
New best mean reward!
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-475.37 +/- 83.88
Episode length: 475.45 +/- 83.61
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-294.57 +/- 159.62
Episode length: 295.21 +/- 159.16
New best mean reward!
Eval num_timesteps=9500, episode_reward=-235.25 +/- 118.19
Episode length: 236.11 +/- 117.87
New best mean reward!
Eval num_timesteps=10000, episode_reward=-211.34 +/- 101.37
Episode length: 212.26 +/- 101.14
New best mean reward!
Eval num_timesteps=10500, episode_reward=-261.53 +/- 138.63
Episode length: 262.31 +/- 138.25
Eval num_timesteps=11000, episode_reward=-402.54 +/- 148.07
Episode length: 402.86 +/- 147.62
Eval num_timesteps=11500, episode_reward=-494.30 +/- 39.90
Episode length: 494.32 +/- 39.76
Eval num_timesteps=12000, episode_reward=-471.44 +/- 91.43
Episode length: 471.53 +/- 91.15
Eval num_timesteps=12500, episode_reward=-334.72 +/- 157.01
Episode length: 335.26 +/- 156.53
Eval num_timesteps=13000, episode_reward=-294.48 +/- 156.51
Episode length: 295.14 +/- 156.06
Eval num_timesteps=13500, episode_reward=-490.71 +/- 53.17
Episode length: 490.74 +/- 53.00
Eval num_timesteps=14000, episode_reward=-497.00 +/- 29.85
Episode length: 497.01 +/- 29.75
Eval num_timesteps=14500, episode_reward=-458.24 +/- 105.17
Episode length: 458.38 +/- 104.83
Eval num_timesteps=15000, episode_reward=-393.34 +/- 152.78
Episode length: 393.68 +/- 152.32
Eval num_timesteps=15500, episode_reward=-374.43 +/- 152.05
Episode length: 374.86 +/- 151.58
Eval num_timesteps=16000, episode_reward=-287.91 +/- 152.53
Episode length: 288.59 +/- 152.08
Eval num_timesteps=16500, episode_reward=-168.90 +/- 50.69
Episode length: 169.89 +/- 50.63
New best mean reward!
Eval num_timesteps=17000, episode_reward=-185.37 +/- 70.79
Episode length: 186.34 +/- 70.66
Eval num_timesteps=17500, episode_reward=-243.60 +/- 135.09
Episode length: 244.40 +/- 134.71
Eval num_timesteps=18000, episode_reward=-274.70 +/- 158.41
Episode length: 275.39 +/- 157.96
Eval num_timesteps=18500, episode_reward=-224.65 +/- 124.46
Episode length: 225.50 +/- 124.12
Eval num_timesteps=19000, episode_reward=-137.52 +/- 95.87
Episode length: 138.47 +/- 95.68
New best mean reward!
Eval num_timesteps=19500, episode_reward=-179.66 +/- 152.48
Episode length: 180.49 +/- 152.12
Eval num_timesteps=20000, episode_reward=-206.88 +/- 167.63
Episode length: 207.64 +/- 167.21
Eval num_timesteps=20500, episode_reward=-194.86 +/- 163.13
Episode length: 195.66 +/- 162.76
Eval num_timesteps=21000, episode_reward=-160.95 +/- 139.46
Episode length: 161.83 +/- 139.17
Eval num_timesteps=21500, episode_reward=-201.16 +/- 174.73
Episode length: 201.92 +/- 174.32
Eval num_timesteps=22000, episode_reward=-198.20 +/- 170.95
Episode length: 198.99 +/- 170.58
Eval num_timesteps=22500, episode_reward=-209.57 +/- 177.01
Episode length: 210.31 +/- 176.58
Eval num_timesteps=23000, episode_reward=-166.22 +/- 148.39
Episode length: 167.06 +/- 148.03
Eval num_timesteps=23500, episode_reward=-165.15 +/- 153.20
Episode length: 165.98 +/- 152.83
Eval num_timesteps=24000, episode_reward=-153.27 +/- 133.77
Episode length: 154.15 +/- 133.46
Eval num_timesteps=24500, episode_reward=-137.26 +/- 120.36
Episode length: 138.17 +/- 120.09
New best mean reward!
Eval num_timesteps=25000, episode_reward=-125.87 +/- 102.48
Episode length: 126.81 +/- 102.26
New best mean reward!
Eval num_timesteps=25500, episode_reward=-101.18 +/- 63.38
Episode length: 102.16 +/- 63.25
New best mean reward!
Eval num_timesteps=26000, episode_reward=-102.86 +/- 65.05
Episode length: 103.85 +/- 64.99
Eval num_timesteps=26500, episode_reward=-95.97 +/- 41.64
Episode length: 96.97 +/- 41.64
New best mean reward!
FINISHED IN 484.4377756000031 s


starting seed  10330 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-407.44 +/- 144.45
Episode length: 407.74 +/- 144.00
New best mean reward!
Eval num_timesteps=11000, episode_reward=-108.34 +/- 58.74
Episode length: 109.32 +/- 58.60
New best mean reward!
Eval num_timesteps=11500, episode_reward=-91.62 +/- 26.01
Episode length: 92.62 +/- 26.01
New best mean reward!
FINISHED IN 289.0856830539997 s


starting seed  10331 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-166.54 +/- 46.46
Episode length: 167.53 +/- 46.39
New best mean reward!
Eval num_timesteps=6000, episode_reward=-211.66 +/- 118.47
Episode length: 212.54 +/- 118.17
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-407.58 +/- 142.73
Episode length: 407.88 +/- 142.28
Eval num_timesteps=8000, episode_reward=-202.44 +/- 97.52
Episode length: 203.36 +/- 97.28
Eval num_timesteps=8500, episode_reward=-356.55 +/- 158.30
Episode length: 357.03 +/- 157.83
Eval num_timesteps=9000, episode_reward=-350.50 +/- 160.85
Episode length: 350.97 +/- 160.36
Eval num_timesteps=9500, episode_reward=-363.50 +/- 160.44
Episode length: 363.94 +/- 159.97
Eval num_timesteps=10000, episode_reward=-255.87 +/- 141.84
Episode length: 256.63 +/- 141.43
Eval num_timesteps=10500, episode_reward=-215.21 +/- 112.01
Episode length: 216.09 +/- 111.71
Eval num_timesteps=11000, episode_reward=-201.79 +/- 95.79
Episode length: 202.71 +/- 95.54
Eval num_timesteps=11500, episode_reward=-251.28 +/- 137.43
Episode length: 252.08 +/- 137.07
Eval num_timesteps=12000, episode_reward=-223.95 +/- 122.90
Episode length: 224.81 +/- 122.59
Eval num_timesteps=12500, episode_reward=-262.88 +/- 149.14
Episode length: 263.61 +/- 148.71
Eval num_timesteps=13000, episode_reward=-266.39 +/- 152.06
Episode length: 267.10 +/- 151.62
Eval num_timesteps=13500, episode_reward=-232.25 +/- 157.28
Episode length: 233.00 +/- 156.85
Eval num_timesteps=14000, episode_reward=-150.30 +/- 114.46
Episode length: 151.21 +/- 114.19
New best mean reward!
Eval num_timesteps=14500, episode_reward=-105.97 +/- 44.41
Episode length: 106.96 +/- 44.32
New best mean reward!
Eval num_timesteps=15000, episode_reward=-137.46 +/- 100.10
Episode length: 138.41 +/- 99.92
Eval num_timesteps=15500, episode_reward=-139.52 +/- 120.63
Episode length: 140.43 +/- 120.36
Eval num_timesteps=16000, episode_reward=-114.38 +/- 85.49
Episode length: 115.34 +/- 85.31
Eval num_timesteps=16500, episode_reward=-100.79 +/- 64.56
Episode length: 101.77 +/- 64.43
New best mean reward!
Eval num_timesteps=17000, episode_reward=-128.48 +/- 105.99
Episode length: 129.41 +/- 105.75
Eval num_timesteps=17500, episode_reward=-158.60 +/- 139.68
Episode length: 159.47 +/- 139.36
Eval num_timesteps=18000, episode_reward=-126.16 +/- 103.04
Episode length: 127.10 +/- 102.82
Eval num_timesteps=18500, episode_reward=-124.59 +/- 106.44
Episode length: 125.52 +/- 106.19
Eval num_timesteps=19000, episode_reward=-159.23 +/- 146.30
Episode length: 160.08 +/- 145.95
Eval num_timesteps=19500, episode_reward=-114.73 +/- 82.34
Episode length: 115.70 +/- 82.20
Eval num_timesteps=20000, episode_reward=-127.14 +/- 110.77
Episode length: 128.07 +/- 110.53
Eval num_timesteps=20500, episode_reward=-91.93 +/- 48.28
Episode length: 92.92 +/- 48.19
New best mean reward!
FINISHED IN 414.2467661530245 s


starting seed  10332 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-343.98 +/- 159.60
Episode length: 344.50 +/- 159.13
New best mean reward!
Eval num_timesteps=9500, episode_reward=-432.40 +/- 129.00
Episode length: 432.62 +/- 128.59
Eval num_timesteps=10000, episode_reward=-487.13 +/- 63.08
Episode length: 487.17 +/- 62.88
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-258.33 +/- 142.62
Episode length: 259.09 +/- 142.22
New best mean reward!
Eval num_timesteps=11500, episode_reward=-234.42 +/- 125.35
Episode length: 235.26 +/- 125.01
New best mean reward!
Eval num_timesteps=12000, episode_reward=-242.73 +/- 132.00
Episode length: 243.54 +/- 131.63
Eval num_timesteps=12500, episode_reward=-193.06 +/- 152.78
Episode length: 193.87 +/- 152.39
New best mean reward!
Eval num_timesteps=13000, episode_reward=-188.60 +/- 162.74
Episode length: 189.39 +/- 162.34
New best mean reward!
Eval num_timesteps=13500, episode_reward=-194.50 +/- 165.55
Episode length: 195.29 +/- 165.16
Eval num_timesteps=14000, episode_reward=-140.22 +/- 124.18
Episode length: 141.12 +/- 123.89
New best mean reward!
Eval num_timesteps=14500, episode_reward=-139.84 +/- 125.03
Episode length: 140.74 +/- 124.74
New best mean reward!
Eval num_timesteps=15000, episode_reward=-127.95 +/- 88.88
Episode length: 128.91 +/- 88.71
New best mean reward!
Eval num_timesteps=15500, episode_reward=-113.89 +/- 92.90
Episode length: 114.85 +/- 92.74
New best mean reward!
Eval num_timesteps=16000, episode_reward=-85.91 +/- 21.49
Episode length: 86.91 +/- 21.49
New best mean reward!
FINISHED IN 314.91865045402665 s


starting seed  10333 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-232.87 +/- 193.63
Episode length: 233.53 +/- 193.16
New best mean reward!
Eval num_timesteps=9500, episode_reward=-220.92 +/- 189.92
Episode length: 221.63 +/- 189.50
New best mean reward!
Eval num_timesteps=10000, episode_reward=-215.84 +/- 188.15
Episode length: 216.55 +/- 187.71
New best mean reward!
Eval num_timesteps=10500, episode_reward=-227.19 +/- 194.71
Episode length: 227.86 +/- 194.24
Eval num_timesteps=11000, episode_reward=-285.65 +/- 201.45
Episode length: 286.20 +/- 200.97
Eval num_timesteps=11500, episode_reward=-315.32 +/- 198.80
Episode length: 315.79 +/- 198.31
Eval num_timesteps=12000, episode_reward=-262.28 +/- 199.16
Episode length: 262.87 +/- 198.67
Eval num_timesteps=12500, episode_reward=-113.21 +/- 94.91
Episode length: 114.16 +/- 94.71
New best mean reward!
Eval num_timesteps=13000, episode_reward=-97.51 +/- 66.43
Episode length: 98.49 +/- 66.31
New best mean reward!
FINISHED IN 292.45055846701143 s


starting seed  10334 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-448.74 +/- 132.68
Episode length: 448.87 +/- 132.35
New best mean reward!
Eval num_timesteps=7000, episode_reward=-397.50 +/- 173.17
Episode length: 397.76 +/- 172.74
New best mean reward!
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-495.87 +/- 41.09
Episode length: 495.88 +/- 40.99
Eval num_timesteps=8500, episode_reward=-80.77 +/- 18.48
Episode length: 81.77 +/- 18.48
New best mean reward!
FINISHED IN 194.03578099398874 s


starting seed  10335 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-250.78 +/- 182.40
Episode length: 251.44 +/- 181.93
New best mean reward!
Eval num_timesteps=7500, episode_reward=-334.00 +/- 173.75
Episode length: 334.48 +/- 173.25
Eval num_timesteps=8000, episode_reward=-400.19 +/- 156.61
Episode length: 400.48 +/- 156.15
Eval num_timesteps=8500, episode_reward=-138.02 +/- 39.67
Episode length: 139.02 +/- 39.67
New best mean reward!
Eval num_timesteps=9000, episode_reward=-179.72 +/- 92.27
Episode length: 180.65 +/- 92.03
Eval num_timesteps=9500, episode_reward=-117.38 +/- 40.34
Episode length: 118.38 +/- 40.34
New best mean reward!
Eval num_timesteps=10000, episode_reward=-150.20 +/- 34.18
Episode length: 151.20 +/- 34.18
Eval num_timesteps=10500, episode_reward=-104.68 +/- 17.02
Episode length: 105.68 +/- 17.02
New best mean reward!
Eval num_timesteps=11000, episode_reward=-113.11 +/- 48.93
Episode length: 114.10 +/- 48.85
Eval num_timesteps=11500, episode_reward=-116.35 +/- 26.32
Episode length: 117.35 +/- 26.32
Eval num_timesteps=12000, episode_reward=-150.77 +/- 60.45
Episode length: 151.75 +/- 60.33
Eval num_timesteps=12500, episode_reward=-137.43 +/- 83.41
Episode length: 138.39 +/- 83.24
Eval num_timesteps=13000, episode_reward=-111.64 +/- 34.10
Episode length: 112.64 +/- 34.10
Eval num_timesteps=13500, episode_reward=-111.41 +/- 57.79
Episode length: 112.40 +/- 57.73
Eval num_timesteps=14000, episode_reward=-103.07 +/- 50.53
Episode length: 104.06 +/- 50.45
New best mean reward!
Eval num_timesteps=14500, episode_reward=-112.50 +/- 53.57
Episode length: 113.49 +/- 53.49
Eval num_timesteps=15000, episode_reward=-132.48 +/- 98.29
Episode length: 133.42 +/- 98.07
Eval num_timesteps=15500, episode_reward=-115.96 +/- 59.66
Episode length: 116.95 +/- 59.60
Eval num_timesteps=16000, episode_reward=-101.22 +/- 47.81
Episode length: 102.21 +/- 47.72
New best mean reward!
Eval num_timesteps=16500, episode_reward=-130.98 +/- 101.66
Episode length: 131.93 +/- 101.48
Eval num_timesteps=17000, episode_reward=-106.06 +/- 66.84
Episode length: 107.04 +/- 66.73
Eval num_timesteps=17500, episode_reward=-98.47 +/- 31.17
Episode length: 99.47 +/- 31.17
New best mean reward!
FINISHED IN 269.0885043779854 s


starting seed  10336 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-483.19 +/- 82.40
Episode length: 483.23 +/- 82.21
New best mean reward!
Eval num_timesteps=10000, episode_reward=-180.71 +/- 170.98
Episode length: 181.49 +/- 170.57
New best mean reward!
Eval num_timesteps=10500, episode_reward=-146.68 +/- 134.96
Episode length: 147.56 +/- 134.65
New best mean reward!
Eval num_timesteps=11000, episode_reward=-159.46 +/- 156.34
Episode length: 160.29 +/- 155.97
Eval num_timesteps=11500, episode_reward=-128.05 +/- 105.06
Episode length: 128.99 +/- 104.84
New best mean reward!
Eval num_timesteps=12000, episode_reward=-112.20 +/- 63.11
Episode length: 113.19 +/- 63.04
New best mean reward!
Eval num_timesteps=12500, episode_reward=-108.07 +/- 68.89
Episode length: 109.05 +/- 68.78
New best mean reward!
Eval num_timesteps=13000, episode_reward=-99.65 +/- 33.67
Episode length: 100.65 +/- 33.67
New best mean reward!
FINISHED IN 283.7767875150312 s


starting seed  10337 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-296.72 +/- 162.91
Episode length: 297.35 +/- 162.45
New best mean reward!
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-198.99 +/- 156.14
Episode length: 199.79 +/- 155.75
New best mean reward!
Eval num_timesteps=12500, episode_reward=-127.98 +/- 80.27
Episode length: 128.95 +/- 80.13
New best mean reward!
Eval num_timesteps=13000, episode_reward=-110.32 +/- 85.33
Episode length: 111.29 +/- 85.19
New best mean reward!
Eval num_timesteps=13500, episode_reward=-93.56 +/- 31.33
Episode length: 94.56 +/- 31.33
New best mean reward!
FINISHED IN 342.9725785300252 s


starting seed  10338 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-206.45 +/- 58.01
Episode length: 207.44 +/- 57.96
New best mean reward!
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-163.18 +/- 49.91
Episode length: 164.17 +/- 49.84
New best mean reward!
Eval num_timesteps=6500, episode_reward=-469.28 +/- 97.75
Episode length: 469.37 +/- 97.46
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-367.17 +/- 190.09
Episode length: 367.50 +/- 189.62
Eval num_timesteps=9500, episode_reward=-356.20 +/- 188.74
Episode length: 356.57 +/- 188.26
Eval num_timesteps=10000, episode_reward=-398.15 +/- 176.28
Episode length: 398.41 +/- 175.85
Eval num_timesteps=10500, episode_reward=-139.25 +/- 134.21
Episode length: 140.13 +/- 133.89
New best mean reward!
Eval num_timesteps=11000, episode_reward=-220.95 +/- 189.02
Episode length: 221.64 +/- 188.56
Eval num_timesteps=11500, episode_reward=-169.83 +/- 161.60
Episode length: 170.64 +/- 161.21
Eval num_timesteps=12000, episode_reward=-125.79 +/- 113.38
Episode length: 126.71 +/- 113.12
New best mean reward!
Eval num_timesteps=12500, episode_reward=-192.83 +/- 175.50
Episode length: 193.59 +/- 175.08
Eval num_timesteps=13000, episode_reward=-153.66 +/- 142.82
Episode length: 154.52 +/- 142.48
Eval num_timesteps=13500, episode_reward=-193.00 +/- 174.46
Episode length: 193.76 +/- 174.04
Eval num_timesteps=14000, episode_reward=-238.84 +/- 193.47
Episode length: 239.50 +/- 193.01
Eval num_timesteps=14500, episode_reward=-130.65 +/- 113.09
Episode length: 131.57 +/- 112.83
Eval num_timesteps=15000, episode_reward=-112.95 +/- 95.34
Episode length: 113.90 +/- 95.14
New best mean reward!
Eval num_timesteps=15500, episode_reward=-86.95 +/- 21.61
Episode length: 87.95 +/- 21.61
New best mean reward!
FINISHED IN 289.33938387501985 s


starting seed  10339 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-484.17 +/- 77.56
Episode length: 484.21 +/- 77.36
New best mean reward!
Eval num_timesteps=10000, episode_reward=-316.93 +/- 200.48
Episode length: 317.39 +/- 199.98
New best mean reward!
Eval num_timesteps=10500, episode_reward=-251.33 +/- 203.88
Episode length: 251.93 +/- 203.40
New best mean reward!
Eval num_timesteps=11000, episode_reward=-285.84 +/- 206.47
Episode length: 286.36 +/- 205.97
Eval num_timesteps=11500, episode_reward=-293.10 +/- 203.90
Episode length: 293.61 +/- 203.40
Eval num_timesteps=12000, episode_reward=-255.25 +/- 201.03
Episode length: 255.85 +/- 200.54
Eval num_timesteps=12500, episode_reward=-109.80 +/- 101.12
Episode length: 110.74 +/- 100.89
New best mean reward!
Eval num_timesteps=13000, episode_reward=-183.68 +/- 178.35
Episode length: 184.44 +/- 177.93
Eval num_timesteps=13500, episode_reward=-120.53 +/- 116.21
Episode length: 121.45 +/- 115.95
Eval num_timesteps=14000, episode_reward=-178.56 +/- 171.89
Episode length: 179.34 +/- 171.48
Eval num_timesteps=14500, episode_reward=-177.43 +/- 172.32
Episode length: 178.21 +/- 171.91
Eval num_timesteps=15000, episode_reward=-173.09 +/- 169.62
Episode length: 173.88 +/- 169.21
Eval num_timesteps=15500, episode_reward=-114.22 +/- 102.27
Episode length: 115.16 +/- 102.04
Eval num_timesteps=16000, episode_reward=-97.49 +/- 73.17
Episode length: 98.46 +/- 73.00
New best mean reward!
FINISHED IN 385.1672651609988 s


starting seed  10340 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-476.96 +/- 91.41
Episode length: 477.02 +/- 91.17
New best mean reward!
Eval num_timesteps=10000, episode_reward=-487.57 +/- 70.73
Episode length: 487.60 +/- 70.56
Eval num_timesteps=10500, episode_reward=-200.38 +/- 179.51
Episode length: 201.12 +/- 179.08
New best mean reward!
Eval num_timesteps=11000, episode_reward=-119.21 +/- 109.57
Episode length: 120.14 +/- 109.33
New best mean reward!
Eval num_timesteps=11500, episode_reward=-116.40 +/- 106.82
Episode length: 117.33 +/- 106.57
New best mean reward!
Eval num_timesteps=12000, episode_reward=-160.54 +/- 158.82
Episode length: 161.37 +/- 158.46
Eval num_timesteps=12500, episode_reward=-185.26 +/- 174.74
Episode length: 186.03 +/- 174.33
Eval num_timesteps=13000, episode_reward=-172.33 +/- 168.77
Episode length: 173.13 +/- 168.38
Eval num_timesteps=13500, episode_reward=-147.49 +/- 147.05
Episode length: 148.35 +/- 146.72
Eval num_timesteps=14000, episode_reward=-111.37 +/- 92.92
Episode length: 112.32 +/- 92.71
New best mean reward!
Eval num_timesteps=14500, episode_reward=-155.37 +/- 154.38
Episode length: 156.21 +/- 154.02
Eval num_timesteps=15000, episode_reward=-126.64 +/- 121.30
Episode length: 127.55 +/- 121.02
Eval num_timesteps=15500, episode_reward=-147.28 +/- 145.42
Episode length: 148.14 +/- 145.09
Eval num_timesteps=16000, episode_reward=-159.02 +/- 153.70
Episode length: 159.86 +/- 153.34
Eval num_timesteps=16500, episode_reward=-113.34 +/- 104.96
Episode length: 114.28 +/- 104.74
Eval num_timesteps=17000, episode_reward=-159.63 +/- 151.48
Episode length: 160.47 +/- 151.12
Eval num_timesteps=17500, episode_reward=-160.47 +/- 157.45
Episode length: 161.30 +/- 157.09
Eval num_timesteps=18000, episode_reward=-132.00 +/- 119.57
Episode length: 132.91 +/- 119.30
Eval num_timesteps=18500, episode_reward=-123.86 +/- 121.25
Episode length: 124.77 +/- 120.97
Eval num_timesteps=19000, episode_reward=-134.39 +/- 132.83
Episode length: 135.28 +/- 132.53
Eval num_timesteps=19500, episode_reward=-122.97 +/- 115.33
Episode length: 123.89 +/- 115.07
Eval num_timesteps=20000, episode_reward=-119.67 +/- 106.58
Episode length: 120.60 +/- 106.33
Eval num_timesteps=20500, episode_reward=-159.32 +/- 148.35
Episode length: 160.17 +/- 148.00
Eval num_timesteps=21000, episode_reward=-106.71 +/- 91.25
Episode length: 107.67 +/- 91.08
New best mean reward!
Eval num_timesteps=21500, episode_reward=-113.49 +/- 104.09
Episode length: 114.43 +/- 103.86
Eval num_timesteps=22000, episode_reward=-99.78 +/- 76.79
Episode length: 100.75 +/- 76.64
New best mean reward!
FINISHED IN 458.456505611015 s


starting seed  10341 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-170.39 +/- 22.59
Episode length: 171.39 +/- 22.59
New best mean reward!
Eval num_timesteps=2000, episode_reward=-377.78 +/- 147.91
Episode length: 378.19 +/- 147.42
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-435.52 +/- 124.35
Episode length: 435.74 +/- 123.94
Eval num_timesteps=3500, episode_reward=-184.89 +/- 27.76
Episode length: 185.89 +/- 27.76
Eval num_timesteps=4000, episode_reward=-194.07 +/- 53.43
Episode length: 195.05 +/- 53.32
Eval num_timesteps=4500, episode_reward=-196.98 +/- 57.24
Episode length: 197.98 +/- 57.24
Eval num_timesteps=5000, episode_reward=-192.86 +/- 55.20
Episode length: 193.84 +/- 55.08
Eval num_timesteps=5500, episode_reward=-183.98 +/- 49.93
Episode length: 184.98 +/- 49.93
Eval num_timesteps=6000, episode_reward=-170.43 +/- 51.35
Episode length: 171.42 +/- 51.28
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-486.15 +/- 61.15
Episode length: 486.20 +/- 60.94
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-473.38 +/- 97.12
Episode length: 473.45 +/- 96.86
Eval num_timesteps=9000, episode_reward=-414.64 +/- 161.01
Episode length: 414.86 +/- 160.60
Eval num_timesteps=9500, episode_reward=-89.82 +/- 10.24
Episode length: 90.82 +/- 10.24
New best mean reward!
FINISHED IN 180.67465190100484 s


starting seed  10342 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-154.23 +/- 34.40
Episode length: 155.23 +/- 34.40
New best mean reward!
Eval num_timesteps=10000, episode_reward=-163.01 +/- 41.15
Episode length: 164.01 +/- 41.15
Eval num_timesteps=10500, episode_reward=-162.59 +/- 48.26
Episode length: 163.58 +/- 48.19
Eval num_timesteps=11000, episode_reward=-182.70 +/- 64.22
Episode length: 183.68 +/- 64.12
Eval num_timesteps=11500, episode_reward=-169.35 +/- 40.57
Episode length: 170.35 +/- 40.57
Eval num_timesteps=12000, episode_reward=-174.66 +/- 54.96
Episode length: 175.65 +/- 54.90
Eval num_timesteps=12500, episode_reward=-190.31 +/- 96.13
Episode length: 191.25 +/- 95.93
Eval num_timesteps=13000, episode_reward=-184.80 +/- 81.26
Episode length: 185.77 +/- 81.15
Eval num_timesteps=13500, episode_reward=-261.68 +/- 139.02
Episode length: 262.44 +/- 138.60
Eval num_timesteps=14000, episode_reward=-177.72 +/- 60.71
Episode length: 178.70 +/- 60.61
Eval num_timesteps=14500, episode_reward=-165.87 +/- 38.37
Episode length: 166.87 +/- 38.37
Eval num_timesteps=15000, episode_reward=-176.11 +/- 60.07
Episode length: 177.09 +/- 59.96
Eval num_timesteps=15500, episode_reward=-175.74 +/- 61.70
Episode length: 176.73 +/- 61.65
Eval num_timesteps=16000, episode_reward=-168.90 +/- 57.28
Episode length: 169.88 +/- 57.17
Eval num_timesteps=16500, episode_reward=-182.75 +/- 82.92
Episode length: 183.70 +/- 82.73
Eval num_timesteps=17000, episode_reward=-209.79 +/- 115.89
Episode length: 210.67 +/- 115.59
Eval num_timesteps=17500, episode_reward=-194.59 +/- 95.96
Episode length: 195.51 +/- 95.71
Eval num_timesteps=18000, episode_reward=-169.78 +/- 102.84
Episode length: 170.70 +/- 102.59
Eval num_timesteps=18500, episode_reward=-180.32 +/- 87.54
Episode length: 181.27 +/- 87.36
Eval num_timesteps=19000, episode_reward=-164.31 +/- 36.94
Episode length: 165.31 +/- 36.94
Eval num_timesteps=19500, episode_reward=-163.66 +/- 53.25
Episode length: 164.65 +/- 53.18
Eval num_timesteps=20000, episode_reward=-170.96 +/- 52.29
Episode length: 171.95 +/- 52.23
Eval num_timesteps=20500, episode_reward=-131.37 +/- 75.23
Episode length: 132.34 +/- 75.08
New best mean reward!
Eval num_timesteps=21000, episode_reward=-160.78 +/- 129.01
Episode length: 161.67 +/- 128.72
Eval num_timesteps=21500, episode_reward=-105.18 +/- 48.07
Episode length: 106.17 +/- 47.99
New best mean reward!
Eval num_timesteps=22000, episode_reward=-103.04 +/- 60.83
Episode length: 104.02 +/- 60.69
New best mean reward!
Eval num_timesteps=22500, episode_reward=-122.17 +/- 98.28
Episode length: 123.11 +/- 98.04
Eval num_timesteps=23000, episode_reward=-112.33 +/- 83.01
Episode length: 113.29 +/- 82.82
Eval num_timesteps=23500, episode_reward=-92.69 +/- 46.65
Episode length: 93.68 +/- 46.56
New best mean reward!
FINISHED IN 412.08046121301595 s


starting seed  10343 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-199.36 +/- 61.42
Episode length: 200.35 +/- 61.38
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-495.96 +/- 30.30
Episode length: 495.98 +/- 30.17
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-454.54 +/- 123.16
Episode length: 454.66 +/- 122.83
Eval num_timesteps=9000, episode_reward=-436.58 +/- 140.34
Episode length: 436.75 +/- 139.96
Eval num_timesteps=9500, episode_reward=-298.58 +/- 177.83
Episode length: 299.15 +/- 177.34
Eval num_timesteps=10000, episode_reward=-213.54 +/- 128.08
Episode length: 214.38 +/- 127.72
Eval num_timesteps=10500, episode_reward=-220.42 +/- 136.31
Episode length: 221.24 +/- 135.95
Eval num_timesteps=11000, episode_reward=-282.59 +/- 158.78
Episode length: 283.25 +/- 158.32
Eval num_timesteps=11500, episode_reward=-369.45 +/- 167.99
Episode length: 369.83 +/- 167.51
Eval num_timesteps=12000, episode_reward=-261.83 +/- 155.90
Episode length: 262.54 +/- 155.46
Eval num_timesteps=12500, episode_reward=-249.18 +/- 152.43
Episode length: 249.92 +/- 152.00
Eval num_timesteps=13000, episode_reward=-273.72 +/- 159.54
Episode length: 274.40 +/- 159.09
Eval num_timesteps=13500, episode_reward=-199.73 +/- 118.87
Episode length: 200.61 +/- 118.57
Eval num_timesteps=14000, episode_reward=-173.72 +/- 83.49
Episode length: 174.67 +/- 83.29
New best mean reward!
Eval num_timesteps=14500, episode_reward=-169.98 +/- 56.68
Episode length: 170.96 +/- 56.56
New best mean reward!
Eval num_timesteps=15000, episode_reward=-164.41 +/- 36.43
Episode length: 165.41 +/- 36.43
New best mean reward!
Eval num_timesteps=15500, episode_reward=-157.95 +/- 33.10
Episode length: 158.95 +/- 33.10
New best mean reward!
Eval num_timesteps=16000, episode_reward=-166.81 +/- 45.74
Episode length: 167.81 +/- 45.74
Eval num_timesteps=16500, episode_reward=-156.45 +/- 40.91
Episode length: 157.45 +/- 40.91
New best mean reward!
Eval num_timesteps=17000, episode_reward=-159.03 +/- 37.57
Episode length: 160.03 +/- 37.57
Eval num_timesteps=17500, episode_reward=-163.03 +/- 47.89
Episode length: 164.03 +/- 47.89
Eval num_timesteps=18000, episode_reward=-154.57 +/- 33.61
Episode length: 155.57 +/- 33.61
New best mean reward!
Eval num_timesteps=18500, episode_reward=-157.77 +/- 38.66
Episode length: 158.77 +/- 38.66
Eval num_timesteps=19000, episode_reward=-161.07 +/- 44.73
Episode length: 162.07 +/- 44.73
Eval num_timesteps=19500, episode_reward=-151.61 +/- 35.79
Episode length: 152.61 +/- 35.79
New best mean reward!
Eval num_timesteps=20000, episode_reward=-168.37 +/- 50.52
Episode length: 169.37 +/- 50.52
Eval num_timesteps=20500, episode_reward=-161.20 +/- 47.95
Episode length: 162.20 +/- 47.95
Eval num_timesteps=21000, episode_reward=-157.08 +/- 44.50
Episode length: 158.08 +/- 44.50
Eval num_timesteps=21500, episode_reward=-164.86 +/- 62.91
Episode length: 165.84 +/- 62.80
Eval num_timesteps=22000, episode_reward=-155.89 +/- 48.00
Episode length: 156.89 +/- 48.00
Eval num_timesteps=22500, episode_reward=-156.89 +/- 38.84
Episode length: 157.89 +/- 38.84
Eval num_timesteps=23000, episode_reward=-154.58 +/- 43.62
Episode length: 155.58 +/- 43.62
Eval num_timesteps=23500, episode_reward=-154.27 +/- 43.60
Episode length: 155.27 +/- 43.60
Eval num_timesteps=24000, episode_reward=-163.14 +/- 60.80
Episode length: 164.12 +/- 60.69
Eval num_timesteps=24500, episode_reward=-159.48 +/- 53.65
Episode length: 160.47 +/- 53.59
Eval num_timesteps=25000, episode_reward=-161.16 +/- 40.29
Episode length: 162.16 +/- 40.29
Eval num_timesteps=25500, episode_reward=-152.32 +/- 48.33
Episode length: 153.31 +/- 48.26
Eval num_timesteps=26000, episode_reward=-148.27 +/- 49.96
Episode length: 149.26 +/- 49.88
New best mean reward!
Eval num_timesteps=26500, episode_reward=-149.91 +/- 28.35
Episode length: 150.91 +/- 28.35
Eval num_timesteps=27000, episode_reward=-160.64 +/- 59.25
Episode length: 161.63 +/- 59.20
Eval num_timesteps=27500, episode_reward=-151.96 +/- 36.19
Episode length: 152.96 +/- 36.19
Eval num_timesteps=28000, episode_reward=-166.36 +/- 42.98
Episode length: 167.36 +/- 42.98
Eval num_timesteps=28500, episode_reward=-172.44 +/- 62.94
Episode length: 173.44 +/- 62.94
Eval num_timesteps=29000, episode_reward=-147.22 +/- 31.29
Episode length: 148.22 +/- 31.29
New best mean reward!
Eval num_timesteps=29500, episode_reward=-146.70 +/- 33.47
Episode length: 147.70 +/- 33.47
New best mean reward!
Eval num_timesteps=30000, episode_reward=-150.23 +/- 33.59
Episode length: 151.23 +/- 33.59
Eval num_timesteps=30500, episode_reward=-151.56 +/- 32.79
Episode length: 152.56 +/- 32.79
Eval num_timesteps=31000, episode_reward=-155.58 +/- 42.42
Episode length: 156.58 +/- 42.42
Eval num_timesteps=31500, episode_reward=-144.04 +/- 30.40
Episode length: 145.04 +/- 30.40
New best mean reward!
Eval num_timesteps=32000, episode_reward=-138.94 +/- 27.93
Episode length: 139.94 +/- 27.93
New best mean reward!
Eval num_timesteps=32500, episode_reward=-140.54 +/- 34.34
Episode length: 141.54 +/- 34.34
Eval num_timesteps=33000, episode_reward=-146.74 +/- 34.25
Episode length: 147.74 +/- 34.25
Eval num_timesteps=33500, episode_reward=-154.84 +/- 61.78
Episode length: 155.82 +/- 61.66
Eval num_timesteps=34000, episode_reward=-152.13 +/- 44.34
Episode length: 153.12 +/- 44.26
Eval num_timesteps=34500, episode_reward=-160.28 +/- 52.87
Episode length: 161.27 +/- 52.81
Eval num_timesteps=35000, episode_reward=-149.28 +/- 57.26
Episode length: 150.26 +/- 57.14
Eval num_timesteps=35500, episode_reward=-145.04 +/- 34.38
Episode length: 146.04 +/- 34.38
Eval num_timesteps=36000, episode_reward=-135.82 +/- 35.39
Episode length: 136.82 +/- 35.39
New best mean reward!
Eval num_timesteps=36500, episode_reward=-147.06 +/- 59.78
Episode length: 148.04 +/- 59.66
Eval num_timesteps=37000, episode_reward=-142.24 +/- 45.32
Episode length: 143.24 +/- 45.32
Eval num_timesteps=37500, episode_reward=-150.68 +/- 61.82
Episode length: 151.66 +/- 61.71
Eval num_timesteps=38000, episode_reward=-156.92 +/- 77.19
Episode length: 157.88 +/- 77.01
Eval num_timesteps=38500, episode_reward=-151.24 +/- 61.31
Episode length: 152.22 +/- 61.20
Eval num_timesteps=39000, episode_reward=-122.37 +/- 46.21
Episode length: 123.36 +/- 46.13
New best mean reward!
Eval num_timesteps=39500, episode_reward=-125.26 +/- 79.67
Episode length: 126.22 +/- 79.48
Eval num_timesteps=40000, episode_reward=-120.49 +/- 63.78
Episode length: 121.47 +/- 63.66
New best mean reward!
Eval num_timesteps=40500, episode_reward=-115.13 +/- 51.15
Episode length: 116.12 +/- 51.07
New best mean reward!
Eval num_timesteps=41000, episode_reward=-115.90 +/- 49.33
Episode length: 116.89 +/- 49.25
Eval num_timesteps=41500, episode_reward=-121.94 +/- 64.10
Episode length: 122.92 +/- 63.98
Eval num_timesteps=42000, episode_reward=-116.41 +/- 60.52
Episode length: 117.39 +/- 60.39
Eval num_timesteps=42500, episode_reward=-107.12 +/- 27.84
Episode length: 108.12 +/- 27.84
New best mean reward!
Eval num_timesteps=43000, episode_reward=-120.07 +/- 57.34
Episode length: 121.07 +/- 57.34
Eval num_timesteps=43500, episode_reward=-125.31 +/- 81.93
Episode length: 126.28 +/- 81.79
Eval num_timesteps=44000, episode_reward=-131.99 +/- 94.05
Episode length: 132.94 +/- 93.85
Eval num_timesteps=44500, episode_reward=-118.12 +/- 69.86
Episode length: 119.10 +/- 69.75
Eval num_timesteps=45000, episode_reward=-118.23 +/- 71.12
Episode length: 119.21 +/- 71.01
Eval num_timesteps=45500, episode_reward=-120.12 +/- 90.23
Episode length: 121.07 +/- 90.02
Eval num_timesteps=46000, episode_reward=-129.72 +/- 93.88
Episode length: 130.67 +/- 93.69
Eval num_timesteps=46500, episode_reward=-112.83 +/- 72.65
Episode length: 113.80 +/- 72.49
Eval num_timesteps=47000, episode_reward=-102.79 +/- 49.71
Episode length: 103.78 +/- 49.63
New best mean reward!
Eval num_timesteps=47500, episode_reward=-111.20 +/- 71.78
Episode length: 112.18 +/- 71.67
Eval num_timesteps=48000, episode_reward=-121.44 +/- 83.88
Episode length: 122.41 +/- 83.74
Eval num_timesteps=48500, episode_reward=-103.47 +/- 31.42
Episode length: 104.47 +/- 31.42
Eval num_timesteps=49000, episode_reward=-113.70 +/- 74.84
Episode length: 114.68 +/- 74.73
Eval num_timesteps=49500, episode_reward=-103.83 +/- 54.76
Episode length: 104.82 +/- 54.69
Eval num_timesteps=50000, episode_reward=-107.85 +/- 73.98
Episode length: 108.82 +/- 73.82
FINISHED IN 682.6821768829832 s


starting seed  10344 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-222.25 +/- 96.21
Episode length: 223.18 +/- 96.01
New best mean reward!
Eval num_timesteps=8000, episode_reward=-398.93 +/- 130.08
Episode length: 399.34 +/- 129.62
Eval num_timesteps=8500, episode_reward=-231.19 +/- 77.91
Episode length: 232.14 +/- 77.74
Eval num_timesteps=9000, episode_reward=-421.10 +/- 107.43
Episode length: 421.50 +/- 106.99
Eval num_timesteps=9500, episode_reward=-420.57 +/- 97.30
Episode length: 421.19 +/- 96.99
Eval num_timesteps=10000, episode_reward=-186.01 +/- 61.75
Episode length: 186.99 +/- 61.65
New best mean reward!
Eval num_timesteps=10500, episode_reward=-153.93 +/- 25.68
Episode length: 154.93 +/- 25.68
New best mean reward!
Eval num_timesteps=11000, episode_reward=-139.76 +/- 51.21
Episode length: 140.75 +/- 51.14
New best mean reward!
Eval num_timesteps=11500, episode_reward=-120.35 +/- 22.73
Episode length: 121.35 +/- 22.73
New best mean reward!
Eval num_timesteps=12000, episode_reward=-116.02 +/- 40.36
Episode length: 117.02 +/- 40.36
New best mean reward!
Eval num_timesteps=12500, episode_reward=-157.69 +/- 124.99
Episode length: 158.59 +/- 124.71
Eval num_timesteps=13000, episode_reward=-130.21 +/- 77.86
Episode length: 131.18 +/- 77.72
Eval num_timesteps=13500, episode_reward=-141.65 +/- 116.26
Episode length: 142.57 +/- 116.01
Eval num_timesteps=14000, episode_reward=-141.80 +/- 113.68
Episode length: 142.72 +/- 113.43
Eval num_timesteps=14500, episode_reward=-134.80 +/- 104.79
Episode length: 135.76 +/- 104.65
Eval num_timesteps=15000, episode_reward=-111.27 +/- 70.50
Episode length: 112.26 +/- 70.44
New best mean reward!
Eval num_timesteps=15500, episode_reward=-194.10 +/- 171.31
Episode length: 194.87 +/- 170.90
Eval num_timesteps=16000, episode_reward=-174.30 +/- 161.59
Episode length: 175.11 +/- 161.21
Eval num_timesteps=16500, episode_reward=-133.28 +/- 96.85
Episode length: 134.23 +/- 96.66
Eval num_timesteps=17000, episode_reward=-160.41 +/- 144.30
Episode length: 161.27 +/- 143.97
Eval num_timesteps=17500, episode_reward=-190.03 +/- 175.95
Episode length: 190.79 +/- 175.53
Eval num_timesteps=18000, episode_reward=-130.98 +/- 118.17
Episode length: 131.90 +/- 117.92
Eval num_timesteps=18500, episode_reward=-138.25 +/- 125.08
Episode length: 139.15 +/- 124.79
Eval num_timesteps=19000, episode_reward=-140.36 +/- 132.95
Episode length: 141.25 +/- 132.65
Eval num_timesteps=19500, episode_reward=-133.58 +/- 125.19
Episode length: 134.49 +/- 124.93
Eval num_timesteps=20000, episode_reward=-136.24 +/- 130.84
Episode length: 137.13 +/- 130.53
Eval num_timesteps=20500, episode_reward=-109.86 +/- 84.14
Episode length: 110.82 +/- 83.95
New best mean reward!
Eval num_timesteps=21000, episode_reward=-118.00 +/- 89.04
Episode length: 118.96 +/- 88.87
Eval num_timesteps=21500, episode_reward=-116.88 +/- 96.73
Episode length: 117.83 +/- 96.54
Eval num_timesteps=22000, episode_reward=-108.61 +/- 70.07
Episode length: 109.60 +/- 70.01
New best mean reward!
Eval num_timesteps=22500, episode_reward=-98.08 +/- 36.46
Episode length: 99.08 +/- 36.46
New best mean reward!
FINISHED IN 366.412540829042 s


starting seed  10345 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-203.96 +/- 153.86
Episode length: 204.76 +/- 153.47
New best mean reward!
Eval num_timesteps=7000, episode_reward=-314.83 +/- 185.58
Episode length: 315.37 +/- 185.12
Eval num_timesteps=7500, episode_reward=-412.02 +/- 157.09
Episode length: 412.26 +/- 156.67
Eval num_timesteps=8000, episode_reward=-149.60 +/- 121.22
Episode length: 150.50 +/- 120.93
New best mean reward!
Eval num_timesteps=8500, episode_reward=-224.35 +/- 109.34
Episode length: 225.23 +/- 109.04
Eval num_timesteps=9000, episode_reward=-158.92 +/- 90.61
Episode length: 159.86 +/- 90.38
Eval num_timesteps=9500, episode_reward=-148.60 +/- 33.56
Episode length: 149.60 +/- 33.56
New best mean reward!
Eval num_timesteps=10000, episode_reward=-120.18 +/- 60.84
Episode length: 121.16 +/- 60.71
New best mean reward!
Eval num_timesteps=10500, episode_reward=-159.31 +/- 48.46
Episode length: 160.30 +/- 48.39
Eval num_timesteps=11000, episode_reward=-161.24 +/- 66.10
Episode length: 162.23 +/- 66.05
Eval num_timesteps=11500, episode_reward=-91.97 +/- 45.08
Episode length: 92.96 +/- 44.99
New best mean reward!
FINISHED IN 222.41970234899782 s


starting seed  10346 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-295.17 +/- 157.10
Episode length: 295.81 +/- 156.63
New best mean reward!
Eval num_timesteps=6000, episode_reward=-405.68 +/- 152.06
Episode length: 405.96 +/- 151.61
Eval num_timesteps=6500, episode_reward=-440.52 +/- 127.64
Episode length: 440.70 +/- 127.26
Eval num_timesteps=7000, episode_reward=-165.54 +/- 61.57
Episode length: 166.52 +/- 61.46
New best mean reward!
Eval num_timesteps=7500, episode_reward=-169.94 +/- 53.25
Episode length: 170.93 +/- 53.19
Eval num_timesteps=8000, episode_reward=-173.10 +/- 60.86
Episode length: 174.08 +/- 60.76
Eval num_timesteps=8500, episode_reward=-165.68 +/- 52.76
Episode length: 166.67 +/- 52.70
Eval num_timesteps=9000, episode_reward=-126.33 +/- 25.30
Episode length: 127.33 +/- 25.30
New best mean reward!
Eval num_timesteps=9500, episode_reward=-93.99 +/- 22.06
Episode length: 94.99 +/- 22.06
New best mean reward!
FINISHED IN 206.60829390096478 s


starting seed  10347 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-495.96 +/- 40.20
Episode length: 495.97 +/- 40.10
New best mean reward!
Eval num_timesteps=8000, episode_reward=-236.54 +/- 195.31
Episode length: 237.19 +/- 194.84
New best mean reward!
Eval num_timesteps=8500, episode_reward=-181.12 +/- 166.41
Episode length: 181.91 +/- 166.01
New best mean reward!
Eval num_timesteps=9000, episode_reward=-92.26 +/- 36.90
Episode length: 93.26 +/- 36.90
New best mean reward!
FINISHED IN 239.4061727260123 s


starting seed  10348 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-484.06 +/- 78.11
Episode length: 484.10 +/- 77.91
New best mean reward!
Eval num_timesteps=6000, episode_reward=-468.36 +/- 107.38
Episode length: 468.44 +/- 107.11
New best mean reward!
Eval num_timesteps=6500, episode_reward=-104.25 +/- 56.04
Episode length: 105.24 +/- 55.97
New best mean reward!
Eval num_timesteps=7000, episode_reward=-89.70 +/- 48.29
Episode length: 90.69 +/- 48.20
New best mean reward!
FINISHED IN 162.94271076103905 s


starting seed  10349 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-477.32 +/- 89.82
Episode length: 477.38 +/- 89.59
New best mean reward!
Eval num_timesteps=9000, episode_reward=-94.78 +/- 21.27
Episode length: 95.78 +/- 21.27
New best mean reward!
FINISHED IN 287.6667818680289 s


starting seed  10350 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-157.31 +/- 139.07
Episode length: 158.18 +/- 138.75
New best mean reward!
Eval num_timesteps=9000, episode_reward=-174.22 +/- 143.75
Episode length: 175.07 +/- 143.41
Eval num_timesteps=9500, episode_reward=-236.82 +/- 190.57
Episode length: 237.48 +/- 190.10
Eval num_timesteps=10000, episode_reward=-286.99 +/- 199.78
Episode length: 287.53 +/- 199.29
Eval num_timesteps=10500, episode_reward=-282.84 +/- 201.10
Episode length: 283.38 +/- 200.60
Eval num_timesteps=11000, episode_reward=-186.23 +/- 169.28
Episode length: 187.01 +/- 168.87
Eval num_timesteps=11500, episode_reward=-187.43 +/- 170.38
Episode length: 188.21 +/- 169.97
Eval num_timesteps=12000, episode_reward=-214.42 +/- 184.96
Episode length: 215.13 +/- 184.52
Eval num_timesteps=12500, episode_reward=-271.34 +/- 197.52
Episode length: 271.93 +/- 197.05
Eval num_timesteps=13000, episode_reward=-295.50 +/- 201.46
Episode length: 296.02 +/- 200.98
Eval num_timesteps=13500, episode_reward=-88.94 +/- 29.69
Episode length: 89.94 +/- 29.69
New best mean reward!
FINISHED IN 306.89854292495875 s


starting seed  10351 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-498.07 +/- 12.99
Episode length: 498.10 +/- 12.84
New best mean reward!
Eval num_timesteps=10500, episode_reward=-475.08 +/- 72.17
Episode length: 475.19 +/- 71.86
New best mean reward!
Eval num_timesteps=11000, episode_reward=-420.92 +/- 107.60
Episode length: 421.32 +/- 107.16
New best mean reward!
Eval num_timesteps=11500, episode_reward=-361.73 +/- 108.50
Episode length: 362.46 +/- 108.15
New best mean reward!
Eval num_timesteps=12000, episode_reward=-363.37 +/- 124.85
Episode length: 363.98 +/- 124.43
Eval num_timesteps=12500, episode_reward=-319.84 +/- 124.17
Episode length: 320.58 +/- 123.79
New best mean reward!
Eval num_timesteps=13000, episode_reward=-319.40 +/- 122.82
Episode length: 320.18 +/- 122.49
New best mean reward!
Eval num_timesteps=13500, episode_reward=-351.42 +/- 116.04
Episode length: 352.13 +/- 115.67
Eval num_timesteps=14000, episode_reward=-359.94 +/- 110.98
Episode length: 360.66 +/- 110.63
Eval num_timesteps=14500, episode_reward=-339.29 +/- 121.25
Episode length: 340.03 +/- 120.91
Eval num_timesteps=15000, episode_reward=-337.86 +/- 118.04
Episode length: 338.62 +/- 117.71
Eval num_timesteps=15500, episode_reward=-349.58 +/- 115.29
Episode length: 350.28 +/- 114.90
Eval num_timesteps=16000, episode_reward=-317.69 +/- 120.10
Episode length: 318.48 +/- 119.78
New best mean reward!
Eval num_timesteps=16500, episode_reward=-353.84 +/- 103.69
Episode length: 354.59 +/- 103.33
Eval num_timesteps=17000, episode_reward=-347.27 +/- 110.72
Episode length: 348.00 +/- 110.35
Eval num_timesteps=17500, episode_reward=-334.56 +/- 112.98
Episode length: 335.35 +/- 112.67
Eval num_timesteps=18000, episode_reward=-351.92 +/- 108.36
Episode length: 352.64 +/- 107.98
Eval num_timesteps=18500, episode_reward=-342.09 +/- 115.31
Episode length: 342.80 +/- 114.92
Eval num_timesteps=19000, episode_reward=-314.48 +/- 118.92
Episode length: 315.25 +/- 118.56
New best mean reward!
Eval num_timesteps=19500, episode_reward=-306.58 +/- 105.98
Episode length: 307.45 +/- 105.74
New best mean reward!
Eval num_timesteps=20000, episode_reward=-333.44 +/- 106.60
Episode length: 334.26 +/- 106.32
Eval num_timesteps=20500, episode_reward=-358.90 +/- 105.31
Episode length: 359.68 +/- 105.01
Eval num_timesteps=21000, episode_reward=-307.99 +/- 97.35
Episode length: 308.87 +/- 97.11
Eval num_timesteps=21500, episode_reward=-204.07 +/- 61.86
Episode length: 205.05 +/- 61.76
New best mean reward!
Eval num_timesteps=22000, episode_reward=-331.90 +/- 94.54
Episode length: 332.70 +/- 94.18
Eval num_timesteps=22500, episode_reward=-333.67 +/- 89.83
Episode length: 334.50 +/- 89.52
Eval num_timesteps=23000, episode_reward=-354.95 +/- 92.99
Episode length: 355.76 +/- 92.69
Eval num_timesteps=23500, episode_reward=-356.74 +/- 101.00
Episode length: 357.48 +/- 100.63
Eval num_timesteps=24000, episode_reward=-316.88 +/- 85.21
Episode length: 317.79 +/- 85.01
Eval num_timesteps=24500, episode_reward=-331.11 +/- 106.28
Episode length: 331.89 +/- 105.93
Eval num_timesteps=25000, episode_reward=-278.00 +/- 84.46
Episode length: 278.90 +/- 84.20
Eval num_timesteps=25500, episode_reward=-317.60 +/- 110.61
Episode length: 318.39 +/- 110.26
Eval num_timesteps=26000, episode_reward=-304.36 +/- 93.55
Episode length: 305.25 +/- 93.32
Eval num_timesteps=26500, episode_reward=-370.55 +/- 110.16
Episode length: 371.28 +/- 109.84
Eval num_timesteps=27000, episode_reward=-339.85 +/- 101.18
Episode length: 340.68 +/- 100.91
Eval num_timesteps=27500, episode_reward=-243.98 +/- 82.02
Episode length: 244.91 +/- 81.80
Eval num_timesteps=28000, episode_reward=-265.35 +/- 93.70
Episode length: 266.25 +/- 93.45
Eval num_timesteps=28500, episode_reward=-265.20 +/- 94.24
Episode length: 266.08 +/- 93.94
Eval num_timesteps=29000, episode_reward=-271.03 +/- 106.04
Episode length: 271.87 +/- 105.69
Eval num_timesteps=29500, episode_reward=-211.88 +/- 82.95
Episode length: 212.83 +/- 82.77
Eval num_timesteps=30000, episode_reward=-237.05 +/- 81.05
Episode length: 237.99 +/- 80.86
Eval num_timesteps=30500, episode_reward=-223.01 +/- 110.37
Episode length: 223.89 +/- 110.07
Eval num_timesteps=31000, episode_reward=-170.31 +/- 77.55
Episode length: 171.27 +/- 77.38
New best mean reward!
Eval num_timesteps=31500, episode_reward=-169.67 +/- 64.97
Episode length: 170.64 +/- 64.82
New best mean reward!
Eval num_timesteps=32000, episode_reward=-165.79 +/- 67.50
Episode length: 166.76 +/- 67.35
New best mean reward!
Eval num_timesteps=32500, episode_reward=-131.74 +/- 43.79
Episode length: 132.73 +/- 43.70
New best mean reward!
Eval num_timesteps=33000, episode_reward=-153.19 +/- 64.86
Episode length: 154.17 +/- 64.76
Eval num_timesteps=33500, episode_reward=-155.17 +/- 92.09
Episode length: 156.11 +/- 91.87
Eval num_timesteps=34000, episode_reward=-118.57 +/- 28.34
Episode length: 119.57 +/- 28.34
New best mean reward!
Eval num_timesteps=34500, episode_reward=-130.05 +/- 66.09
Episode length: 131.03 +/- 65.98
Eval num_timesteps=35000, episode_reward=-128.53 +/- 66.84
Episode length: 129.51 +/- 66.73
Eval num_timesteps=35500, episode_reward=-121.74 +/- 49.13
Episode length: 122.73 +/- 49.06
Eval num_timesteps=36000, episode_reward=-118.33 +/- 37.84
Episode length: 119.33 +/- 37.84
New best mean reward!
Eval num_timesteps=36500, episode_reward=-128.03 +/- 73.56
Episode length: 129.01 +/- 73.46
Eval num_timesteps=37000, episode_reward=-120.64 +/- 55.13
Episode length: 121.64 +/- 55.13
Eval num_timesteps=37500, episode_reward=-120.05 +/- 55.79
Episode length: 121.04 +/- 55.73
Eval num_timesteps=38000, episode_reward=-110.68 +/- 45.64
Episode length: 111.67 +/- 45.55
New best mean reward!
Eval num_timesteps=38500, episode_reward=-116.78 +/- 38.68
Episode length: 117.78 +/- 38.68
Eval num_timesteps=39000, episode_reward=-111.07 +/- 29.01
Episode length: 112.07 +/- 29.01
Eval num_timesteps=39500, episode_reward=-109.15 +/- 43.32
Episode length: 110.14 +/- 43.23
New best mean reward!
Eval num_timesteps=40000, episode_reward=-115.61 +/- 33.94
Episode length: 116.61 +/- 33.94
Eval num_timesteps=40500, episode_reward=-100.26 +/- 14.81
Episode length: 101.26 +/- 14.81
New best mean reward!
Eval num_timesteps=41000, episode_reward=-108.83 +/- 35.50
Episode length: 109.83 +/- 35.50
Eval num_timesteps=41500, episode_reward=-102.85 +/- 25.05
Episode length: 103.85 +/- 25.05
Eval num_timesteps=42000, episode_reward=-100.97 +/- 24.17
Episode length: 101.97 +/- 24.17
Eval num_timesteps=42500, episode_reward=-106.20 +/- 28.22
Episode length: 107.20 +/- 28.22
Eval num_timesteps=43000, episode_reward=-99.86 +/- 22.44
Episode length: 100.86 +/- 22.44
New best mean reward!
FINISHED IN 837.0761857150355 s


starting seed  10352 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-355.74 +/- 167.12
Episode length: 356.17 +/- 166.63
New best mean reward!
Eval num_timesteps=5500, episode_reward=-98.38 +/- 61.83
Episode length: 99.36 +/- 61.70
New best mean reward!
FINISHED IN 131.39047584700165 s


starting seed  10353 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-489.39 +/- 60.37
Episode length: 489.42 +/- 60.19
New best mean reward!
Eval num_timesteps=9000, episode_reward=-156.63 +/- 28.05
Episode length: 157.63 +/- 28.05
New best mean reward!
Eval num_timesteps=9500, episode_reward=-167.69 +/- 47.55
Episode length: 168.68 +/- 47.48
Eval num_timesteps=10000, episode_reward=-170.85 +/- 74.26
Episode length: 171.81 +/- 74.08
Eval num_timesteps=10500, episode_reward=-170.78 +/- 46.93
Episode length: 171.77 +/- 46.86
Eval num_timesteps=11000, episode_reward=-185.30 +/- 169.47
Episode length: 186.08 +/- 169.07
Eval num_timesteps=11500, episode_reward=-153.42 +/- 142.26
Episode length: 154.28 +/- 141.92
New best mean reward!
Eval num_timesteps=12000, episode_reward=-149.15 +/- 144.00
Episode length: 150.01 +/- 143.66
New best mean reward!
Eval num_timesteps=12500, episode_reward=-406.18 +/- 171.83
Episode length: 406.41 +/- 171.41
Eval num_timesteps=13000, episode_reward=-99.75 +/- 23.42
Episode length: 100.75 +/- 23.42
New best mean reward!
FINISHED IN 273.73610733199166 s


starting seed  10354 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-395.16 +/- 139.66
Episode length: 395.60 +/- 139.24
New best mean reward!
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-493.35 +/- 46.64
Episode length: 493.37 +/- 46.50
Eval num_timesteps=6500, episode_reward=-256.13 +/- 190.04
Episode length: 256.77 +/- 189.58
New best mean reward!
Eval num_timesteps=7000, episode_reward=-100.18 +/- 26.69
Episode length: 101.18 +/- 26.69
New best mean reward!
Eval num_timesteps=7500, episode_reward=-110.57 +/- 52.86
Episode length: 111.56 +/- 52.78
Eval num_timesteps=8000, episode_reward=-118.61 +/- 33.42
Episode length: 119.61 +/- 33.42
Eval num_timesteps=8500, episode_reward=-118.79 +/- 45.08
Episode length: 119.79 +/- 45.08
Eval num_timesteps=9000, episode_reward=-112.11 +/- 37.23
Episode length: 113.11 +/- 37.23
Eval num_timesteps=9500, episode_reward=-114.99 +/- 34.17
Episode length: 115.99 +/- 34.17
Eval num_timesteps=10000, episode_reward=-112.71 +/- 51.13
Episode length: 113.70 +/- 51.06
Eval num_timesteps=10500, episode_reward=-107.57 +/- 40.87
Episode length: 108.57 +/- 40.87
Eval num_timesteps=11000, episode_reward=-98.16 +/- 28.64
Episode length: 99.16 +/- 28.64
New best mean reward!
FINISHED IN 232.76022046996513 s


starting seed  10355 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-115.46 +/- 25.85
Episode length: 116.46 +/- 25.85
New best mean reward!
Eval num_timesteps=7000, episode_reward=-384.37 +/- 169.64
Episode length: 384.70 +/- 169.19
Eval num_timesteps=7500, episode_reward=-97.26 +/- 41.03
Episode length: 98.26 +/- 41.03
New best mean reward!
FINISHED IN 174.54982506897068 s


starting seed  10356 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-126.25 +/- 52.01
Episode length: 127.24 +/- 51.94
New best mean reward!
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-85.16 +/- 19.39
Episode length: 86.16 +/- 19.39
New best mean reward!
FINISHED IN 209.7375835920102 s


starting seed  10357 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-224.83 +/- 185.47
Episode length: 225.52 +/- 185.01
New best mean reward!
Eval num_timesteps=11000, episode_reward=-149.73 +/- 136.39
Episode length: 150.61 +/- 136.08
New best mean reward!
Eval num_timesteps=11500, episode_reward=-117.70 +/- 45.67
Episode length: 118.69 +/- 45.59
New best mean reward!
Eval num_timesteps=12000, episode_reward=-147.71 +/- 107.15
Episode length: 148.63 +/- 106.88
Eval num_timesteps=12500, episode_reward=-125.04 +/- 80.50
Episode length: 126.00 +/- 80.32
Eval num_timesteps=13000, episode_reward=-119.70 +/- 29.35
Episode length: 120.70 +/- 29.35
Eval num_timesteps=13500, episode_reward=-103.86 +/- 14.54
Episode length: 104.86 +/- 14.54
New best mean reward!
Eval num_timesteps=14000, episode_reward=-87.89 +/- 24.95
Episode length: 88.89 +/- 24.95
New best mean reward!
FINISHED IN 264.87942029803526 s


starting seed  10358 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-498.97 +/- 7.42
Episode length: 498.99 +/- 7.29
New best mean reward!
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-390.31 +/- 75.76
Episode length: 391.15 +/- 75.53
New best mean reward!
Eval num_timesteps=4500, episode_reward=-440.99 +/- 122.83
Episode length: 441.19 +/- 122.45
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-440.42 +/- 102.23
Episode length: 440.70 +/- 101.81
Eval num_timesteps=6000, episode_reward=-467.95 +/- 69.99
Episode length: 468.16 +/- 69.62
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-492.48 +/- 52.67
Episode length: 492.50 +/- 52.53
Eval num_timesteps=9500, episode_reward=-335.08 +/- 194.74
Episode length: 335.50 +/- 194.25
New best mean reward!
Eval num_timesteps=10000, episode_reward=-88.79 +/- 29.72
Episode length: 89.79 +/- 29.72
New best mean reward!
FINISHED IN 239.3784491789993 s


starting seed  10359 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-145.51 +/- 75.32
Episode length: 146.48 +/- 75.18
New best mean reward!
Eval num_timesteps=8000, episode_reward=-205.15 +/- 146.47
Episode length: 205.97 +/- 146.11
Eval num_timesteps=8500, episode_reward=-192.98 +/- 154.79
Episode length: 193.79 +/- 154.42
Eval num_timesteps=9000, episode_reward=-196.55 +/- 156.81
Episode length: 197.36 +/- 156.44
Eval num_timesteps=9500, episode_reward=-146.81 +/- 103.54
Episode length: 147.74 +/- 103.30
Eval num_timesteps=10000, episode_reward=-242.33 +/- 184.33
Episode length: 243.00 +/- 183.87
Eval num_timesteps=10500, episode_reward=-173.38 +/- 144.01
Episode length: 174.24 +/- 143.69
Eval num_timesteps=11000, episode_reward=-122.79 +/- 63.78
Episode length: 123.79 +/- 63.78
New best mean reward!
Eval num_timesteps=11500, episode_reward=-120.81 +/- 90.91
Episode length: 121.77 +/- 90.74
New best mean reward!
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-92.91 +/- 48.08
Episode length: 93.90 +/- 47.99
New best mean reward!
FINISHED IN 261.3011513929814 s


starting seed  10360 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-157.25 +/- 53.14
Episode length: 158.24 +/- 53.08
New best mean reward!
Eval num_timesteps=8000, episode_reward=-144.75 +/- 32.23
Episode length: 145.75 +/- 32.23
New best mean reward!
Eval num_timesteps=8500, episode_reward=-102.33 +/- 21.20
Episode length: 103.33 +/- 21.20
New best mean reward!
Eval num_timesteps=9000, episode_reward=-91.26 +/- 29.44
Episode length: 92.26 +/- 29.44
New best mean reward!
FINISHED IN 200.12178140698234 s


starting seed  10361 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-362.71 +/- 159.96
Episode length: 363.15 +/- 159.48
New best mean reward!
Eval num_timesteps=1500, episode_reward=-227.05 +/- 140.32
Episode length: 227.86 +/- 139.95
New best mean reward!
Eval num_timesteps=2000, episode_reward=-498.08 +/- 18.90
Episode length: 498.10 +/- 18.80
Eval num_timesteps=2500, episode_reward=-317.69 +/- 84.73
Episode length: 318.62 +/- 84.58
Eval num_timesteps=3000, episode_reward=-258.62 +/- 65.29
Episode length: 259.62 +/- 65.29
Eval num_timesteps=3500, episode_reward=-184.84 +/- 58.22
Episode length: 185.84 +/- 58.22
New best mean reward!
Eval num_timesteps=4000, episode_reward=-181.68 +/- 32.60
Episode length: 182.68 +/- 32.60
New best mean reward!
Eval num_timesteps=4500, episode_reward=-175.06 +/- 34.78
Episode length: 176.06 +/- 34.78
New best mean reward!
Eval num_timesteps=5000, episode_reward=-182.61 +/- 63.83
Episode length: 183.59 +/- 63.73
Eval num_timesteps=5500, episode_reward=-172.76 +/- 41.77
Episode length: 173.76 +/- 41.77
New best mean reward!
Eval num_timesteps=6000, episode_reward=-167.86 +/- 40.03
Episode length: 168.86 +/- 40.03
New best mean reward!
Eval num_timesteps=6500, episode_reward=-164.13 +/- 36.27
Episode length: 165.13 +/- 36.27
New best mean reward!
Eval num_timesteps=7000, episode_reward=-169.02 +/- 36.07
Episode length: 170.02 +/- 36.07
Eval num_timesteps=7500, episode_reward=-234.17 +/- 148.65
Episode length: 234.94 +/- 148.24
Eval num_timesteps=8000, episode_reward=-197.04 +/- 122.21
Episode length: 197.92 +/- 121.91
Eval num_timesteps=8500, episode_reward=-146.42 +/- 110.20
Episode length: 147.34 +/- 109.94
New best mean reward!
Eval num_timesteps=9000, episode_reward=-164.74 +/- 131.28
Episode length: 165.62 +/- 130.97
Eval num_timesteps=9500, episode_reward=-172.53 +/- 142.85
Episode length: 173.38 +/- 142.50
Eval num_timesteps=10000, episode_reward=-138.97 +/- 116.46
Episode length: 139.88 +/- 116.18
New best mean reward!
Eval num_timesteps=10500, episode_reward=-148.00 +/- 124.17
Episode length: 148.90 +/- 123.88
Eval num_timesteps=11000, episode_reward=-160.99 +/- 146.65
Episode length: 161.85 +/- 146.33
Eval num_timesteps=11500, episode_reward=-206.26 +/- 174.45
Episode length: 207.01 +/- 174.02
Eval num_timesteps=12000, episode_reward=-138.53 +/- 109.73
Episode length: 139.47 +/- 109.53
New best mean reward!
Eval num_timesteps=12500, episode_reward=-179.46 +/- 153.39
Episode length: 180.29 +/- 153.04
Eval num_timesteps=13000, episode_reward=-140.25 +/- 117.92
Episode length: 141.17 +/- 117.68
Eval num_timesteps=13500, episode_reward=-169.64 +/- 140.47
Episode length: 170.50 +/- 140.14
Eval num_timesteps=14000, episode_reward=-254.61 +/- 193.38
Episode length: 255.24 +/- 192.91
Eval num_timesteps=14500, episode_reward=-202.27 +/- 171.61
Episode length: 203.04 +/- 171.22
Eval num_timesteps=15000, episode_reward=-162.64 +/- 141.35
Episode length: 163.50 +/- 141.01
Eval num_timesteps=15500, episode_reward=-172.78 +/- 153.41
Episode length: 173.64 +/- 153.12
Eval num_timesteps=16000, episode_reward=-99.72 +/- 51.46
Episode length: 100.72 +/- 51.46
New best mean reward!
FINISHED IN 283.5901932370034 s


starting seed  10362 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-473.30 +/- 90.60
Episode length: 473.38 +/- 90.33
New best mean reward!
Eval num_timesteps=13500, episode_reward=-376.43 +/- 172.75
Episode length: 376.78 +/- 172.28
New best mean reward!
Eval num_timesteps=14000, episode_reward=-329.67 +/- 189.44
Episode length: 330.13 +/- 188.95
New best mean reward!
Eval num_timesteps=14500, episode_reward=-316.73 +/- 183.25
Episode length: 317.24 +/- 182.76
New best mean reward!
Eval num_timesteps=15000, episode_reward=-197.52 +/- 168.50
Episode length: 198.30 +/- 168.10
New best mean reward!
Eval num_timesteps=15500, episode_reward=-238.62 +/- 182.87
Episode length: 239.30 +/- 182.42
Eval num_timesteps=16000, episode_reward=-214.04 +/- 168.67
Episode length: 214.80 +/- 168.27
Eval num_timesteps=16500, episode_reward=-204.88 +/- 161.80
Episode length: 205.66 +/- 161.40
Eval num_timesteps=17000, episode_reward=-236.49 +/- 178.66
Episode length: 237.20 +/- 178.23
Eval num_timesteps=17500, episode_reward=-276.51 +/- 188.31
Episode length: 277.12 +/- 187.84
Eval num_timesteps=18000, episode_reward=-164.85 +/- 140.76
Episode length: 165.72 +/- 140.45
New best mean reward!
Eval num_timesteps=18500, episode_reward=-165.54 +/- 143.25
Episode length: 166.40 +/- 142.92
Eval num_timesteps=19000, episode_reward=-138.10 +/- 117.76
Episode length: 139.01 +/- 117.49
New best mean reward!
Eval num_timesteps=19500, episode_reward=-137.47 +/- 117.96
Episode length: 138.38 +/- 117.68
New best mean reward!
Eval num_timesteps=20000, episode_reward=-137.71 +/- 118.43
Episode length: 138.62 +/- 118.15
Eval num_timesteps=20500, episode_reward=-117.82 +/- 84.18
Episode length: 118.78 +/- 83.99
New best mean reward!
Eval num_timesteps=21000, episode_reward=-130.34 +/- 108.79
Episode length: 131.29 +/- 108.62
Eval num_timesteps=21500, episode_reward=-119.35 +/- 87.99
Episode length: 120.31 +/- 87.82
Eval num_timesteps=22000, episode_reward=-115.13 +/- 79.78
Episode length: 116.10 +/- 79.64
New best mean reward!
Eval num_timesteps=22500, episode_reward=-132.29 +/- 97.67
Episode length: 133.25 +/- 97.51
Eval num_timesteps=23000, episode_reward=-112.41 +/- 65.77
Episode length: 113.39 +/- 65.65
New best mean reward!
Eval num_timesteps=23500, episode_reward=-107.74 +/- 77.63
Episode length: 108.71 +/- 77.48
New best mean reward!
Eval num_timesteps=24000, episode_reward=-94.09 +/- 32.70
Episode length: 95.09 +/- 32.70
New best mean reward!
FINISHED IN 550.7354894749587 s


starting seed  10363 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-350.44 +/- 170.31
Episode length: 350.88 +/- 169.81
New best mean reward!
Eval num_timesteps=8000, episode_reward=-152.62 +/- 37.19
Episode length: 153.62 +/- 37.19
New best mean reward!
Eval num_timesteps=8500, episode_reward=-182.55 +/- 90.87
Episode length: 183.48 +/- 90.62
Eval num_timesteps=9000, episode_reward=-163.56 +/- 50.32
Episode length: 164.55 +/- 50.25
Eval num_timesteps=9500, episode_reward=-156.51 +/- 36.41
Episode length: 157.51 +/- 36.41
Eval num_timesteps=10000, episode_reward=-165.01 +/- 73.33
Episode length: 165.97 +/- 73.14
Eval num_timesteps=10500, episode_reward=-156.18 +/- 30.93
Episode length: 157.18 +/- 30.93
Eval num_timesteps=11000, episode_reward=-159.79 +/- 30.48
Episode length: 160.79 +/- 30.48
Eval num_timesteps=11500, episode_reward=-156.00 +/- 39.79
Episode length: 157.00 +/- 39.79
Eval num_timesteps=12000, episode_reward=-166.66 +/- 57.29
Episode length: 167.64 +/- 57.17
Eval num_timesteps=12500, episode_reward=-177.25 +/- 93.56
Episode length: 178.18 +/- 93.32
Eval num_timesteps=13000, episode_reward=-191.79 +/- 109.72
Episode length: 192.69 +/- 109.44
Eval num_timesteps=13500, episode_reward=-393.76 +/- 156.11
Episode length: 394.09 +/- 155.65
Eval num_timesteps=14000, episode_reward=-451.10 +/- 117.39
Episode length: 451.25 +/- 117.04
Eval num_timesteps=14500, episode_reward=-307.55 +/- 171.16
Episode length: 308.12 +/- 170.67
Eval num_timesteps=15000, episode_reward=-247.40 +/- 151.15
Episode length: 248.14 +/- 150.71
Eval num_timesteps=15500, episode_reward=-283.11 +/- 165.64
Episode length: 283.75 +/- 165.17
Eval num_timesteps=16000, episode_reward=-155.14 +/- 34.56
Episode length: 156.14 +/- 34.56
Eval num_timesteps=16500, episode_reward=-159.96 +/- 48.43
Episode length: 160.95 +/- 48.36
Eval num_timesteps=17000, episode_reward=-161.76 +/- 37.58
Episode length: 162.76 +/- 37.58
Eval num_timesteps=17500, episode_reward=-158.41 +/- 35.24
Episode length: 159.41 +/- 35.24
Eval num_timesteps=18000, episode_reward=-156.45 +/- 35.70
Episode length: 157.45 +/- 35.70
Eval num_timesteps=18500, episode_reward=-157.86 +/- 29.46
Episode length: 158.86 +/- 29.46
Eval num_timesteps=19000, episode_reward=-155.14 +/- 43.37
Episode length: 156.14 +/- 43.37
Eval num_timesteps=19500, episode_reward=-156.95 +/- 33.62
Episode length: 157.95 +/- 33.62
Eval num_timesteps=20000, episode_reward=-164.74 +/- 40.99
Episode length: 165.74 +/- 40.99
Eval num_timesteps=20500, episode_reward=-163.74 +/- 43.32
Episode length: 164.74 +/- 43.32
Eval num_timesteps=21000, episode_reward=-154.46 +/- 31.13
Episode length: 155.46 +/- 31.13
Eval num_timesteps=21500, episode_reward=-158.86 +/- 41.43
Episode length: 159.86 +/- 41.43
Eval num_timesteps=22000, episode_reward=-160.57 +/- 38.97
Episode length: 161.57 +/- 38.97
Eval num_timesteps=22500, episode_reward=-162.31 +/- 46.09
Episode length: 163.30 +/- 46.02
Eval num_timesteps=23000, episode_reward=-161.64 +/- 43.48
Episode length: 162.64 +/- 43.48
Eval num_timesteps=23500, episode_reward=-155.71 +/- 29.95
Episode length: 156.71 +/- 29.95
Eval num_timesteps=24000, episode_reward=-154.12 +/- 33.70
Episode length: 155.12 +/- 33.70
Eval num_timesteps=24500, episode_reward=-157.00 +/- 28.68
Episode length: 158.00 +/- 28.68
Eval num_timesteps=25000, episode_reward=-156.88 +/- 31.16
Episode length: 157.88 +/- 31.16
Eval num_timesteps=25500, episode_reward=-159.55 +/- 42.95
Episode length: 160.54 +/- 42.87
Eval num_timesteps=26000, episode_reward=-156.12 +/- 46.56
Episode length: 157.11 +/- 46.48
Eval num_timesteps=26500, episode_reward=-155.71 +/- 37.19
Episode length: 156.71 +/- 37.19
Eval num_timesteps=27000, episode_reward=-153.40 +/- 29.08
Episode length: 154.40 +/- 29.08
Eval num_timesteps=27500, episode_reward=-155.87 +/- 32.32
Episode length: 156.87 +/- 32.32
Eval num_timesteps=28000, episode_reward=-160.27 +/- 52.98
Episode length: 161.26 +/- 52.91
Eval num_timesteps=28500, episode_reward=-156.91 +/- 28.78
Episode length: 157.91 +/- 28.78
Eval num_timesteps=29000, episode_reward=-161.13 +/- 37.49
Episode length: 162.13 +/- 37.49
Eval num_timesteps=29500, episode_reward=-157.64 +/- 46.99
Episode length: 158.63 +/- 46.92
Eval num_timesteps=30000, episode_reward=-158.60 +/- 38.45
Episode length: 159.60 +/- 38.45
Eval num_timesteps=30500, episode_reward=-154.97 +/- 31.26
Episode length: 155.97 +/- 31.26
Eval num_timesteps=31000, episode_reward=-161.57 +/- 50.76
Episode length: 162.56 +/- 50.70
Eval num_timesteps=31500, episode_reward=-158.52 +/- 31.67
Episode length: 159.52 +/- 31.67
Eval num_timesteps=32000, episode_reward=-163.06 +/- 47.03
Episode length: 164.06 +/- 47.03
Eval num_timesteps=32500, episode_reward=-161.42 +/- 41.43
Episode length: 162.42 +/- 41.43
Eval num_timesteps=33000, episode_reward=-159.84 +/- 43.82
Episode length: 160.83 +/- 43.74
Eval num_timesteps=33500, episode_reward=-128.40 +/- 26.60
Episode length: 129.40 +/- 26.60
New best mean reward!
Eval num_timesteps=34000, episode_reward=-161.15 +/- 49.42
Episode length: 162.14 +/- 49.36
Eval num_timesteps=34500, episode_reward=-106.99 +/- 19.70
Episode length: 107.99 +/- 19.70
New best mean reward!
Eval num_timesteps=35000, episode_reward=-105.75 +/- 27.66
Episode length: 106.75 +/- 27.66
New best mean reward!
Eval num_timesteps=35500, episode_reward=-106.88 +/- 35.04
Episode length: 107.88 +/- 35.04
Eval num_timesteps=36000, episode_reward=-108.56 +/- 38.69
Episode length: 109.56 +/- 38.69
Eval num_timesteps=36500, episode_reward=-107.04 +/- 49.54
Episode length: 108.03 +/- 49.46
Eval num_timesteps=37000, episode_reward=-107.44 +/- 39.04
Episode length: 108.44 +/- 39.04
Eval num_timesteps=37500, episode_reward=-105.88 +/- 38.59
Episode length: 106.88 +/- 38.59
Eval num_timesteps=38000, episode_reward=-105.43 +/- 27.88
Episode length: 106.43 +/- 27.88
New best mean reward!
Eval num_timesteps=38500, episode_reward=-104.60 +/- 32.05
Episode length: 105.60 +/- 32.05
New best mean reward!
Eval num_timesteps=39000, episode_reward=-120.49 +/- 56.93
Episode length: 121.48 +/- 56.86
Eval num_timesteps=39500, episode_reward=-107.98 +/- 40.00
Episode length: 108.98 +/- 40.00
Eval num_timesteps=40000, episode_reward=-105.30 +/- 35.93
Episode length: 106.30 +/- 35.93
Eval num_timesteps=40500, episode_reward=-110.25 +/- 42.73
Episode length: 111.25 +/- 42.73
Eval num_timesteps=41000, episode_reward=-97.92 +/- 22.20
Episode length: 98.92 +/- 22.20
New best mean reward!
FINISHED IN 568.6060865799664 s


starting seed  10364 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-459.72 +/- 105.00
Episode length: 459.86 +/- 104.67
New best mean reward!
Eval num_timesteps=1000, episode_reward=-119.74 +/- 15.94
Episode length: 120.74 +/- 15.94
New best mean reward!
Eval num_timesteps=1500, episode_reward=-198.28 +/- 160.76
Episode length: 199.07 +/- 160.36
Eval num_timesteps=2000, episode_reward=-472.48 +/- 63.81
Episode length: 472.65 +/- 63.45
Eval num_timesteps=2500, episode_reward=-197.29 +/- 46.41
Episode length: 198.28 +/- 46.34
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-355.02 +/- 85.15
Episode length: 355.84 +/- 84.85
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-466.24 +/- 91.61
Episode length: 466.36 +/- 91.28
Eval num_timesteps=9000, episode_reward=-497.03 +/- 29.55
Episode length: 497.04 +/- 29.45
Eval num_timesteps=9500, episode_reward=-191.29 +/- 64.27
Episode length: 192.27 +/- 64.17
Eval num_timesteps=10000, episode_reward=-196.95 +/- 97.59
Episode length: 197.88 +/- 97.37
Eval num_timesteps=10500, episode_reward=-174.60 +/- 56.72
Episode length: 175.58 +/- 56.60
Eval num_timesteps=11000, episode_reward=-132.07 +/- 87.39
Episode length: 133.03 +/- 87.22
Eval num_timesteps=11500, episode_reward=-169.40 +/- 45.34
Episode length: 170.39 +/- 45.27
Eval num_timesteps=12000, episode_reward=-185.79 +/- 46.48
Episode length: 186.78 +/- 46.42
Eval num_timesteps=12500, episode_reward=-193.29 +/- 81.29
Episode length: 194.23 +/- 81.07
Eval num_timesteps=13000, episode_reward=-193.55 +/- 67.41
Episode length: 194.52 +/- 67.27
Eval num_timesteps=13500, episode_reward=-182.97 +/- 46.11
Episode length: 183.96 +/- 46.04
Eval num_timesteps=14000, episode_reward=-280.40 +/- 36.81
Episode length: 281.40 +/- 36.81
Eval num_timesteps=14500, episode_reward=-499.82 +/- 1.27
Episode length: 499.84 +/- 1.13
Eval num_timesteps=15000, episode_reward=-491.24 +/- 17.97
Episode length: 491.53 +/- 17.63
Eval num_timesteps=15500, episode_reward=-185.65 +/- 40.25
Episode length: 186.64 +/- 40.17
Eval num_timesteps=16000, episode_reward=-142.62 +/- 56.09
Episode length: 143.60 +/- 55.97
Eval num_timesteps=16500, episode_reward=-175.42 +/- 63.97
Episode length: 176.40 +/- 63.87
Eval num_timesteps=17000, episode_reward=-157.47 +/- 30.39
Episode length: 158.47 +/- 30.39
Eval num_timesteps=17500, episode_reward=-128.29 +/- 22.02
Episode length: 129.29 +/- 22.02
Eval num_timesteps=18000, episode_reward=-175.71 +/- 47.51
Episode length: 176.70 +/- 47.44
Eval num_timesteps=18500, episode_reward=-172.40 +/- 30.05
Episode length: 173.40 +/- 30.05
Eval num_timesteps=19000, episode_reward=-180.12 +/- 48.27
Episode length: 181.11 +/- 48.20
Eval num_timesteps=19500, episode_reward=-172.12 +/- 34.35
Episode length: 173.12 +/- 34.35
Eval num_timesteps=20000, episode_reward=-177.09 +/- 46.83
Episode length: 178.08 +/- 46.77
Eval num_timesteps=20500, episode_reward=-175.94 +/- 38.19
Episode length: 176.94 +/- 38.19
Eval num_timesteps=21000, episode_reward=-166.23 +/- 33.00
Episode length: 167.23 +/- 33.00
Eval num_timesteps=21500, episode_reward=-168.49 +/- 44.23
Episode length: 169.48 +/- 44.15
Eval num_timesteps=22000, episode_reward=-127.92 +/- 22.64
Episode length: 128.92 +/- 22.64
Eval num_timesteps=22500, episode_reward=-90.58 +/- 20.68
Episode length: 91.58 +/- 20.68
New best mean reward!
FINISHED IN 461.13403103803284 s


starting seed  10365 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=17000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=17500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=18000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=18500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=19000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=19500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=20000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=20500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=21000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=21500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=22000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=22500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=23000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=23500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=24000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=24500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=25000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=25500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=26000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=26500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=27000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=27500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=28000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=28500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=29000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=29500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=30000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=30500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=31000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=31500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=32000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=32500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=33000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=33500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=34000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=34500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=35000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=35500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=36000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=36500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=37000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=37500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=38000, episode_reward=-487.90 +/- 68.82
Episode length: 487.93 +/- 68.65
New best mean reward!
Eval num_timesteps=38500, episode_reward=-460.06 +/- 120.02
Episode length: 460.16 +/- 119.72
New best mean reward!
Eval num_timesteps=39000, episode_reward=-419.55 +/- 160.99
Episode length: 419.75 +/- 160.59
New best mean reward!
Eval num_timesteps=39500, episode_reward=-408.68 +/- 164.97
Episode length: 408.92 +/- 164.55
New best mean reward!
Eval num_timesteps=40000, episode_reward=-396.58 +/- 175.22
Episode length: 396.84 +/- 174.79
New best mean reward!
Eval num_timesteps=40500, episode_reward=-416.21 +/- 159.08
Episode length: 416.43 +/- 158.67
Eval num_timesteps=41000, episode_reward=-344.31 +/- 199.17
Episode length: 344.69 +/- 198.68
New best mean reward!
Eval num_timesteps=41500, episode_reward=-342.69 +/- 198.13
Episode length: 343.08 +/- 197.64
New best mean reward!
Eval num_timesteps=42000, episode_reward=-341.59 +/- 198.96
Episode length: 341.98 +/- 198.47
New best mean reward!
Eval num_timesteps=42500, episode_reward=-305.45 +/- 207.22
Episode length: 305.92 +/- 206.72
New best mean reward!
Eval num_timesteps=43000, episode_reward=-303.30 +/- 205.27
Episode length: 303.78 +/- 204.77
New best mean reward!
Eval num_timesteps=43500, episode_reward=-356.78 +/- 195.52
Episode length: 357.13 +/- 195.04
Eval num_timesteps=44000, episode_reward=-282.41 +/- 205.29
Episode length: 282.94 +/- 204.80
New best mean reward!
Eval num_timesteps=44500, episode_reward=-313.75 +/- 204.00
Episode length: 314.21 +/- 203.51
Eval num_timesteps=45000, episode_reward=-295.22 +/- 205.56
Episode length: 295.72 +/- 205.06
Eval num_timesteps=45500, episode_reward=-323.91 +/- 203.21
Episode length: 324.34 +/- 202.71
Eval num_timesteps=46000, episode_reward=-278.62 +/- 205.45
Episode length: 279.16 +/- 204.95
New best mean reward!
Eval num_timesteps=46500, episode_reward=-238.63 +/- 199.50
Episode length: 239.27 +/- 199.03
New best mean reward!
Eval num_timesteps=47000, episode_reward=-247.00 +/- 202.81
Episode length: 247.61 +/- 202.32
Eval num_timesteps=47500, episode_reward=-295.04 +/- 207.59
Episode length: 295.54 +/- 207.10
Eval num_timesteps=48000, episode_reward=-224.79 +/- 194.13
Episode length: 225.46 +/- 193.66
New best mean reward!
Eval num_timesteps=48500, episode_reward=-276.09 +/- 206.17
Episode length: 276.64 +/- 205.68
Eval num_timesteps=49000, episode_reward=-243.34 +/- 201.72
Episode length: 243.97 +/- 201.25
Eval num_timesteps=49500, episode_reward=-270.80 +/- 203.72
Episode length: 271.37 +/- 203.23
Eval num_timesteps=50000, episode_reward=-264.32 +/- 203.18
Episode length: 264.90 +/- 202.69
FINISHED IN 1335.6946676509688 s


starting seed  10366 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-265.55 +/- 55.48
Episode length: 266.54 +/- 55.44
New best mean reward!
Eval num_timesteps=6500, episode_reward=-304.38 +/- 151.01
Episode length: 305.02 +/- 150.54
Eval num_timesteps=7000, episode_reward=-211.62 +/- 95.81
Episode length: 212.54 +/- 95.57
New best mean reward!
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-460.78 +/- 102.75
Episode length: 460.91 +/- 102.42
Eval num_timesteps=11000, episode_reward=-427.82 +/- 130.20
Episode length: 428.06 +/- 129.78
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-295.33 +/- 155.94
Episode length: 295.97 +/- 155.47
Eval num_timesteps=13000, episode_reward=-271.81 +/- 143.68
Episode length: 272.54 +/- 143.25
Eval num_timesteps=13500, episode_reward=-255.57 +/- 140.27
Episode length: 256.33 +/- 139.85
Eval num_timesteps=14000, episode_reward=-191.30 +/- 93.52
Episode length: 192.24 +/- 93.32
New best mean reward!
Eval num_timesteps=14500, episode_reward=-220.56 +/- 120.44
Episode length: 221.43 +/- 120.14
Eval num_timesteps=15000, episode_reward=-202.12 +/- 111.08
Episode length: 203.01 +/- 110.78
Eval num_timesteps=15500, episode_reward=-176.50 +/- 47.68
Episode length: 177.50 +/- 47.68
New best mean reward!
Eval num_timesteps=16000, episode_reward=-165.69 +/- 36.98
Episode length: 166.69 +/- 36.98
New best mean reward!
Eval num_timesteps=16500, episode_reward=-187.90 +/- 71.62
Episode length: 188.87 +/- 71.49
Eval num_timesteps=17000, episode_reward=-189.35 +/- 89.17
Episode length: 190.29 +/- 88.96
Eval num_timesteps=17500, episode_reward=-172.11 +/- 60.67
Episode length: 173.09 +/- 60.57
Eval num_timesteps=18000, episode_reward=-193.53 +/- 92.04
Episode length: 194.46 +/- 91.80
Eval num_timesteps=18500, episode_reward=-187.62 +/- 83.40
Episode length: 188.57 +/- 83.21
Eval num_timesteps=19000, episode_reward=-208.93 +/- 104.76
Episode length: 209.83 +/- 104.48
Eval num_timesteps=19500, episode_reward=-190.18 +/- 83.23
Episode length: 191.13 +/- 83.05
Eval num_timesteps=20000, episode_reward=-178.43 +/- 55.93
Episode length: 179.41 +/- 55.82
Eval num_timesteps=20500, episode_reward=-184.56 +/- 75.23
Episode length: 185.52 +/- 75.06
Eval num_timesteps=21000, episode_reward=-205.87 +/- 116.91
Episode length: 206.75 +/- 116.61
Eval num_timesteps=21500, episode_reward=-149.34 +/- 87.53
Episode length: 150.29 +/- 87.33
New best mean reward!
Eval num_timesteps=22000, episode_reward=-210.10 +/- 117.43
Episode length: 210.97 +/- 117.11
Eval num_timesteps=22500, episode_reward=-255.44 +/- 154.11
Episode length: 256.19 +/- 153.72
Eval num_timesteps=23000, episode_reward=-264.63 +/- 147.13
Episode length: 265.36 +/- 146.70
Eval num_timesteps=23500, episode_reward=-233.70 +/- 130.46
Episode length: 234.53 +/- 130.11
Eval num_timesteps=24000, episode_reward=-278.36 +/- 181.55
Episode length: 278.97 +/- 181.07
Eval num_timesteps=24500, episode_reward=-246.66 +/- 165.02
Episode length: 247.38 +/- 164.59
Eval num_timesteps=25000, episode_reward=-245.89 +/- 179.42
Episode length: 246.57 +/- 178.97
Eval num_timesteps=25500, episode_reward=-169.28 +/- 158.06
Episode length: 170.10 +/- 157.69
Eval num_timesteps=26000, episode_reward=-100.03 +/- 77.81
Episode length: 101.00 +/- 77.66
New best mean reward!
Eval num_timesteps=26500, episode_reward=-119.21 +/- 114.39
Episode length: 120.13 +/- 114.13
Eval num_timesteps=27000, episode_reward=-148.59 +/- 145.97
Episode length: 149.45 +/- 145.63
Eval num_timesteps=27500, episode_reward=-126.36 +/- 114.15
Episode length: 127.29 +/- 113.92
Eval num_timesteps=28000, episode_reward=-123.92 +/- 56.07
Episode length: 124.91 +/- 56.00
Eval num_timesteps=28500, episode_reward=-143.65 +/- 118.93
Episode length: 144.56 +/- 118.66
Eval num_timesteps=29000, episode_reward=-179.21 +/- 137.55
Episode length: 180.06 +/- 137.20
Eval num_timesteps=29500, episode_reward=-129.11 +/- 121.33
Episode length: 130.02 +/- 121.05
Eval num_timesteps=30000, episode_reward=-108.19 +/- 91.96
Episode length: 109.14 +/- 91.75
Eval num_timesteps=30500, episode_reward=-122.28 +/- 113.49
Episode length: 123.21 +/- 113.26
Eval num_timesteps=31000, episode_reward=-123.96 +/- 120.40
Episode length: 124.87 +/- 120.12
Eval num_timesteps=31500, episode_reward=-116.28 +/- 105.04
Episode length: 117.22 +/- 104.82
Eval num_timesteps=32000, episode_reward=-130.44 +/- 130.90
Episode length: 131.33 +/- 130.59
Eval num_timesteps=32500, episode_reward=-134.56 +/- 134.29
Episode length: 135.46 +/- 134.01
Eval num_timesteps=33000, episode_reward=-120.65 +/- 110.25
Episode length: 121.58 +/- 110.01
Eval num_timesteps=33500, episode_reward=-128.81 +/- 125.65
Episode length: 129.71 +/- 125.35
Eval num_timesteps=34000, episode_reward=-124.68 +/- 112.67
Episode length: 125.61 +/- 112.44
Eval num_timesteps=34500, episode_reward=-118.56 +/- 107.89
Episode length: 119.49 +/- 107.64
Eval num_timesteps=35000, episode_reward=-119.34 +/- 103.28
Episode length: 120.28 +/- 103.06
Eval num_timesteps=35500, episode_reward=-139.14 +/- 134.99
Episode length: 140.02 +/- 134.67
Eval num_timesteps=36000, episode_reward=-152.19 +/- 145.15
Episode length: 153.05 +/- 144.82
Eval num_timesteps=36500, episode_reward=-115.21 +/- 103.82
Episode length: 116.16 +/- 103.64
Eval num_timesteps=37000, episode_reward=-115.05 +/- 106.71
Episode length: 115.99 +/- 106.50
Eval num_timesteps=37500, episode_reward=-124.70 +/- 115.82
Episode length: 125.62 +/- 115.56
Eval num_timesteps=38000, episode_reward=-113.33 +/- 101.31
Episode length: 114.27 +/- 101.08
Eval num_timesteps=38500, episode_reward=-90.48 +/- 56.09
Episode length: 91.47 +/- 56.02
New best mean reward!
FINISHED IN 613.4455949839903 s


starting seed  10367 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-496.78 +/- 32.04
Episode length: 496.79 +/- 31.94
New best mean reward!
Eval num_timesteps=11000, episode_reward=-456.52 +/- 112.32
Episode length: 456.66 +/- 111.99
New best mean reward!
Eval num_timesteps=11500, episode_reward=-327.71 +/- 168.19
Episode length: 328.23 +/- 167.69
New best mean reward!
Eval num_timesteps=12000, episode_reward=-349.93 +/- 169.07
Episode length: 350.39 +/- 168.59
Eval num_timesteps=12500, episode_reward=-337.83 +/- 173.24
Episode length: 338.30 +/- 172.74
Eval num_timesteps=13000, episode_reward=-408.09 +/- 145.41
Episode length: 408.39 +/- 144.97
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-493.19 +/- 47.86
Episode length: 493.21 +/- 47.73
Eval num_timesteps=14500, episode_reward=-362.87 +/- 170.33
Episode length: 363.27 +/- 169.84
Eval num_timesteps=15000, episode_reward=-301.08 +/- 171.18
Episode length: 301.66 +/- 170.69
New best mean reward!
Eval num_timesteps=15500, episode_reward=-277.24 +/- 182.45
Episode length: 277.84 +/- 181.96
New best mean reward!
Eval num_timesteps=16000, episode_reward=-228.61 +/- 182.75
Episode length: 229.30 +/- 182.29
New best mean reward!
Eval num_timesteps=16500, episode_reward=-223.60 +/- 180.40
Episode length: 224.32 +/- 179.97
New best mean reward!
Eval num_timesteps=17000, episode_reward=-227.04 +/- 192.84
Episode length: 227.71 +/- 192.37
Eval num_timesteps=17500, episode_reward=-126.50 +/- 97.07
Episode length: 127.44 +/- 96.84
New best mean reward!
Eval num_timesteps=18000, episode_reward=-107.35 +/- 61.38
Episode length: 108.33 +/- 61.25
New best mean reward!
Eval num_timesteps=18500, episode_reward=-113.88 +/- 74.82
Episode length: 114.85 +/- 74.66
Eval num_timesteps=19000, episode_reward=-105.30 +/- 50.08
Episode length: 106.29 +/- 50.00
New best mean reward!
Eval num_timesteps=19500, episode_reward=-131.78 +/- 125.09
Episode length: 132.68 +/- 124.79
Eval num_timesteps=20000, episode_reward=-165.54 +/- 154.75
Episode length: 166.38 +/- 154.40
Eval num_timesteps=20500, episode_reward=-112.30 +/- 84.67
Episode length: 113.26 +/- 84.49
Eval num_timesteps=21000, episode_reward=-127.07 +/- 101.41
Episode length: 128.01 +/- 101.19
Eval num_timesteps=21500, episode_reward=-119.77 +/- 94.93
Episode length: 120.72 +/- 94.73
Eval num_timesteps=22000, episode_reward=-119.65 +/- 102.24
Episode length: 120.59 +/- 102.01
Eval num_timesteps=22500, episode_reward=-123.05 +/- 113.12
Episode length: 123.97 +/- 112.85
Eval num_timesteps=23000, episode_reward=-124.01 +/- 119.30
Episode length: 124.92 +/- 119.02
Eval num_timesteps=23500, episode_reward=-136.24 +/- 135.16
Episode length: 137.12 +/- 134.83
Eval num_timesteps=24000, episode_reward=-88.61 +/- 22.03
Episode length: 89.61 +/- 22.03
New best mean reward!
FINISHED IN 537.8511577119934 s


starting seed  10368 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-200.89 +/- 45.06
Episode length: 201.89 +/- 45.06
New best mean reward!
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-119.05 +/- 60.80
Episode length: 120.03 +/- 60.68
New best mean reward!
Eval num_timesteps=8000, episode_reward=-111.36 +/- 37.12
Episode length: 112.36 +/- 37.12
New best mean reward!
Eval num_timesteps=8500, episode_reward=-100.77 +/- 39.17
Episode length: 101.77 +/- 39.17
New best mean reward!
Eval num_timesteps=9000, episode_reward=-105.76 +/- 26.78
Episode length: 106.76 +/- 26.78
Eval num_timesteps=9500, episode_reward=-144.21 +/- 115.45
Episode length: 145.12 +/- 115.17
Eval num_timesteps=10000, episode_reward=-252.41 +/- 176.26
Episode length: 253.08 +/- 175.80
Eval num_timesteps=10500, episode_reward=-102.94 +/- 28.01
Episode length: 103.94 +/- 28.01
Eval num_timesteps=11000, episode_reward=-102.29 +/- 21.18
Episode length: 103.29 +/- 21.18
Eval num_timesteps=11500, episode_reward=-113.24 +/- 19.58
Episode length: 114.24 +/- 19.58
Eval num_timesteps=12000, episode_reward=-89.06 +/- 18.40
Episode length: 90.06 +/- 18.40
New best mean reward!
FINISHED IN 217.08973683498334 s


starting seed  10369 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-91.30 +/- 50.46
Episode length: 92.29 +/- 50.38
New best mean reward!
FINISHED IN 278.7532078709919 s


starting seed  10370 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-170.28 +/- 48.19
Episode length: 171.27 +/- 48.12
New best mean reward!
Eval num_timesteps=15000, episode_reward=-172.34 +/- 31.29
Episode length: 173.34 +/- 31.29
Eval num_timesteps=15500, episode_reward=-238.50 +/- 55.80
Episode length: 239.47 +/- 55.66
Eval num_timesteps=16000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16500, episode_reward=-243.21 +/- 47.61
Episode length: 244.19 +/- 47.50
Eval num_timesteps=17000, episode_reward=-170.13 +/- 91.13
Episode length: 171.07 +/- 90.91
New best mean reward!
Eval num_timesteps=17500, episode_reward=-268.98 +/- 91.50
Episode length: 269.86 +/- 91.20
Eval num_timesteps=18000, episode_reward=-348.68 +/- 126.14
Episode length: 349.30 +/- 125.69
Eval num_timesteps=18500, episode_reward=-355.93 +/- 67.75
Episode length: 356.80 +/- 67.48
Eval num_timesteps=19000, episode_reward=-359.47 +/- 49.30
Episode length: 360.42 +/- 49.16
Eval num_timesteps=19500, episode_reward=-126.48 +/- 18.70
Episode length: 127.48 +/- 18.70
New best mean reward!
Eval num_timesteps=20000, episode_reward=-109.30 +/- 28.96
Episode length: 110.30 +/- 28.96
New best mean reward!
Eval num_timesteps=20500, episode_reward=-137.24 +/- 22.85
Episode length: 138.24 +/- 22.85
Eval num_timesteps=21000, episode_reward=-273.23 +/- 97.79
Episode length: 274.10 +/- 97.49
Eval num_timesteps=21500, episode_reward=-148.74 +/- 63.91
Episode length: 149.72 +/- 63.80
Eval num_timesteps=22000, episode_reward=-157.09 +/- 91.19
Episode length: 158.03 +/- 90.96
Eval num_timesteps=22500, episode_reward=-249.88 +/- 138.98
Episode length: 250.66 +/- 138.59
Eval num_timesteps=23000, episode_reward=-157.80 +/- 90.45
Episode length: 158.74 +/- 90.22
Eval num_timesteps=23500, episode_reward=-136.73 +/- 28.95
Episode length: 137.73 +/- 28.95
Eval num_timesteps=24000, episode_reward=-118.62 +/- 26.46
Episode length: 119.62 +/- 26.46
Eval num_timesteps=24500, episode_reward=-112.22 +/- 32.13
Episode length: 113.22 +/- 32.13
Eval num_timesteps=25000, episode_reward=-106.35 +/- 26.59
Episode length: 107.35 +/- 26.59
New best mean reward!
Eval num_timesteps=25500, episode_reward=-104.55 +/- 49.49
Episode length: 105.54 +/- 49.41
New best mean reward!
Eval num_timesteps=26000, episode_reward=-87.98 +/- 24.84
Episode length: 88.98 +/- 24.84
New best mean reward!
FINISHED IN 530.0622723990236 s


starting seed  10371 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-479.11 +/- 63.61
Episode length: 479.21 +/- 63.31
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-456.89 +/- 66.67
Episode length: 457.28 +/- 66.27
New best mean reward!
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-196.00 +/- 62.45
Episode length: 196.97 +/- 62.31
New best mean reward!
Eval num_timesteps=8500, episode_reward=-182.08 +/- 49.06
Episode length: 183.07 +/- 49.00
New best mean reward!
Eval num_timesteps=9000, episode_reward=-177.59 +/- 42.37
Episode length: 178.58 +/- 42.30
New best mean reward!
Eval num_timesteps=9500, episode_reward=-176.20 +/- 34.05
Episode length: 177.20 +/- 34.05
New best mean reward!
Eval num_timesteps=10000, episode_reward=-259.42 +/- 53.07
Episode length: 260.41 +/- 53.02
Eval num_timesteps=10500, episode_reward=-289.99 +/- 52.27
Episode length: 290.98 +/- 52.23
Eval num_timesteps=11000, episode_reward=-268.00 +/- 60.78
Episode length: 268.97 +/- 60.66
Eval num_timesteps=11500, episode_reward=-221.52 +/- 44.88
Episode length: 222.51 +/- 44.81
Eval num_timesteps=12000, episode_reward=-230.34 +/- 52.71
Episode length: 231.33 +/- 52.65
Eval num_timesteps=12500, episode_reward=-326.85 +/- 63.92
Episode length: 327.85 +/- 63.92
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-287.12 +/- 111.21
Episode length: 287.93 +/- 110.85
Eval num_timesteps=15000, episode_reward=-231.70 +/- 112.70
Episode length: 232.56 +/- 112.37
Eval num_timesteps=15500, episode_reward=-279.99 +/- 146.90
Episode length: 280.70 +/- 146.47
Eval num_timesteps=16000, episode_reward=-284.90 +/- 149.93
Episode length: 285.58 +/- 149.47
Eval num_timesteps=16500, episode_reward=-276.67 +/- 151.14
Episode length: 277.36 +/- 150.68
Eval num_timesteps=17000, episode_reward=-276.94 +/- 149.22
Episode length: 277.64 +/- 148.78
Eval num_timesteps=17500, episode_reward=-334.61 +/- 160.12
Episode length: 335.13 +/- 159.62
Eval num_timesteps=18000, episode_reward=-389.51 +/- 151.73
Episode length: 389.86 +/- 151.26
Eval num_timesteps=18500, episode_reward=-328.90 +/- 193.58
Episode length: 329.34 +/- 193.08
Eval num_timesteps=19000, episode_reward=-412.00 +/- 165.82
Episode length: 412.22 +/- 165.40
Eval num_timesteps=19500, episode_reward=-436.21 +/- 146.21
Episode length: 436.37 +/- 145.84
Eval num_timesteps=20000, episode_reward=-414.36 +/- 166.32
Episode length: 414.57 +/- 165.91
Eval num_timesteps=20500, episode_reward=-381.82 +/- 185.46
Episode length: 382.11 +/- 185.01
Eval num_timesteps=21000, episode_reward=-380.05 +/- 184.31
Episode length: 380.35 +/- 183.86
Eval num_timesteps=21500, episode_reward=-392.76 +/- 181.14
Episode length: 393.02 +/- 180.71
Eval num_timesteps=22000, episode_reward=-377.86 +/- 186.90
Episode length: 378.16 +/- 186.44
Eval num_timesteps=22500, episode_reward=-381.14 +/- 186.24
Episode length: 381.43 +/- 185.79
Eval num_timesteps=23000, episode_reward=-413.00 +/- 168.97
Episode length: 413.21 +/- 168.56
Eval num_timesteps=23500, episode_reward=-408.14 +/- 173.06
Episode length: 408.36 +/- 172.64
Eval num_timesteps=24000, episode_reward=-400.49 +/- 177.23
Episode length: 400.73 +/- 176.81
Eval num_timesteps=24500, episode_reward=-326.72 +/- 203.99
Episode length: 327.14 +/- 203.50
Eval num_timesteps=25000, episode_reward=-346.85 +/- 200.16
Episode length: 347.22 +/- 199.68
Eval num_timesteps=25500, episode_reward=-344.73 +/- 196.90
Episode length: 345.12 +/- 196.42
Eval num_timesteps=26000, episode_reward=-350.97 +/- 198.85
Episode length: 351.33 +/- 198.37
Eval num_timesteps=26500, episode_reward=-337.72 +/- 199.48
Episode length: 338.12 +/- 198.99
Eval num_timesteps=27000, episode_reward=-344.31 +/- 199.12
Episode length: 344.69 +/- 198.63
Eval num_timesteps=27500, episode_reward=-352.70 +/- 196.69
Episode length: 353.06 +/- 196.21
Eval num_timesteps=28000, episode_reward=-356.42 +/- 194.17
Episode length: 356.78 +/- 193.69
Eval num_timesteps=28500, episode_reward=-363.72 +/- 195.22
Episode length: 364.05 +/- 194.76
Eval num_timesteps=29000, episode_reward=-293.18 +/- 207.46
Episode length: 293.68 +/- 206.96
Eval num_timesteps=29500, episode_reward=-286.02 +/- 205.95
Episode length: 286.54 +/- 205.45
Eval num_timesteps=30000, episode_reward=-313.43 +/- 206.62
Episode length: 313.88 +/- 206.13
Eval num_timesteps=30500, episode_reward=-298.33 +/- 206.39
Episode length: 298.82 +/- 205.90
Eval num_timesteps=31000, episode_reward=-272.52 +/- 202.76
Episode length: 273.08 +/- 202.27
Eval num_timesteps=31500, episode_reward=-273.92 +/- 204.24
Episode length: 274.48 +/- 203.75
Eval num_timesteps=32000, episode_reward=-284.74 +/- 204.14
Episode length: 285.27 +/- 203.64
Eval num_timesteps=32500, episode_reward=-297.11 +/- 203.64
Episode length: 297.61 +/- 203.15
Eval num_timesteps=33000, episode_reward=-225.39 +/- 190.76
Episode length: 226.07 +/- 190.30
Eval num_timesteps=33500, episode_reward=-240.93 +/- 194.02
Episode length: 241.58 +/- 193.56
Eval num_timesteps=34000, episode_reward=-244.54 +/- 197.78
Episode length: 245.17 +/- 197.30
Eval num_timesteps=34500, episode_reward=-272.09 +/- 203.53
Episode length: 272.65 +/- 203.04
Eval num_timesteps=35000, episode_reward=-302.44 +/- 200.40
Episode length: 302.94 +/- 199.91
Eval num_timesteps=35500, episode_reward=-282.70 +/- 203.38
Episode length: 283.24 +/- 202.89
Eval num_timesteps=36000, episode_reward=-258.87 +/- 202.36
Episode length: 259.46 +/- 201.88
Eval num_timesteps=36500, episode_reward=-313.34 +/- 203.46
Episode length: 313.80 +/- 202.97
Eval num_timesteps=37000, episode_reward=-315.62 +/- 204.50
Episode length: 316.07 +/- 204.00
Eval num_timesteps=37500, episode_reward=-302.02 +/- 203.36
Episode length: 302.51 +/- 202.86
Eval num_timesteps=38000, episode_reward=-340.03 +/- 200.53
Episode length: 340.42 +/- 200.04
Eval num_timesteps=38500, episode_reward=-354.59 +/- 198.29
Episode length: 354.94 +/- 197.82
Eval num_timesteps=39000, episode_reward=-288.80 +/- 207.81
Episode length: 289.31 +/- 207.31
Eval num_timesteps=39500, episode_reward=-283.00 +/- 203.33
Episode length: 283.54 +/- 202.84
Eval num_timesteps=40000, episode_reward=-323.31 +/- 202.33
Episode length: 323.75 +/- 201.85
Eval num_timesteps=40500, episode_reward=-301.33 +/- 204.70
Episode length: 301.82 +/- 204.20
Eval num_timesteps=41000, episode_reward=-293.49 +/- 203.79
Episode length: 294.00 +/- 203.30
Eval num_timesteps=41500, episode_reward=-312.23 +/- 205.56
Episode length: 312.69 +/- 205.06
Eval num_timesteps=42000, episode_reward=-336.07 +/- 198.50
Episode length: 336.48 +/- 198.01
Eval num_timesteps=42500, episode_reward=-343.36 +/- 200.69
Episode length: 343.74 +/- 200.21
Eval num_timesteps=43000, episode_reward=-309.90 +/- 202.94
Episode length: 310.37 +/- 202.44
Eval num_timesteps=43500, episode_reward=-299.34 +/- 202.86
Episode length: 299.84 +/- 202.36
Eval num_timesteps=44000, episode_reward=-339.12 +/- 201.54
Episode length: 339.51 +/- 201.05
Eval num_timesteps=44500, episode_reward=-295.58 +/- 202.28
Episode length: 296.09 +/- 201.79
Eval num_timesteps=45000, episode_reward=-328.79 +/- 202.60
Episode length: 329.21 +/- 202.11
Eval num_timesteps=45500, episode_reward=-331.35 +/- 197.80
Episode length: 331.78 +/- 197.31
Eval num_timesteps=46000, episode_reward=-363.19 +/- 191.69
Episode length: 363.53 +/- 191.22
Eval num_timesteps=46500, episode_reward=-318.82 +/- 202.06
Episode length: 319.27 +/- 201.57
Eval num_timesteps=47000, episode_reward=-329.24 +/- 202.43
Episode length: 329.67 +/- 201.95
Eval num_timesteps=47500, episode_reward=-399.00 +/- 173.99
Episode length: 399.26 +/- 173.56
Eval num_timesteps=48000, episode_reward=-341.40 +/- 197.30
Episode length: 341.80 +/- 196.82
Eval num_timesteps=48500, episode_reward=-335.38 +/- 200.41
Episode length: 335.79 +/- 199.92
Eval num_timesteps=49000, episode_reward=-320.44 +/- 200.78
Episode length: 320.89 +/- 200.29
Eval num_timesteps=49500, episode_reward=-361.58 +/- 193.31
Episode length: 361.92 +/- 192.84
Eval num_timesteps=50000, episode_reward=-317.28 +/- 200.10
Episode length: 317.74 +/- 199.60
FINISHED IN 1062.0186023809947 s


starting seed  10372 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-492.79 +/- 50.47
Episode length: 492.81 +/- 50.33
New best mean reward!
Eval num_timesteps=11500, episode_reward=-359.35 +/- 180.26
Episode length: 359.73 +/- 179.77
New best mean reward!
Eval num_timesteps=12000, episode_reward=-424.69 +/- 151.01
Episode length: 424.89 +/- 150.62
Eval num_timesteps=12500, episode_reward=-369.92 +/- 183.37
Episode length: 370.27 +/- 182.91
Eval num_timesteps=13000, episode_reward=-327.31 +/- 196.88
Episode length: 327.75 +/- 196.39
New best mean reward!
Eval num_timesteps=13500, episode_reward=-213.87 +/- 183.91
Episode length: 214.58 +/- 183.46
New best mean reward!
Eval num_timesteps=14000, episode_reward=-181.30 +/- 155.90
Episode length: 182.11 +/- 155.51
New best mean reward!
Eval num_timesteps=14500, episode_reward=-157.00 +/- 129.91
Episode length: 157.88 +/- 129.60
New best mean reward!
Eval num_timesteps=15000, episode_reward=-116.04 +/- 84.26
Episode length: 117.00 +/- 84.08
New best mean reward!
Eval num_timesteps=15500, episode_reward=-116.92 +/- 60.87
Episode length: 117.90 +/- 60.74
Eval num_timesteps=16000, episode_reward=-119.77 +/- 55.75
Episode length: 120.76 +/- 55.69
Eval num_timesteps=16500, episode_reward=-113.61 +/- 45.35
Episode length: 114.60 +/- 45.27
New best mean reward!
Eval num_timesteps=17000, episode_reward=-132.98 +/- 90.50
Episode length: 133.93 +/- 90.30
Eval num_timesteps=17500, episode_reward=-110.44 +/- 70.58
Episode length: 111.41 +/- 70.42
New best mean reward!
Eval num_timesteps=18000, episode_reward=-97.05 +/- 44.10
Episode length: 98.04 +/- 44.01
New best mean reward!
FINISHED IN 388.4628765179659 s


starting seed  10373 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-456.09 +/- 83.79
Episode length: 456.36 +/- 83.41
New best mean reward!
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-236.63 +/- 93.36
Episode length: 237.56 +/- 93.16
New best mean reward!
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-443.44 +/- 132.16
Episode length: 443.60 +/- 131.80
Eval num_timesteps=11000, episode_reward=-482.60 +/- 78.11
Episode length: 482.65 +/- 77.89
Eval num_timesteps=11500, episode_reward=-345.07 +/- 190.05
Episode length: 345.49 +/- 189.58
Eval num_timesteps=12000, episode_reward=-266.06 +/- 194.81
Episode length: 266.67 +/- 194.34
Eval num_timesteps=12500, episode_reward=-267.98 +/- 195.19
Episode length: 268.58 +/- 194.72
Eval num_timesteps=13000, episode_reward=-260.27 +/- 198.16
Episode length: 260.87 +/- 197.68
Eval num_timesteps=13500, episode_reward=-244.26 +/- 188.24
Episode length: 244.93 +/- 187.79
Eval num_timesteps=14000, episode_reward=-163.24 +/- 150.56
Episode length: 164.08 +/- 150.21
New best mean reward!
Eval num_timesteps=14500, episode_reward=-150.07 +/- 133.84
Episode length: 150.96 +/- 133.56
New best mean reward!
Eval num_timesteps=15000, episode_reward=-144.56 +/- 131.73
Episode length: 145.45 +/- 131.44
New best mean reward!
Eval num_timesteps=15500, episode_reward=-132.06 +/- 119.08
Episode length: 132.97 +/- 118.80
New best mean reward!
Eval num_timesteps=16000, episode_reward=-124.04 +/- 106.14
Episode length: 124.97 +/- 105.89
New best mean reward!
Eval num_timesteps=16500, episode_reward=-138.11 +/- 115.09
Episode length: 139.03 +/- 114.84
Eval num_timesteps=17000, episode_reward=-124.14 +/- 100.37
Episode length: 125.10 +/- 100.23
Eval num_timesteps=17500, episode_reward=-106.35 +/- 75.47
Episode length: 107.32 +/- 75.31
New best mean reward!
Eval num_timesteps=18000, episode_reward=-90.50 +/- 31.80
Episode length: 91.50 +/- 31.80
New best mean reward!
FINISHED IN 378.9407009249553 s


starting seed  10374 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-287.22 +/- 42.62
Episode length: 288.22 +/- 42.62
New best mean reward!
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-466.66 +/- 97.68
Episode length: 466.77 +/- 97.38
Eval num_timesteps=4500, episode_reward=-174.85 +/- 49.43
Episode length: 175.84 +/- 49.37
New best mean reward!
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-238.06 +/- 119.39
Episode length: 238.90 +/- 119.03
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-400.36 +/- 109.02
Episode length: 400.89 +/- 108.60
Eval num_timesteps=7500, episode_reward=-437.36 +/- 108.62
Episode length: 437.65 +/- 108.21
Eval num_timesteps=8000, episode_reward=-473.38 +/- 70.80
Episode length: 473.53 +/- 70.48
Eval num_timesteps=8500, episode_reward=-182.62 +/- 54.98
Episode length: 183.61 +/- 54.92
Eval num_timesteps=9000, episode_reward=-311.56 +/- 139.14
Episode length: 312.31 +/- 138.80
Eval num_timesteps=9500, episode_reward=-260.45 +/- 86.00
Episode length: 261.45 +/- 86.00
Eval num_timesteps=10000, episode_reward=-147.06 +/- 43.12
Episode length: 148.06 +/- 43.12
New best mean reward!
Eval num_timesteps=10500, episode_reward=-194.56 +/- 46.09
Episode length: 195.56 +/- 46.09
Eval num_timesteps=11000, episode_reward=-157.56 +/- 57.40
Episode length: 158.55 +/- 57.34
Eval num_timesteps=11500, episode_reward=-400.09 +/- 118.63
Episode length: 400.55 +/- 118.18
Eval num_timesteps=12000, episode_reward=-103.33 +/- 44.77
Episode length: 104.33 +/- 44.77
New best mean reward!
Eval num_timesteps=12500, episode_reward=-82.81 +/- 18.38
Episode length: 83.81 +/- 18.38
New best mean reward!
FINISHED IN 243.1405284649809 s


starting seed  10375 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-412.60 +/- 148.54
Episode length: 412.86 +/- 148.11
New best mean reward!
Eval num_timesteps=9500, episode_reward=-445.54 +/- 125.02
Episode length: 445.70 +/- 124.66
Eval num_timesteps=10000, episode_reward=-473.23 +/- 91.22
Episode length: 473.31 +/- 90.95
Eval num_timesteps=10500, episode_reward=-398.47 +/- 167.09
Episode length: 398.74 +/- 166.65
New best mean reward!
Eval num_timesteps=11000, episode_reward=-400.32 +/- 172.74
Episode length: 400.57 +/- 172.31
Eval num_timesteps=11500, episode_reward=-176.88 +/- 126.74
Episode length: 177.75 +/- 126.41
New best mean reward!
Eval num_timesteps=12000, episode_reward=-117.26 +/- 57.21
Episode length: 118.24 +/- 57.07
New best mean reward!
Eval num_timesteps=12500, episode_reward=-105.43 +/- 58.78
Episode length: 106.41 +/- 58.64
New best mean reward!
Eval num_timesteps=13000, episode_reward=-110.49 +/- 37.20
Episode length: 111.49 +/- 37.20
Eval num_timesteps=13500, episode_reward=-97.56 +/- 21.68
Episode length: 98.56 +/- 21.68
New best mean reward!
FINISHED IN 321.99195210501784 s


starting seed  10376 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-496.36 +/- 36.22
Episode length: 496.37 +/- 36.12
New best mean reward!
Eval num_timesteps=6500, episode_reward=-149.92 +/- 145.01
Episode length: 150.78 +/- 144.67
New best mean reward!
Eval num_timesteps=7000, episode_reward=-196.10 +/- 95.63
Episode length: 197.02 +/- 95.37
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-270.27 +/- 204.77
Episode length: 270.83 +/- 204.28
Eval num_timesteps=8500, episode_reward=-270.72 +/- 200.31
Episode length: 271.29 +/- 199.82
Eval num_timesteps=9000, episode_reward=-356.36 +/- 188.15
Episode length: 356.73 +/- 187.67
Eval num_timesteps=9500, episode_reward=-430.86 +/- 147.68
Episode length: 431.04 +/- 147.30
Eval num_timesteps=10000, episode_reward=-331.04 +/- 194.98
Episode length: 331.47 +/- 194.48
Eval num_timesteps=10500, episode_reward=-348.20 +/- 187.66
Episode length: 348.60 +/- 187.17
Eval num_timesteps=11000, episode_reward=-315.82 +/- 192.97
Episode length: 316.30 +/- 192.48
Eval num_timesteps=11500, episode_reward=-148.47 +/- 146.67
Episode length: 149.33 +/- 146.33
New best mean reward!
Eval num_timesteps=12000, episode_reward=-109.90 +/- 107.03
Episode length: 110.84 +/- 106.81
New best mean reward!
Eval num_timesteps=12500, episode_reward=-233.39 +/- 193.46
Episode length: 234.05 +/- 193.00
Eval num_timesteps=13000, episode_reward=-259.85 +/- 197.47
Episode length: 260.45 +/- 196.99
Eval num_timesteps=13500, episode_reward=-208.42 +/- 175.27
Episode length: 209.16 +/- 174.84
Eval num_timesteps=14000, episode_reward=-114.11 +/- 103.50
Episode length: 115.05 +/- 103.28
Eval num_timesteps=14500, episode_reward=-92.63 +/- 34.15
Episode length: 93.63 +/- 34.15
New best mean reward!
FINISHED IN 301.9297851139563 s


starting seed  10377 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-464.07 +/- 57.71
Episode length: 464.51 +/- 57.36
New best mean reward!
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-367.66 +/- 189.67
Episode length: 367.99 +/- 189.20
New best mean reward!
Eval num_timesteps=13500, episode_reward=-292.09 +/- 201.26
Episode length: 292.61 +/- 200.77
New best mean reward!
Eval num_timesteps=14000, episode_reward=-251.25 +/- 197.07
Episode length: 251.87 +/- 196.59
New best mean reward!
Eval num_timesteps=14500, episode_reward=-306.82 +/- 199.53
Episode length: 307.31 +/- 199.03
Eval num_timesteps=15000, episode_reward=-230.75 +/- 192.02
Episode length: 231.42 +/- 191.56
New best mean reward!
Eval num_timesteps=15500, episode_reward=-156.34 +/- 152.21
Episode length: 157.18 +/- 151.85
New best mean reward!
Eval num_timesteps=16000, episode_reward=-150.71 +/- 144.89
Episode length: 151.57 +/- 144.55
New best mean reward!
Eval num_timesteps=16500, episode_reward=-160.16 +/- 156.99
Episode length: 160.99 +/- 156.62
Eval num_timesteps=17000, episode_reward=-114.97 +/- 102.05
Episode length: 115.91 +/- 101.82
New best mean reward!
Eval num_timesteps=17500, episode_reward=-102.29 +/- 86.31
Episode length: 103.25 +/- 86.13
New best mean reward!
Eval num_timesteps=18000, episode_reward=-87.54 +/- 24.02
Episode length: 88.54 +/- 24.02
New best mean reward!
FINISHED IN 412.2887922890368 s


starting seed  10378 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-358.74 +/- 182.45
Episode length: 359.12 +/- 181.97
New best mean reward!
Eval num_timesteps=7500, episode_reward=-380.66 +/- 174.69
Episode length: 380.98 +/- 174.22
Eval num_timesteps=8000, episode_reward=-152.09 +/- 141.76
Episode length: 152.95 +/- 141.42
New best mean reward!
Eval num_timesteps=8500, episode_reward=-104.32 +/- 67.18
Episode length: 105.30 +/- 67.06
New best mean reward!
Eval num_timesteps=9000, episode_reward=-136.18 +/- 118.73
Episode length: 137.10 +/- 118.48
Eval num_timesteps=9500, episode_reward=-119.35 +/- 102.77
Episode length: 120.29 +/- 102.55
Eval num_timesteps=10000, episode_reward=-158.86 +/- 151.57
Episode length: 159.70 +/- 151.21
Eval num_timesteps=10500, episode_reward=-148.21 +/- 139.62
Episode length: 149.08 +/- 139.29
Eval num_timesteps=11000, episode_reward=-174.78 +/- 165.85
Episode length: 175.59 +/- 165.47
Eval num_timesteps=11500, episode_reward=-144.63 +/- 132.89
Episode length: 145.51 +/- 132.57
Eval num_timesteps=12000, episode_reward=-164.06 +/- 159.03
Episode length: 164.88 +/- 158.65
Eval num_timesteps=12500, episode_reward=-156.16 +/- 152.95
Episode length: 157.00 +/- 152.59
Eval num_timesteps=13000, episode_reward=-202.27 +/- 183.27
Episode length: 203.00 +/- 182.83
Eval num_timesteps=13500, episode_reward=-177.40 +/- 166.49
Episode length: 178.20 +/- 166.11
Eval num_timesteps=14000, episode_reward=-169.91 +/- 159.64
Episode length: 170.73 +/- 159.27
Eval num_timesteps=14500, episode_reward=-164.26 +/- 148.87
Episode length: 165.12 +/- 148.55
Eval num_timesteps=15000, episode_reward=-149.89 +/- 144.01
Episode length: 150.75 +/- 143.67
Eval num_timesteps=15500, episode_reward=-158.77 +/- 149.98
Episode length: 159.62 +/- 149.64
Eval num_timesteps=16000, episode_reward=-151.96 +/- 142.02
Episode length: 152.84 +/- 141.73
Eval num_timesteps=16500, episode_reward=-169.45 +/- 153.44
Episode length: 170.28 +/- 153.08
Eval num_timesteps=17000, episode_reward=-168.45 +/- 163.78
Episode length: 169.26 +/- 163.40
Eval num_timesteps=17500, episode_reward=-152.77 +/- 146.92
Episode length: 153.64 +/- 146.62
Eval num_timesteps=18000, episode_reward=-144.33 +/- 143.46
Episode length: 145.20 +/- 143.14
Eval num_timesteps=18500, episode_reward=-132.21 +/- 119.99
Episode length: 133.12 +/- 119.71
Eval num_timesteps=19000, episode_reward=-102.70 +/- 72.06
Episode length: 103.68 +/- 71.95
New best mean reward!
Eval num_timesteps=19500, episode_reward=-94.10 +/- 33.97
Episode length: 95.10 +/- 33.97
New best mean reward!
FINISHED IN 369.4356063620071 s


starting seed  10379 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-323.85 +/- 173.86
Episode length: 324.36 +/- 173.36
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-430.52 +/- 153.62
Episode length: 430.69 +/- 153.25
Eval num_timesteps=11500, episode_reward=-415.62 +/- 164.06
Episode length: 415.83 +/- 163.65
Eval num_timesteps=12000, episode_reward=-427.33 +/- 155.40
Episode length: 427.51 +/- 155.02
Eval num_timesteps=12500, episode_reward=-418.03 +/- 164.01
Episode length: 418.23 +/- 163.61
Eval num_timesteps=13000, episode_reward=-110.44 +/- 93.02
Episode length: 111.39 +/- 92.81
New best mean reward!
Eval num_timesteps=13500, episode_reward=-137.52 +/- 135.31
Episode length: 138.40 +/- 134.99
Eval num_timesteps=14000, episode_reward=-91.77 +/- 28.25
Episode length: 92.77 +/- 28.25
New best mean reward!
FINISHED IN 344.52915172901703 s


starting seed  10380 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-302.46 +/- 96.32
Episode length: 303.30 +/- 96.00
New best mean reward!
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-350.09 +/- 146.18
Episode length: 350.61 +/- 145.69
Eval num_timesteps=9000, episode_reward=-305.31 +/- 148.97
Episode length: 305.95 +/- 148.50
Eval num_timesteps=9500, episode_reward=-155.85 +/- 34.90
Episode length: 156.85 +/- 34.90
New best mean reward!
Eval num_timesteps=10000, episode_reward=-163.09 +/- 37.41
Episode length: 164.09 +/- 37.41
Eval num_timesteps=10500, episode_reward=-170.48 +/- 68.53
Episode length: 171.46 +/- 68.44
Eval num_timesteps=11000, episode_reward=-170.77 +/- 69.82
Episode length: 171.74 +/- 69.68
Eval num_timesteps=11500, episode_reward=-168.28 +/- 50.38
Episode length: 169.27 +/- 50.31
Eval num_timesteps=12000, episode_reward=-161.68 +/- 47.19
Episode length: 162.67 +/- 47.12
Eval num_timesteps=12500, episode_reward=-151.34 +/- 31.47
Episode length: 152.34 +/- 31.47
New best mean reward!
Eval num_timesteps=13000, episode_reward=-161.51 +/- 31.24
Episode length: 162.51 +/- 31.24
Eval num_timesteps=13500, episode_reward=-193.28 +/- 96.97
Episode length: 194.20 +/- 96.71
Eval num_timesteps=14000, episode_reward=-270.96 +/- 154.50
Episode length: 271.66 +/- 154.05
Eval num_timesteps=14500, episode_reward=-207.82 +/- 102.01
Episode length: 208.74 +/- 101.79
Eval num_timesteps=15000, episode_reward=-208.25 +/- 119.51
Episode length: 209.12 +/- 119.19
Eval num_timesteps=15500, episode_reward=-155.61 +/- 45.20
Episode length: 156.60 +/- 45.12
Eval num_timesteps=16000, episode_reward=-108.79 +/- 25.57
Episode length: 109.79 +/- 25.57
New best mean reward!
Eval num_timesteps=16500, episode_reward=-101.65 +/- 31.23
Episode length: 102.65 +/- 31.23
New best mean reward!
Eval num_timesteps=17000, episode_reward=-101.40 +/- 30.18
Episode length: 102.40 +/- 30.18
New best mean reward!
Eval num_timesteps=17500, episode_reward=-100.47 +/- 25.12
Episode length: 101.47 +/- 25.12
New best mean reward!
Eval num_timesteps=18000, episode_reward=-96.14 +/- 25.85
Episode length: 97.14 +/- 25.85
New best mean reward!
FINISHED IN 351.3560982889612 s


starting seed  10381 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-159.34 +/- 41.60
Episode length: 160.33 +/- 41.52
New best mean reward!
Eval num_timesteps=8000, episode_reward=-169.33 +/- 62.82
Episode length: 170.30 +/- 62.66
Eval num_timesteps=8500, episode_reward=-157.73 +/- 32.95
Episode length: 158.73 +/- 32.95
New best mean reward!
Eval num_timesteps=9000, episode_reward=-162.22 +/- 46.57
Episode length: 163.21 +/- 46.50
Eval num_timesteps=9500, episode_reward=-295.85 +/- 193.29
Episode length: 296.38 +/- 192.79
Eval num_timesteps=10000, episode_reward=-87.05 +/- 27.82
Episode length: 88.05 +/- 27.82
New best mean reward!
FINISHED IN 230.56518322200282 s


starting seed  10382 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-351.19 +/- 185.74
Episode length: 351.59 +/- 185.26
New best mean reward!
Eval num_timesteps=10000, episode_reward=-468.74 +/- 106.10
Episode length: 468.82 +/- 105.83
Eval num_timesteps=10500, episode_reward=-247.88 +/- 194.05
Episode length: 248.51 +/- 193.57
New best mean reward!
Eval num_timesteps=11000, episode_reward=-125.66 +/- 94.58
Episode length: 126.61 +/- 94.38
New best mean reward!
Eval num_timesteps=11500, episode_reward=-133.19 +/- 107.72
Episode length: 134.13 +/- 107.51
Eval num_timesteps=12000, episode_reward=-110.33 +/- 57.63
Episode length: 111.32 +/- 57.57
New best mean reward!
Eval num_timesteps=12500, episode_reward=-115.26 +/- 83.38
Episode length: 116.22 +/- 83.20
Eval num_timesteps=13000, episode_reward=-116.86 +/- 92.07
Episode length: 117.81 +/- 91.86
Eval num_timesteps=13500, episode_reward=-112.83 +/- 86.06
Episode length: 113.79 +/- 85.88
Eval num_timesteps=14000, episode_reward=-106.35 +/- 57.92
Episode length: 107.34 +/- 57.86
New best mean reward!
Eval num_timesteps=14500, episode_reward=-120.48 +/- 94.09
Episode length: 121.43 +/- 93.89
Eval num_timesteps=15000, episode_reward=-123.91 +/- 95.02
Episode length: 124.86 +/- 94.82
Eval num_timesteps=15500, episode_reward=-120.32 +/- 87.05
Episode length: 121.29 +/- 86.92
Eval num_timesteps=16000, episode_reward=-98.88 +/- 30.51
Episode length: 99.88 +/- 30.51
New best mean reward!
FINISHED IN 349.40722276701126 s


starting seed  10383 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-216.39 +/- 103.39
Episode length: 217.29 +/- 103.11
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-454.40 +/- 75.11
Episode length: 454.76 +/- 74.72
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-388.00 +/- 160.56
Episode length: 388.33 +/- 160.10
Eval num_timesteps=6500, episode_reward=-203.58 +/- 99.04
Episode length: 204.50 +/- 98.80
New best mean reward!
Eval num_timesteps=7000, episode_reward=-164.54 +/- 92.39
Episode length: 165.48 +/- 92.17
New best mean reward!
Eval num_timesteps=7500, episode_reward=-246.41 +/- 154.63
Episode length: 247.15 +/- 154.20
Eval num_timesteps=8000, episode_reward=-294.15 +/- 168.48
Episode length: 294.78 +/- 168.03
Eval num_timesteps=8500, episode_reward=-392.17 +/- 159.41
Episode length: 392.50 +/- 158.95
Eval num_timesteps=9000, episode_reward=-275.70 +/- 162.20
Episode length: 276.37 +/- 161.74
Eval num_timesteps=9500, episode_reward=-315.32 +/- 171.39
Episode length: 315.86 +/- 170.89
Eval num_timesteps=10000, episode_reward=-207.11 +/- 133.76
Episode length: 207.95 +/- 133.41
Eval num_timesteps=10500, episode_reward=-198.54 +/- 125.07
Episode length: 199.40 +/- 124.73
Eval num_timesteps=11000, episode_reward=-150.84 +/- 50.85
Episode length: 151.83 +/- 50.78
New best mean reward!
Eval num_timesteps=11500, episode_reward=-197.30 +/- 130.45
Episode length: 198.15 +/- 130.10
Eval num_timesteps=12000, episode_reward=-252.59 +/- 165.04
Episode length: 253.29 +/- 164.59
Eval num_timesteps=12500, episode_reward=-240.63 +/- 157.71
Episode length: 241.38 +/- 157.30
Eval num_timesteps=13000, episode_reward=-126.52 +/- 55.10
Episode length: 127.51 +/- 55.03
New best mean reward!
Eval num_timesteps=13500, episode_reward=-83.61 +/- 17.94
Episode length: 84.61 +/- 17.94
New best mean reward!
FINISHED IN 253.15386992902495 s


starting seed  10384 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-485.30 +/- 64.85
Episode length: 485.35 +/- 64.63
New best mean reward!
Eval num_timesteps=10000, episode_reward=-188.51 +/- 86.26
Episode length: 189.45 +/- 86.04
New best mean reward!
Eval num_timesteps=10500, episode_reward=-241.66 +/- 143.31
Episode length: 242.44 +/- 142.91
Eval num_timesteps=11000, episode_reward=-223.71 +/- 129.76
Episode length: 224.54 +/- 129.40
Eval num_timesteps=11500, episode_reward=-277.66 +/- 156.68
Episode length: 278.34 +/- 156.23
Eval num_timesteps=12000, episode_reward=-288.02 +/- 160.80
Episode length: 288.66 +/- 160.32
Eval num_timesteps=12500, episode_reward=-341.35 +/- 163.36
Episode length: 341.84 +/- 162.86
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-405.06 +/- 169.35
Episode length: 405.30 +/- 168.93
Eval num_timesteps=14500, episode_reward=-340.02 +/- 196.65
Episode length: 340.42 +/- 196.16
Eval num_timesteps=15000, episode_reward=-308.63 +/- 207.64
Episode length: 309.09 +/- 207.14
Eval num_timesteps=15500, episode_reward=-313.41 +/- 202.98
Episode length: 313.87 +/- 202.48
Eval num_timesteps=16000, episode_reward=-277.91 +/- 205.84
Episode length: 278.45 +/- 205.34
Eval num_timesteps=16500, episode_reward=-278.75 +/- 205.21
Episode length: 279.29 +/- 204.72
Eval num_timesteps=17000, episode_reward=-202.31 +/- 181.90
Episode length: 203.04 +/- 181.46
Eval num_timesteps=17500, episode_reward=-160.08 +/- 157.29
Episode length: 160.91 +/- 156.92
New best mean reward!
Eval num_timesteps=18000, episode_reward=-171.92 +/- 165.11
Episode length: 172.72 +/- 164.71
Eval num_timesteps=18500, episode_reward=-156.41 +/- 153.95
Episode length: 157.25 +/- 153.60
New best mean reward!
Eval num_timesteps=19000, episode_reward=-163.07 +/- 159.72
Episode length: 163.89 +/- 159.34
Eval num_timesteps=19500, episode_reward=-154.58 +/- 157.02
Episode length: 155.41 +/- 156.65
New best mean reward!
Eval num_timesteps=20000, episode_reward=-126.94 +/- 119.93
Episode length: 127.85 +/- 119.65
New best mean reward!
Eval num_timesteps=20500, episode_reward=-156.30 +/- 156.24
Episode length: 157.13 +/- 155.87
Eval num_timesteps=21000, episode_reward=-118.72 +/- 115.08
Episode length: 119.64 +/- 114.82
New best mean reward!
Eval num_timesteps=21500, episode_reward=-105.98 +/- 87.24
Episode length: 106.94 +/- 87.06
New best mean reward!
Eval num_timesteps=22000, episode_reward=-93.26 +/- 53.15
Episode length: 94.25 +/- 53.08
New best mean reward!
FINISHED IN 447.0772332010092 s


starting seed  10385 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-414.36 +/- 135.56
Episode length: 414.65 +/- 135.11
New best mean reward!
Eval num_timesteps=6500, episode_reward=-394.71 +/- 152.40
Episode length: 395.04 +/- 151.93
New best mean reward!
Eval num_timesteps=7000, episode_reward=-432.80 +/- 127.05
Episode length: 433.02 +/- 126.64
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=17000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=17500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=18000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=18500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=19000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=19500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=20000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=20500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=21000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=21500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=22000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=22500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=23000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=23500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=24000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=24500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=25000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=25500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=26000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=26500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=27000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=27500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=28000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=28500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=29000, episode_reward=-432.04 +/- 150.32
Episode length: 432.21 +/- 149.95
Eval num_timesteps=29500, episode_reward=-381.52 +/- 181.32
Episode length: 381.82 +/- 180.87
New best mean reward!
Eval num_timesteps=30000, episode_reward=-395.23 +/- 176.86
Episode length: 395.49 +/- 176.42
Eval num_timesteps=30500, episode_reward=-305.69 +/- 202.61
Episode length: 306.17 +/- 202.11
New best mean reward!
Eval num_timesteps=31000, episode_reward=-286.93 +/- 201.64
Episode length: 287.46 +/- 201.15
New best mean reward!
Eval num_timesteps=31500, episode_reward=-279.03 +/- 205.21
Episode length: 279.57 +/- 204.71
New best mean reward!
Eval num_timesteps=32000, episode_reward=-254.62 +/- 205.16
Episode length: 255.21 +/- 204.67
New best mean reward!
Eval num_timesteps=32500, episode_reward=-262.45 +/- 206.72
Episode length: 263.02 +/- 206.23
Eval num_timesteps=33000, episode_reward=-253.46 +/- 202.88
Episode length: 254.06 +/- 202.39
New best mean reward!
Eval num_timesteps=33500, episode_reward=-240.73 +/- 199.27
Episode length: 241.36 +/- 198.79
New best mean reward!
Eval num_timesteps=34000, episode_reward=-208.93 +/- 191.05
Episode length: 209.63 +/- 190.59
New best mean reward!
Eval num_timesteps=34500, episode_reward=-264.34 +/- 205.84
Episode length: 264.91 +/- 205.35
Eval num_timesteps=35000, episode_reward=-236.41 +/- 202.58
Episode length: 237.04 +/- 202.10
Eval num_timesteps=35500, episode_reward=-157.58 +/- 161.07
Episode length: 158.40 +/- 160.69
New best mean reward!
Eval num_timesteps=36000, episode_reward=-146.33 +/- 150.04
Episode length: 147.18 +/- 149.69
New best mean reward!
Eval num_timesteps=36500, episode_reward=-185.59 +/- 182.25
Episode length: 186.34 +/- 181.82
Eval num_timesteps=37000, episode_reward=-198.06 +/- 188.80
Episode length: 198.78 +/- 188.35
Eval num_timesteps=37500, episode_reward=-164.90 +/- 163.61
Episode length: 165.71 +/- 163.22
Eval num_timesteps=38000, episode_reward=-138.89 +/- 141.62
Episode length: 139.76 +/- 141.29
New best mean reward!
Eval num_timesteps=38500, episode_reward=-136.88 +/- 137.43
Episode length: 137.76 +/- 137.12
New best mean reward!
Eval num_timesteps=39000, episode_reward=-132.64 +/- 136.96
Episode length: 133.52 +/- 136.63
New best mean reward!
Eval num_timesteps=39500, episode_reward=-125.20 +/- 121.47
Episode length: 126.11 +/- 121.20
New best mean reward!
Eval num_timesteps=40000, episode_reward=-108.36 +/- 100.85
Episode length: 109.30 +/- 100.62
New best mean reward!
Eval num_timesteps=40500, episode_reward=-120.80 +/- 120.70
Episode length: 121.71 +/- 120.41
Eval num_timesteps=41000, episode_reward=-102.96 +/- 92.26
Episode length: 103.91 +/- 92.05
New best mean reward!
Eval num_timesteps=41500, episode_reward=-127.46 +/- 120.42
Episode length: 128.37 +/- 120.14
Eval num_timesteps=42000, episode_reward=-103.99 +/- 92.68
Episode length: 104.94 +/- 92.47
Eval num_timesteps=42500, episode_reward=-96.98 +/- 73.92
Episode length: 97.95 +/- 73.76
New best mean reward!
FINISHED IN 1000.009971545951 s


starting seed  10386 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-99.30 +/- 38.16
Episode length: 100.30 +/- 38.16
New best mean reward!
FINISHED IN 4.207430086040404 s


starting seed  10387 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-303.68 +/- 200.63
Episode length: 304.17 +/- 200.14
New best mean reward!
Eval num_timesteps=10500, episode_reward=-370.28 +/- 185.26
Episode length: 370.61 +/- 184.80
Eval num_timesteps=11000, episode_reward=-374.42 +/- 180.10
Episode length: 374.75 +/- 179.63
Eval num_timesteps=11500, episode_reward=-241.25 +/- 184.27
Episode length: 241.93 +/- 183.82
New best mean reward!
Eval num_timesteps=12000, episode_reward=-160.10 +/- 128.44
Episode length: 160.99 +/- 128.15
New best mean reward!
Eval num_timesteps=12500, episode_reward=-195.59 +/- 164.36
Episode length: 196.38 +/- 163.97
Eval num_timesteps=13000, episode_reward=-135.15 +/- 102.16
Episode length: 136.09 +/- 101.95
New best mean reward!
Eval num_timesteps=13500, episode_reward=-121.47 +/- 85.93
Episode length: 122.43 +/- 85.76
New best mean reward!
Eval num_timesteps=14000, episode_reward=-145.84 +/- 116.21
Episode length: 146.76 +/- 115.97
Eval num_timesteps=14500, episode_reward=-178.32 +/- 150.53
Episode length: 179.16 +/- 150.19
Eval num_timesteps=15000, episode_reward=-149.24 +/- 121.66
Episode length: 150.14 +/- 121.37
Eval num_timesteps=15500, episode_reward=-175.40 +/- 157.88
Episode length: 176.22 +/- 157.51
Eval num_timesteps=16000, episode_reward=-265.17 +/- 200.70
Episode length: 265.76 +/- 200.22
Eval num_timesteps=16500, episode_reward=-262.67 +/- 198.86
Episode length: 263.26 +/- 198.37
Eval num_timesteps=17000, episode_reward=-202.77 +/- 181.18
Episode length: 203.51 +/- 180.76
Eval num_timesteps=17500, episode_reward=-142.88 +/- 126.34
Episode length: 143.78 +/- 126.06
Eval num_timesteps=18000, episode_reward=-151.55 +/- 132.16
Episode length: 152.45 +/- 131.90
Eval num_timesteps=18500, episode_reward=-141.33 +/- 115.52
Episode length: 142.25 +/- 115.27
Eval num_timesteps=19000, episode_reward=-166.11 +/- 150.93
Episode length: 166.95 +/- 150.58
Eval num_timesteps=19500, episode_reward=-169.09 +/- 155.24
Episode length: 169.92 +/- 154.88
Eval num_timesteps=20000, episode_reward=-191.16 +/- 167.99
Episode length: 191.94 +/- 167.59
Eval num_timesteps=20500, episode_reward=-137.63 +/- 117.60
Episode length: 138.55 +/- 117.35
Eval num_timesteps=21000, episode_reward=-177.85 +/- 163.12
Episode length: 178.67 +/- 162.76
Eval num_timesteps=21500, episode_reward=-142.22 +/- 124.58
Episode length: 143.12 +/- 124.30
Eval num_timesteps=22000, episode_reward=-132.95 +/- 118.70
Episode length: 133.86 +/- 118.42
Eval num_timesteps=22500, episode_reward=-132.10 +/- 112.73
Episode length: 133.02 +/- 112.47
Eval num_timesteps=23000, episode_reward=-131.57 +/- 100.32
Episode length: 132.52 +/- 100.14
Eval num_timesteps=23500, episode_reward=-131.59 +/- 111.28
Episode length: 132.53 +/- 111.08
Eval num_timesteps=24000, episode_reward=-154.52 +/- 139.29
Episode length: 155.39 +/- 138.97
Eval num_timesteps=24500, episode_reward=-174.75 +/- 159.78
Episode length: 175.57 +/- 159.41
Eval num_timesteps=25000, episode_reward=-119.24 +/- 99.12
Episode length: 120.18 +/- 98.89
New best mean reward!
Eval num_timesteps=25500, episode_reward=-170.79 +/- 160.38
Episode length: 171.60 +/- 159.99
Eval num_timesteps=26000, episode_reward=-198.07 +/- 179.85
Episode length: 198.82 +/- 179.43
Eval num_timesteps=26500, episode_reward=-228.41 +/- 195.15
Episode length: 229.08 +/- 194.69
Eval num_timesteps=27000, episode_reward=-168.21 +/- 161.96
Episode length: 169.02 +/- 161.58
Eval num_timesteps=27500, episode_reward=-187.76 +/- 172.25
Episode length: 188.53 +/- 171.83
Eval num_timesteps=28000, episode_reward=-216.59 +/- 186.56
Episode length: 217.30 +/- 186.12
Eval num_timesteps=28500, episode_reward=-191.01 +/- 174.75
Episode length: 191.78 +/- 174.34
Eval num_timesteps=29000, episode_reward=-235.60 +/- 192.45
Episode length: 236.26 +/- 191.98
Eval num_timesteps=29500, episode_reward=-280.05 +/- 204.06
Episode length: 280.59 +/- 203.56
Eval num_timesteps=30000, episode_reward=-252.33 +/- 197.86
Episode length: 252.95 +/- 197.39
Eval num_timesteps=30500, episode_reward=-306.07 +/- 202.33
Episode length: 306.56 +/- 201.84
Eval num_timesteps=31000, episode_reward=-250.00 +/- 200.63
Episode length: 250.61 +/- 200.15
Eval num_timesteps=31500, episode_reward=-268.55 +/- 205.09
Episode length: 269.12 +/- 204.61
Eval num_timesteps=32000, episode_reward=-245.61 +/- 196.77
Episode length: 246.24 +/- 196.30
Eval num_timesteps=32500, episode_reward=-224.36 +/- 194.35
Episode length: 225.04 +/- 193.89
Eval num_timesteps=33000, episode_reward=-218.29 +/- 191.00
Episode length: 218.98 +/- 190.54
Eval num_timesteps=33500, episode_reward=-153.99 +/- 147.12
Episode length: 154.84 +/- 146.77
Eval num_timesteps=34000, episode_reward=-192.07 +/- 172.89
Episode length: 192.85 +/- 172.50
Eval num_timesteps=34500, episode_reward=-235.75 +/- 194.12
Episode length: 236.41 +/- 193.66
Eval num_timesteps=35000, episode_reward=-199.76 +/- 183.75
Episode length: 200.49 +/- 183.31
Eval num_timesteps=35500, episode_reward=-186.50 +/- 178.26
Episode length: 187.26 +/- 177.84
Eval num_timesteps=36000, episode_reward=-242.83 +/- 198.16
Episode length: 243.47 +/- 197.69
Eval num_timesteps=36500, episode_reward=-201.85 +/- 182.62
Episode length: 202.58 +/- 182.18
Eval num_timesteps=37000, episode_reward=-214.96 +/- 189.19
Episode length: 215.66 +/- 188.74
Eval num_timesteps=37500, episode_reward=-224.91 +/- 192.03
Episode length: 225.59 +/- 191.58
Eval num_timesteps=38000, episode_reward=-253.22 +/- 197.17
Episode length: 253.84 +/- 196.69
Eval num_timesteps=38500, episode_reward=-262.24 +/- 203.15
Episode length: 262.82 +/- 202.66
Eval num_timesteps=39000, episode_reward=-284.00 +/- 204.47
Episode length: 284.53 +/- 203.97
Eval num_timesteps=39500, episode_reward=-274.53 +/- 200.22
Episode length: 275.10 +/- 199.74
Eval num_timesteps=40000, episode_reward=-272.89 +/- 202.77
Episode length: 273.45 +/- 202.28
Eval num_timesteps=40500, episode_reward=-274.48 +/- 204.74
Episode length: 275.03 +/- 204.24
Eval num_timesteps=41000, episode_reward=-284.14 +/- 204.94
Episode length: 284.67 +/- 204.45
Eval num_timesteps=41500, episode_reward=-290.02 +/- 206.33
Episode length: 290.53 +/- 205.83
Eval num_timesteps=42000, episode_reward=-261.13 +/- 201.38
Episode length: 261.72 +/- 200.89
Eval num_timesteps=42500, episode_reward=-234.27 +/- 191.97
Episode length: 234.94 +/- 191.52
Eval num_timesteps=43000, episode_reward=-249.05 +/- 195.12
Episode length: 249.69 +/- 194.66
Eval num_timesteps=43500, episode_reward=-311.01 +/- 205.64
Episode length: 311.47 +/- 205.14
Eval num_timesteps=44000, episode_reward=-290.95 +/- 201.94
Episode length: 291.48 +/- 201.45
Eval num_timesteps=44500, episode_reward=-236.24 +/- 194.87
Episode length: 236.90 +/- 194.41
Eval num_timesteps=45000, episode_reward=-205.95 +/- 184.24
Episode length: 206.67 +/- 183.79
Eval num_timesteps=45500, episode_reward=-225.33 +/- 194.93
Episode length: 226.00 +/- 194.47
Eval num_timesteps=46000, episode_reward=-249.88 +/- 199.56
Episode length: 250.50 +/- 199.09
Eval num_timesteps=46500, episode_reward=-224.61 +/- 194.36
Episode length: 225.28 +/- 193.89
Eval num_timesteps=47000, episode_reward=-213.09 +/- 185.72
Episode length: 213.80 +/- 185.27
Eval num_timesteps=47500, episode_reward=-254.77 +/- 201.22
Episode length: 255.37 +/- 200.74
Eval num_timesteps=48000, episode_reward=-274.83 +/- 204.79
Episode length: 275.38 +/- 204.30
Eval num_timesteps=48500, episode_reward=-261.03 +/- 199.14
Episode length: 261.64 +/- 198.68
Eval num_timesteps=49000, episode_reward=-247.26 +/- 198.91
Episode length: 247.89 +/- 198.44
Eval num_timesteps=49500, episode_reward=-237.63 +/- 197.90
Episode length: 238.27 +/- 197.43
Eval num_timesteps=50000, episode_reward=-213.05 +/- 188.05
Episode length: 213.76 +/- 187.61
FINISHED IN 1091.0628035870031 s


starting seed  10388 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-486.68 +/- 65.29
Episode length: 486.72 +/- 65.09
New best mean reward!
Eval num_timesteps=8000, episode_reward=-83.76 +/- 16.24
Episode length: 84.76 +/- 16.24
New best mean reward!
FINISHED IN 250.29739333398174 s


starting seed  10389 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-323.88 +/- 84.14
Episode length: 324.82 +/- 84.02
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-297.81 +/- 61.76
Episode length: 298.79 +/- 61.70
New best mean reward!
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-496.49 +/- 34.92
Episode length: 496.50 +/- 34.82
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-492.25 +/- 54.25
Episode length: 492.27 +/- 54.11
Eval num_timesteps=9500, episode_reward=-225.72 +/- 186.70
Episode length: 226.43 +/- 186.28
New best mean reward!
Eval num_timesteps=10000, episode_reward=-267.86 +/- 198.67
Episode length: 268.45 +/- 198.19
Eval num_timesteps=10500, episode_reward=-322.97 +/- 197.14
Episode length: 323.42 +/- 196.64
Eval num_timesteps=11000, episode_reward=-247.14 +/- 198.42
Episode length: 247.76 +/- 197.93
Eval num_timesteps=11500, episode_reward=-108.96 +/- 69.11
Episode length: 109.94 +/- 68.99
New best mean reward!
Eval num_timesteps=12000, episode_reward=-109.74 +/- 70.98
Episode length: 110.72 +/- 70.87
Eval num_timesteps=12500, episode_reward=-100.10 +/- 62.78
Episode length: 101.09 +/- 62.71
New best mean reward!
Eval num_timesteps=13000, episode_reward=-109.26 +/- 66.34
Episode length: 110.24 +/- 66.22
Eval num_timesteps=13500, episode_reward=-95.01 +/- 47.05
Episode length: 96.01 +/- 47.05
New best mean reward!
FINISHED IN 243.56242716498673 s


starting seed  10390 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-88.72 +/- 28.60
Episode length: 89.72 +/- 28.60
New best mean reward!
FINISHED IN 193.95847219997086 s


starting seed  10391 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-82.36 +/- 21.82
Episode length: 83.36 +/- 21.82
New best mean reward!
FINISHED IN 151.3692913579871 s


starting seed  10392 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-370.86 +/- 189.02
Episode length: 371.18 +/- 188.56
New best mean reward!
Eval num_timesteps=12000, episode_reward=-392.03 +/- 177.84
Episode length: 392.30 +/- 177.40
Eval num_timesteps=12500, episode_reward=-363.99 +/- 190.34
Episode length: 364.33 +/- 189.86
New best mean reward!
Eval num_timesteps=13000, episode_reward=-155.00 +/- 151.59
Episode length: 155.85 +/- 151.24
New best mean reward!
Eval num_timesteps=13500, episode_reward=-133.07 +/- 124.60
Episode length: 133.97 +/- 124.31
New best mean reward!
Eval num_timesteps=14000, episode_reward=-122.82 +/- 108.91
Episode length: 123.75 +/- 108.66
New best mean reward!
Eval num_timesteps=14500, episode_reward=-94.78 +/- 29.69
Episode length: 95.78 +/- 29.69
New best mean reward!
FINISHED IN 251.50265379296616 s


starting seed  10393 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-497.81 +/- 21.79
Episode length: 497.82 +/- 21.69
New best mean reward!
Eval num_timesteps=6000, episode_reward=-446.09 +/- 128.90
Episode length: 446.24 +/- 128.54
New best mean reward!
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-452.54 +/- 113.64
Episode length: 452.69 +/- 113.28
Eval num_timesteps=9000, episode_reward=-487.11 +/- 63.42
Episode length: 487.15 +/- 63.22
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-487.18 +/- 72.92
Episode length: 487.21 +/- 72.75
Eval num_timesteps=14000, episode_reward=-409.15 +/- 166.90
Episode length: 409.38 +/- 166.49
New best mean reward!
Eval num_timesteps=14500, episode_reward=-351.26 +/- 195.05
Episode length: 351.63 +/- 194.57
New best mean reward!
Eval num_timesteps=15000, episode_reward=-314.06 +/- 202.37
Episode length: 314.52 +/- 201.88
New best mean reward!
Eval num_timesteps=15500, episode_reward=-360.91 +/- 191.06
Episode length: 361.26 +/- 190.58
Eval num_timesteps=16000, episode_reward=-265.32 +/- 201.73
Episode length: 265.90 +/- 201.24
New best mean reward!
Eval num_timesteps=16500, episode_reward=-235.51 +/- 196.29
Episode length: 236.17 +/- 195.84
New best mean reward!
Eval num_timesteps=17000, episode_reward=-187.59 +/- 171.93
Episode length: 188.37 +/- 171.53
New best mean reward!
Eval num_timesteps=17500, episode_reward=-193.11 +/- 179.89
Episode length: 193.86 +/- 179.46
Eval num_timesteps=18000, episode_reward=-192.07 +/- 173.67
Episode length: 192.84 +/- 173.27
Eval num_timesteps=18500, episode_reward=-174.32 +/- 166.63
Episode length: 175.13 +/- 166.26
New best mean reward!
Eval num_timesteps=19000, episode_reward=-147.01 +/- 139.61
Episode length: 147.88 +/- 139.28
New best mean reward!
Eval num_timesteps=19500, episode_reward=-181.55 +/- 170.46
Episode length: 182.33 +/- 170.05
Eval num_timesteps=20000, episode_reward=-153.31 +/- 142.67
Episode length: 154.17 +/- 142.32
Eval num_timesteps=20500, episode_reward=-179.68 +/- 172.06
Episode length: 180.46 +/- 171.65
Eval num_timesteps=21000, episode_reward=-173.59 +/- 165.23
Episode length: 174.39 +/- 164.84
Eval num_timesteps=21500, episode_reward=-175.88 +/- 162.78
Episode length: 176.70 +/- 162.42
Eval num_timesteps=22000, episode_reward=-148.32 +/- 144.57
Episode length: 149.18 +/- 144.23
Eval num_timesteps=22500, episode_reward=-142.11 +/- 136.26
Episode length: 142.99 +/- 135.95
New best mean reward!
Eval num_timesteps=23000, episode_reward=-150.39 +/- 145.94
Episode length: 151.26 +/- 145.63
Eval num_timesteps=23500, episode_reward=-156.23 +/- 151.20
Episode length: 157.09 +/- 150.88
Eval num_timesteps=24000, episode_reward=-152.92 +/- 144.95
Episode length: 153.78 +/- 144.61
Eval num_timesteps=24500, episode_reward=-121.38 +/- 108.41
Episode length: 122.31 +/- 108.16
New best mean reward!
Eval num_timesteps=25000, episode_reward=-125.42 +/- 109.00
Episode length: 126.35 +/- 108.75
Eval num_timesteps=25500, episode_reward=-131.81 +/- 129.00
Episode length: 132.71 +/- 128.72
Eval num_timesteps=26000, episode_reward=-128.24 +/- 120.32
Episode length: 129.16 +/- 120.08
Eval num_timesteps=26500, episode_reward=-117.43 +/- 104.14
Episode length: 118.38 +/- 103.96
New best mean reward!
Eval num_timesteps=27000, episode_reward=-116.70 +/- 95.35
Episode length: 117.66 +/- 95.19
New best mean reward!
Eval num_timesteps=27500, episode_reward=-116.70 +/- 102.27
Episode length: 117.64 +/- 102.04
Eval num_timesteps=28000, episode_reward=-156.79 +/- 155.37
Episode length: 157.63 +/- 155.02
Eval num_timesteps=28500, episode_reward=-163.57 +/- 159.21
Episode length: 164.39 +/- 158.83
Eval num_timesteps=29000, episode_reward=-123.34 +/- 114.44
Episode length: 124.26 +/- 114.18
Eval num_timesteps=29500, episode_reward=-128.25 +/- 118.15
Episode length: 129.17 +/- 117.90
Eval num_timesteps=30000, episode_reward=-101.12 +/- 64.61
Episode length: 102.10 +/- 64.48
New best mean reward!
Eval num_timesteps=30500, episode_reward=-111.07 +/- 94.75
Episode length: 112.02 +/- 94.55
Eval num_timesteps=31000, episode_reward=-136.77 +/- 131.17
Episode length: 137.66 +/- 130.87
Eval num_timesteps=31500, episode_reward=-101.95 +/- 69.45
Episode length: 102.93 +/- 69.34
Eval num_timesteps=32000, episode_reward=-99.02 +/- 62.19
Episode length: 100.00 +/- 62.06
New best mean reward!
FINISHED IN 439.6620386810391 s


starting seed  10394 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-108.37 +/- 75.92
Episode length: 109.34 +/- 75.76
New best mean reward!
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-107.00 +/- 62.99
Episode length: 107.99 +/- 62.93
New best mean reward!
Eval num_timesteps=8500, episode_reward=-94.34 +/- 39.16
Episode length: 95.34 +/- 39.16
New best mean reward!
FINISHED IN 146.44382650998887 s


starting seed  10395 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-452.02 +/- 57.26
Episode length: 452.61 +/- 56.92
New best mean reward!
Eval num_timesteps=9000, episode_reward=-434.91 +/- 126.73
Episode length: 435.12 +/- 126.32
New best mean reward!
Eval num_timesteps=9500, episode_reward=-497.11 +/- 28.76
Episode length: 497.12 +/- 28.66
Eval num_timesteps=10000, episode_reward=-321.02 +/- 152.54
Episode length: 321.61 +/- 152.06
New best mean reward!
Eval num_timesteps=10500, episode_reward=-339.08 +/- 150.31
Episode length: 339.64 +/- 149.84
Eval num_timesteps=11000, episode_reward=-353.12 +/- 146.79
Episode length: 353.64 +/- 146.31
Eval num_timesteps=11500, episode_reward=-372.83 +/- 145.72
Episode length: 373.27 +/- 145.23
Eval num_timesteps=12000, episode_reward=-495.46 +/- 31.95
Episode length: 495.48 +/- 31.81
Eval num_timesteps=12500, episode_reward=-235.29 +/- 86.64
Episode length: 236.23 +/- 86.46
New best mean reward!
Eval num_timesteps=13000, episode_reward=-332.36 +/- 111.34
Episode length: 333.09 +/- 110.93
Eval num_timesteps=13500, episode_reward=-291.14 +/- 86.34
Episode length: 292.05 +/- 86.12
Eval num_timesteps=14000, episode_reward=-332.94 +/- 98.95
Episode length: 333.72 +/- 98.58
Eval num_timesteps=14500, episode_reward=-297.38 +/- 110.37
Episode length: 298.18 +/- 110.00
Eval num_timesteps=15000, episode_reward=-233.03 +/- 128.60
Episode length: 233.86 +/- 128.24
New best mean reward!
Eval num_timesteps=15500, episode_reward=-205.71 +/- 126.22
Episode length: 206.57 +/- 125.89
New best mean reward!
Eval num_timesteps=16000, episode_reward=-164.65 +/- 102.02
Episode length: 165.58 +/- 101.79
New best mean reward!
Eval num_timesteps=16500, episode_reward=-140.20 +/- 25.22
Episode length: 141.20 +/- 25.22
New best mean reward!
Eval num_timesteps=17000, episode_reward=-182.08 +/- 72.28
Episode length: 183.05 +/- 72.15
Eval num_timesteps=17500, episode_reward=-317.29 +/- 79.82
Episode length: 318.22 +/- 79.66
Eval num_timesteps=18000, episode_reward=-207.77 +/- 65.35
Episode length: 208.75 +/- 65.26
Eval num_timesteps=18500, episode_reward=-210.95 +/- 52.21
Episode length: 211.94 +/- 52.16
Eval num_timesteps=19000, episode_reward=-200.81 +/- 58.50
Episode length: 201.79 +/- 58.40
Eval num_timesteps=19500, episode_reward=-191.10 +/- 45.89
Episode length: 192.10 +/- 45.89
Eval num_timesteps=20000, episode_reward=-172.71 +/- 47.37
Episode length: 173.70 +/- 47.30
Eval num_timesteps=20500, episode_reward=-169.44 +/- 54.30
Episode length: 170.43 +/- 54.24
Eval num_timesteps=21000, episode_reward=-133.00 +/- 47.22
Episode length: 133.99 +/- 47.14
New best mean reward!
Eval num_timesteps=21500, episode_reward=-99.36 +/- 31.04
Episode length: 100.36 +/- 31.04
New best mean reward!
FINISHED IN 411.3148780770134 s


starting seed  10396 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-496.98 +/- 13.95
Episode length: 497.04 +/- 13.75
New best mean reward!
Eval num_timesteps=9000, episode_reward=-220.76 +/- 192.51
Episode length: 221.45 +/- 192.06
New best mean reward!
Eval num_timesteps=9500, episode_reward=-246.29 +/- 196.61
Episode length: 246.92 +/- 196.14
Eval num_timesteps=10000, episode_reward=-224.38 +/- 190.41
Episode length: 225.06 +/- 189.94
Eval num_timesteps=10500, episode_reward=-208.01 +/- 186.29
Episode length: 208.73 +/- 185.85
New best mean reward!
Eval num_timesteps=11000, episode_reward=-203.08 +/- 186.66
Episode length: 203.80 +/- 186.21
New best mean reward!
Eval num_timesteps=11500, episode_reward=-362.82 +/- 188.90
Episode length: 363.17 +/- 188.43
Eval num_timesteps=12000, episode_reward=-227.11 +/- 196.64
Episode length: 227.77 +/- 196.17
Eval num_timesteps=12500, episode_reward=-320.06 +/- 203.98
Episode length: 320.50 +/- 203.49
Eval num_timesteps=13000, episode_reward=-310.87 +/- 209.35
Episode length: 311.32 +/- 208.86
Eval num_timesteps=13500, episode_reward=-295.68 +/- 205.17
Episode length: 296.18 +/- 204.68
Eval num_timesteps=14000, episode_reward=-194.90 +/- 178.89
Episode length: 195.65 +/- 178.46
New best mean reward!
Eval num_timesteps=14500, episode_reward=-187.37 +/- 177.17
Episode length: 188.13 +/- 176.74
New best mean reward!
Eval num_timesteps=15000, episode_reward=-172.74 +/- 167.53
Episode length: 173.54 +/- 167.14
New best mean reward!
Eval num_timesteps=15500, episode_reward=-142.76 +/- 140.42
Episode length: 143.64 +/- 140.12
New best mean reward!
Eval num_timesteps=16000, episode_reward=-124.28 +/- 113.71
Episode length: 125.20 +/- 113.45
New best mean reward!
Eval num_timesteps=16500, episode_reward=-161.26 +/- 157.63
Episode length: 162.09 +/- 157.27
Eval num_timesteps=17000, episode_reward=-127.87 +/- 110.69
Episode length: 128.80 +/- 110.45
Eval num_timesteps=17500, episode_reward=-92.37 +/- 49.27
Episode length: 93.36 +/- 49.18
New best mean reward!
FINISHED IN 246.84290288301418 s


starting seed  10397 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-322.70 +/- 60.59
Episode length: 323.70 +/- 60.59
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-133.13 +/- 75.33
Episode length: 134.11 +/- 75.23
New best mean reward!
Eval num_timesteps=9500, episode_reward=-176.29 +/- 134.58
Episode length: 177.17 +/- 134.29
Eval num_timesteps=10000, episode_reward=-489.20 +/- 61.80
Episode length: 489.23 +/- 61.63
Eval num_timesteps=10500, episode_reward=-104.66 +/- 53.28
Episode length: 105.65 +/- 53.21
New best mean reward!
Eval num_timesteps=11000, episode_reward=-174.65 +/- 141.77
Episode length: 175.51 +/- 141.44
Eval num_timesteps=11500, episode_reward=-114.00 +/- 71.74
Episode length: 114.98 +/- 71.63
Eval num_timesteps=12000, episode_reward=-173.59 +/- 154.65
Episode length: 174.42 +/- 154.30
Eval num_timesteps=12500, episode_reward=-125.26 +/- 100.11
Episode length: 126.21 +/- 99.92
Eval num_timesteps=13000, episode_reward=-129.46 +/- 103.11
Episode length: 130.41 +/- 102.93
Eval num_timesteps=13500, episode_reward=-108.21 +/- 52.27
Episode length: 109.20 +/- 52.19
Eval num_timesteps=14000, episode_reward=-100.38 +/- 46.98
Episode length: 101.37 +/- 46.89
New best mean reward!
Eval num_timesteps=14500, episode_reward=-94.67 +/- 25.63
Episode length: 95.67 +/- 25.63
New best mean reward!
FINISHED IN 188.48278394300723 s


starting seed  10398 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-198.10 +/- 53.26
Episode length: 199.08 +/- 53.15
New best mean reward!
Eval num_timesteps=12000, episode_reward=-259.55 +/- 128.24
Episode length: 260.35 +/- 127.87
Eval num_timesteps=12500, episode_reward=-136.50 +/- 43.68
Episode length: 137.50 +/- 43.68
New best mean reward!
Eval num_timesteps=13000, episode_reward=-282.20 +/- 180.14
Episode length: 282.81 +/- 179.67
Eval num_timesteps=13500, episode_reward=-160.57 +/- 111.69
Episode length: 161.50 +/- 111.47
Eval num_timesteps=14000, episode_reward=-380.21 +/- 176.97
Episode length: 380.53 +/- 176.51
Eval num_timesteps=14500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=17000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=17500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=18000, episode_reward=-479.46 +/- 89.58
Episode length: 479.51 +/- 89.37
Eval num_timesteps=18500, episode_reward=-471.57 +/- 103.63
Episode length: 471.64 +/- 103.37
Eval num_timesteps=19000, episode_reward=-400.47 +/- 177.25
Episode length: 400.71 +/- 176.83
Eval num_timesteps=19500, episode_reward=-342.00 +/- 197.92
Episode length: 342.39 +/- 197.43
Eval num_timesteps=20000, episode_reward=-357.99 +/- 194.31
Episode length: 358.34 +/- 193.84
Eval num_timesteps=20500, episode_reward=-367.07 +/- 190.83
Episode length: 367.40 +/- 190.36
Eval num_timesteps=21000, episode_reward=-356.26 +/- 195.13
Episode length: 356.62 +/- 194.66
Eval num_timesteps=21500, episode_reward=-298.40 +/- 206.17
Episode length: 298.89 +/- 205.67
Eval num_timesteps=22000, episode_reward=-262.25 +/- 199.69
Episode length: 262.84 +/- 199.20
Eval num_timesteps=22500, episode_reward=-202.53 +/- 182.15
Episode length: 203.26 +/- 181.71
Eval num_timesteps=23000, episode_reward=-195.69 +/- 181.89
Episode length: 196.44 +/- 181.47
Eval num_timesteps=23500, episode_reward=-208.29 +/- 184.44
Episode length: 209.01 +/- 184.00
Eval num_timesteps=24000, episode_reward=-176.12 +/- 168.11
Episode length: 176.93 +/- 167.74
Eval num_timesteps=24500, episode_reward=-221.40 +/- 190.80
Episode length: 222.09 +/- 190.35
Eval num_timesteps=25000, episode_reward=-202.45 +/- 186.29
Episode length: 203.17 +/- 185.84
Eval num_timesteps=25500, episode_reward=-167.42 +/- 167.28
Episode length: 168.22 +/- 166.88
Eval num_timesteps=26000, episode_reward=-147.64 +/- 142.74
Episode length: 148.52 +/- 142.44
Eval num_timesteps=26500, episode_reward=-150.68 +/- 151.12
Episode length: 151.53 +/- 150.78
Eval num_timesteps=27000, episode_reward=-135.79 +/- 130.96
Episode length: 136.68 +/- 130.66
New best mean reward!
Eval num_timesteps=27500, episode_reward=-131.74 +/- 126.96
Episode length: 132.64 +/- 126.67
New best mean reward!
Eval num_timesteps=28000, episode_reward=-101.98 +/- 85.21
Episode length: 102.94 +/- 85.02
New best mean reward!
Eval num_timesteps=28500, episode_reward=-118.78 +/- 114.55
Episode length: 119.70 +/- 114.28
Eval num_timesteps=29000, episode_reward=-137.68 +/- 137.09
Episode length: 138.56 +/- 136.77
Eval num_timesteps=29500, episode_reward=-120.76 +/- 115.52
Episode length: 121.68 +/- 115.26
Eval num_timesteps=30000, episode_reward=-95.88 +/- 73.15
Episode length: 96.85 +/- 72.98
New best mean reward!
FINISHED IN 570.6562790910248 s


starting seed  10399 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-470.29 +/- 84.14
Episode length: 470.41 +/- 83.83
New best mean reward!
Eval num_timesteps=8000, episode_reward=-285.69 +/- 149.75
Episode length: 286.38 +/- 149.31
New best mean reward!
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-177.44 +/- 64.22
Episode length: 178.43 +/- 64.17
New best mean reward!
Eval num_timesteps=9500, episode_reward=-156.93 +/- 48.82
Episode length: 157.93 +/- 48.82
New best mean reward!
Eval num_timesteps=10000, episode_reward=-174.39 +/- 51.74
Episode length: 175.39 +/- 51.74
Eval num_timesteps=10500, episode_reward=-189.81 +/- 76.37
Episode length: 190.77 +/- 76.21
Eval num_timesteps=11000, episode_reward=-181.22 +/- 59.74
Episode length: 182.21 +/- 59.69
Eval num_timesteps=11500, episode_reward=-163.51 +/- 43.62
Episode length: 164.51 +/- 43.62
Eval num_timesteps=12000, episode_reward=-171.95 +/- 50.72
Episode length: 172.94 +/- 50.65
Eval num_timesteps=12500, episode_reward=-186.44 +/- 44.80
Episode length: 187.44 +/- 44.80
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-484.37 +/- 53.76
Episode length: 484.46 +/- 53.50
Eval num_timesteps=14000, episode_reward=-325.99 +/- 150.44
Episode length: 326.58 +/- 149.96
Eval num_timesteps=14500, episode_reward=-437.28 +/- 105.56
Episode length: 437.59 +/- 105.15
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16000, episode_reward=-162.17 +/- 155.38
Episode length: 163.01 +/- 155.03
Eval num_timesteps=16500, episode_reward=-98.89 +/- 68.40
Episode length: 99.87 +/- 68.28
New best mean reward!
FINISHED IN 205.09865769097814 s
AVG TIME: 361.3006249466114
