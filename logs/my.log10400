nohup: ignoring input


starting seed  10400 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-206.92 +/- 122.08
Episode length: 207.78 +/- 121.74
New best mean reward!
Eval num_timesteps=9000, episode_reward=-187.47 +/- 99.99
Episode length: 188.39 +/- 99.74
New best mean reward!
Eval num_timesteps=9500, episode_reward=-220.71 +/- 161.88
Episode length: 221.46 +/- 161.45
Eval num_timesteps=10000, episode_reward=-178.76 +/- 92.45
Episode length: 179.69 +/- 92.20
New best mean reward!
Eval num_timesteps=10500, episode_reward=-144.58 +/- 131.34
Episode length: 145.47 +/- 131.04
New best mean reward!
Eval num_timesteps=11000, episode_reward=-125.07 +/- 90.90
Episode length: 126.02 +/- 90.69
New best mean reward!
Eval num_timesteps=11500, episode_reward=-218.47 +/- 113.67
Episode length: 219.36 +/- 113.40
Eval num_timesteps=12000, episode_reward=-183.12 +/- 155.96
Episode length: 183.93 +/- 155.58
Eval num_timesteps=12500, episode_reward=-290.88 +/- 178.79
Episode length: 291.46 +/- 178.29
Eval num_timesteps=13000, episode_reward=-307.88 +/- 185.18
Episode length: 308.40 +/- 184.68
Eval num_timesteps=13500, episode_reward=-128.89 +/- 114.86
Episode length: 129.81 +/- 114.61
Eval num_timesteps=14000, episode_reward=-100.97 +/- 72.20
Episode length: 101.94 +/- 72.04
New best mean reward!
Eval num_timesteps=14500, episode_reward=-108.55 +/- 92.47
Episode length: 109.50 +/- 92.26
Eval num_timesteps=15000, episode_reward=-149.80 +/- 144.74
Episode length: 150.66 +/- 144.41
Eval num_timesteps=15500, episode_reward=-212.23 +/- 186.70
Episode length: 212.94 +/- 186.26
Eval num_timesteps=16000, episode_reward=-218.00 +/- 188.66
Episode length: 218.70 +/- 188.22
Eval num_timesteps=16500, episode_reward=-192.72 +/- 176.14
Episode length: 193.48 +/- 175.72
Eval num_timesteps=17000, episode_reward=-287.62 +/- 203.65
Episode length: 288.15 +/- 203.16
Eval num_timesteps=17500, episode_reward=-290.95 +/- 205.48
Episode length: 291.46 +/- 204.98
Eval num_timesteps=18000, episode_reward=-264.88 +/- 204.44
Episode length: 265.46 +/- 203.96
Eval num_timesteps=18500, episode_reward=-380.76 +/- 187.02
Episode length: 381.05 +/- 186.57
Eval num_timesteps=19000, episode_reward=-355.93 +/- 196.65
Episode length: 356.28 +/- 196.17
Eval num_timesteps=19500, episode_reward=-267.79 +/- 202.86
Episode length: 268.36 +/- 202.36
Eval num_timesteps=20000, episode_reward=-160.09 +/- 157.51
Episode length: 160.92 +/- 157.15
Eval num_timesteps=20500, episode_reward=-173.38 +/- 168.93
Episode length: 174.17 +/- 168.52
Eval num_timesteps=21000, episode_reward=-175.50 +/- 170.13
Episode length: 176.29 +/- 169.73
Eval num_timesteps=21500, episode_reward=-166.63 +/- 161.25
Episode length: 167.45 +/- 160.87
Eval num_timesteps=22000, episode_reward=-118.04 +/- 108.64
Episode length: 118.97 +/- 108.39
Eval num_timesteps=22500, episode_reward=-131.28 +/- 127.99
Episode length: 132.18 +/- 127.70
Eval num_timesteps=23000, episode_reward=-126.33 +/- 119.71
Episode length: 127.25 +/- 119.46
Eval num_timesteps=23500, episode_reward=-129.74 +/- 125.55
Episode length: 130.64 +/- 125.26
Eval num_timesteps=24000, episode_reward=-113.66 +/- 100.95
Episode length: 114.60 +/- 100.72
Eval num_timesteps=24500, episode_reward=-106.18 +/- 91.60
Episode length: 107.13 +/- 91.39
Eval num_timesteps=25000, episode_reward=-114.06 +/- 97.94
Episode length: 115.01 +/- 97.74
Eval num_timesteps=25500, episode_reward=-114.41 +/- 96.39
Episode length: 115.36 +/- 96.19
Eval num_timesteps=26000, episode_reward=-106.15 +/- 86.88
Episode length: 107.11 +/- 86.70
Eval num_timesteps=26500, episode_reward=-117.95 +/- 104.09
Episode length: 118.89 +/- 103.87
Eval num_timesteps=27000, episode_reward=-120.23 +/- 114.61
Episode length: 121.15 +/- 114.34
Eval num_timesteps=27500, episode_reward=-95.63 +/- 52.63
Episode length: 96.62 +/- 52.55
New best mean reward!
FINISHED IN 472.0905940919765 s


starting seed  10401 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-235.80 +/- 64.63
Episode length: 236.76 +/- 64.46
New best mean reward!
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-153.09 +/- 33.53
Episode length: 154.09 +/- 33.53
New best mean reward!
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-156.94 +/- 90.98
Episode length: 157.88 +/- 90.75
Eval num_timesteps=5500, episode_reward=-299.46 +/- 156.24
Episode length: 300.09 +/- 155.77
Eval num_timesteps=6000, episode_reward=-486.17 +/- 60.70
Episode length: 486.22 +/- 60.49
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-489.80 +/- 55.29
Episode length: 489.84 +/- 55.12
Eval num_timesteps=9000, episode_reward=-187.69 +/- 158.12
Episode length: 188.50 +/- 157.74
Eval num_timesteps=9500, episode_reward=-169.83 +/- 146.14
Episode length: 170.67 +/- 145.78
Eval num_timesteps=10000, episode_reward=-181.67 +/- 156.03
Episode length: 182.48 +/- 155.64
Eval num_timesteps=10500, episode_reward=-124.52 +/- 103.69
Episode length: 125.46 +/- 103.47
New best mean reward!
Eval num_timesteps=11000, episode_reward=-106.00 +/- 32.21
Episode length: 107.00 +/- 32.21
New best mean reward!
Eval num_timesteps=11500, episode_reward=-221.24 +/- 183.45
Episode length: 221.94 +/- 183.00
Eval num_timesteps=12000, episode_reward=-221.72 +/- 191.59
Episode length: 222.41 +/- 191.14
Eval num_timesteps=12500, episode_reward=-121.32 +/- 108.10
Episode length: 122.25 +/- 107.86
Eval num_timesteps=13000, episode_reward=-132.85 +/- 124.61
Episode length: 133.76 +/- 124.35
Eval num_timesteps=13500, episode_reward=-161.16 +/- 146.10
Episode length: 162.02 +/- 145.77
Eval num_timesteps=14000, episode_reward=-120.64 +/- 99.46
Episode length: 121.58 +/- 99.24
Eval num_timesteps=14500, episode_reward=-138.43 +/- 136.50
Episode length: 139.31 +/- 136.18
Eval num_timesteps=15000, episode_reward=-137.41 +/- 129.42
Episode length: 138.30 +/- 129.11
Eval num_timesteps=15500, episode_reward=-113.39 +/- 100.76
Episode length: 114.33 +/- 100.53
Eval num_timesteps=16000, episode_reward=-82.78 +/- 16.96
Episode length: 83.78 +/- 16.96
New best mean reward!
FINISHED IN 304.9335963749909 s


starting seed  10402 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-302.27 +/- 201.44
Episode length: 302.77 +/- 200.95
New best mean reward!
Eval num_timesteps=15000, episode_reward=-260.71 +/- 201.24
Episode length: 261.31 +/- 200.77
New best mean reward!
Eval num_timesteps=15500, episode_reward=-203.62 +/- 181.67
Episode length: 204.36 +/- 181.25
New best mean reward!
Eval num_timesteps=16000, episode_reward=-198.13 +/- 179.75
Episode length: 198.87 +/- 179.31
New best mean reward!
Eval num_timesteps=16500, episode_reward=-218.98 +/- 183.40
Episode length: 219.70 +/- 182.98
Eval num_timesteps=17000, episode_reward=-207.54 +/- 180.03
Episode length: 208.27 +/- 179.59
Eval num_timesteps=17500, episode_reward=-218.50 +/- 187.28
Episode length: 219.21 +/- 186.84
Eval num_timesteps=18000, episode_reward=-150.34 +/- 148.44
Episode length: 151.19 +/- 148.08
New best mean reward!
Eval num_timesteps=18500, episode_reward=-162.87 +/- 158.70
Episode length: 163.70 +/- 158.34
Eval num_timesteps=19000, episode_reward=-170.53 +/- 162.65
Episode length: 171.35 +/- 162.28
Eval num_timesteps=19500, episode_reward=-182.03 +/- 167.09
Episode length: 182.82 +/- 166.69
Eval num_timesteps=20000, episode_reward=-200.24 +/- 176.64
Episode length: 200.99 +/- 176.22
Eval num_timesteps=20500, episode_reward=-159.60 +/- 155.49
Episode length: 160.43 +/- 155.12
Eval num_timesteps=21000, episode_reward=-150.49 +/- 146.37
Episode length: 151.35 +/- 146.04
Eval num_timesteps=21500, episode_reward=-171.02 +/- 160.95
Episode length: 171.84 +/- 160.58
Eval num_timesteps=22000, episode_reward=-177.34 +/- 168.21
Episode length: 178.13 +/- 167.81
Eval num_timesteps=22500, episode_reward=-156.96 +/- 148.63
Episode length: 157.81 +/- 148.28
Eval num_timesteps=23000, episode_reward=-142.70 +/- 139.46
Episode length: 143.57 +/- 139.13
New best mean reward!
Eval num_timesteps=23500, episode_reward=-131.84 +/- 130.32
Episode length: 132.73 +/- 130.01
New best mean reward!
Eval num_timesteps=24000, episode_reward=-137.86 +/- 133.95
Episode length: 138.75 +/- 133.66
Eval num_timesteps=24500, episode_reward=-107.33 +/- 90.11
Episode length: 108.29 +/- 89.93
New best mean reward!
Eval num_timesteps=25000, episode_reward=-107.54 +/- 78.18
Episode length: 108.51 +/- 78.03
Eval num_timesteps=25500, episode_reward=-105.48 +/- 83.97
Episode length: 106.45 +/- 83.83
New best mean reward!
Eval num_timesteps=26000, episode_reward=-127.47 +/- 118.05
Episode length: 128.39 +/- 117.80
Eval num_timesteps=26500, episode_reward=-124.70 +/- 105.86
Episode length: 125.64 +/- 105.65
Eval num_timesteps=27000, episode_reward=-115.68 +/- 102.83
Episode length: 116.62 +/- 102.61
Eval num_timesteps=27500, episode_reward=-125.13 +/- 113.47
Episode length: 126.06 +/- 113.24
Eval num_timesteps=28000, episode_reward=-116.06 +/- 106.66
Episode length: 116.99 +/- 106.41
Eval num_timesteps=28500, episode_reward=-133.72 +/- 131.30
Episode length: 134.61 +/- 131.00
Eval num_timesteps=29000, episode_reward=-143.07 +/- 137.03
Episode length: 143.95 +/- 136.72
Eval num_timesteps=29500, episode_reward=-130.28 +/- 121.02
Episode length: 131.19 +/- 120.74
Eval num_timesteps=30000, episode_reward=-119.29 +/- 106.05
Episode length: 120.24 +/- 105.87
Eval num_timesteps=30500, episode_reward=-133.78 +/- 126.10
Episode length: 134.68 +/- 125.81
Eval num_timesteps=31000, episode_reward=-108.03 +/- 86.35
Episode length: 108.99 +/- 86.17
Eval num_timesteps=31500, episode_reward=-100.20 +/- 76.03
Episode length: 101.17 +/- 75.87
New best mean reward!
Eval num_timesteps=32000, episode_reward=-111.81 +/- 97.52
Episode length: 112.76 +/- 97.32
Eval num_timesteps=32500, episode_reward=-135.27 +/- 128.54
Episode length: 136.17 +/- 128.26
Eval num_timesteps=33000, episode_reward=-91.26 +/- 45.59
Episode length: 92.25 +/- 45.50
New best mean reward!
FINISHED IN 693.5467932640458 s


starting seed  10403 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-270.88 +/- 200.23
Episode length: 271.46 +/- 199.75
New best mean reward!
Eval num_timesteps=9500, episode_reward=-141.78 +/- 123.10
Episode length: 142.68 +/- 122.81
New best mean reward!
Eval num_timesteps=10000, episode_reward=-140.47 +/- 112.65
Episode length: 141.40 +/- 112.42
New best mean reward!
Eval num_timesteps=10500, episode_reward=-138.32 +/- 118.47
Episode length: 139.23 +/- 118.20
New best mean reward!
Eval num_timesteps=11000, episode_reward=-147.80 +/- 129.20
Episode length: 148.69 +/- 128.90
Eval num_timesteps=11500, episode_reward=-105.45 +/- 63.13
Episode length: 106.44 +/- 63.07
New best mean reward!
Eval num_timesteps=12000, episode_reward=-103.55 +/- 62.94
Episode length: 104.53 +/- 62.81
New best mean reward!
Eval num_timesteps=12500, episode_reward=-97.58 +/- 39.61
Episode length: 98.58 +/- 39.61
New best mean reward!
FINISHED IN 255.21141377196182 s


starting seed  10404 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-86.05 +/- 18.39
Episode length: 87.05 +/- 18.39
New best mean reward!
FINISHED IN 194.81294693396194 s


starting seed  10405 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-471.81 +/- 84.44
Episode length: 471.92 +/- 84.14
New best mean reward!
Eval num_timesteps=3500, episode_reward=-184.73 +/- 64.60
Episode length: 185.70 +/- 64.45
New best mean reward!
Eval num_timesteps=4000, episode_reward=-213.31 +/- 74.10
Episode length: 214.26 +/- 73.91
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-345.27 +/- 111.22
Episode length: 345.98 +/- 110.82
Eval num_timesteps=6000, episode_reward=-469.01 +/- 62.52
Episode length: 469.25 +/- 62.14
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-466.00 +/- 81.20
Episode length: 466.16 +/- 80.84
Eval num_timesteps=8000, episode_reward=-323.31 +/- 140.45
Episode length: 323.94 +/- 139.98
Eval num_timesteps=8500, episode_reward=-137.97 +/- 48.41
Episode length: 138.96 +/- 48.34
New best mean reward!
Eval num_timesteps=9000, episode_reward=-96.90 +/- 24.56
Episode length: 97.90 +/- 24.56
New best mean reward!
FINISHED IN 193.48825973301427 s


starting seed  10406 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-231.96 +/- 137.34
Episode length: 232.76 +/- 136.95
New best mean reward!
Eval num_timesteps=1500, episode_reward=-352.51 +/- 164.33
Episode length: 352.96 +/- 163.84
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-474.70 +/- 92.42
Episode length: 474.77 +/- 92.16
Eval num_timesteps=10500, episode_reward=-406.18 +/- 162.22
Episode length: 406.44 +/- 161.80
Eval num_timesteps=11000, episode_reward=-313.10 +/- 199.50
Episode length: 313.57 +/- 199.00
Eval num_timesteps=11500, episode_reward=-307.53 +/- 200.74
Episode length: 308.01 +/- 200.24
Eval num_timesteps=12000, episode_reward=-289.08 +/- 197.36
Episode length: 289.62 +/- 196.87
Eval num_timesteps=12500, episode_reward=-256.66 +/- 191.90
Episode length: 257.30 +/- 191.45
Eval num_timesteps=13000, episode_reward=-257.55 +/- 198.96
Episode length: 258.15 +/- 198.47
Eval num_timesteps=13500, episode_reward=-226.49 +/- 191.25
Episode length: 227.17 +/- 190.80
New best mean reward!
Eval num_timesteps=14000, episode_reward=-192.91 +/- 171.81
Episode length: 193.69 +/- 171.42
New best mean reward!
Eval num_timesteps=14500, episode_reward=-207.98 +/- 181.00
Episode length: 208.71 +/- 180.56
Eval num_timesteps=15000, episode_reward=-160.95 +/- 150.79
Episode length: 161.79 +/- 150.43
New best mean reward!
Eval num_timesteps=15500, episode_reward=-191.06 +/- 173.79
Episode length: 191.83 +/- 173.38
Eval num_timesteps=16000, episode_reward=-141.78 +/- 135.59
Episode length: 142.66 +/- 135.28
New best mean reward!
Eval num_timesteps=16500, episode_reward=-165.09 +/- 155.55
Episode length: 165.95 +/- 155.25
Eval num_timesteps=17000, episode_reward=-153.93 +/- 144.73
Episode length: 154.79 +/- 144.40
Eval num_timesteps=17500, episode_reward=-160.90 +/- 148.15
Episode length: 161.75 +/- 147.81
Eval num_timesteps=18000, episode_reward=-152.50 +/- 147.68
Episode length: 153.35 +/- 147.33
Eval num_timesteps=18500, episode_reward=-136.83 +/- 131.89
Episode length: 137.72 +/- 131.59
New best mean reward!
Eval num_timesteps=19000, episode_reward=-133.41 +/- 129.31
Episode length: 134.31 +/- 129.03
New best mean reward!
Eval num_timesteps=19500, episode_reward=-140.68 +/- 130.61
Episode length: 141.57 +/- 130.31
Eval num_timesteps=20000, episode_reward=-152.61 +/- 145.29
Episode length: 153.47 +/- 144.96
Eval num_timesteps=20500, episode_reward=-157.12 +/- 149.29
Episode length: 157.97 +/- 148.95
Eval num_timesteps=21000, episode_reward=-131.57 +/- 120.46
Episode length: 132.48 +/- 120.19
New best mean reward!
Eval num_timesteps=21500, episode_reward=-132.41 +/- 126.92
Episode length: 133.31 +/- 126.63
Eval num_timesteps=22000, episode_reward=-132.76 +/- 128.28
Episode length: 133.66 +/- 127.99
Eval num_timesteps=22500, episode_reward=-103.20 +/- 76.90
Episode length: 104.17 +/- 76.75
New best mean reward!
Eval num_timesteps=23000, episode_reward=-105.38 +/- 75.95
Episode length: 106.35 +/- 75.79
Eval num_timesteps=23500, episode_reward=-93.22 +/- 52.21
Episode length: 94.21 +/- 52.13
New best mean reward!
FINISHED IN 448.8085919499863 s


starting seed  10407 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-261.23 +/- 141.15
Episode length: 261.99 +/- 140.74
New best mean reward!
Eval num_timesteps=8000, episode_reward=-427.45 +/- 47.57
Episode length: 428.31 +/- 47.35
Eval num_timesteps=8500, episode_reward=-258.66 +/- 130.72
Episode length: 259.45 +/- 130.34
New best mean reward!
Eval num_timesteps=9000, episode_reward=-271.98 +/- 119.29
Episode length: 272.79 +/- 118.93
Eval num_timesteps=9500, episode_reward=-178.50 +/- 30.03
Episode length: 179.50 +/- 30.03
New best mean reward!
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-201.96 +/- 174.51
Episode length: 202.71 +/- 174.09
Eval num_timesteps=13500, episode_reward=-192.45 +/- 166.30
Episode length: 193.23 +/- 165.89
Eval num_timesteps=14000, episode_reward=-246.14 +/- 192.61
Episode length: 246.78 +/- 192.13
Eval num_timesteps=14500, episode_reward=-246.40 +/- 186.48
Episode length: 247.07 +/- 186.04
Eval num_timesteps=15000, episode_reward=-277.67 +/- 198.02
Episode length: 278.23 +/- 197.53
Eval num_timesteps=15500, episode_reward=-331.05 +/- 189.39
Episode length: 331.50 +/- 188.90
Eval num_timesteps=16000, episode_reward=-331.62 +/- 187.29
Episode length: 332.07 +/- 186.80
Eval num_timesteps=16500, episode_reward=-303.83 +/- 192.90
Episode length: 304.34 +/- 192.40
Eval num_timesteps=17000, episode_reward=-199.71 +/- 175.08
Episode length: 200.46 +/- 174.66
Eval num_timesteps=17500, episode_reward=-198.00 +/- 175.69
Episode length: 198.75 +/- 175.26
Eval num_timesteps=18000, episode_reward=-185.00 +/- 165.61
Episode length: 185.79 +/- 165.21
Eval num_timesteps=18500, episode_reward=-151.20 +/- 140.34
Episode length: 152.07 +/- 140.02
New best mean reward!
Eval num_timesteps=19000, episode_reward=-125.24 +/- 110.28
Episode length: 126.17 +/- 110.04
New best mean reward!
Eval num_timesteps=19500, episode_reward=-97.96 +/- 73.27
Episode length: 98.93 +/- 73.11
New best mean reward!
FINISHED IN 419.15015379397664 s


starting seed  10408 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-124.82 +/- 25.31
Episode length: 125.82 +/- 25.31
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-499.00 +/- 9.95
Episode length: 499.01 +/- 9.85
Eval num_timesteps=3000, episode_reward=-499.72 +/- 1.92
Episode length: 499.75 +/- 1.78
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-498.07 +/- 17.40
Episode length: 498.09 +/- 17.29
Eval num_timesteps=4500, episode_reward=-497.21 +/- 27.76
Episode length: 497.22 +/- 27.66
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-498.19 +/- 18.01
Episode length: 498.20 +/- 17.91
Eval num_timesteps=8000, episode_reward=-499.01 +/- 6.13
Episode length: 499.05 +/- 5.98
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-451.79 +/- 97.68
Episode length: 452.00 +/- 97.29
Eval num_timesteps=12000, episode_reward=-408.06 +/- 156.46
Episode length: 408.32 +/- 156.02
Eval num_timesteps=12500, episode_reward=-380.38 +/- 179.04
Episode length: 380.69 +/- 178.58
Eval num_timesteps=13000, episode_reward=-342.50 +/- 181.35
Episode length: 342.94 +/- 180.86
Eval num_timesteps=13500, episode_reward=-361.82 +/- 182.81
Episode length: 362.19 +/- 182.33
Eval num_timesteps=14000, episode_reward=-390.12 +/- 164.85
Episode length: 390.43 +/- 164.39
Eval num_timesteps=14500, episode_reward=-368.17 +/- 155.55
Episode length: 368.60 +/- 155.06
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-495.67 +/- 30.32
Episode length: 495.69 +/- 30.18
Eval num_timesteps=16000, episode_reward=-237.30 +/- 186.99
Episode length: 237.98 +/- 186.54
Eval num_timesteps=16500, episode_reward=-224.99 +/- 180.22
Episode length: 225.71 +/- 179.79
Eval num_timesteps=17000, episode_reward=-192.30 +/- 168.25
Episode length: 193.08 +/- 167.85
Eval num_timesteps=17500, episode_reward=-155.66 +/- 142.01
Episode length: 156.53 +/- 141.70
Eval num_timesteps=18000, episode_reward=-152.11 +/- 150.26
Episode length: 152.96 +/- 149.91
Eval num_timesteps=18500, episode_reward=-127.11 +/- 112.81
Episode length: 128.03 +/- 112.55
Eval num_timesteps=19000, episode_reward=-146.91 +/- 133.76
Episode length: 147.80 +/- 133.47
Eval num_timesteps=19500, episode_reward=-168.16 +/- 158.31
Episode length: 169.00 +/- 157.98
Eval num_timesteps=20000, episode_reward=-150.52 +/- 135.37
Episode length: 151.40 +/- 135.06
Eval num_timesteps=20500, episode_reward=-129.44 +/- 120.48
Episode length: 130.35 +/- 120.20
Eval num_timesteps=21000, episode_reward=-101.63 +/- 66.05
Episode length: 102.61 +/- 65.93
New best mean reward!
Eval num_timesteps=21500, episode_reward=-129.16 +/- 115.78
Episode length: 130.08 +/- 115.53
Eval num_timesteps=22000, episode_reward=-115.86 +/- 94.81
Episode length: 116.81 +/- 94.61
Eval num_timesteps=22500, episode_reward=-114.14 +/- 92.69
Episode length: 115.09 +/- 92.48
Eval num_timesteps=23000, episode_reward=-109.62 +/- 79.28
Episode length: 110.59 +/- 79.13
Eval num_timesteps=23500, episode_reward=-101.95 +/- 73.92
Episode length: 102.93 +/- 73.81
Eval num_timesteps=24000, episode_reward=-104.25 +/- 68.32
Episode length: 105.23 +/- 68.20
Eval num_timesteps=24500, episode_reward=-93.87 +/- 61.90
Episode length: 94.85 +/- 61.77
New best mean reward!
FINISHED IN 503.4984382740222 s


starting seed  10409 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-419.16 +/- 117.74
Episode length: 419.49 +/- 117.28
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-112.77 +/- 100.04
Episode length: 113.71 +/- 99.81
New best mean reward!
Eval num_timesteps=9000, episode_reward=-86.04 +/- 24.14
Episode length: 87.04 +/- 24.14
New best mean reward!
FINISHED IN 213.1350631180103 s


starting seed  10410 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-433.77 +/- 151.92
Episode length: 433.93 +/- 151.55
New best mean reward!
Eval num_timesteps=1000, episode_reward=-367.11 +/- 190.81
Episode length: 367.44 +/- 190.35
New best mean reward!
Eval num_timesteps=1500, episode_reward=-85.20 +/- 14.51
Episode length: 86.20 +/- 14.51
New best mean reward!
FINISHED IN 26.682301120017655 s


starting seed  10411 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-499.69 +/- 3.08
Episode length: 499.70 +/- 2.98
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-491.86 +/- 57.00
Episode length: 491.88 +/- 56.86
New best mean reward!
Eval num_timesteps=12000, episode_reward=-422.18 +/- 160.73
Episode length: 422.37 +/- 160.34
New best mean reward!
Eval num_timesteps=12500, episode_reward=-451.71 +/- 130.84
Episode length: 451.83 +/- 130.51
Eval num_timesteps=13000, episode_reward=-374.80 +/- 188.38
Episode length: 375.11 +/- 187.92
New best mean reward!
Eval num_timesteps=13500, episode_reward=-293.52 +/- 203.73
Episode length: 294.03 +/- 203.24
New best mean reward!
Eval num_timesteps=14000, episode_reward=-293.28 +/- 201.45
Episode length: 293.80 +/- 200.95
New best mean reward!
Eval num_timesteps=14500, episode_reward=-301.44 +/- 205.09
Episode length: 301.93 +/- 204.60
Eval num_timesteps=15000, episode_reward=-174.33 +/- 169.31
Episode length: 175.13 +/- 168.92
New best mean reward!
Eval num_timesteps=15500, episode_reward=-130.72 +/- 121.93
Episode length: 131.63 +/- 121.66
New best mean reward!
Eval num_timesteps=16000, episode_reward=-138.45 +/- 138.72
Episode length: 139.33 +/- 138.40
Eval num_timesteps=16500, episode_reward=-128.04 +/- 121.64
Episode length: 128.95 +/- 121.36
New best mean reward!
Eval num_timesteps=17000, episode_reward=-170.19 +/- 166.42
Episode length: 170.99 +/- 166.03
Eval num_timesteps=17500, episode_reward=-167.81 +/- 162.57
Episode length: 168.63 +/- 162.20
Eval num_timesteps=18000, episode_reward=-125.92 +/- 119.37
Episode length: 126.83 +/- 119.08
New best mean reward!
Eval num_timesteps=18500, episode_reward=-140.59 +/- 134.19
Episode length: 141.48 +/- 133.90
Eval num_timesteps=19000, episode_reward=-117.39 +/- 111.47
Episode length: 118.32 +/- 111.23
New best mean reward!
Eval num_timesteps=19500, episode_reward=-100.41 +/- 70.78
Episode length: 101.39 +/- 70.67
New best mean reward!
Eval num_timesteps=20000, episode_reward=-89.94 +/- 30.74
Episode length: 90.94 +/- 30.74
New best mean reward!
FINISHED IN 456.05702361103613 s


starting seed  10412 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-306.48 +/- 167.95
Episode length: 307.06 +/- 167.47
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-273.46 +/- 179.09
Episode length: 274.09 +/- 178.62
New best mean reward!
Eval num_timesteps=9500, episode_reward=-184.47 +/- 87.33
Episode length: 185.41 +/- 87.11
New best mean reward!
Eval num_timesteps=10000, episode_reward=-207.92 +/- 114.93
Episode length: 208.79 +/- 114.60
Eval num_timesteps=10500, episode_reward=-196.55 +/- 91.64
Episode length: 197.49 +/- 91.44
Eval num_timesteps=11000, episode_reward=-158.15 +/- 35.43
Episode length: 159.15 +/- 35.43
New best mean reward!
Eval num_timesteps=11500, episode_reward=-173.02 +/- 60.03
Episode length: 174.00 +/- 59.92
Eval num_timesteps=12000, episode_reward=-233.09 +/- 166.85
Episode length: 233.81 +/- 166.41
Eval num_timesteps=12500, episode_reward=-122.37 +/- 48.07
Episode length: 123.36 +/- 47.99
New best mean reward!
Eval num_timesteps=13000, episode_reward=-107.32 +/- 44.35
Episode length: 108.31 +/- 44.26
New best mean reward!
Eval num_timesteps=13500, episode_reward=-102.44 +/- 20.69
Episode length: 103.44 +/- 20.69
New best mean reward!
Eval num_timesteps=14000, episode_reward=-96.28 +/- 19.08
Episode length: 97.28 +/- 19.08
New best mean reward!
FINISHED IN 315.7413858270156 s


starting seed  10413 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-248.29 +/- 197.72
Episode length: 248.91 +/- 197.23
New best mean reward!
Eval num_timesteps=10500, episode_reward=-141.69 +/- 132.11
Episode length: 142.58 +/- 131.81
New best mean reward!
Eval num_timesteps=11000, episode_reward=-313.71 +/- 195.09
Episode length: 314.19 +/- 194.59
Eval num_timesteps=11500, episode_reward=-89.79 +/- 22.33
Episode length: 90.79 +/- 22.33
New best mean reward!
FINISHED IN 277.48247816000367 s


starting seed  10414 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-417.60 +/- 165.03
Episode length: 417.80 +/- 164.63
New best mean reward!
Eval num_timesteps=13500, episode_reward=-359.49 +/- 196.20
Episode length: 359.83 +/- 195.73
New best mean reward!
Eval num_timesteps=14000, episode_reward=-303.95 +/- 204.49
Episode length: 304.44 +/- 204.00
New best mean reward!
Eval num_timesteps=14500, episode_reward=-318.09 +/- 202.50
Episode length: 318.54 +/- 202.00
Eval num_timesteps=15000, episode_reward=-335.67 +/- 201.82
Episode length: 336.07 +/- 201.33
Eval num_timesteps=15500, episode_reward=-293.02 +/- 204.85
Episode length: 293.53 +/- 204.36
New best mean reward!
Eval num_timesteps=16000, episode_reward=-288.12 +/- 202.37
Episode length: 288.65 +/- 201.88
New best mean reward!
Eval num_timesteps=16500, episode_reward=-125.45 +/- 113.64
Episode length: 126.37 +/- 113.38
New best mean reward!
Eval num_timesteps=17000, episode_reward=-127.95 +/- 112.15
Episode length: 128.88 +/- 111.92
Eval num_timesteps=17500, episode_reward=-114.44 +/- 102.24
Episode length: 115.38 +/- 102.01
New best mean reward!
Eval num_timesteps=18000, episode_reward=-108.14 +/- 90.04
Episode length: 109.10 +/- 89.87
New best mean reward!
Eval num_timesteps=18500, episode_reward=-97.98 +/- 51.49
Episode length: 98.98 +/- 51.49
New best mean reward!
FINISHED IN 393.84369913395494 s


starting seed  10415 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-222.81 +/- 161.21
Episode length: 223.56 +/- 160.78
New best mean reward!
Eval num_timesteps=7500, episode_reward=-107.23 +/- 32.10
Episode length: 108.23 +/- 32.10
New best mean reward!
Eval num_timesteps=8000, episode_reward=-102.48 +/- 62.28
Episode length: 103.46 +/- 62.15
New best mean reward!
Eval num_timesteps=8500, episode_reward=-136.13 +/- 26.39
Episode length: 137.13 +/- 26.39
Eval num_timesteps=9000, episode_reward=-114.32 +/- 91.51
Episode length: 115.27 +/- 91.30
Eval num_timesteps=9500, episode_reward=-86.94 +/- 24.55
Episode length: 87.94 +/- 24.55
New best mean reward!
FINISHED IN 188.826902752975 s


starting seed  10416 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-358.51 +/- 164.81
Episode length: 358.94 +/- 164.33
New best mean reward!
Eval num_timesteps=5000, episode_reward=-459.92 +/- 90.03
Episode length: 460.10 +/- 89.67
Eval num_timesteps=5500, episode_reward=-166.71 +/- 43.24
Episode length: 167.71 +/- 43.24
New best mean reward!
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-156.45 +/- 31.17
Episode length: 157.45 +/- 31.17
New best mean reward!
Eval num_timesteps=7000, episode_reward=-338.02 +/- 170.02
Episode length: 338.50 +/- 169.53
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-497.87 +/- 21.19
Episode length: 497.88 +/- 21.09
Eval num_timesteps=9000, episode_reward=-217.26 +/- 80.37
Episode length: 218.21 +/- 80.19
Eval num_timesteps=9500, episode_reward=-265.98 +/- 100.33
Episode length: 266.86 +/- 100.05
Eval num_timesteps=10000, episode_reward=-305.82 +/- 111.61
Episode length: 306.65 +/- 111.31
Eval num_timesteps=10500, episode_reward=-275.29 +/- 112.32
Episode length: 276.12 +/- 111.98
Eval num_timesteps=11000, episode_reward=-177.49 +/- 60.56
Episode length: 178.47 +/- 60.45
Eval num_timesteps=11500, episode_reward=-170.30 +/- 44.72
Episode length: 171.30 +/- 44.72
Eval num_timesteps=12000, episode_reward=-170.96 +/- 60.26
Episode length: 171.94 +/- 60.15
Eval num_timesteps=12500, episode_reward=-167.25 +/- 31.22
Episode length: 168.25 +/- 31.22
Eval num_timesteps=13000, episode_reward=-187.42 +/- 57.78
Episode length: 188.40 +/- 57.67
Eval num_timesteps=13500, episode_reward=-188.88 +/- 50.66
Episode length: 189.88 +/- 50.66
Eval num_timesteps=14000, episode_reward=-441.92 +/- 99.02
Episode length: 442.23 +/- 98.61
Eval num_timesteps=14500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=16500, episode_reward=-492.74 +/- 41.41
Episode length: 492.77 +/- 41.24
Eval num_timesteps=17000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=17500, episode_reward=-458.16 +/- 96.74
Episode length: 458.34 +/- 96.38
Eval num_timesteps=18000, episode_reward=-464.96 +/- 92.28
Episode length: 465.10 +/- 91.95
Eval num_timesteps=18500, episode_reward=-443.44 +/- 116.52
Episode length: 443.64 +/- 116.13
Eval num_timesteps=19000, episode_reward=-319.69 +/- 165.29
Episode length: 320.25 +/- 164.81
Eval num_timesteps=19500, episode_reward=-307.43 +/- 166.56
Episode length: 308.02 +/- 166.08
Eval num_timesteps=20000, episode_reward=-272.64 +/- 155.50
Episode length: 273.34 +/- 155.06
Eval num_timesteps=20500, episode_reward=-251.65 +/- 147.25
Episode length: 252.40 +/- 146.83
Eval num_timesteps=21000, episode_reward=-234.46 +/- 140.75
Episode length: 235.26 +/- 140.37
Eval num_timesteps=21500, episode_reward=-237.82 +/- 155.98
Episode length: 238.57 +/- 155.56
Eval num_timesteps=22000, episode_reward=-194.66 +/- 140.28
Episode length: 195.50 +/- 139.93
Eval num_timesteps=22500, episode_reward=-181.51 +/- 144.33
Episode length: 182.36 +/- 144.00
Eval num_timesteps=23000, episode_reward=-216.11 +/- 181.33
Episode length: 216.84 +/- 180.90
Eval num_timesteps=23500, episode_reward=-163.22 +/- 149.48
Episode length: 164.06 +/- 149.12
Eval num_timesteps=24000, episode_reward=-149.88 +/- 142.88
Episode length: 150.76 +/- 142.58
New best mean reward!
Eval num_timesteps=24500, episode_reward=-136.28 +/- 124.09
Episode length: 137.20 +/- 123.85
New best mean reward!
Eval num_timesteps=25000, episode_reward=-112.77 +/- 89.80
Episode length: 113.73 +/- 89.63
New best mean reward!
Eval num_timesteps=25500, episode_reward=-117.71 +/- 102.62
Episode length: 118.65 +/- 102.40
Eval num_timesteps=26000, episode_reward=-132.47 +/- 124.16
Episode length: 133.38 +/- 123.89
Eval num_timesteps=26500, episode_reward=-125.06 +/- 104.29
Episode length: 126.00 +/- 104.08
Eval num_timesteps=27000, episode_reward=-130.57 +/- 119.65
Episode length: 131.48 +/- 119.38
Eval num_timesteps=27500, episode_reward=-147.83 +/- 137.08
Episode length: 148.71 +/- 136.77
Eval num_timesteps=28000, episode_reward=-154.63 +/- 145.58
Episode length: 155.49 +/- 145.25
Eval num_timesteps=28500, episode_reward=-147.90 +/- 139.54
Episode length: 148.77 +/- 139.21
Eval num_timesteps=29000, episode_reward=-173.52 +/- 160.41
Episode length: 174.33 +/- 160.02
Eval num_timesteps=29500, episode_reward=-171.02 +/- 158.76
Episode length: 171.84 +/- 158.39
Eval num_timesteps=30000, episode_reward=-145.26 +/- 134.65
Episode length: 146.14 +/- 134.33
Eval num_timesteps=30500, episode_reward=-121.56 +/- 101.41
Episode length: 122.51 +/- 101.23
Eval num_timesteps=31000, episode_reward=-134.12 +/- 124.60
Episode length: 135.02 +/- 124.30
Eval num_timesteps=31500, episode_reward=-122.83 +/- 110.51
Episode length: 123.76 +/- 110.27
Eval num_timesteps=32000, episode_reward=-137.96 +/- 129.67
Episode length: 138.85 +/- 129.37
Eval num_timesteps=32500, episode_reward=-134.41 +/- 128.13
Episode length: 135.31 +/- 127.84
Eval num_timesteps=33000, episode_reward=-116.52 +/- 106.95
Episode length: 117.45 +/- 106.70
Eval num_timesteps=33500, episode_reward=-133.26 +/- 123.93
Episode length: 134.17 +/- 123.66
Eval num_timesteps=34000, episode_reward=-117.30 +/- 102.39
Episode length: 118.24 +/- 102.16
Eval num_timesteps=34500, episode_reward=-104.71 +/- 83.96
Episode length: 105.67 +/- 83.77
New best mean reward!
Eval num_timesteps=35000, episode_reward=-105.22 +/- 78.76
Episode length: 106.19 +/- 78.61
Eval num_timesteps=35500, episode_reward=-88.86 +/- 23.70
Episode length: 89.86 +/- 23.70
New best mean reward!
FINISHED IN 586.9466549950303 s


starting seed  10417 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-351.98 +/- 171.37
Episode length: 352.41 +/- 170.87
New best mean reward!
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-429.67 +/- 120.81
Episode length: 429.94 +/- 120.39
Eval num_timesteps=3000, episode_reward=-280.09 +/- 148.63
Episode length: 280.79 +/- 148.18
New best mean reward!
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-325.57 +/- 190.57
Episode length: 326.03 +/- 190.08
Eval num_timesteps=6000, episode_reward=-313.13 +/- 169.68
Episode length: 313.69 +/- 169.19
Eval num_timesteps=6500, episode_reward=-152.99 +/- 134.93
Episode length: 153.87 +/- 134.63
New best mean reward!
Eval num_timesteps=7000, episode_reward=-121.88 +/- 100.43
Episode length: 122.82 +/- 100.20
New best mean reward!
Eval num_timesteps=7500, episode_reward=-94.00 +/- 51.24
Episode length: 94.99 +/- 51.16
New best mean reward!
FINISHED IN 158.88461647101212 s


starting seed  10418 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-153.57 +/- 36.34
Episode length: 154.57 +/- 36.34
New best mean reward!
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-487.16 +/- 62.98
Episode length: 487.20 +/- 62.78
Eval num_timesteps=4000, episode_reward=-330.99 +/- 170.08
Episode length: 331.49 +/- 169.58
Eval num_timesteps=4500, episode_reward=-309.37 +/- 167.80
Episode length: 309.94 +/- 167.31
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-490.19 +/- 55.80
Episode length: 490.22 +/- 55.63
Eval num_timesteps=9500, episode_reward=-176.06 +/- 75.97
Episode length: 177.02 +/- 75.80
Eval num_timesteps=10000, episode_reward=-166.21 +/- 51.00
Episode length: 167.20 +/- 50.93
Eval num_timesteps=10500, episode_reward=-155.10 +/- 25.30
Episode length: 156.10 +/- 25.30
Eval num_timesteps=11000, episode_reward=-161.30 +/- 34.46
Episode length: 162.30 +/- 34.46
Eval num_timesteps=11500, episode_reward=-159.52 +/- 31.54
Episode length: 160.52 +/- 31.54
Eval num_timesteps=12000, episode_reward=-165.02 +/- 42.34
Episode length: 166.02 +/- 42.34
Eval num_timesteps=12500, episode_reward=-162.53 +/- 40.00
Episode length: 163.53 +/- 40.00
Eval num_timesteps=13000, episode_reward=-156.59 +/- 38.96
Episode length: 157.59 +/- 38.96
Eval num_timesteps=13500, episode_reward=-162.21 +/- 40.78
Episode length: 163.21 +/- 40.78
Eval num_timesteps=14000, episode_reward=-160.45 +/- 41.98
Episode length: 161.45 +/- 41.98
Eval num_timesteps=14500, episode_reward=-163.17 +/- 47.04
Episode length: 164.16 +/- 46.97
Eval num_timesteps=15000, episode_reward=-163.04 +/- 38.72
Episode length: 164.04 +/- 38.72
Eval num_timesteps=15500, episode_reward=-172.03 +/- 68.74
Episode length: 173.00 +/- 68.59
Eval num_timesteps=16000, episode_reward=-162.33 +/- 31.31
Episode length: 163.33 +/- 31.31
Eval num_timesteps=16500, episode_reward=-150.13 +/- 21.01
Episode length: 151.13 +/- 21.01
New best mean reward!
Eval num_timesteps=17000, episode_reward=-125.39 +/- 29.45
Episode length: 126.39 +/- 29.45
New best mean reward!
Eval num_timesteps=17500, episode_reward=-136.55 +/- 43.26
Episode length: 137.54 +/- 43.17
Eval num_timesteps=18000, episode_reward=-165.51 +/- 56.95
Episode length: 166.49 +/- 56.83
Eval num_timesteps=18500, episode_reward=-97.48 +/- 47.86
Episode length: 98.47 +/- 47.78
New best mean reward!
FINISHED IN 314.3728417799575 s


starting seed  10419 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-289.92 +/- 160.58
Episode length: 290.56 +/- 160.11
New best mean reward!
Eval num_timesteps=10000, episode_reward=-215.36 +/- 131.28
Episode length: 216.19 +/- 130.91
New best mean reward!
Eval num_timesteps=10500, episode_reward=-189.20 +/- 132.17
Episode length: 190.05 +/- 131.82
New best mean reward!
Eval num_timesteps=11000, episode_reward=-214.40 +/- 118.13
Episode length: 215.27 +/- 117.82
Eval num_timesteps=11500, episode_reward=-253.71 +/- 155.79
Episode length: 254.45 +/- 155.38
Eval num_timesteps=12000, episode_reward=-472.01 +/- 89.34
Episode length: 472.10 +/- 89.06
Eval num_timesteps=12500, episode_reward=-487.34 +/- 62.11
Episode length: 487.38 +/- 61.92
Eval num_timesteps=13000, episode_reward=-215.81 +/- 135.38
Episode length: 216.63 +/- 135.01
Eval num_timesteps=13500, episode_reward=-407.36 +/- 150.28
Episode length: 407.64 +/- 149.84
Eval num_timesteps=14000, episode_reward=-364.75 +/- 164.29
Episode length: 365.16 +/- 163.80
Eval num_timesteps=14500, episode_reward=-128.76 +/- 60.84
Episode length: 129.74 +/- 60.72
New best mean reward!
Eval num_timesteps=15000, episode_reward=-132.98 +/- 58.83
Episode length: 133.96 +/- 58.71
Eval num_timesteps=15500, episode_reward=-99.82 +/- 22.40
Episode length: 100.82 +/- 22.40
New best mean reward!
FINISHED IN 321.918246590998 s


starting seed  10420 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-103.85 +/- 28.02
Episode length: 104.85 +/- 28.02
New best mean reward!
Eval num_timesteps=10000, episode_reward=-109.89 +/- 43.43
Episode length: 110.89 +/- 43.43
Eval num_timesteps=10500, episode_reward=-115.91 +/- 23.90
Episode length: 116.91 +/- 23.90
Eval num_timesteps=11000, episode_reward=-115.89 +/- 50.36
Episode length: 116.88 +/- 50.28
Eval num_timesteps=11500, episode_reward=-117.45 +/- 24.13
Episode length: 118.45 +/- 24.13
Eval num_timesteps=12000, episode_reward=-156.97 +/- 112.28
Episode length: 157.88 +/- 112.00
Eval num_timesteps=12500, episode_reward=-210.57 +/- 124.43
Episode length: 211.42 +/- 124.08
Eval num_timesteps=13000, episode_reward=-121.01 +/- 50.61
Episode length: 122.00 +/- 50.54
Eval num_timesteps=13500, episode_reward=-120.57 +/- 67.50
Episode length: 121.55 +/- 67.39
Eval num_timesteps=14000, episode_reward=-120.46 +/- 49.11
Episode length: 121.45 +/- 49.03
Eval num_timesteps=14500, episode_reward=-158.40 +/- 40.64
Episode length: 159.40 +/- 40.64
Eval num_timesteps=15000, episode_reward=-128.49 +/- 46.67
Episode length: 129.48 +/- 46.59
Eval num_timesteps=15500, episode_reward=-111.39 +/- 48.50
Episode length: 112.38 +/- 48.42
Eval num_timesteps=16000, episode_reward=-109.62 +/- 36.61
Episode length: 110.62 +/- 36.61
Eval num_timesteps=16500, episode_reward=-127.23 +/- 23.91
Episode length: 128.23 +/- 23.91
Eval num_timesteps=17000, episode_reward=-112.57 +/- 67.43
Episode length: 113.55 +/- 67.31
Eval num_timesteps=17500, episode_reward=-92.99 +/- 22.11
Episode length: 93.99 +/- 22.11
New best mean reward!
FINISHED IN 310.53518065501703 s


starting seed  10421 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-496.55 +/- 34.33
Episode length: 496.56 +/- 34.23
New best mean reward!
Eval num_timesteps=7500, episode_reward=-460.78 +/- 106.37
Episode length: 460.90 +/- 106.04
New best mean reward!
Eval num_timesteps=8000, episode_reward=-470.73 +/- 93.21
Episode length: 470.82 +/- 92.93
Eval num_timesteps=8500, episode_reward=-412.09 +/- 145.50
Episode length: 412.36 +/- 145.06
New best mean reward!
Eval num_timesteps=9000, episode_reward=-423.58 +/- 140.91
Episode length: 423.81 +/- 140.49
Eval num_timesteps=9500, episode_reward=-200.62 +/- 118.54
Episode length: 201.50 +/- 118.24
New best mean reward!
Eval num_timesteps=10000, episode_reward=-164.00 +/- 60.08
Episode length: 164.98 +/- 59.96
New best mean reward!
Eval num_timesteps=10500, episode_reward=-158.76 +/- 38.29
Episode length: 159.76 +/- 38.29
New best mean reward!
Eval num_timesteps=11000, episode_reward=-165.49 +/- 55.18
Episode length: 166.48 +/- 55.12
Eval num_timesteps=11500, episode_reward=-175.56 +/- 81.78
Episode length: 176.51 +/- 81.58
Eval num_timesteps=12000, episode_reward=-171.29 +/- 71.01
Episode length: 172.26 +/- 70.88
Eval num_timesteps=12500, episode_reward=-160.92 +/- 55.29
Episode length: 161.90 +/- 55.17
Eval num_timesteps=13000, episode_reward=-158.33 +/- 38.56
Episode length: 159.33 +/- 38.56
New best mean reward!
Eval num_timesteps=13500, episode_reward=-174.78 +/- 82.04
Episode length: 175.73 +/- 81.85
Eval num_timesteps=14000, episode_reward=-199.50 +/- 115.07
Episode length: 200.38 +/- 114.76
Eval num_timesteps=14500, episode_reward=-188.71 +/- 106.29
Episode length: 189.62 +/- 106.03
Eval num_timesteps=15000, episode_reward=-242.85 +/- 143.39
Episode length: 243.62 +/- 142.98
Eval num_timesteps=15500, episode_reward=-240.84 +/- 140.92
Episode length: 241.63 +/- 140.53
Eval num_timesteps=16000, episode_reward=-225.53 +/- 142.48
Episode length: 226.33 +/- 142.09
Eval num_timesteps=16500, episode_reward=-217.07 +/- 128.86
Episode length: 217.91 +/- 128.50
Eval num_timesteps=17000, episode_reward=-215.02 +/- 127.41
Episode length: 215.87 +/- 127.08
Eval num_timesteps=17500, episode_reward=-250.31 +/- 146.48
Episode length: 251.07 +/- 146.07
Eval num_timesteps=18000, episode_reward=-241.01 +/- 145.47
Episode length: 241.78 +/- 145.06
Eval num_timesteps=18500, episode_reward=-236.85 +/- 145.27
Episode length: 237.63 +/- 144.87
Eval num_timesteps=19000, episode_reward=-204.73 +/- 120.35
Episode length: 205.61 +/- 120.06
Eval num_timesteps=19500, episode_reward=-188.61 +/- 102.15
Episode length: 189.53 +/- 101.90
Eval num_timesteps=20000, episode_reward=-219.45 +/- 135.00
Episode length: 220.27 +/- 134.63
Eval num_timesteps=20500, episode_reward=-227.17 +/- 146.43
Episode length: 227.95 +/- 146.02
Eval num_timesteps=21000, episode_reward=-230.71 +/- 141.97
Episode length: 231.50 +/- 141.58
Eval num_timesteps=21500, episode_reward=-236.48 +/- 150.31
Episode length: 237.24 +/- 149.88
Eval num_timesteps=22000, episode_reward=-180.85 +/- 87.24
Episode length: 181.79 +/- 87.02
Eval num_timesteps=22500, episode_reward=-173.80 +/- 82.09
Episode length: 174.76 +/- 81.93
Eval num_timesteps=23000, episode_reward=-183.13 +/- 94.49
Episode length: 184.06 +/- 94.26
Eval num_timesteps=23500, episode_reward=-189.40 +/- 105.99
Episode length: 190.30 +/- 105.70
Eval num_timesteps=24000, episode_reward=-197.86 +/- 115.09
Episode length: 198.74 +/- 114.78
Eval num_timesteps=24500, episode_reward=-192.85 +/- 105.96
Episode length: 193.75 +/- 105.67
Eval num_timesteps=25000, episode_reward=-179.01 +/- 85.61
Episode length: 179.96 +/- 85.42
Eval num_timesteps=25500, episode_reward=-166.77 +/- 64.18
Episode length: 167.75 +/- 64.07
Eval num_timesteps=26000, episode_reward=-185.28 +/- 98.45
Episode length: 186.21 +/- 98.22
Eval num_timesteps=26500, episode_reward=-160.13 +/- 55.46
Episode length: 161.11 +/- 55.34
Eval num_timesteps=27000, episode_reward=-142.21 +/- 84.87
Episode length: 143.16 +/- 84.66
New best mean reward!
Eval num_timesteps=27500, episode_reward=-131.26 +/- 80.99
Episode length: 132.22 +/- 80.81
New best mean reward!
Eval num_timesteps=28000, episode_reward=-124.42 +/- 87.67
Episode length: 125.38 +/- 87.50
New best mean reward!
Eval num_timesteps=28500, episode_reward=-117.25 +/- 59.70
Episode length: 118.23 +/- 59.57
New best mean reward!
Eval num_timesteps=29000, episode_reward=-128.48 +/- 51.26
Episode length: 129.47 +/- 51.19
Eval num_timesteps=29500, episode_reward=-125.50 +/- 21.67
Episode length: 126.50 +/- 21.67
Eval num_timesteps=30000, episode_reward=-108.94 +/- 48.01
Episode length: 109.93 +/- 47.93
New best mean reward!
Eval num_timesteps=30500, episode_reward=-101.48 +/- 29.54
Episode length: 102.48 +/- 29.54
New best mean reward!
Eval num_timesteps=31000, episode_reward=-85.93 +/- 22.22
Episode length: 86.93 +/- 22.22
New best mean reward!
FINISHED IN 514.6853711080039 s


starting seed  10422 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-370.24 +/- 125.33
Episode length: 370.78 +/- 124.86
New best mean reward!
Eval num_timesteps=1500, episode_reward=-133.60 +/- 80.69
Episode length: 134.56 +/- 80.50
New best mean reward!
Eval num_timesteps=2000, episode_reward=-126.55 +/- 26.50
Episode length: 127.55 +/- 26.50
New best mean reward!
Eval num_timesteps=2500, episode_reward=-428.97 +/- 92.10
Episode length: 429.37 +/- 91.63
Eval num_timesteps=3000, episode_reward=-451.22 +/- 47.09
Episode length: 451.91 +/- 46.77
Eval num_timesteps=3500, episode_reward=-121.77 +/- 20.37
Episode length: 122.77 +/- 20.37
New best mean reward!
Eval num_timesteps=4000, episode_reward=-121.44 +/- 31.54
Episode length: 122.44 +/- 31.54
New best mean reward!
Eval num_timesteps=4500, episode_reward=-117.33 +/- 26.44
Episode length: 118.33 +/- 26.44
New best mean reward!
Eval num_timesteps=5000, episode_reward=-148.55 +/- 30.90
Episode length: 149.55 +/- 30.90
Eval num_timesteps=5500, episode_reward=-339.57 +/- 55.68
Episode length: 340.55 +/- 55.62
Eval num_timesteps=6000, episode_reward=-136.01 +/- 29.21
Episode length: 137.01 +/- 29.21
Eval num_timesteps=6500, episode_reward=-262.81 +/- 48.75
Episode length: 263.80 +/- 48.70
Eval num_timesteps=7000, episode_reward=-113.49 +/- 19.08
Episode length: 114.49 +/- 19.08
New best mean reward!
Eval num_timesteps=7500, episode_reward=-121.56 +/- 43.75
Episode length: 122.56 +/- 43.75
Eval num_timesteps=8000, episode_reward=-118.87 +/- 37.73
Episode length: 119.87 +/- 37.73
Eval num_timesteps=8500, episode_reward=-435.71 +/- 139.09
Episode length: 435.89 +/- 138.71
Eval num_timesteps=9000, episode_reward=-434.94 +/- 136.87
Episode length: 435.13 +/- 136.49
Eval num_timesteps=9500, episode_reward=-380.77 +/- 173.96
Episode length: 381.09 +/- 173.50
Eval num_timesteps=10000, episode_reward=-234.08 +/- 163.02
Episode length: 234.81 +/- 162.58
Eval num_timesteps=10500, episode_reward=-462.02 +/- 108.15
Episode length: 462.13 +/- 107.84
Eval num_timesteps=11000, episode_reward=-455.02 +/- 116.79
Episode length: 455.15 +/- 116.45
Eval num_timesteps=11500, episode_reward=-166.80 +/- 90.21
Episode length: 167.74 +/- 89.98
Eval num_timesteps=12000, episode_reward=-164.40 +/- 84.44
Episode length: 165.35 +/- 84.24
Eval num_timesteps=12500, episode_reward=-148.92 +/- 61.64
Episode length: 149.90 +/- 61.53
Eval num_timesteps=13000, episode_reward=-155.90 +/- 48.53
Episode length: 156.89 +/- 48.46
Eval num_timesteps=13500, episode_reward=-227.45 +/- 142.93
Episode length: 228.24 +/- 142.53
Eval num_timesteps=14000, episode_reward=-377.83 +/- 163.82
Episode length: 378.19 +/- 163.34
Eval num_timesteps=14500, episode_reward=-401.48 +/- 155.08
Episode length: 401.77 +/- 154.63
Eval num_timesteps=15000, episode_reward=-452.79 +/- 122.19
Episode length: 452.92 +/- 121.85
Eval num_timesteps=15500, episode_reward=-135.80 +/- 26.77
Episode length: 136.80 +/- 26.77
Eval num_timesteps=16000, episode_reward=-132.71 +/- 45.35
Episode length: 133.70 +/- 45.27
Eval num_timesteps=16500, episode_reward=-142.97 +/- 57.28
Episode length: 143.95 +/- 57.15
Eval num_timesteps=17000, episode_reward=-160.80 +/- 82.80
Episode length: 161.76 +/- 82.63
Eval num_timesteps=17500, episode_reward=-124.78 +/- 31.88
Episode length: 125.78 +/- 31.88
Eval num_timesteps=18000, episode_reward=-111.76 +/- 16.08
Episode length: 112.76 +/- 16.08
New best mean reward!
Eval num_timesteps=18500, episode_reward=-127.31 +/- 32.95
Episode length: 128.31 +/- 32.95
Eval num_timesteps=19000, episode_reward=-107.66 +/- 32.20
Episode length: 108.66 +/- 32.20
New best mean reward!
Eval num_timesteps=19500, episode_reward=-102.52 +/- 26.18
Episode length: 103.52 +/- 26.18
New best mean reward!
Eval num_timesteps=20000, episode_reward=-96.53 +/- 43.20
Episode length: 97.52 +/- 43.10
New best mean reward!
FINISHED IN 303.5041246350156 s


starting seed  10423 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-116.00 +/- 58.91
Episode length: 116.99 +/- 58.85
New best mean reward!
Eval num_timesteps=1000, episode_reward=-381.32 +/- 137.94
Episode length: 381.82 +/- 137.51
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-492.27 +/- 54.25
Episode length: 492.29 +/- 54.11
Eval num_timesteps=12000, episode_reward=-465.82 +/- 107.66
Episode length: 465.92 +/- 107.37
Eval num_timesteps=12500, episode_reward=-454.37 +/- 123.79
Episode length: 454.49 +/- 123.47
Eval num_timesteps=13000, episode_reward=-435.56 +/- 144.85
Episode length: 435.73 +/- 144.48
Eval num_timesteps=13500, episode_reward=-270.30 +/- 201.74
Episode length: 270.87 +/- 201.25
Eval num_timesteps=14000, episode_reward=-174.74 +/- 164.47
Episode length: 175.54 +/- 164.08
Eval num_timesteps=14500, episode_reward=-161.21 +/- 154.55
Episode length: 162.05 +/- 154.20
Eval num_timesteps=15000, episode_reward=-223.66 +/- 189.51
Episode length: 224.35 +/- 189.06
Eval num_timesteps=15500, episode_reward=-214.53 +/- 186.61
Episode length: 215.24 +/- 186.17
Eval num_timesteps=16000, episode_reward=-251.09 +/- 199.77
Episode length: 251.72 +/- 199.31
Eval num_timesteps=16500, episode_reward=-275.83 +/- 201.16
Episode length: 276.41 +/- 200.69
Eval num_timesteps=17000, episode_reward=-264.85 +/- 197.56
Episode length: 265.45 +/- 197.08
Eval num_timesteps=17500, episode_reward=-184.67 +/- 174.96
Episode length: 185.44 +/- 174.55
Eval num_timesteps=18000, episode_reward=-207.37 +/- 181.52
Episode length: 208.12 +/- 181.12
Eval num_timesteps=18500, episode_reward=-244.74 +/- 197.78
Episode length: 245.38 +/- 197.32
Eval num_timesteps=19000, episode_reward=-209.84 +/- 184.17
Episode length: 210.56 +/- 183.73
Eval num_timesteps=19500, episode_reward=-168.39 +/- 159.75
Episode length: 169.21 +/- 159.38
Eval num_timesteps=20000, episode_reward=-201.34 +/- 181.46
Episode length: 202.08 +/- 181.03
Eval num_timesteps=20500, episode_reward=-180.20 +/- 170.50
Episode length: 180.99 +/- 170.10
Eval num_timesteps=21000, episode_reward=-184.24 +/- 167.44
Episode length: 185.04 +/- 167.07
Eval num_timesteps=21500, episode_reward=-172.39 +/- 166.69
Episode length: 173.20 +/- 166.32
Eval num_timesteps=22000, episode_reward=-165.23 +/- 160.11
Episode length: 166.05 +/- 159.74
Eval num_timesteps=22500, episode_reward=-129.99 +/- 120.82
Episode length: 130.90 +/- 120.54
Eval num_timesteps=23000, episode_reward=-94.73 +/- 47.31
Episode length: 95.72 +/- 47.23
New best mean reward!
FINISHED IN 551.8108551130281 s


starting seed  10424 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-404.60 +/- 146.27
Episode length: 404.90 +/- 145.82
New best mean reward!
Eval num_timesteps=1000, episode_reward=-396.61 +/- 136.85
Episode length: 396.98 +/- 136.38
New best mean reward!
Eval num_timesteps=1500, episode_reward=-304.53 +/- 143.79
Episode length: 305.21 +/- 143.35
New best mean reward!
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-111.71 +/- 36.29
Episode length: 112.71 +/- 36.29
New best mean reward!
Eval num_timesteps=9000, episode_reward=-103.54 +/- 27.18
Episode length: 104.54 +/- 27.18
New best mean reward!
Eval num_timesteps=9500, episode_reward=-258.55 +/- 119.62
Episode length: 259.38 +/- 119.27
Eval num_timesteps=10000, episode_reward=-124.49 +/- 20.65
Episode length: 125.49 +/- 20.65
Eval num_timesteps=10500, episode_reward=-106.93 +/- 19.04
Episode length: 107.93 +/- 19.04
Eval num_timesteps=11000, episode_reward=-101.61 +/- 24.27
Episode length: 102.61 +/- 24.27
New best mean reward!
Eval num_timesteps=11500, episode_reward=-101.61 +/- 33.56
Episode length: 102.61 +/- 33.56
Eval num_timesteps=12000, episode_reward=-99.55 +/- 30.82
Episode length: 100.55 +/- 30.82
New best mean reward!
FINISHED IN 222.04350938997231 s


starting seed  10425 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-165.84 +/- 34.60
Episode length: 166.84 +/- 34.60
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-370.24 +/- 93.84
Episode length: 371.09 +/- 93.63
Eval num_timesteps=5500, episode_reward=-489.71 +/- 39.58
Episode length: 489.81 +/- 39.35
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-337.07 +/- 113.25
Episode length: 337.84 +/- 112.92
Eval num_timesteps=7500, episode_reward=-169.04 +/- 38.40
Episode length: 170.04 +/- 38.40
Eval num_timesteps=8000, episode_reward=-179.24 +/- 38.90
Episode length: 180.24 +/- 38.90
Eval num_timesteps=8500, episode_reward=-182.13 +/- 44.40
Episode length: 183.13 +/- 44.40
Eval num_timesteps=9000, episode_reward=-207.12 +/- 55.87
Episode length: 208.10 +/- 55.77
Eval num_timesteps=9500, episode_reward=-194.25 +/- 34.01
Episode length: 195.25 +/- 34.01
Eval num_timesteps=10000, episode_reward=-210.14 +/- 66.29
Episode length: 211.11 +/- 66.16
Eval num_timesteps=10500, episode_reward=-369.89 +/- 154.69
Episode length: 370.32 +/- 154.21
Eval num_timesteps=11000, episode_reward=-290.79 +/- 183.23
Episode length: 291.38 +/- 182.76
Eval num_timesteps=11500, episode_reward=-445.64 +/- 135.23
Episode length: 445.78 +/- 134.88
Eval num_timesteps=12000, episode_reward=-90.33 +/- 33.44
Episode length: 91.33 +/- 33.44
New best mean reward!
FINISHED IN 267.3386159189977 s


starting seed  10426 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-171.21 +/- 38.28
Episode length: 172.21 +/- 38.28
New best mean reward!
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-94.45 +/- 47.67
Episode length: 95.44 +/- 47.59
New best mean reward!
FINISHED IN 176.69304972997634 s


starting seed  10427 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-448.94 +/- 72.72
Episode length: 449.34 +/- 72.30
New best mean reward!
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-495.95 +/- 40.30
Episode length: 495.96 +/- 40.20
Eval num_timesteps=14500, episode_reward=-426.70 +/- 156.60
Episode length: 426.88 +/- 156.22
New best mean reward!
Eval num_timesteps=15000, episode_reward=-401.31 +/- 176.06
Episode length: 401.55 +/- 175.64
New best mean reward!
Eval num_timesteps=15500, episode_reward=-382.81 +/- 182.92
Episode length: 383.11 +/- 182.47
New best mean reward!
Eval num_timesteps=16000, episode_reward=-406.62 +/- 171.39
Episode length: 406.85 +/- 170.97
Eval num_timesteps=16500, episode_reward=-369.27 +/- 190.82
Episode length: 369.59 +/- 190.35
New best mean reward!
Eval num_timesteps=17000, episode_reward=-318.76 +/- 204.69
Episode length: 319.20 +/- 204.20
New best mean reward!
Eval num_timesteps=17500, episode_reward=-240.12 +/- 195.73
Episode length: 240.77 +/- 195.26
New best mean reward!
Eval num_timesteps=18000, episode_reward=-222.88 +/- 195.30
Episode length: 223.55 +/- 194.84
New best mean reward!
Eval num_timesteps=18500, episode_reward=-210.60 +/- 187.31
Episode length: 211.32 +/- 186.88
New best mean reward!
Eval num_timesteps=19000, episode_reward=-267.71 +/- 202.80
Episode length: 268.28 +/- 202.31
Eval num_timesteps=19500, episode_reward=-249.48 +/- 201.39
Episode length: 250.09 +/- 200.90
Eval num_timesteps=20000, episode_reward=-198.99 +/- 180.94
Episode length: 199.73 +/- 180.50
New best mean reward!
Eval num_timesteps=20500, episode_reward=-196.96 +/- 178.56
Episode length: 197.71 +/- 178.14
New best mean reward!
Eval num_timesteps=21000, episode_reward=-232.04 +/- 194.33
Episode length: 232.71 +/- 193.87
Eval num_timesteps=21500, episode_reward=-202.46 +/- 183.73
Episode length: 203.19 +/- 183.29
Eval num_timesteps=22000, episode_reward=-222.78 +/- 191.21
Episode length: 223.46 +/- 190.75
Eval num_timesteps=22500, episode_reward=-204.42 +/- 185.16
Episode length: 205.14 +/- 184.71
Eval num_timesteps=23000, episode_reward=-174.51 +/- 165.29
Episode length: 175.31 +/- 164.89
New best mean reward!
Eval num_timesteps=23500, episode_reward=-185.35 +/- 175.89
Episode length: 186.12 +/- 175.48
Eval num_timesteps=24000, episode_reward=-177.90 +/- 167.88
Episode length: 178.69 +/- 167.48
Eval num_timesteps=24500, episode_reward=-155.33 +/- 151.93
Episode length: 156.18 +/- 151.59
New best mean reward!
Eval num_timesteps=25000, episode_reward=-159.54 +/- 150.37
Episode length: 160.39 +/- 150.03
Eval num_timesteps=25500, episode_reward=-173.96 +/- 169.05
Episode length: 174.75 +/- 168.65
Eval num_timesteps=26000, episode_reward=-177.80 +/- 164.66
Episode length: 178.61 +/- 164.29
Eval num_timesteps=26500, episode_reward=-169.21 +/- 162.42
Episode length: 170.02 +/- 162.03
Eval num_timesteps=27000, episode_reward=-181.48 +/- 173.42
Episode length: 182.26 +/- 173.02
Eval num_timesteps=27500, episode_reward=-167.77 +/- 162.72
Episode length: 168.58 +/- 162.34
Eval num_timesteps=28000, episode_reward=-173.15 +/- 154.50
Episode length: 173.99 +/- 154.16
Eval num_timesteps=28500, episode_reward=-135.58 +/- 136.26
Episode length: 136.46 +/- 135.94
New best mean reward!
Eval num_timesteps=29000, episode_reward=-116.40 +/- 108.35
Episode length: 117.33 +/- 108.10
New best mean reward!
Eval num_timesteps=29500, episode_reward=-149.98 +/- 140.31
Episode length: 150.86 +/- 140.01
Eval num_timesteps=30000, episode_reward=-128.80 +/- 118.78
Episode length: 129.72 +/- 118.53
Eval num_timesteps=30500, episode_reward=-127.46 +/- 119.62
Episode length: 128.37 +/- 119.34
Eval num_timesteps=31000, episode_reward=-126.72 +/- 120.30
Episode length: 127.63 +/- 120.02
Eval num_timesteps=31500, episode_reward=-107.06 +/- 87.50
Episode length: 108.02 +/- 87.32
New best mean reward!
Eval num_timesteps=32000, episode_reward=-130.15 +/- 118.19
Episode length: 131.07 +/- 117.94
Eval num_timesteps=32500, episode_reward=-164.56 +/- 156.84
Episode length: 165.40 +/- 156.50
Eval num_timesteps=33000, episode_reward=-131.76 +/- 125.95
Episode length: 132.66 +/- 125.66
Eval num_timesteps=33500, episode_reward=-131.02 +/- 128.88
Episode length: 131.92 +/- 128.59
Eval num_timesteps=34000, episode_reward=-130.13 +/- 122.24
Episode length: 131.05 +/- 122.00
Eval num_timesteps=34500, episode_reward=-135.59 +/- 126.78
Episode length: 136.49 +/- 126.50
Eval num_timesteps=35000, episode_reward=-111.67 +/- 94.58
Episode length: 112.62 +/- 94.38
Eval num_timesteps=35500, episode_reward=-111.77 +/- 94.29
Episode length: 112.72 +/- 94.08
Eval num_timesteps=36000, episode_reward=-121.78 +/- 111.29
Episode length: 122.73 +/- 111.12
Eval num_timesteps=36500, episode_reward=-123.70 +/- 110.47
Episode length: 124.63 +/- 110.24
Eval num_timesteps=37000, episode_reward=-116.84 +/- 109.26
Episode length: 117.77 +/- 109.02
Eval num_timesteps=37500, episode_reward=-117.59 +/- 101.50
Episode length: 118.54 +/- 101.31
Eval num_timesteps=38000, episode_reward=-103.51 +/- 71.79
Episode length: 104.50 +/- 71.73
New best mean reward!
Eval num_timesteps=38500, episode_reward=-94.57 +/- 51.72
Episode length: 95.56 +/- 51.64
New best mean reward!
FINISHED IN 674.363186114002 s


starting seed  10428 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15500, episode_reward=-390.80 +/- 177.26
Episode length: 391.08 +/- 176.82
New best mean reward!
Eval num_timesteps=16000, episode_reward=-151.65 +/- 136.34
Episode length: 152.54 +/- 136.06
New best mean reward!
Eval num_timesteps=16500, episode_reward=-166.22 +/- 149.40
Episode length: 167.06 +/- 149.04
Eval num_timesteps=17000, episode_reward=-139.59 +/- 124.36
Episode length: 140.49 +/- 124.07
New best mean reward!
Eval num_timesteps=17500, episode_reward=-199.03 +/- 180.36
Episode length: 199.77 +/- 179.93
Eval num_timesteps=18000, episode_reward=-223.24 +/- 191.27
Episode length: 223.92 +/- 190.81
Eval num_timesteps=18500, episode_reward=-212.01 +/- 184.57
Episode length: 212.73 +/- 184.13
Eval num_timesteps=19000, episode_reward=-143.85 +/- 135.53
Episode length: 144.73 +/- 135.22
Eval num_timesteps=19500, episode_reward=-167.08 +/- 151.46
Episode length: 167.92 +/- 151.11
Eval num_timesteps=20000, episode_reward=-118.83 +/- 91.70
Episode length: 119.78 +/- 91.49
New best mean reward!
Eval num_timesteps=20500, episode_reward=-127.84 +/- 112.56
Episode length: 128.77 +/- 112.33
Eval num_timesteps=21000, episode_reward=-167.77 +/- 149.13
Episode length: 168.61 +/- 148.78
Eval num_timesteps=21500, episode_reward=-101.97 +/- 61.62
Episode length: 102.95 +/- 61.49
New best mean reward!
Eval num_timesteps=22000, episode_reward=-107.78 +/- 63.10
Episode length: 108.77 +/- 63.04
Eval num_timesteps=22500, episode_reward=-111.23 +/- 71.48
Episode length: 112.21 +/- 71.37
Eval num_timesteps=23000, episode_reward=-95.89 +/- 25.25
Episode length: 96.89 +/- 25.25
New best mean reward!
FINISHED IN 519.1596348029561 s


starting seed  10429 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-412.87 +/- 128.83
Episode length: 413.20 +/- 128.38
New best mean reward!
Eval num_timesteps=5500, episode_reward=-441.89 +/- 104.66
Episode length: 442.15 +/- 104.25
Eval num_timesteps=6000, episode_reward=-156.45 +/- 63.48
Episode length: 157.42 +/- 63.32
New best mean reward!
Eval num_timesteps=6500, episode_reward=-368.68 +/- 112.66
Episode length: 369.31 +/- 112.23
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-477.38 +/- 73.65
Episode length: 477.48 +/- 73.38
Eval num_timesteps=10000, episode_reward=-170.06 +/- 65.55
Episode length: 171.03 +/- 65.39
Eval num_timesteps=10500, episode_reward=-179.30 +/- 59.21
Episode length: 180.28 +/- 59.10
Eval num_timesteps=11000, episode_reward=-109.19 +/- 25.75
Episode length: 110.19 +/- 25.75
New best mean reward!
Eval num_timesteps=11500, episode_reward=-102.92 +/- 24.48
Episode length: 103.92 +/- 24.48
New best mean reward!
Eval num_timesteps=12000, episode_reward=-100.05 +/- 32.22
Episode length: 101.05 +/- 32.22
New best mean reward!
Eval num_timesteps=12500, episode_reward=-99.66 +/- 47.66
Episode length: 100.65 +/- 47.57
New best mean reward!
FINISHED IN 261.476128890994 s


starting seed  10430 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-84.44 +/- 11.42
Episode length: 85.44 +/- 11.42
New best mean reward!
FINISHED IN 234.0337721729884 s


starting seed  10431 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-388.21 +/- 179.99
Episode length: 388.49 +/- 179.54
New best mean reward!
Eval num_timesteps=11000, episode_reward=-445.84 +/- 140.14
Episode length: 445.97 +/- 139.81
Eval num_timesteps=11500, episode_reward=-346.79 +/- 200.34
Episode length: 347.16 +/- 199.86
New best mean reward!
Eval num_timesteps=12000, episode_reward=-194.67 +/- 177.87
Episode length: 195.42 +/- 177.44
New best mean reward!
Eval num_timesteps=12500, episode_reward=-163.62 +/- 156.86
Episode length: 164.45 +/- 156.49
New best mean reward!
Eval num_timesteps=13000, episode_reward=-141.43 +/- 136.34
Episode length: 142.31 +/- 136.02
New best mean reward!
Eval num_timesteps=13500, episode_reward=-142.96 +/- 145.02
Episode length: 143.82 +/- 144.68
Eval num_timesteps=14000, episode_reward=-133.05 +/- 124.95
Episode length: 133.95 +/- 124.65
New best mean reward!
Eval num_timesteps=14500, episode_reward=-126.33 +/- 122.38
Episode length: 127.24 +/- 122.11
New best mean reward!
Eval num_timesteps=15000, episode_reward=-101.52 +/- 73.60
Episode length: 102.49 +/- 73.44
New best mean reward!
Eval num_timesteps=15500, episode_reward=-102.23 +/- 83.85
Episode length: 103.19 +/- 83.66
Eval num_timesteps=16000, episode_reward=-84.19 +/- 14.61
Episode length: 85.19 +/- 14.61
New best mean reward!
FINISHED IN 315.2125068609603 s


starting seed  10432 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-165.69 +/- 31.66
Episode length: 166.69 +/- 31.66
New best mean reward!
Eval num_timesteps=11000, episode_reward=-172.37 +/- 44.22
Episode length: 173.37 +/- 44.22
Eval num_timesteps=11500, episode_reward=-168.66 +/- 31.96
Episode length: 169.66 +/- 31.96
Eval num_timesteps=12000, episode_reward=-174.25 +/- 43.50
Episode length: 175.25 +/- 43.50
Eval num_timesteps=12500, episode_reward=-168.82 +/- 33.63
Episode length: 169.82 +/- 33.63
Eval num_timesteps=13000, episode_reward=-162.84 +/- 49.21
Episode length: 163.83 +/- 49.14
New best mean reward!
Eval num_timesteps=13500, episode_reward=-148.95 +/- 27.09
Episode length: 149.95 +/- 27.09
New best mean reward!
Eval num_timesteps=14000, episode_reward=-113.86 +/- 27.60
Episode length: 114.86 +/- 27.60
New best mean reward!
Eval num_timesteps=14500, episode_reward=-126.21 +/- 65.22
Episode length: 127.19 +/- 65.11
Eval num_timesteps=15000, episode_reward=-127.14 +/- 97.44
Episode length: 128.09 +/- 97.25
Eval num_timesteps=15500, episode_reward=-103.60 +/- 49.59
Episode length: 104.59 +/- 49.51
New best mean reward!
Eval num_timesteps=16000, episode_reward=-119.24 +/- 79.29
Episode length: 120.21 +/- 79.15
Eval num_timesteps=16500, episode_reward=-104.93 +/- 74.05
Episode length: 105.90 +/- 73.89
Eval num_timesteps=17000, episode_reward=-136.87 +/- 118.58
Episode length: 137.78 +/- 118.31
Eval num_timesteps=17500, episode_reward=-102.57 +/- 61.50
Episode length: 103.55 +/- 61.37
New best mean reward!
Eval num_timesteps=18000, episode_reward=-174.73 +/- 154.35
Episode length: 175.55 +/- 153.97
Eval num_timesteps=18500, episode_reward=-100.66 +/- 57.54
Episode length: 101.65 +/- 57.47
New best mean reward!
Eval num_timesteps=19000, episode_reward=-86.84 +/- 20.36
Episode length: 87.84 +/- 20.36
New best mean reward!
FINISHED IN 325.98603645002004 s


starting seed  10433 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-358.71 +/- 193.54
Episode length: 359.06 +/- 193.06
New best mean reward!
Eval num_timesteps=8500, episode_reward=-487.98 +/- 68.48
Episode length: 488.01 +/- 68.31
Eval num_timesteps=9000, episode_reward=-291.93 +/- 205.45
Episode length: 292.44 +/- 204.96
New best mean reward!
Eval num_timesteps=9500, episode_reward=-353.53 +/- 199.71
Episode length: 353.88 +/- 199.24
Eval num_timesteps=10000, episode_reward=-293.31 +/- 204.97
Episode length: 293.83 +/- 204.49
Eval num_timesteps=10500, episode_reward=-85.03 +/- 21.72
Episode length: 86.03 +/- 21.72
New best mean reward!
FINISHED IN 291.7030078499811 s


starting seed  10434 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-499.25 +/- 7.46
Episode length: 499.26 +/- 7.36
New best mean reward!
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-346.83 +/- 158.89
Episode length: 347.34 +/- 158.42
New best mean reward!
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-497.12 +/- 28.66
Episode length: 497.13 +/- 28.56
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-453.93 +/- 108.79
Episode length: 454.09 +/- 108.44
Eval num_timesteps=8000, episode_reward=-202.73 +/- 90.77
Episode length: 203.66 +/- 90.55
New best mean reward!
Eval num_timesteps=8500, episode_reward=-176.90 +/- 61.12
Episode length: 177.88 +/- 61.01
New best mean reward!
Eval num_timesteps=9000, episode_reward=-189.43 +/- 73.76
Episode length: 190.39 +/- 73.59
Eval num_timesteps=9500, episode_reward=-282.72 +/- 143.10
Episode length: 283.44 +/- 142.68
Eval num_timesteps=10000, episode_reward=-206.85 +/- 98.88
Episode length: 207.76 +/- 98.62
Eval num_timesteps=10500, episode_reward=-313.61 +/- 162.21
Episode length: 314.20 +/- 161.74
Eval num_timesteps=11000, episode_reward=-289.79 +/- 150.09
Episode length: 290.47 +/- 149.64
Eval num_timesteps=11500, episode_reward=-290.34 +/- 156.59
Episode length: 290.99 +/- 156.12
Eval num_timesteps=12000, episode_reward=-493.95 +/- 42.72
Episode length: 493.97 +/- 42.58
Eval num_timesteps=12500, episode_reward=-264.27 +/- 148.93
Episode length: 264.99 +/- 148.49
Eval num_timesteps=13000, episode_reward=-348.70 +/- 164.79
Episode length: 349.17 +/- 164.30
Eval num_timesteps=13500, episode_reward=-133.31 +/- 24.38
Episode length: 134.31 +/- 24.38
New best mean reward!
Eval num_timesteps=14000, episode_reward=-92.98 +/- 30.53
Episode length: 93.98 +/- 30.53
New best mean reward!
FINISHED IN 369.35327664000215 s


starting seed  10435 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-306.58 +/- 175.93
Episode length: 307.13 +/- 175.44
New best mean reward!
Eval num_timesteps=3500, episode_reward=-222.25 +/- 53.82
Episode length: 223.25 +/- 53.82
New best mean reward!
Eval num_timesteps=4000, episode_reward=-164.63 +/- 46.43
Episode length: 165.63 +/- 46.43
New best mean reward!
Eval num_timesteps=4500, episode_reward=-149.56 +/- 41.83
Episode length: 150.55 +/- 41.75
New best mean reward!
Eval num_timesteps=5000, episode_reward=-202.23 +/- 103.62
Episode length: 203.14 +/- 103.36
Eval num_timesteps=5500, episode_reward=-306.06 +/- 165.87
Episode length: 306.64 +/- 165.38
Eval num_timesteps=6000, episode_reward=-396.30 +/- 165.26
Episode length: 396.59 +/- 164.81
Eval num_timesteps=6500, episode_reward=-208.43 +/- 151.29
Episode length: 209.22 +/- 150.88
Eval num_timesteps=7000, episode_reward=-118.37 +/- 15.52
Episode length: 119.37 +/- 15.52
New best mean reward!
Eval num_timesteps=7500, episode_reward=-169.34 +/- 66.08
Episode length: 170.31 +/- 65.93
Eval num_timesteps=8000, episode_reward=-146.62 +/- 65.79
Episode length: 147.59 +/- 65.63
Eval num_timesteps=8500, episode_reward=-168.50 +/- 83.64
Episode length: 169.45 +/- 83.44
Eval num_timesteps=9000, episode_reward=-164.09 +/- 46.23
Episode length: 165.08 +/- 46.16
Eval num_timesteps=9500, episode_reward=-118.83 +/- 22.85
Episode length: 119.83 +/- 22.85
Eval num_timesteps=10000, episode_reward=-123.14 +/- 30.19
Episode length: 124.14 +/- 30.19
Eval num_timesteps=10500, episode_reward=-115.63 +/- 31.87
Episode length: 116.63 +/- 31.87
New best mean reward!
Eval num_timesteps=11000, episode_reward=-100.55 +/- 26.77
Episode length: 101.55 +/- 26.77
New best mean reward!
Eval num_timesteps=11500, episode_reward=-116.07 +/- 64.08
Episode length: 117.05 +/- 63.96
Eval num_timesteps=12000, episode_reward=-106.52 +/- 25.27
Episode length: 107.52 +/- 25.27
Eval num_timesteps=12500, episode_reward=-100.61 +/- 45.26
Episode length: 101.61 +/- 45.26
Eval num_timesteps=13000, episode_reward=-90.12 +/- 20.08
Episode length: 91.12 +/- 20.08
New best mean reward!
FINISHED IN 192.7800373529899 s


starting seed  10436 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-116.54 +/- 33.40
Episode length: 117.54 +/- 33.40
New best mean reward!
Eval num_timesteps=1000, episode_reward=-470.42 +/- 52.13
Episode length: 470.75 +/- 51.75
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-475.83 +/- 88.37
Episode length: 475.90 +/- 88.12
Eval num_timesteps=8000, episode_reward=-261.33 +/- 170.02
Episode length: 262.01 +/- 169.57
Eval num_timesteps=8500, episode_reward=-134.42 +/- 119.20
Episode length: 135.33 +/- 118.92
Eval num_timesteps=9000, episode_reward=-157.21 +/- 142.29
Episode length: 158.07 +/- 141.96
Eval num_timesteps=9500, episode_reward=-123.03 +/- 100.12
Episode length: 123.98 +/- 99.93
Eval num_timesteps=10000, episode_reward=-126.02 +/- 116.37
Episode length: 126.94 +/- 116.12
Eval num_timesteps=10500, episode_reward=-119.59 +/- 102.32
Episode length: 120.53 +/- 102.09
Eval num_timesteps=11000, episode_reward=-173.77 +/- 157.98
Episode length: 174.59 +/- 157.61
Eval num_timesteps=11500, episode_reward=-114.30 +/- 67.82
Episode length: 115.28 +/- 67.70
New best mean reward!
Eval num_timesteps=12000, episode_reward=-86.09 +/- 23.52
Episode length: 87.09 +/- 23.52
New best mean reward!
FINISHED IN 227.34652447898407 s


starting seed  10437 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-151.42 +/- 31.48
Episode length: 152.42 +/- 31.48
New best mean reward!
Eval num_timesteps=4000, episode_reward=-214.50 +/- 133.52
Episode length: 215.33 +/- 133.16
Eval num_timesteps=4500, episode_reward=-193.36 +/- 116.26
Episode length: 194.24 +/- 115.95
Eval num_timesteps=5000, episode_reward=-317.97 +/- 173.67
Episode length: 318.50 +/- 173.18
Eval num_timesteps=5500, episode_reward=-223.44 +/- 134.89
Episode length: 224.26 +/- 134.52
Eval num_timesteps=6000, episode_reward=-264.78 +/- 164.45
Episode length: 265.46 +/- 163.99
Eval num_timesteps=6500, episode_reward=-288.50 +/- 170.92
Episode length: 289.11 +/- 170.44
Eval num_timesteps=7000, episode_reward=-156.01 +/- 67.87
Episode length: 156.98 +/- 67.72
Eval num_timesteps=7500, episode_reward=-217.76 +/- 135.20
Episode length: 218.59 +/- 134.85
Eval num_timesteps=8000, episode_reward=-205.78 +/- 128.79
Episode length: 206.63 +/- 128.44
Eval num_timesteps=8500, episode_reward=-217.85 +/- 142.49
Episode length: 218.65 +/- 142.09
Eval num_timesteps=9000, episode_reward=-178.10 +/- 103.30
Episode length: 179.01 +/- 103.02
Eval num_timesteps=9500, episode_reward=-192.51 +/- 113.67
Episode length: 193.40 +/- 113.37
Eval num_timesteps=10000, episode_reward=-200.76 +/- 118.70
Episode length: 201.63 +/- 118.38
Eval num_timesteps=10500, episode_reward=-206.71 +/- 122.44
Episode length: 207.57 +/- 122.10
Eval num_timesteps=11000, episode_reward=-236.59 +/- 154.01
Episode length: 237.34 +/- 153.58
Eval num_timesteps=11500, episode_reward=-170.91 +/- 123.77
Episode length: 171.79 +/- 123.46
Eval num_timesteps=12000, episode_reward=-159.67 +/- 121.63
Episode length: 160.56 +/- 121.32
Eval num_timesteps=12500, episode_reward=-138.39 +/- 78.43
Episode length: 139.35 +/- 78.24
New best mean reward!
Eval num_timesteps=13000, episode_reward=-139.09 +/- 59.75
Episode length: 140.07 +/- 59.63
Eval num_timesteps=13500, episode_reward=-128.93 +/- 42.84
Episode length: 129.92 +/- 42.75
New best mean reward!
Eval num_timesteps=14000, episode_reward=-135.23 +/- 32.76
Episode length: 136.23 +/- 32.76
Eval num_timesteps=14500, episode_reward=-120.80 +/- 24.94
Episode length: 121.80 +/- 24.94
New best mean reward!
Eval num_timesteps=15000, episode_reward=-124.32 +/- 21.13
Episode length: 125.32 +/- 21.13
Eval num_timesteps=15500, episode_reward=-124.48 +/- 23.64
Episode length: 125.48 +/- 23.64
Eval num_timesteps=16000, episode_reward=-108.54 +/- 18.34
Episode length: 109.54 +/- 18.34
New best mean reward!
Eval num_timesteps=16500, episode_reward=-112.97 +/- 25.29
Episode length: 113.97 +/- 25.29
Eval num_timesteps=17000, episode_reward=-105.67 +/- 21.84
Episode length: 106.67 +/- 21.84
New best mean reward!
Eval num_timesteps=17500, episode_reward=-114.46 +/- 47.29
Episode length: 115.45 +/- 47.21
Eval num_timesteps=18000, episode_reward=-109.43 +/- 59.11
Episode length: 110.41 +/- 58.98
Eval num_timesteps=18500, episode_reward=-89.81 +/- 25.87
Episode length: 90.81 +/- 25.87
New best mean reward!
FINISHED IN 244.92253548896406 s


starting seed  10438 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-364.57 +/- 189.13
Episode length: 364.91 +/- 188.66
New best mean reward!
Eval num_timesteps=8000, episode_reward=-94.52 +/- 49.63
Episode length: 95.52 +/- 49.63
New best mean reward!
FINISHED IN 185.03436439001234 s


starting seed  10439 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-447.11 +/- 133.34
Episode length: 447.25 +/- 133.00
New best mean reward!
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-492.22 +/- 54.51
Episode length: 492.24 +/- 54.37
Eval num_timesteps=3500, episode_reward=-94.79 +/- 32.45
Episode length: 95.79 +/- 32.45
New best mean reward!
FINISHED IN 79.92074108199449 s


starting seed  10440 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-277.08 +/- 158.10
Episode length: 277.76 +/- 157.65
New best mean reward!
Eval num_timesteps=1000, episode_reward=-349.11 +/- 159.19
Episode length: 349.59 +/- 158.69
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-487.85 +/- 59.78
Episode length: 487.89 +/- 59.59
Eval num_timesteps=3000, episode_reward=-485.16 +/- 59.36
Episode length: 485.22 +/- 59.13
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-338.26 +/- 163.19
Episode length: 338.76 +/- 162.69
Eval num_timesteps=4500, episode_reward=-190.54 +/- 70.24
Episode length: 191.50 +/- 70.06
New best mean reward!
Eval num_timesteps=5000, episode_reward=-240.55 +/- 141.79
Episode length: 241.33 +/- 141.39
Eval num_timesteps=5500, episode_reward=-162.33 +/- 33.53
Episode length: 163.33 +/- 33.53
New best mean reward!
Eval num_timesteps=6000, episode_reward=-205.22 +/- 105.97
Episode length: 206.12 +/- 105.69
Eval num_timesteps=6500, episode_reward=-278.21 +/- 151.02
Episode length: 278.90 +/- 150.56
Eval num_timesteps=7000, episode_reward=-164.02 +/- 42.03
Episode length: 165.01 +/- 41.95
Eval num_timesteps=7500, episode_reward=-238.42 +/- 138.48
Episode length: 239.21 +/- 138.08
Eval num_timesteps=8000, episode_reward=-208.68 +/- 120.80
Episode length: 209.54 +/- 120.47
Eval num_timesteps=8500, episode_reward=-164.84 +/- 43.80
Episode length: 165.84 +/- 43.80
Eval num_timesteps=9000, episode_reward=-164.55 +/- 40.31
Episode length: 165.55 +/- 40.31
Eval num_timesteps=9500, episode_reward=-154.09 +/- 21.35
Episode length: 155.09 +/- 21.35
New best mean reward!
Eval num_timesteps=10000, episode_reward=-162.62 +/- 41.59
Episode length: 163.62 +/- 41.59
Eval num_timesteps=10500, episode_reward=-165.27 +/- 50.11
Episode length: 166.26 +/- 50.04
Eval num_timesteps=11000, episode_reward=-164.60 +/- 35.68
Episode length: 165.60 +/- 35.68
Eval num_timesteps=11500, episode_reward=-164.31 +/- 55.90
Episode length: 165.29 +/- 55.78
Eval num_timesteps=12000, episode_reward=-160.49 +/- 38.18
Episode length: 161.49 +/- 38.18
Eval num_timesteps=12500, episode_reward=-163.26 +/- 36.15
Episode length: 164.26 +/- 36.15
Eval num_timesteps=13000, episode_reward=-200.42 +/- 109.52
Episode length: 201.31 +/- 109.22
Eval num_timesteps=13500, episode_reward=-202.90 +/- 108.85
Episode length: 203.79 +/- 108.55
Eval num_timesteps=14000, episode_reward=-270.92 +/- 156.17
Episode length: 271.61 +/- 155.72
Eval num_timesteps=14500, episode_reward=-293.31 +/- 166.66
Episode length: 293.92 +/- 166.18
Eval num_timesteps=15000, episode_reward=-233.55 +/- 139.61
Episode length: 234.34 +/- 139.21
Eval num_timesteps=15500, episode_reward=-206.83 +/- 114.14
Episode length: 207.71 +/- 113.83
Eval num_timesteps=16000, episode_reward=-203.64 +/- 114.13
Episode length: 204.52 +/- 113.82
Eval num_timesteps=16500, episode_reward=-193.79 +/- 104.59
Episode length: 194.69 +/- 104.30
Eval num_timesteps=17000, episode_reward=-164.47 +/- 48.19
Episode length: 165.46 +/- 48.12
Eval num_timesteps=17500, episode_reward=-165.92 +/- 49.33
Episode length: 166.91 +/- 49.26
Eval num_timesteps=18000, episode_reward=-155.86 +/- 24.68
Episode length: 156.86 +/- 24.68
Eval num_timesteps=18500, episode_reward=-169.54 +/- 59.81
Episode length: 170.52 +/- 59.70
Eval num_timesteps=19000, episode_reward=-164.87 +/- 39.07
Episode length: 165.87 +/- 39.07
Eval num_timesteps=19500, episode_reward=-164.93 +/- 40.75
Episode length: 165.93 +/- 40.75
Eval num_timesteps=20000, episode_reward=-159.25 +/- 29.76
Episode length: 160.25 +/- 29.76
Eval num_timesteps=20500, episode_reward=-162.26 +/- 34.75
Episode length: 163.26 +/- 34.75
Eval num_timesteps=21000, episode_reward=-155.06 +/- 26.70
Episode length: 156.06 +/- 26.70
Eval num_timesteps=21500, episode_reward=-164.20 +/- 37.89
Episode length: 165.20 +/- 37.89
Eval num_timesteps=22000, episode_reward=-160.28 +/- 36.25
Episode length: 161.28 +/- 36.25
Eval num_timesteps=22500, episode_reward=-162.54 +/- 34.90
Episode length: 163.54 +/- 34.90
Eval num_timesteps=23000, episode_reward=-167.38 +/- 43.90
Episode length: 168.38 +/- 43.90
Eval num_timesteps=23500, episode_reward=-164.28 +/- 56.40
Episode length: 165.26 +/- 56.28
Eval num_timesteps=24000, episode_reward=-164.34 +/- 40.42
Episode length: 165.34 +/- 40.42
Eval num_timesteps=24500, episode_reward=-158.00 +/- 35.04
Episode length: 159.00 +/- 35.04
Eval num_timesteps=25000, episode_reward=-157.18 +/- 36.16
Episode length: 158.18 +/- 36.16
Eval num_timesteps=25500, episode_reward=-164.93 +/- 49.51
Episode length: 165.92 +/- 49.45
Eval num_timesteps=26000, episode_reward=-158.82 +/- 30.83
Episode length: 159.82 +/- 30.83
Eval num_timesteps=26500, episode_reward=-163.61 +/- 46.88
Episode length: 164.61 +/- 46.88
Eval num_timesteps=27000, episode_reward=-156.71 +/- 30.76
Episode length: 157.71 +/- 30.76
Eval num_timesteps=27500, episode_reward=-163.08 +/- 36.10
Episode length: 164.08 +/- 36.10
Eval num_timesteps=28000, episode_reward=-168.17 +/- 50.86
Episode length: 169.16 +/- 50.80
Eval num_timesteps=28500, episode_reward=-174.77 +/- 82.91
Episode length: 175.72 +/- 82.71
Eval num_timesteps=29000, episode_reward=-198.77 +/- 110.89
Episode length: 199.66 +/- 110.59
Eval num_timesteps=29500, episode_reward=-196.68 +/- 106.87
Episode length: 197.59 +/- 106.62
Eval num_timesteps=30000, episode_reward=-176.04 +/- 69.60
Episode length: 177.01 +/- 69.46
Eval num_timesteps=30500, episode_reward=-193.65 +/- 97.66
Episode length: 194.58 +/- 97.44
Eval num_timesteps=31000, episode_reward=-187.87 +/- 95.65
Episode length: 188.80 +/- 95.43
Eval num_timesteps=31500, episode_reward=-176.90 +/- 79.57
Episode length: 177.86 +/- 79.41
Eval num_timesteps=32000, episode_reward=-170.37 +/- 57.61
Episode length: 171.35 +/- 57.50
Eval num_timesteps=32500, episode_reward=-219.26 +/- 123.90
Episode length: 220.11 +/- 123.56
Eval num_timesteps=33000, episode_reward=-218.49 +/- 124.98
Episode length: 219.34 +/- 124.64
Eval num_timesteps=33500, episode_reward=-194.47 +/- 110.10
Episode length: 195.36 +/- 109.79
Eval num_timesteps=34000, episode_reward=-213.15 +/- 123.98
Episode length: 214.00 +/- 123.63
Eval num_timesteps=34500, episode_reward=-199.91 +/- 113.55
Episode length: 200.80 +/- 113.26
Eval num_timesteps=35000, episode_reward=-206.55 +/- 118.97
Episode length: 207.42 +/- 118.65
Eval num_timesteps=35500, episode_reward=-201.24 +/- 107.05
Episode length: 202.14 +/- 106.77
Eval num_timesteps=36000, episode_reward=-217.16 +/- 132.62
Episode length: 217.99 +/- 132.25
Eval num_timesteps=36500, episode_reward=-184.04 +/- 98.34
Episode length: 184.97 +/- 98.12
Eval num_timesteps=37000, episode_reward=-176.85 +/- 89.42
Episode length: 177.79 +/- 89.20
Eval num_timesteps=37500, episode_reward=-158.63 +/- 56.48
Episode length: 159.61 +/- 56.35
Eval num_timesteps=38000, episode_reward=-151.80 +/- 54.60
Episode length: 152.78 +/- 54.47
New best mean reward!
Eval num_timesteps=38500, episode_reward=-148.88 +/- 100.12
Episode length: 149.81 +/- 99.88
New best mean reward!
Eval num_timesteps=39000, episode_reward=-135.56 +/- 78.49
Episode length: 136.52 +/- 78.31
New best mean reward!
Eval num_timesteps=39500, episode_reward=-149.01 +/- 91.11
Episode length: 149.95 +/- 90.88
Eval num_timesteps=40000, episode_reward=-157.59 +/- 74.81
Episode length: 158.55 +/- 74.63
Eval num_timesteps=40500, episode_reward=-159.85 +/- 81.52
Episode length: 160.80 +/- 81.31
Eval num_timesteps=41000, episode_reward=-159.59 +/- 111.16
Episode length: 160.50 +/- 110.89
Eval num_timesteps=41500, episode_reward=-153.89 +/- 69.72
Episode length: 154.86 +/- 69.57
Eval num_timesteps=42000, episode_reward=-152.74 +/- 73.85
Episode length: 153.71 +/- 73.71
Eval num_timesteps=42500, episode_reward=-137.33 +/- 83.89
Episode length: 138.29 +/- 83.72
Eval num_timesteps=43000, episode_reward=-138.00 +/- 71.12
Episode length: 138.97 +/- 70.96
Eval num_timesteps=43500, episode_reward=-131.23 +/- 71.98
Episode length: 132.20 +/- 71.82
New best mean reward!
Eval num_timesteps=44000, episode_reward=-115.99 +/- 62.14
Episode length: 116.97 +/- 62.02
New best mean reward!
Eval num_timesteps=44500, episode_reward=-133.77 +/- 98.25
Episode length: 134.72 +/- 98.06
Eval num_timesteps=45000, episode_reward=-124.18 +/- 74.57
Episode length: 125.15 +/- 74.41
Eval num_timesteps=45500, episode_reward=-121.03 +/- 71.37
Episode length: 122.00 +/- 71.21
Eval num_timesteps=46000, episode_reward=-114.30 +/- 49.89
Episode length: 115.29 +/- 49.81
New best mean reward!
Eval num_timesteps=46500, episode_reward=-130.56 +/- 95.88
Episode length: 131.50 +/- 95.64
Eval num_timesteps=47000, episode_reward=-120.46 +/- 76.49
Episode length: 121.43 +/- 76.34
Eval num_timesteps=47500, episode_reward=-111.31 +/- 52.41
Episode length: 112.30 +/- 52.34
New best mean reward!
Eval num_timesteps=48000, episode_reward=-111.55 +/- 48.12
Episode length: 112.54 +/- 48.04
Eval num_timesteps=48500, episode_reward=-120.38 +/- 70.02
Episode length: 121.35 +/- 69.86
Eval num_timesteps=49000, episode_reward=-116.93 +/- 62.68
Episode length: 117.91 +/- 62.56
Eval num_timesteps=49500, episode_reward=-108.60 +/- 26.54
Episode length: 109.60 +/- 26.54
New best mean reward!
Eval num_timesteps=50000, episode_reward=-120.89 +/- 75.92
Episode length: 121.86 +/- 75.77
FINISHED IN 638.8207876579836 s


starting seed  10441 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-194.61 +/- 56.30
Episode length: 195.60 +/- 56.25
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-472.70 +/- 99.64
Episode length: 472.77 +/- 99.39
Eval num_timesteps=13500, episode_reward=-444.12 +/- 138.69
Episode length: 444.26 +/- 138.35
Eval num_timesteps=14000, episode_reward=-475.88 +/- 95.50
Episode length: 475.94 +/- 95.26
Eval num_timesteps=14500, episode_reward=-364.06 +/- 189.73
Episode length: 364.40 +/- 189.26
Eval num_timesteps=15000, episode_reward=-235.25 +/- 195.31
Episode length: 235.90 +/- 194.83
Eval num_timesteps=15500, episode_reward=-269.47 +/- 201.78
Episode length: 270.04 +/- 201.29
Eval num_timesteps=16000, episode_reward=-279.76 +/- 199.84
Episode length: 280.31 +/- 199.34
Eval num_timesteps=16500, episode_reward=-188.16 +/- 171.21
Episode length: 188.94 +/- 170.81
New best mean reward!
Eval num_timesteps=17000, episode_reward=-150.89 +/- 153.39
Episode length: 151.73 +/- 153.03
New best mean reward!
Eval num_timesteps=17500, episode_reward=-128.15 +/- 123.73
Episode length: 129.06 +/- 123.46
New best mean reward!
Eval num_timesteps=18000, episode_reward=-133.19 +/- 130.58
Episode length: 134.08 +/- 130.27
Eval num_timesteps=18500, episode_reward=-113.13 +/- 93.68
Episode length: 114.08 +/- 93.47
New best mean reward!
Eval num_timesteps=19000, episode_reward=-128.92 +/- 120.89
Episode length: 129.83 +/- 120.61
Eval num_timesteps=19500, episode_reward=-145.29 +/- 140.63
Episode length: 146.16 +/- 140.31
Eval num_timesteps=20000, episode_reward=-119.11 +/- 106.67
Episode length: 120.05 +/- 106.46
Eval num_timesteps=20500, episode_reward=-124.16 +/- 119.52
Episode length: 125.07 +/- 119.24
Eval num_timesteps=21000, episode_reward=-101.23 +/- 75.42
Episode length: 102.20 +/- 75.26
New best mean reward!
Eval num_timesteps=21500, episode_reward=-135.67 +/- 124.31
Episode length: 136.58 +/- 124.04
Eval num_timesteps=22000, episode_reward=-162.80 +/- 158.58
Episode length: 163.63 +/- 158.21
Eval num_timesteps=22500, episode_reward=-183.09 +/- 170.23
Episode length: 183.87 +/- 169.82
Eval num_timesteps=23000, episode_reward=-206.91 +/- 183.76
Episode length: 207.64 +/- 183.33
Eval num_timesteps=23500, episode_reward=-158.96 +/- 145.34
Episode length: 159.82 +/- 145.01
Eval num_timesteps=24000, episode_reward=-196.82 +/- 177.69
Episode length: 197.57 +/- 177.27
Eval num_timesteps=24500, episode_reward=-138.90 +/- 125.77
Episode length: 139.80 +/- 125.48
Eval num_timesteps=25000, episode_reward=-140.69 +/- 130.87
Episode length: 141.58 +/- 130.57
Eval num_timesteps=25500, episode_reward=-105.77 +/- 83.94
Episode length: 106.73 +/- 83.76
Eval num_timesteps=26000, episode_reward=-126.01 +/- 120.67
Episode length: 126.92 +/- 120.39
Eval num_timesteps=26500, episode_reward=-122.36 +/- 107.09
Episode length: 123.29 +/- 106.84
Eval num_timesteps=27000, episode_reward=-136.39 +/- 125.60
Episode length: 137.29 +/- 125.31
Eval num_timesteps=27500, episode_reward=-158.01 +/- 154.11
Episode length: 158.85 +/- 153.76
Eval num_timesteps=28000, episode_reward=-139.85 +/- 124.77
Episode length: 140.76 +/- 124.51
Eval num_timesteps=28500, episode_reward=-154.96 +/- 148.71
Episode length: 155.81 +/- 148.37
Eval num_timesteps=29000, episode_reward=-120.26 +/- 102.28
Episode length: 121.21 +/- 102.10
Eval num_timesteps=29500, episode_reward=-107.89 +/- 78.12
Episode length: 108.86 +/- 77.97
Eval num_timesteps=30000, episode_reward=-112.05 +/- 92.69
Episode length: 113.00 +/- 92.48
Eval num_timesteps=30500, episode_reward=-105.16 +/- 76.33
Episode length: 106.14 +/- 76.23
Eval num_timesteps=31000, episode_reward=-159.20 +/- 152.83
Episode length: 160.04 +/- 152.47
Eval num_timesteps=31500, episode_reward=-112.40 +/- 92.58
Episode length: 113.35 +/- 92.37
Eval num_timesteps=32000, episode_reward=-115.60 +/- 92.33
Episode length: 116.56 +/- 92.16
Eval num_timesteps=32500, episode_reward=-121.37 +/- 98.96
Episode length: 122.32 +/- 98.77
Eval num_timesteps=33000, episode_reward=-117.30 +/- 90.71
Episode length: 118.26 +/- 90.54
Eval num_timesteps=33500, episode_reward=-133.24 +/- 111.52
Episode length: 134.17 +/- 111.29
Eval num_timesteps=34000, episode_reward=-109.64 +/- 79.93
Episode length: 110.61 +/- 79.79
Eval num_timesteps=34500, episode_reward=-142.19 +/- 120.33
Episode length: 143.10 +/- 120.06
Eval num_timesteps=35000, episode_reward=-134.73 +/- 123.36
Episode length: 135.64 +/- 123.10
Eval num_timesteps=35500, episode_reward=-134.98 +/- 124.07
Episode length: 135.89 +/- 123.81
Eval num_timesteps=36000, episode_reward=-167.72 +/- 156.78
Episode length: 168.55 +/- 156.42
Eval num_timesteps=36500, episode_reward=-164.06 +/- 151.35
Episode length: 164.90 +/- 150.99
Eval num_timesteps=37000, episode_reward=-150.16 +/- 145.51
Episode length: 151.03 +/- 145.19
Eval num_timesteps=37500, episode_reward=-170.36 +/- 161.06
Episode length: 171.18 +/- 160.69
Eval num_timesteps=38000, episode_reward=-181.66 +/- 164.80
Episode length: 182.47 +/- 164.43
Eval num_timesteps=38500, episode_reward=-164.23 +/- 151.12
Episode length: 165.08 +/- 150.79
Eval num_timesteps=39000, episode_reward=-155.35 +/- 141.97
Episode length: 156.22 +/- 141.65
Eval num_timesteps=39500, episode_reward=-120.14 +/- 101.54
Episode length: 121.08 +/- 101.32
Eval num_timesteps=40000, episode_reward=-96.51 +/- 49.70
Episode length: 97.50 +/- 49.62
New best mean reward!
FINISHED IN 711.2304375200183 s


starting seed  10442 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-166.58 +/- 123.33
Episode length: 167.48 +/- 123.06
New best mean reward!
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-486.09 +/- 68.59
Episode length: 486.13 +/- 68.40
Eval num_timesteps=8500, episode_reward=-215.92 +/- 163.98
Episode length: 216.69 +/- 163.58
Eval num_timesteps=9000, episode_reward=-84.94 +/- 22.40
Episode length: 85.94 +/- 22.40
New best mean reward!
FINISHED IN 220.02409504400566 s


starting seed  10443 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-344.82 +/- 156.48
Episode length: 345.32 +/- 155.98
New best mean reward!
Eval num_timesteps=8000, episode_reward=-201.29 +/- 113.41
Episode length: 202.18 +/- 113.12
New best mean reward!
Eval num_timesteps=8500, episode_reward=-158.51 +/- 72.07
Episode length: 159.49 +/- 71.97
New best mean reward!
Eval num_timesteps=9000, episode_reward=-305.48 +/- 103.77
Episode length: 306.36 +/- 103.54
Eval num_timesteps=9500, episode_reward=-174.18 +/- 68.24
Episode length: 175.16 +/- 68.14
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-310.96 +/- 92.81
Episode length: 311.85 +/- 92.58
Eval num_timesteps=11000, episode_reward=-494.93 +/- 28.70
Episode length: 494.97 +/- 28.53
Eval num_timesteps=11500, episode_reward=-292.86 +/- 108.42
Episode length: 293.76 +/- 108.23
Eval num_timesteps=12000, episode_reward=-246.11 +/- 94.49
Episode length: 247.08 +/- 94.41
Eval num_timesteps=12500, episode_reward=-123.70 +/- 18.13
Episode length: 124.70 +/- 18.13
New best mean reward!
Eval num_timesteps=13000, episode_reward=-359.66 +/- 79.18
Episode length: 360.52 +/- 78.93
Eval num_timesteps=13500, episode_reward=-127.79 +/- 45.93
Episode length: 128.78 +/- 45.85
Eval num_timesteps=14000, episode_reward=-232.33 +/- 47.25
Episode length: 233.33 +/- 47.25
Eval num_timesteps=14500, episode_reward=-235.84 +/- 64.19
Episode length: 236.83 +/- 64.15
Eval num_timesteps=15000, episode_reward=-239.47 +/- 56.86
Episode length: 240.46 +/- 56.81
Eval num_timesteps=15500, episode_reward=-441.21 +/- 64.31
Episode length: 441.83 +/- 63.96
Eval num_timesteps=16000, episode_reward=-228.92 +/- 53.40
Episode length: 229.90 +/- 53.30
Eval num_timesteps=16500, episode_reward=-207.42 +/- 49.44
Episode length: 208.42 +/- 49.44
Eval num_timesteps=17000, episode_reward=-177.74 +/- 65.39
Episode length: 178.71 +/- 65.24
Eval num_timesteps=17500, episode_reward=-192.28 +/- 72.13
Episode length: 193.25 +/- 72.00
Eval num_timesteps=18000, episode_reward=-175.66 +/- 54.02
Episode length: 176.64 +/- 53.90
Eval num_timesteps=18500, episode_reward=-187.64 +/- 60.18
Episode length: 188.61 +/- 60.02
Eval num_timesteps=19000, episode_reward=-202.79 +/- 74.95
Episode length: 203.75 +/- 74.79
Eval num_timesteps=19500, episode_reward=-200.83 +/- 73.24
Episode length: 201.80 +/- 73.12
Eval num_timesteps=20000, episode_reward=-140.37 +/- 25.10
Episode length: 141.37 +/- 25.10
Eval num_timesteps=20500, episode_reward=-106.75 +/- 41.23
Episode length: 107.75 +/- 41.23
New best mean reward!
Eval num_timesteps=21000, episode_reward=-94.41 +/- 44.50
Episode length: 95.40 +/- 44.41
New best mean reward!
FINISHED IN 424.87039671099046 s


starting seed  10444 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-142.37 +/- 103.46
Episode length: 143.30 +/- 103.22
New best mean reward!
Eval num_timesteps=6500, episode_reward=-188.10 +/- 45.45
Episode length: 189.09 +/- 45.38
Eval num_timesteps=7000, episode_reward=-482.86 +/- 74.79
Episode length: 482.91 +/- 74.58
Eval num_timesteps=7500, episode_reward=-287.68 +/- 191.97
Episode length: 288.24 +/- 191.49
Eval num_timesteps=8000, episode_reward=-497.64 +/- 13.02
Episode length: 497.68 +/- 12.85
Eval num_timesteps=8500, episode_reward=-494.64 +/- 31.10
Episode length: 494.67 +/- 30.93
Eval num_timesteps=9000, episode_reward=-235.17 +/- 178.62
Episode length: 235.86 +/- 178.16
Eval num_timesteps=9500, episode_reward=-398.54 +/- 159.53
Episode length: 398.83 +/- 159.08
Eval num_timesteps=10000, episode_reward=-307.03 +/- 189.47
Episode length: 307.55 +/- 188.98
Eval num_timesteps=10500, episode_reward=-277.83 +/- 182.10
Episode length: 278.44 +/- 181.62
Eval num_timesteps=11000, episode_reward=-225.87 +/- 180.84
Episode length: 226.57 +/- 180.38
Eval num_timesteps=11500, episode_reward=-275.14 +/- 189.04
Episode length: 275.74 +/- 188.57
Eval num_timesteps=12000, episode_reward=-179.41 +/- 150.50
Episode length: 180.24 +/- 150.14
Eval num_timesteps=12500, episode_reward=-97.18 +/- 47.15
Episode length: 98.17 +/- 47.06
New best mean reward!
FINISHED IN 271.86246456298977 s


starting seed  10445 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-237.86 +/- 168.37
Episode length: 238.57 +/- 167.92
New best mean reward!
Eval num_timesteps=10500, episode_reward=-274.32 +/- 184.42
Episode length: 274.93 +/- 183.94
Eval num_timesteps=11000, episode_reward=-133.73 +/- 71.24
Episode length: 134.71 +/- 71.13
New best mean reward!
Eval num_timesteps=11500, episode_reward=-123.60 +/- 53.85
Episode length: 124.59 +/- 53.78
New best mean reward!
Eval num_timesteps=12000, episode_reward=-109.11 +/- 22.07
Episode length: 110.11 +/- 22.07
New best mean reward!
Eval num_timesteps=12500, episode_reward=-116.28 +/- 32.24
Episode length: 117.28 +/- 32.24
Eval num_timesteps=13000, episode_reward=-115.50 +/- 26.67
Episode length: 116.50 +/- 26.67
Eval num_timesteps=13500, episode_reward=-228.82 +/- 170.49
Episode length: 229.54 +/- 170.04
Eval num_timesteps=14000, episode_reward=-275.91 +/- 183.86
Episode length: 276.51 +/- 183.37
Eval num_timesteps=14500, episode_reward=-125.76 +/- 84.87
Episode length: 126.72 +/- 84.69
Eval num_timesteps=15000, episode_reward=-111.52 +/- 37.04
Episode length: 112.52 +/- 37.04
Eval num_timesteps=15500, episode_reward=-131.50 +/- 104.25
Episode length: 132.43 +/- 104.00
Eval num_timesteps=16000, episode_reward=-135.86 +/- 104.02
Episode length: 136.80 +/- 103.81
Eval num_timesteps=16500, episode_reward=-154.81 +/- 130.25
Episode length: 155.69 +/- 129.93
Eval num_timesteps=17000, episode_reward=-219.40 +/- 174.40
Episode length: 220.13 +/- 173.96
Eval num_timesteps=17500, episode_reward=-290.22 +/- 199.24
Episode length: 290.75 +/- 198.75
Eval num_timesteps=18000, episode_reward=-170.92 +/- 161.25
Episode length: 171.74 +/- 160.89
Eval num_timesteps=18500, episode_reward=-179.53 +/- 167.33
Episode length: 180.33 +/- 166.95
Eval num_timesteps=19000, episode_reward=-133.54 +/- 126.24
Episode length: 134.44 +/- 125.95
Eval num_timesteps=19500, episode_reward=-95.79 +/- 48.61
Episode length: 96.78 +/- 48.53
New best mean reward!
FINISHED IN 418.2228145749541 s


starting seed  10446 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-386.63 +/- 143.99
Episode length: 387.02 +/- 143.51
New best mean reward!
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-200.45 +/- 130.69
Episode length: 201.30 +/- 130.35
New best mean reward!
Eval num_timesteps=11000, episode_reward=-141.27 +/- 116.96
Episode length: 142.18 +/- 116.68
New best mean reward!
Eval num_timesteps=11500, episode_reward=-119.65 +/- 77.48
Episode length: 120.62 +/- 77.33
New best mean reward!
Eval num_timesteps=12000, episode_reward=-107.91 +/- 31.54
Episode length: 108.91 +/- 31.54
New best mean reward!
Eval num_timesteps=12500, episode_reward=-114.04 +/- 27.58
Episode length: 115.04 +/- 27.58
Eval num_timesteps=13000, episode_reward=-115.40 +/- 70.89
Episode length: 116.37 +/- 70.73
Eval num_timesteps=13500, episode_reward=-147.09 +/- 121.47
Episode length: 147.99 +/- 121.17
Eval num_timesteps=14000, episode_reward=-84.88 +/- 24.22
Episode length: 85.88 +/- 24.22
New best mean reward!
FINISHED IN 327.4859208029811 s


starting seed  10447 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-460.09 +/- 119.77
Episode length: 460.19 +/- 119.47
New best mean reward!
Eval num_timesteps=10500, episode_reward=-233.70 +/- 191.84
Episode length: 234.37 +/- 191.38
New best mean reward!
Eval num_timesteps=11000, episode_reward=-188.93 +/- 136.71
Episode length: 189.77 +/- 136.34
New best mean reward!
Eval num_timesteps=11500, episode_reward=-142.66 +/- 66.08
Episode length: 143.63 +/- 65.92
New best mean reward!
Eval num_timesteps=12000, episode_reward=-127.33 +/- 43.31
Episode length: 128.32 +/- 43.22
New best mean reward!
Eval num_timesteps=12500, episode_reward=-133.30 +/- 24.12
Episode length: 134.30 +/- 24.12
Eval num_timesteps=13000, episode_reward=-109.99 +/- 45.40
Episode length: 110.98 +/- 45.32
New best mean reward!
Eval num_timesteps=13500, episode_reward=-137.74 +/- 26.79
Episode length: 138.74 +/- 26.79
Eval num_timesteps=14000, episode_reward=-127.52 +/- 21.50
Episode length: 128.52 +/- 21.50
Eval num_timesteps=14500, episode_reward=-152.09 +/- 49.07
Episode length: 153.08 +/- 49.00
Eval num_timesteps=15000, episode_reward=-196.56 +/- 129.87
Episode length: 197.42 +/- 129.54
Eval num_timesteps=15500, episode_reward=-188.17 +/- 99.94
Episode length: 189.10 +/- 99.72
Eval num_timesteps=16000, episode_reward=-166.49 +/- 61.07
Episode length: 167.47 +/- 60.96
Eval num_timesteps=16500, episode_reward=-154.11 +/- 48.38
Episode length: 155.10 +/- 48.31
Eval num_timesteps=17000, episode_reward=-139.05 +/- 31.69
Episode length: 140.05 +/- 31.69
Eval num_timesteps=17500, episode_reward=-140.47 +/- 25.30
Episode length: 141.47 +/- 25.30
Eval num_timesteps=18000, episode_reward=-141.95 +/- 32.53
Episode length: 142.95 +/- 32.53
Eval num_timesteps=18500, episode_reward=-139.20 +/- 28.01
Episode length: 140.20 +/- 28.01
Eval num_timesteps=19000, episode_reward=-139.90 +/- 24.00
Episode length: 140.90 +/- 24.00
Eval num_timesteps=19500, episode_reward=-148.56 +/- 48.35
Episode length: 149.55 +/- 48.28
Eval num_timesteps=20000, episode_reward=-151.14 +/- 46.40
Episode length: 152.13 +/- 46.33
Eval num_timesteps=20500, episode_reward=-153.32 +/- 36.21
Episode length: 154.32 +/- 36.21
Eval num_timesteps=21000, episode_reward=-143.27 +/- 31.93
Episode length: 144.27 +/- 31.93
Eval num_timesteps=21500, episode_reward=-139.36 +/- 23.42
Episode length: 140.36 +/- 23.42
Eval num_timesteps=22000, episode_reward=-137.21 +/- 23.30
Episode length: 138.21 +/- 23.30
Eval num_timesteps=22500, episode_reward=-147.03 +/- 61.88
Episode length: 148.01 +/- 61.77
Eval num_timesteps=23000, episode_reward=-140.23 +/- 18.51
Episode length: 141.23 +/- 18.51
Eval num_timesteps=23500, episode_reward=-140.71 +/- 25.08
Episode length: 141.71 +/- 25.08
Eval num_timesteps=24000, episode_reward=-126.87 +/- 28.04
Episode length: 127.87 +/- 28.04
Eval num_timesteps=24500, episode_reward=-127.65 +/- 33.91
Episode length: 128.65 +/- 33.91
Eval num_timesteps=25000, episode_reward=-127.85 +/- 30.33
Episode length: 128.85 +/- 30.33
Eval num_timesteps=25500, episode_reward=-104.38 +/- 26.85
Episode length: 105.38 +/- 26.85
New best mean reward!
Eval num_timesteps=26000, episode_reward=-107.67 +/- 36.34
Episode length: 108.67 +/- 36.34
Eval num_timesteps=26500, episode_reward=-94.65 +/- 23.39
Episode length: 95.65 +/- 23.39
New best mean reward!
FINISHED IN 408.4751847269945 s


starting seed  10448 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-472.38 +/- 93.71
Episode length: 472.46 +/- 93.44
New best mean reward!
Eval num_timesteps=12000, episode_reward=-488.38 +/- 66.13
Episode length: 488.41 +/- 65.96
Eval num_timesteps=12500, episode_reward=-95.78 +/- 48.06
Episode length: 96.77 +/- 47.97
New best mean reward!
FINISHED IN 398.5903191869729 s


starting seed  10449 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-449.39 +/- 91.71
Episode length: 449.67 +/- 91.31
New best mean reward!
Eval num_timesteps=14000, episode_reward=-458.21 +/- 84.88
Episode length: 458.44 +/- 84.50
Eval num_timesteps=14500, episode_reward=-407.56 +/- 142.37
Episode length: 407.86 +/- 141.92
New best mean reward!
Eval num_timesteps=15000, episode_reward=-354.62 +/- 163.11
Episode length: 355.07 +/- 162.62
New best mean reward!
Eval num_timesteps=15500, episode_reward=-136.73 +/- 25.98
Episode length: 137.73 +/- 25.98
New best mean reward!
Eval num_timesteps=16000, episode_reward=-226.31 +/- 149.66
Episode length: 227.09 +/- 149.26
Eval num_timesteps=16500, episode_reward=-256.58 +/- 180.11
Episode length: 257.24 +/- 179.65
Eval num_timesteps=17000, episode_reward=-344.93 +/- 185.08
Episode length: 345.35 +/- 184.59
Eval num_timesteps=17500, episode_reward=-265.26 +/- 189.64
Episode length: 265.87 +/- 189.15
Eval num_timesteps=18000, episode_reward=-125.16 +/- 80.50
Episode length: 126.13 +/- 80.36
New best mean reward!
Eval num_timesteps=18500, episode_reward=-107.02 +/- 60.17
Episode length: 108.00 +/- 60.04
New best mean reward!
Eval num_timesteps=19000, episode_reward=-185.84 +/- 163.51
Episode length: 186.63 +/- 163.10
Eval num_timesteps=19500, episode_reward=-273.98 +/- 196.85
Episode length: 274.56 +/- 196.37
Eval num_timesteps=20000, episode_reward=-301.10 +/- 198.34
Episode length: 301.62 +/- 197.85
Eval num_timesteps=20500, episode_reward=-183.03 +/- 159.58
Episode length: 183.83 +/- 159.18
Eval num_timesteps=21000, episode_reward=-121.36 +/- 109.37
Episode length: 122.29 +/- 109.13
Eval num_timesteps=21500, episode_reward=-96.81 +/- 32.31
Episode length: 97.81 +/- 32.31
New best mean reward!
FINISHED IN 504.00887343200156 s


starting seed  10450 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-234.92 +/- 71.48
Episode length: 235.90 +/- 71.40
New best mean reward!
Eval num_timesteps=5000, episode_reward=-480.34 +/- 85.76
Episode length: 480.39 +/- 85.54
Eval num_timesteps=5500, episode_reward=-472.22 +/- 95.22
Episode length: 472.30 +/- 94.96
Eval num_timesteps=6000, episode_reward=-273.79 +/- 101.55
Episode length: 274.73 +/- 101.42
Eval num_timesteps=6500, episode_reward=-257.29 +/- 86.69
Episode length: 258.26 +/- 86.61
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-237.84 +/- 58.72
Episode length: 238.83 +/- 58.67
Eval num_timesteps=8000, episode_reward=-94.30 +/- 27.78
Episode length: 95.30 +/- 27.78
New best mean reward!
FINISHED IN 174.99705880100373 s


starting seed  10451 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-490.78 +/- 46.15
Episode length: 490.82 +/- 45.96
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-277.54 +/- 185.28
Episode length: 278.14 +/- 184.80
New best mean reward!
Eval num_timesteps=6500, episode_reward=-185.65 +/- 140.56
Episode length: 186.50 +/- 140.23
New best mean reward!
Eval num_timesteps=7000, episode_reward=-119.44 +/- 68.31
Episode length: 120.42 +/- 68.20
New best mean reward!
Eval num_timesteps=7500, episode_reward=-115.31 +/- 55.64
Episode length: 116.30 +/- 55.57
New best mean reward!
Eval num_timesteps=8000, episode_reward=-98.65 +/- 41.96
Episode length: 99.65 +/- 41.96
New best mean reward!
FINISHED IN 172.9770276119816 s


starting seed  10452 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-496.89 +/- 30.94
Episode length: 496.90 +/- 30.84
New best mean reward!
Eval num_timesteps=1000, episode_reward=-459.45 +/- 77.87
Episode length: 459.75 +/- 77.50
New best mean reward!
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-313.30 +/- 163.08
Episode length: 313.88 +/- 162.60
New best mean reward!
Eval num_timesteps=8500, episode_reward=-237.94 +/- 167.73
Episode length: 238.66 +/- 167.30
New best mean reward!
Eval num_timesteps=9000, episode_reward=-464.11 +/- 74.07
Episode length: 464.35 +/- 73.71
Eval num_timesteps=9500, episode_reward=-426.35 +/- 148.69
Episode length: 426.55 +/- 148.30
Eval num_timesteps=10000, episode_reward=-104.00 +/- 33.82
Episode length: 105.00 +/- 33.82
New best mean reward!
Eval num_timesteps=10500, episode_reward=-230.51 +/- 143.52
Episode length: 231.30 +/- 143.13
Eval num_timesteps=11000, episode_reward=-282.48 +/- 176.95
Episode length: 283.09 +/- 176.47
Eval num_timesteps=11500, episode_reward=-497.82 +/- 16.14
Episode length: 497.85 +/- 16.01
Eval num_timesteps=12000, episode_reward=-498.36 +/- 16.32
Episode length: 498.37 +/- 16.22
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-461.90 +/- 90.64
Episode length: 462.06 +/- 90.28
Eval num_timesteps=13500, episode_reward=-437.81 +/- 137.89
Episode length: 437.98 +/- 137.52
Eval num_timesteps=14000, episode_reward=-492.44 +/- 52.92
Episode length: 492.46 +/- 52.78
Eval num_timesteps=14500, episode_reward=-485.98 +/- 69.10
Episode length: 486.02 +/- 68.91
Eval num_timesteps=15000, episode_reward=-400.49 +/- 171.77
Episode length: 400.75 +/- 171.34
Eval num_timesteps=15500, episode_reward=-312.43 +/- 201.42
Episode length: 312.90 +/- 200.92
Eval num_timesteps=16000, episode_reward=-180.82 +/- 171.89
Episode length: 181.60 +/- 171.49
Eval num_timesteps=16500, episode_reward=-174.36 +/- 164.84
Episode length: 175.17 +/- 164.46
Eval num_timesteps=17000, episode_reward=-209.71 +/- 183.27
Episode length: 210.45 +/- 182.86
Eval num_timesteps=17500, episode_reward=-150.87 +/- 143.92
Episode length: 151.74 +/- 143.60
Eval num_timesteps=18000, episode_reward=-122.72 +/- 111.18
Episode length: 123.65 +/- 110.94
Eval num_timesteps=18500, episode_reward=-141.95 +/- 130.21
Episode length: 142.84 +/- 129.91
Eval num_timesteps=19000, episode_reward=-128.11 +/- 125.58
Episode length: 129.01 +/- 125.28
Eval num_timesteps=19500, episode_reward=-119.58 +/- 97.62
Episode length: 120.53 +/- 97.43
Eval num_timesteps=20000, episode_reward=-108.00 +/- 77.68
Episode length: 108.97 +/- 77.53
Eval num_timesteps=20500, episode_reward=-91.82 +/- 46.63
Episode length: 92.81 +/- 46.54
New best mean reward!
FINISHED IN 438.10525104700355 s


starting seed  10453 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-495.96 +/- 40.20
Episode length: 495.97 +/- 40.10
New best mean reward!
Eval num_timesteps=11500, episode_reward=-269.55 +/- 201.58
Episode length: 270.12 +/- 201.09
New best mean reward!
Eval num_timesteps=12000, episode_reward=-400.58 +/- 173.38
Episode length: 400.83 +/- 172.95
Eval num_timesteps=12500, episode_reward=-178.24 +/- 166.49
Episode length: 179.04 +/- 166.11
New best mean reward!
Eval num_timesteps=13000, episode_reward=-180.53 +/- 171.41
Episode length: 181.31 +/- 171.00
Eval num_timesteps=13500, episode_reward=-185.37 +/- 173.18
Episode length: 186.15 +/- 172.78
Eval num_timesteps=14000, episode_reward=-194.81 +/- 174.24
Episode length: 195.57 +/- 173.82
Eval num_timesteps=14500, episode_reward=-175.18 +/- 160.14
Episode length: 176.00 +/- 159.77
New best mean reward!
Eval num_timesteps=15000, episode_reward=-121.11 +/- 115.42
Episode length: 122.03 +/- 115.16
New best mean reward!
Eval num_timesteps=15500, episode_reward=-95.10 +/- 70.32
Episode length: 96.08 +/- 70.21
New best mean reward!
FINISHED IN 361.3157386100502 s


starting seed  10454 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-493.12 +/- 48.16
Episode length: 493.14 +/- 48.02
New best mean reward!
Eval num_timesteps=11500, episode_reward=-301.63 +/- 162.34
Episode length: 302.25 +/- 161.88
New best mean reward!
Eval num_timesteps=12000, episode_reward=-281.02 +/- 166.32
Episode length: 281.66 +/- 165.84
New best mean reward!
Eval num_timesteps=12500, episode_reward=-228.74 +/- 145.70
Episode length: 229.52 +/- 145.29
New best mean reward!
Eval num_timesteps=13000, episode_reward=-286.93 +/- 163.78
Episode length: 287.57 +/- 163.32
Eval num_timesteps=13500, episode_reward=-422.76 +/- 143.13
Episode length: 422.99 +/- 142.72
Eval num_timesteps=14000, episode_reward=-480.49 +/- 77.57
Episode length: 480.55 +/- 77.33
Eval num_timesteps=14500, episode_reward=-429.48 +/- 133.89
Episode length: 429.70 +/- 133.48
Eval num_timesteps=15000, episode_reward=-208.74 +/- 112.38
Episode length: 209.64 +/- 112.12
New best mean reward!
Eval num_timesteps=15500, episode_reward=-185.67 +/- 89.25
Episode length: 186.62 +/- 89.08
New best mean reward!
Eval num_timesteps=16000, episode_reward=-161.88 +/- 44.54
Episode length: 162.87 +/- 44.46
New best mean reward!
Eval num_timesteps=16500, episode_reward=-152.59 +/- 26.04
Episode length: 153.59 +/- 26.04
New best mean reward!
Eval num_timesteps=17000, episode_reward=-164.91 +/- 48.55
Episode length: 165.90 +/- 48.48
Eval num_timesteps=17500, episode_reward=-160.66 +/- 32.68
Episode length: 161.66 +/- 32.68
Eval num_timesteps=18000, episode_reward=-147.80 +/- 83.73
Episode length: 148.76 +/- 83.57
New best mean reward!
Eval num_timesteps=18500, episode_reward=-156.98 +/- 46.48
Episode length: 157.98 +/- 46.48
Eval num_timesteps=19000, episode_reward=-188.00 +/- 95.60
Episode length: 188.94 +/- 95.40
Eval num_timesteps=19500, episode_reward=-203.14 +/- 112.95
Episode length: 204.03 +/- 112.67
Eval num_timesteps=20000, episode_reward=-137.20 +/- 104.68
Episode length: 138.13 +/- 104.44
New best mean reward!
Eval num_timesteps=20500, episode_reward=-176.70 +/- 142.03
Episode length: 177.55 +/- 141.69
Eval num_timesteps=21000, episode_reward=-184.46 +/- 154.07
Episode length: 185.27 +/- 153.68
Eval num_timesteps=21500, episode_reward=-131.36 +/- 119.15
Episode length: 132.27 +/- 118.87
New best mean reward!
Eval num_timesteps=22000, episode_reward=-182.39 +/- 166.04
Episode length: 183.19 +/- 165.66
Eval num_timesteps=22500, episode_reward=-162.30 +/- 155.27
Episode length: 163.13 +/- 154.90
Eval num_timesteps=23000, episode_reward=-203.33 +/- 183.63
Episode length: 204.07 +/- 183.21
Eval num_timesteps=23500, episode_reward=-156.39 +/- 157.05
Episode length: 157.22 +/- 156.67
Eval num_timesteps=24000, episode_reward=-190.88 +/- 178.29
Episode length: 191.64 +/- 177.88
Eval num_timesteps=24500, episode_reward=-203.84 +/- 186.76
Episode length: 204.56 +/- 186.32
Eval num_timesteps=25000, episode_reward=-203.53 +/- 186.44
Episode length: 204.25 +/- 186.00
Eval num_timesteps=25500, episode_reward=-186.84 +/- 177.67
Episode length: 187.60 +/- 177.24
Eval num_timesteps=26000, episode_reward=-147.01 +/- 149.15
Episode length: 147.86 +/- 148.80
Eval num_timesteps=26500, episode_reward=-158.18 +/- 149.79
Episode length: 159.03 +/- 149.45
Eval num_timesteps=27000, episode_reward=-137.92 +/- 135.75
Episode length: 138.80 +/- 135.43
Eval num_timesteps=27500, episode_reward=-132.38 +/- 130.48
Episode length: 133.27 +/- 130.17
Eval num_timesteps=28000, episode_reward=-144.56 +/- 140.88
Episode length: 145.43 +/- 140.55
Eval num_timesteps=28500, episode_reward=-111.35 +/- 90.17
Episode length: 112.31 +/- 90.00
New best mean reward!
Eval num_timesteps=29000, episode_reward=-106.67 +/- 84.55
Episode length: 107.63 +/- 84.36
New best mean reward!
Eval num_timesteps=29500, episode_reward=-99.13 +/- 69.23
Episode length: 100.11 +/- 69.11
New best mean reward!
FINISHED IN 553.0589178699884 s


starting seed  10455 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-493.05 +/- 49.02
Episode length: 493.07 +/- 48.88
New best mean reward!
Eval num_timesteps=7500, episode_reward=-306.30 +/- 203.57
Episode length: 306.78 +/- 203.07
New best mean reward!
Eval num_timesteps=8000, episode_reward=-214.65 +/- 185.17
Episode length: 215.37 +/- 184.74
New best mean reward!
Eval num_timesteps=8500, episode_reward=-212.28 +/- 181.45
Episode length: 213.01 +/- 181.02
New best mean reward!
Eval num_timesteps=9000, episode_reward=-203.05 +/- 183.24
Episode length: 203.78 +/- 182.80
New best mean reward!
Eval num_timesteps=9500, episode_reward=-198.00 +/- 173.15
Episode length: 198.77 +/- 172.75
New best mean reward!
Eval num_timesteps=10000, episode_reward=-105.10 +/- 75.20
Episode length: 106.07 +/- 75.05
New best mean reward!
Eval num_timesteps=10500, episode_reward=-135.16 +/- 131.50
Episode length: 136.06 +/- 131.23
Eval num_timesteps=11000, episode_reward=-95.69 +/- 50.42
Episode length: 96.68 +/- 50.34
New best mean reward!
FINISHED IN 227.80900846404256 s


starting seed  10456 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-389.99 +/- 108.70
Episode length: 390.61 +/- 108.31
New best mean reward!
Eval num_timesteps=8500, episode_reward=-499.84 +/- 1.59
Episode length: 499.85 +/- 1.49
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-181.43 +/- 39.61
Episode length: 182.43 +/- 39.61
New best mean reward!
Eval num_timesteps=10000, episode_reward=-196.33 +/- 44.09
Episode length: 197.32 +/- 44.02
Eval num_timesteps=10500, episode_reward=-178.69 +/- 45.78
Episode length: 179.69 +/- 45.78
New best mean reward!
Eval num_timesteps=11000, episode_reward=-139.91 +/- 22.88
Episode length: 140.91 +/- 22.88
New best mean reward!
Eval num_timesteps=11500, episode_reward=-157.12 +/- 27.57
Episode length: 158.12 +/- 27.57
Eval num_timesteps=12000, episode_reward=-126.53 +/- 33.07
Episode length: 127.53 +/- 33.07
New best mean reward!
Eval num_timesteps=12500, episode_reward=-115.57 +/- 29.87
Episode length: 116.57 +/- 29.87
New best mean reward!
Eval num_timesteps=13000, episode_reward=-154.75 +/- 117.17
Episode length: 155.65 +/- 116.87
Eval num_timesteps=13500, episode_reward=-108.78 +/- 49.03
Episode length: 109.77 +/- 48.95
New best mean reward!
Eval num_timesteps=14000, episode_reward=-104.45 +/- 29.69
Episode length: 105.45 +/- 29.69
New best mean reward!
Eval num_timesteps=14500, episode_reward=-103.44 +/- 31.90
Episode length: 104.44 +/- 31.90
New best mean reward!
Eval num_timesteps=15000, episode_reward=-102.95 +/- 53.84
Episode length: 103.94 +/- 53.77
New best mean reward!
Eval num_timesteps=15500, episode_reward=-96.90 +/- 41.72
Episode length: 97.90 +/- 41.72
New best mean reward!
FINISHED IN 299.69368816696806 s


starting seed  10457 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-281.66 +/- 124.28
Episode length: 282.49 +/- 123.98
New best mean reward!
Eval num_timesteps=10500, episode_reward=-368.88 +/- 146.43
Episode length: 369.36 +/- 145.96
Eval num_timesteps=11000, episode_reward=-310.02 +/- 160.46
Episode length: 310.63 +/- 159.99
Eval num_timesteps=11500, episode_reward=-497.89 +/- 15.91
Episode length: 497.92 +/- 15.79
Eval num_timesteps=12000, episode_reward=-262.56 +/- 118.50
Episode length: 263.45 +/- 118.28
New best mean reward!
Eval num_timesteps=12500, episode_reward=-107.86 +/- 32.65
Episode length: 108.86 +/- 32.65
New best mean reward!
Eval num_timesteps=13000, episode_reward=-91.81 +/- 23.24
Episode length: 92.81 +/- 23.24
New best mean reward!
FINISHED IN 313.1152152160066 s


starting seed  10458 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-204.15 +/- 164.42
Episode length: 204.92 +/- 164.01
New best mean reward!
Eval num_timesteps=6500, episode_reward=-234.85 +/- 199.85
Episode length: 235.49 +/- 199.37
Eval num_timesteps=7000, episode_reward=-255.55 +/- 196.14
Episode length: 256.16 +/- 195.66
Eval num_timesteps=7500, episode_reward=-315.34 +/- 204.49
Episode length: 315.79 +/- 204.00
Eval num_timesteps=8000, episode_reward=-173.51 +/- 127.35
Episode length: 174.38 +/- 127.02
New best mean reward!
Eval num_timesteps=8500, episode_reward=-168.55 +/- 141.95
Episode length: 169.40 +/- 141.60
New best mean reward!
Eval num_timesteps=9000, episode_reward=-151.37 +/- 112.67
Episode length: 152.28 +/- 112.39
New best mean reward!
Eval num_timesteps=9500, episode_reward=-141.82 +/- 74.24
Episode length: 142.79 +/- 74.09
New best mean reward!
Eval num_timesteps=10000, episode_reward=-139.88 +/- 109.71
Episode length: 140.80 +/- 109.45
New best mean reward!
Eval num_timesteps=10500, episode_reward=-225.60 +/- 166.98
Episode length: 226.34 +/- 166.55
Eval num_timesteps=11000, episode_reward=-180.94 +/- 135.81
Episode length: 181.80 +/- 135.49
Eval num_timesteps=11500, episode_reward=-160.08 +/- 110.82
Episode length: 160.99 +/- 110.54
Eval num_timesteps=12000, episode_reward=-173.47 +/- 107.58
Episode length: 174.39 +/- 107.33
Eval num_timesteps=12500, episode_reward=-131.00 +/- 58.84
Episode length: 131.98 +/- 58.71
New best mean reward!
Eval num_timesteps=13000, episode_reward=-131.32 +/- 58.67
Episode length: 132.30 +/- 58.54
Eval num_timesteps=13500, episode_reward=-127.15 +/- 28.86
Episode length: 128.15 +/- 28.86
New best mean reward!
Eval num_timesteps=14000, episode_reward=-126.29 +/- 24.77
Episode length: 127.29 +/- 24.77
New best mean reward!
Eval num_timesteps=14500, episode_reward=-92.32 +/- 48.30
Episode length: 93.31 +/- 48.22
New best mean reward!
FINISHED IN 250.68970143096521 s


starting seed  10459 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-429.91 +/- 150.65
Episode length: 430.09 +/- 150.27
New best mean reward!
Eval num_timesteps=7500, episode_reward=-263.24 +/- 197.28
Episode length: 263.84 +/- 196.80
New best mean reward!
Eval num_timesteps=8000, episode_reward=-400.22 +/- 177.71
Episode length: 400.46 +/- 177.29
Eval num_timesteps=8500, episode_reward=-307.20 +/- 202.79
Episode length: 307.69 +/- 202.30
Eval num_timesteps=9000, episode_reward=-197.99 +/- 180.66
Episode length: 198.75 +/- 180.26
New best mean reward!
Eval num_timesteps=9500, episode_reward=-248.20 +/- 196.85
Episode length: 248.85 +/- 196.40
Eval num_timesteps=10000, episode_reward=-272.12 +/- 201.17
Episode length: 272.70 +/- 200.69
Eval num_timesteps=10500, episode_reward=-340.97 +/- 200.72
Episode length: 341.37 +/- 200.25
Eval num_timesteps=11000, episode_reward=-241.38 +/- 196.69
Episode length: 242.03 +/- 196.23
Eval num_timesteps=11500, episode_reward=-196.98 +/- 179.58
Episode length: 197.73 +/- 179.16
New best mean reward!
Eval num_timesteps=12000, episode_reward=-153.94 +/- 147.06
Episode length: 154.79 +/- 146.71
New best mean reward!
Eval num_timesteps=12500, episode_reward=-158.71 +/- 152.62
Episode length: 159.55 +/- 152.26
Eval num_timesteps=13000, episode_reward=-115.31 +/- 88.88
Episode length: 116.27 +/- 88.71
New best mean reward!
Eval num_timesteps=13500, episode_reward=-117.67 +/- 96.22
Episode length: 118.62 +/- 96.02
Eval num_timesteps=14000, episode_reward=-107.53 +/- 69.96
Episode length: 108.51 +/- 69.85
New best mean reward!
Eval num_timesteps=14500, episode_reward=-100.30 +/- 68.08
Episode length: 101.28 +/- 67.96
New best mean reward!
Eval num_timesteps=15000, episode_reward=-106.58 +/- 89.99
Episode length: 107.54 +/- 89.82
Eval num_timesteps=15500, episode_reward=-145.15 +/- 142.85
Episode length: 146.02 +/- 142.53
Eval num_timesteps=16000, episode_reward=-112.13 +/- 97.75
Episode length: 113.08 +/- 97.55
Eval num_timesteps=16500, episode_reward=-113.91 +/- 102.12
Episode length: 114.85 +/- 101.89
Eval num_timesteps=17000, episode_reward=-127.17 +/- 119.54
Episode length: 128.09 +/- 119.29
Eval num_timesteps=17500, episode_reward=-131.69 +/- 122.19
Episode length: 132.61 +/- 121.95
Eval num_timesteps=18000, episode_reward=-126.85 +/- 118.39
Episode length: 127.77 +/- 118.14
Eval num_timesteps=18500, episode_reward=-99.14 +/- 74.55
Episode length: 100.11 +/- 74.39
New best mean reward!
FINISHED IN 336.3508491750108 s


starting seed  10460 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-498.52 +/- 11.36
Episode length: 498.54 +/- 11.24
New best mean reward!
Eval num_timesteps=8500, episode_reward=-273.35 +/- 82.68
Episode length: 274.31 +/- 82.57
New best mean reward!
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-479.26 +/- 83.77
Episode length: 479.32 +/- 83.54
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=14000, episode_reward=-468.68 +/- 106.33
Episode length: 468.76 +/- 106.06
Eval num_timesteps=14500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-399.40 +/- 171.72
Episode length: 399.66 +/- 171.29
Eval num_timesteps=15500, episode_reward=-355.69 +/- 190.23
Episode length: 356.06 +/- 189.76
Eval num_timesteps=16000, episode_reward=-442.59 +/- 137.12
Episode length: 442.74 +/- 136.77
Eval num_timesteps=16500, episode_reward=-423.30 +/- 154.72
Episode length: 423.50 +/- 154.33
Eval num_timesteps=17000, episode_reward=-390.33 +/- 176.50
Episode length: 390.61 +/- 176.05
Eval num_timesteps=17500, episode_reward=-368.17 +/- 180.44
Episode length: 368.53 +/- 179.97
Eval num_timesteps=18000, episode_reward=-365.34 +/- 184.75
Episode length: 365.69 +/- 184.28
Eval num_timesteps=18500, episode_reward=-319.44 +/- 196.17
Episode length: 319.91 +/- 195.69
Eval num_timesteps=19000, episode_reward=-257.09 +/- 192.96
Episode length: 257.71 +/- 192.48
New best mean reward!
Eval num_timesteps=19500, episode_reward=-250.03 +/- 193.34
Episode length: 250.66 +/- 192.86
New best mean reward!
Eval num_timesteps=20000, episode_reward=-213.17 +/- 180.67
Episode length: 213.89 +/- 180.22
New best mean reward!
Eval num_timesteps=20500, episode_reward=-157.52 +/- 127.63
Episode length: 158.41 +/- 127.33
New best mean reward!
Eval num_timesteps=21000, episode_reward=-183.44 +/- 159.12
Episode length: 184.25 +/- 158.74
Eval num_timesteps=21500, episode_reward=-189.19 +/- 163.06
Episode length: 189.98 +/- 162.66
Eval num_timesteps=22000, episode_reward=-145.20 +/- 114.21
Episode length: 146.13 +/- 113.99
New best mean reward!
Eval num_timesteps=22500, episode_reward=-150.96 +/- 131.95
Episode length: 151.84 +/- 131.63
Eval num_timesteps=23000, episode_reward=-142.33 +/- 111.34
Episode length: 143.25 +/- 111.09
New best mean reward!
Eval num_timesteps=23500, episode_reward=-181.54 +/- 159.65
Episode length: 182.35 +/- 159.28
Eval num_timesteps=24000, episode_reward=-174.75 +/- 151.84
Episode length: 175.58 +/- 151.48
Eval num_timesteps=24500, episode_reward=-151.00 +/- 123.29
Episode length: 151.90 +/- 123.01
Eval num_timesteps=25000, episode_reward=-136.55 +/- 112.97
Episode length: 137.47 +/- 112.71
New best mean reward!
Eval num_timesteps=25500, episode_reward=-144.48 +/- 127.27
Episode length: 145.37 +/- 126.97
Eval num_timesteps=26000, episode_reward=-130.06 +/- 105.14
Episode length: 131.00 +/- 104.93
New best mean reward!
Eval num_timesteps=26500, episode_reward=-143.42 +/- 124.54
Episode length: 144.32 +/- 124.25
Eval num_timesteps=27000, episode_reward=-129.50 +/- 97.98
Episode length: 130.45 +/- 97.79
New best mean reward!
Eval num_timesteps=27500, episode_reward=-146.90 +/- 116.84
Episode length: 147.82 +/- 116.60
Eval num_timesteps=28000, episode_reward=-138.23 +/- 107.48
Episode length: 139.16 +/- 107.25
Eval num_timesteps=28500, episode_reward=-140.47 +/- 122.56
Episode length: 141.37 +/- 122.26
Eval num_timesteps=29000, episode_reward=-150.48 +/- 126.90
Episode length: 151.37 +/- 126.60
Eval num_timesteps=29500, episode_reward=-178.96 +/- 157.57
Episode length: 179.78 +/- 157.20
Eval num_timesteps=30000, episode_reward=-136.47 +/- 108.37
Episode length: 137.40 +/- 108.13
Eval num_timesteps=30500, episode_reward=-143.95 +/- 123.69
Episode length: 144.85 +/- 123.40
Eval num_timesteps=31000, episode_reward=-120.33 +/- 83.87
Episode length: 121.29 +/- 83.69
New best mean reward!
Eval num_timesteps=31500, episode_reward=-124.77 +/- 83.96
Episode length: 125.74 +/- 83.83
Eval num_timesteps=32000, episode_reward=-116.65 +/- 80.87
Episode length: 117.62 +/- 80.73
New best mean reward!
Eval num_timesteps=32500, episode_reward=-109.34 +/- 75.54
Episode length: 110.31 +/- 75.39
New best mean reward!
Eval num_timesteps=33000, episode_reward=-109.35 +/- 67.41
Episode length: 110.33 +/- 67.29
Eval num_timesteps=33500, episode_reward=-115.73 +/- 92.05
Episode length: 116.69 +/- 91.88
Eval num_timesteps=34000, episode_reward=-125.16 +/- 99.99
Episode length: 126.12 +/- 99.84
Eval num_timesteps=34500, episode_reward=-116.14 +/- 75.97
Episode length: 117.12 +/- 75.86
Eval num_timesteps=35000, episode_reward=-103.45 +/- 55.41
Episode length: 104.44 +/- 55.34
New best mean reward!
Eval num_timesteps=35500, episode_reward=-120.17 +/- 89.11
Episode length: 121.13 +/- 88.94
Eval num_timesteps=36000, episode_reward=-116.81 +/- 94.57
Episode length: 117.76 +/- 94.36
Eval num_timesteps=36500, episode_reward=-116.15 +/- 83.14
Episode length: 117.13 +/- 83.04
Eval num_timesteps=37000, episode_reward=-109.74 +/- 64.02
Episode length: 110.72 +/- 63.90
Eval num_timesteps=37500, episode_reward=-116.88 +/- 82.19
Episode length: 117.84 +/- 82.01
Eval num_timesteps=38000, episode_reward=-146.25 +/- 123.46
Episode length: 147.15 +/- 123.17
Eval num_timesteps=38500, episode_reward=-122.22 +/- 87.95
Episode length: 123.18 +/- 87.77
Eval num_timesteps=39000, episode_reward=-131.09 +/- 107.69
Episode length: 132.02 +/- 107.45
Eval num_timesteps=39500, episode_reward=-122.98 +/- 92.42
Episode length: 123.94 +/- 92.25
Eval num_timesteps=40000, episode_reward=-105.55 +/- 65.52
Episode length: 106.53 +/- 65.40
Eval num_timesteps=40500, episode_reward=-122.23 +/- 92.84
Episode length: 123.18 +/- 92.64
Eval num_timesteps=41000, episode_reward=-116.85 +/- 78.61
Episode length: 117.82 +/- 78.46
Eval num_timesteps=41500, episode_reward=-136.65 +/- 113.01
Episode length: 137.58 +/- 112.78
Eval num_timesteps=42000, episode_reward=-122.71 +/- 91.24
Episode length: 123.66 +/- 91.04
Eval num_timesteps=42500, episode_reward=-116.77 +/- 85.11
Episode length: 117.73 +/- 84.93
Eval num_timesteps=43000, episode_reward=-128.74 +/- 95.43
Episode length: 129.70 +/- 95.27
Eval num_timesteps=43500, episode_reward=-107.70 +/- 72.66
Episode length: 108.67 +/- 72.49
Eval num_timesteps=44000, episode_reward=-110.57 +/- 66.56
Episode length: 111.55 +/- 66.44
Eval num_timesteps=44500, episode_reward=-140.74 +/- 129.06
Episode length: 141.63 +/- 128.76
Eval num_timesteps=45000, episode_reward=-123.66 +/- 93.15
Episode length: 124.61 +/- 92.95
Eval num_timesteps=45500, episode_reward=-121.38 +/- 82.03
Episode length: 122.35 +/- 81.89
Eval num_timesteps=46000, episode_reward=-121.81 +/- 92.51
Episode length: 122.76 +/- 92.30
Eval num_timesteps=46500, episode_reward=-107.21 +/- 62.63
Episode length: 108.19 +/- 62.51
Eval num_timesteps=47000, episode_reward=-113.53 +/- 81.48
Episode length: 114.51 +/- 81.38
Eval num_timesteps=47500, episode_reward=-130.31 +/- 106.50
Episode length: 131.25 +/- 106.30
Eval num_timesteps=48000, episode_reward=-111.39 +/- 85.75
Episode length: 112.35 +/- 85.57
Eval num_timesteps=48500, episode_reward=-120.04 +/- 88.56
Episode length: 121.00 +/- 88.39
Eval num_timesteps=49000, episode_reward=-110.75 +/- 79.86
Episode length: 111.72 +/- 79.72
Eval num_timesteps=49500, episode_reward=-121.39 +/- 93.42
Episode length: 122.34 +/- 93.22
Eval num_timesteps=50000, episode_reward=-127.05 +/- 99.67
Episode length: 127.99 +/- 99.45
FINISHED IN 1005.6469120869879 s


starting seed  10461 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-371.24 +/- 181.57
Episode length: 371.58 +/- 181.10
New best mean reward!
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-349.02 +/- 194.63
Episode length: 349.40 +/- 194.15
New best mean reward!
Eval num_timesteps=14000, episode_reward=-265.26 +/- 203.48
Episode length: 265.84 +/- 203.00
New best mean reward!
Eval num_timesteps=14500, episode_reward=-302.25 +/- 197.62
Episode length: 302.76 +/- 197.13
Eval num_timesteps=15000, episode_reward=-151.09 +/- 143.34
Episode length: 151.95 +/- 143.00
New best mean reward!
Eval num_timesteps=15500, episode_reward=-162.99 +/- 149.30
Episode length: 163.84 +/- 148.96
Eval num_timesteps=16000, episode_reward=-249.13 +/- 194.00
Episode length: 249.77 +/- 193.54
Eval num_timesteps=16500, episode_reward=-310.96 +/- 192.42
Episode length: 311.47 +/- 191.94
Eval num_timesteps=17000, episode_reward=-366.57 +/- 185.02
Episode length: 366.92 +/- 184.55
Eval num_timesteps=17500, episode_reward=-103.23 +/- 64.42
Episode length: 104.21 +/- 64.30
New best mean reward!
Eval num_timesteps=18000, episode_reward=-89.98 +/- 23.84
Episode length: 90.98 +/- 23.84
New best mean reward!
FINISHED IN 394.9224483850412 s


starting seed  10462 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-177.32 +/- 39.41
Episode length: 178.32 +/- 39.41
New best mean reward!
Eval num_timesteps=9500, episode_reward=-183.02 +/- 38.09
Episode length: 184.02 +/- 38.09
Eval num_timesteps=10000, episode_reward=-499.78 +/- 2.19
Episode length: 499.79 +/- 2.09
Eval num_timesteps=10500, episode_reward=-324.02 +/- 181.91
Episode length: 324.51 +/- 181.42
Eval num_timesteps=11000, episode_reward=-277.87 +/- 183.01
Episode length: 278.50 +/- 182.56
Eval num_timesteps=11500, episode_reward=-136.48 +/- 104.53
Episode length: 137.43 +/- 104.36
New best mean reward!
Eval num_timesteps=12000, episode_reward=-207.53 +/- 170.13
Episode length: 208.30 +/- 169.73
Eval num_timesteps=12500, episode_reward=-414.83 +/- 148.03
Episode length: 415.11 +/- 147.61
Eval num_timesteps=13000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13500, episode_reward=-472.54 +/- 100.23
Episode length: 472.61 +/- 99.98
Eval num_timesteps=14000, episode_reward=-438.21 +/- 144.77
Episode length: 438.37 +/- 144.41
Eval num_timesteps=14500, episode_reward=-366.36 +/- 191.65
Episode length: 366.69 +/- 191.18
Eval num_timesteps=15000, episode_reward=-149.84 +/- 145.30
Episode length: 150.70 +/- 144.96
Eval num_timesteps=15500, episode_reward=-130.40 +/- 124.44
Episode length: 131.30 +/- 124.15
New best mean reward!
Eval num_timesteps=16000, episode_reward=-101.76 +/- 73.82
Episode length: 102.73 +/- 73.66
New best mean reward!
Eval num_timesteps=16500, episode_reward=-117.84 +/- 107.16
Episode length: 118.77 +/- 106.91
Eval num_timesteps=17000, episode_reward=-122.06 +/- 109.67
Episode length: 122.99 +/- 109.43
Eval num_timesteps=17500, episode_reward=-117.83 +/- 107.73
Episode length: 118.76 +/- 107.48
Eval num_timesteps=18000, episode_reward=-91.63 +/- 54.92
Episode length: 92.62 +/- 54.84
New best mean reward!
FINISHED IN 376.1681007219595 s


starting seed  10463 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-145.30 +/- 133.26
Episode length: 146.19 +/- 132.96
New best mean reward!
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-85.24 +/- 22.28
Episode length: 86.24 +/- 22.28
New best mean reward!
FINISHED IN 191.73525817203335 s


starting seed  10464 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-365.15 +/- 166.48
Episode length: 365.55 +/- 165.99
New best mean reward!
Eval num_timesteps=8000, episode_reward=-314.73 +/- 167.71
Episode length: 315.29 +/- 167.22
New best mean reward!
Eval num_timesteps=8500, episode_reward=-170.51 +/- 64.04
Episode length: 171.48 +/- 63.89
New best mean reward!
Eval num_timesteps=9000, episode_reward=-166.04 +/- 50.05
Episode length: 167.03 +/- 49.98
New best mean reward!
Eval num_timesteps=9500, episode_reward=-180.87 +/- 79.49
Episode length: 181.82 +/- 79.29
Eval num_timesteps=10000, episode_reward=-176.65 +/- 68.53
Episode length: 177.62 +/- 68.39
Eval num_timesteps=10500, episode_reward=-173.80 +/- 76.76
Episode length: 174.76 +/- 76.59
Eval num_timesteps=11000, episode_reward=-164.00 +/- 21.42
Episode length: 165.00 +/- 21.42
New best mean reward!
Eval num_timesteps=11500, episode_reward=-176.49 +/- 68.34
Episode length: 177.46 +/- 68.20
Eval num_timesteps=12000, episode_reward=-195.22 +/- 100.66
Episode length: 196.13 +/- 100.38
Eval num_timesteps=12500, episode_reward=-256.54 +/- 148.72
Episode length: 257.28 +/- 148.30
Eval num_timesteps=13000, episode_reward=-245.58 +/- 143.00
Episode length: 246.36 +/- 142.61
Eval num_timesteps=13500, episode_reward=-170.76 +/- 39.85
Episode length: 171.76 +/- 39.85
Eval num_timesteps=14000, episode_reward=-180.09 +/- 62.88
Episode length: 181.07 +/- 62.78
Eval num_timesteps=14500, episode_reward=-269.87 +/- 147.61
Episode length: 270.60 +/- 147.19
Eval num_timesteps=15000, episode_reward=-187.62 +/- 78.76
Episode length: 188.58 +/- 78.60
Eval num_timesteps=15500, episode_reward=-163.76 +/- 40.14
Episode length: 164.76 +/- 40.14
New best mean reward!
Eval num_timesteps=16000, episode_reward=-257.24 +/- 147.80
Episode length: 257.99 +/- 147.39
Eval num_timesteps=16500, episode_reward=-173.12 +/- 66.43
Episode length: 174.10 +/- 66.33
Eval num_timesteps=17000, episode_reward=-256.59 +/- 150.13
Episode length: 257.34 +/- 149.72
Eval num_timesteps=17500, episode_reward=-280.23 +/- 160.70
Episode length: 280.89 +/- 160.23
Eval num_timesteps=18000, episode_reward=-277.35 +/- 158.79
Episode length: 278.03 +/- 158.34
Eval num_timesteps=18500, episode_reward=-227.22 +/- 127.71
Episode length: 228.06 +/- 127.37
Eval num_timesteps=19000, episode_reward=-217.25 +/- 128.33
Episode length: 218.09 +/- 127.97
Eval num_timesteps=19500, episode_reward=-212.23 +/- 115.00
Episode length: 213.11 +/- 114.70
Eval num_timesteps=20000, episode_reward=-183.16 +/- 77.57
Episode length: 184.13 +/- 77.44
Eval num_timesteps=20500, episode_reward=-207.66 +/- 113.80
Episode length: 208.54 +/- 113.49
Eval num_timesteps=21000, episode_reward=-193.98 +/- 94.83
Episode length: 194.92 +/- 94.64
Eval num_timesteps=21500, episode_reward=-163.33 +/- 38.81
Episode length: 164.33 +/- 38.81
New best mean reward!
Eval num_timesteps=22000, episode_reward=-271.05 +/- 152.56
Episode length: 271.78 +/- 152.16
Eval num_timesteps=22500, episode_reward=-251.99 +/- 143.41
Episode length: 252.78 +/- 143.04
Eval num_timesteps=23000, episode_reward=-356.47 +/- 164.03
Episode length: 356.92 +/- 163.55
Eval num_timesteps=23500, episode_reward=-316.16 +/- 165.85
Episode length: 316.73 +/- 165.37
Eval num_timesteps=24000, episode_reward=-312.47 +/- 168.21
Episode length: 313.04 +/- 167.73
Eval num_timesteps=24500, episode_reward=-375.21 +/- 163.63
Episode length: 375.59 +/- 163.16
Eval num_timesteps=25000, episode_reward=-392.87 +/- 154.86
Episode length: 393.20 +/- 154.40
Eval num_timesteps=25500, episode_reward=-459.94 +/- 105.14
Episode length: 460.07 +/- 104.81
Eval num_timesteps=26000, episode_reward=-402.59 +/- 159.94
Episode length: 402.87 +/- 159.50
Eval num_timesteps=26500, episode_reward=-204.97 +/- 175.27
Episode length: 205.72 +/- 174.85
Eval num_timesteps=27000, episode_reward=-146.79 +/- 135.05
Episode length: 147.67 +/- 134.74
New best mean reward!
Eval num_timesteps=27500, episode_reward=-156.20 +/- 146.14
Episode length: 157.07 +/- 145.83
Eval num_timesteps=28000, episode_reward=-118.83 +/- 108.43
Episode length: 119.76 +/- 108.18
New best mean reward!
Eval num_timesteps=28500, episode_reward=-92.28 +/- 46.33
Episode length: 93.27 +/- 46.25
New best mean reward!
FINISHED IN 600.0946932259831 s


starting seed  10465 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-484.37 +/- 76.66
Episode length: 484.41 +/- 76.47
New best mean reward!
Eval num_timesteps=12000, episode_reward=-168.33 +/- 152.07
Episode length: 169.17 +/- 151.72
New best mean reward!
Eval num_timesteps=12500, episode_reward=-116.25 +/- 94.71
Episode length: 117.21 +/- 94.55
New best mean reward!
Eval num_timesteps=13000, episode_reward=-124.73 +/- 119.49
Episode length: 125.64 +/- 119.21
Eval num_timesteps=13500, episode_reward=-108.59 +/- 92.45
Episode length: 109.54 +/- 92.24
New best mean reward!
Eval num_timesteps=14000, episode_reward=-102.60 +/- 57.86
Episode length: 103.59 +/- 57.79
New best mean reward!
Eval num_timesteps=14500, episode_reward=-134.92 +/- 126.49
Episode length: 135.82 +/- 126.20
Eval num_timesteps=15000, episode_reward=-95.27 +/- 45.47
Episode length: 96.26 +/- 45.38
New best mean reward!
FINISHED IN 332.80687964102253 s


starting seed  10466 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-93.66 +/- 51.95
Episode length: 94.65 +/- 51.88
New best mean reward!
FINISHED IN 15.31313142500585 s


starting seed  10467 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-442.59 +/- 142.34
Episode length: 442.73 +/- 141.99
New best mean reward!
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-472.24 +/- 101.24
Episode length: 472.31 +/- 100.98
Eval num_timesteps=3500, episode_reward=-349.24 +/- 193.10
Episode length: 349.62 +/- 192.62
New best mean reward!
Eval num_timesteps=4000, episode_reward=-373.09 +/- 185.23
Episode length: 373.41 +/- 184.76
Eval num_timesteps=4500, episode_reward=-496.03 +/- 39.50
Episode length: 496.04 +/- 39.40
Eval num_timesteps=5000, episode_reward=-496.19 +/- 37.91
Episode length: 496.20 +/- 37.81
Eval num_timesteps=5500, episode_reward=-411.74 +/- 166.29
Episode length: 411.96 +/- 165.88
Eval num_timesteps=6000, episode_reward=-176.11 +/- 147.57
Episode length: 176.95 +/- 147.22
New best mean reward!
Eval num_timesteps=6500, episode_reward=-242.74 +/- 189.65
Episode length: 243.40 +/- 189.19
Eval num_timesteps=7000, episode_reward=-233.44 +/- 177.82
Episode length: 234.14 +/- 177.37
Eval num_timesteps=7500, episode_reward=-345.35 +/- 194.26
Episode length: 345.74 +/- 193.77
Eval num_timesteps=8000, episode_reward=-162.77 +/- 131.92
Episode length: 163.65 +/- 131.61
New best mean reward!
Eval num_timesteps=8500, episode_reward=-96.70 +/- 17.33
Episode length: 97.70 +/- 17.33
New best mean reward!
FINISHED IN 167.05834071303252 s


starting seed  10468 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-427.31 +/- 135.99
Episode length: 427.54 +/- 135.58
New best mean reward!
Eval num_timesteps=1000, episode_reward=-168.18 +/- 56.74
Episode length: 169.16 +/- 56.62
New best mean reward!
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-467.81 +/- 96.71
Episode length: 467.91 +/- 96.41
Eval num_timesteps=2500, episode_reward=-408.01 +/- 144.60
Episode length: 408.30 +/- 144.14
Eval num_timesteps=3000, episode_reward=-443.56 +/- 119.99
Episode length: 443.75 +/- 119.61
Eval num_timesteps=3500, episode_reward=-351.78 +/- 166.46
Episode length: 352.23 +/- 165.97
Eval num_timesteps=4000, episode_reward=-266.93 +/- 155.09
Episode length: 267.63 +/- 154.64
Eval num_timesteps=4500, episode_reward=-174.40 +/- 67.15
Episode length: 175.37 +/- 67.00
Eval num_timesteps=5000, episode_reward=-177.62 +/- 64.07
Episode length: 178.60 +/- 63.96
Eval num_timesteps=5500, episode_reward=-175.89 +/- 66.56
Episode length: 176.86 +/- 66.42
Eval num_timesteps=6000, episode_reward=-182.33 +/- 84.14
Episode length: 183.27 +/- 83.91
Eval num_timesteps=6500, episode_reward=-176.99 +/- 64.81
Episode length: 177.97 +/- 64.71
Eval num_timesteps=7000, episode_reward=-166.57 +/- 46.81
Episode length: 167.57 +/- 46.81
New best mean reward!
Eval num_timesteps=7500, episode_reward=-160.55 +/- 29.65
Episode length: 161.55 +/- 29.65
New best mean reward!
Eval num_timesteps=8000, episode_reward=-162.89 +/- 46.77
Episode length: 163.88 +/- 46.70
Eval num_timesteps=8500, episode_reward=-170.28 +/- 51.99
Episode length: 171.28 +/- 51.99
Eval num_timesteps=9000, episode_reward=-162.74 +/- 32.48
Episode length: 163.74 +/- 32.48
Eval num_timesteps=9500, episode_reward=-166.66 +/- 40.78
Episode length: 167.66 +/- 40.78
Eval num_timesteps=10000, episode_reward=-166.92 +/- 35.07
Episode length: 167.92 +/- 35.07
Eval num_timesteps=10500, episode_reward=-158.79 +/- 30.33
Episode length: 159.79 +/- 30.33
New best mean reward!
Eval num_timesteps=11000, episode_reward=-164.42 +/- 45.19
Episode length: 165.41 +/- 45.12
Eval num_timesteps=11500, episode_reward=-186.37 +/- 86.32
Episode length: 187.31 +/- 86.10
Eval num_timesteps=12000, episode_reward=-178.18 +/- 57.16
Episode length: 179.16 +/- 57.05
Eval num_timesteps=12500, episode_reward=-163.79 +/- 43.37
Episode length: 164.79 +/- 43.37
Eval num_timesteps=13000, episode_reward=-157.34 +/- 23.12
Episode length: 158.34 +/- 23.12
New best mean reward!
Eval num_timesteps=13500, episode_reward=-163.25 +/- 39.96
Episode length: 164.25 +/- 39.96
Eval num_timesteps=14000, episode_reward=-166.76 +/- 47.28
Episode length: 167.75 +/- 47.21
Eval num_timesteps=14500, episode_reward=-166.87 +/- 46.76
Episode length: 167.87 +/- 46.76
Eval num_timesteps=15000, episode_reward=-169.23 +/- 45.71
Episode length: 170.23 +/- 45.71
Eval num_timesteps=15500, episode_reward=-167.19 +/- 49.29
Episode length: 168.19 +/- 49.29
Eval num_timesteps=16000, episode_reward=-158.42 +/- 33.15
Episode length: 159.42 +/- 33.15
Eval num_timesteps=16500, episode_reward=-163.44 +/- 39.23
Episode length: 164.44 +/- 39.23
Eval num_timesteps=17000, episode_reward=-167.11 +/- 39.89
Episode length: 168.11 +/- 39.89
Eval num_timesteps=17500, episode_reward=-162.63 +/- 34.84
Episode length: 163.63 +/- 34.84
Eval num_timesteps=18000, episode_reward=-161.55 +/- 31.33
Episode length: 162.55 +/- 31.33
Eval num_timesteps=18500, episode_reward=-158.57 +/- 29.53
Episode length: 159.57 +/- 29.53
Eval num_timesteps=19000, episode_reward=-169.64 +/- 37.04
Episode length: 170.64 +/- 37.04
Eval num_timesteps=19500, episode_reward=-167.49 +/- 45.81
Episode length: 168.49 +/- 45.81
Eval num_timesteps=20000, episode_reward=-173.83 +/- 42.09
Episode length: 174.83 +/- 42.09
Eval num_timesteps=20500, episode_reward=-166.72 +/- 43.31
Episode length: 167.71 +/- 43.23
Eval num_timesteps=21000, episode_reward=-171.34 +/- 47.85
Episode length: 172.33 +/- 47.78
Eval num_timesteps=21500, episode_reward=-169.50 +/- 33.67
Episode length: 170.50 +/- 33.67
Eval num_timesteps=22000, episode_reward=-170.72 +/- 40.15
Episode length: 171.72 +/- 40.15
Eval num_timesteps=22500, episode_reward=-164.90 +/- 39.86
Episode length: 165.90 +/- 39.86
Eval num_timesteps=23000, episode_reward=-172.55 +/- 40.92
Episode length: 173.55 +/- 40.92
Eval num_timesteps=23500, episode_reward=-169.44 +/- 55.56
Episode length: 170.43 +/- 55.50
Eval num_timesteps=24000, episode_reward=-165.31 +/- 35.08
Episode length: 166.31 +/- 35.08
Eval num_timesteps=24500, episode_reward=-182.53 +/- 68.31
Episode length: 183.50 +/- 68.17
Eval num_timesteps=25000, episode_reward=-175.19 +/- 64.87
Episode length: 176.16 +/- 64.72
Eval num_timesteps=25500, episode_reward=-177.21 +/- 62.30
Episode length: 178.20 +/- 62.24
Eval num_timesteps=26000, episode_reward=-181.67 +/- 61.44
Episode length: 182.66 +/- 61.38
Eval num_timesteps=26500, episode_reward=-168.06 +/- 45.54
Episode length: 169.05 +/- 45.47
Eval num_timesteps=27000, episode_reward=-169.31 +/- 45.09
Episode length: 170.30 +/- 45.02
Eval num_timesteps=27500, episode_reward=-172.96 +/- 48.45
Episode length: 173.96 +/- 48.45
Eval num_timesteps=28000, episode_reward=-169.66 +/- 37.93
Episode length: 170.66 +/- 37.93
Eval num_timesteps=28500, episode_reward=-161.72 +/- 29.39
Episode length: 162.72 +/- 29.39
Eval num_timesteps=29000, episode_reward=-178.04 +/- 54.64
Episode length: 179.03 +/- 54.58
Eval num_timesteps=29500, episode_reward=-161.93 +/- 34.13
Episode length: 162.93 +/- 34.13
Eval num_timesteps=30000, episode_reward=-168.30 +/- 48.61
Episode length: 169.29 +/- 48.54
Eval num_timesteps=30500, episode_reward=-164.28 +/- 34.61
Episode length: 165.28 +/- 34.61
Eval num_timesteps=31000, episode_reward=-163.70 +/- 40.30
Episode length: 164.70 +/- 40.30
Eval num_timesteps=31500, episode_reward=-170.78 +/- 44.10
Episode length: 171.78 +/- 44.10
Eval num_timesteps=32000, episode_reward=-164.70 +/- 35.35
Episode length: 165.70 +/- 35.35
Eval num_timesteps=32500, episode_reward=-170.25 +/- 50.32
Episode length: 171.24 +/- 50.25
Eval num_timesteps=33000, episode_reward=-166.19 +/- 48.84
Episode length: 167.18 +/- 48.77
Eval num_timesteps=33500, episode_reward=-162.17 +/- 34.78
Episode length: 163.17 +/- 34.78
Eval num_timesteps=34000, episode_reward=-160.41 +/- 30.34
Episode length: 161.41 +/- 30.34
Eval num_timesteps=34500, episode_reward=-169.89 +/- 51.80
Episode length: 170.89 +/- 51.80
Eval num_timesteps=35000, episode_reward=-160.99 +/- 24.44
Episode length: 161.99 +/- 24.44
Eval num_timesteps=35500, episode_reward=-161.26 +/- 33.50
Episode length: 162.26 +/- 33.50
Eval num_timesteps=36000, episode_reward=-164.94 +/- 36.07
Episode length: 165.94 +/- 36.07
Eval num_timesteps=36500, episode_reward=-162.97 +/- 31.80
Episode length: 163.97 +/- 31.80
Eval num_timesteps=37000, episode_reward=-161.91 +/- 40.17
Episode length: 162.91 +/- 40.17
Eval num_timesteps=37500, episode_reward=-169.18 +/- 43.20
Episode length: 170.18 +/- 43.20
Eval num_timesteps=38000, episode_reward=-160.55 +/- 33.70
Episode length: 161.55 +/- 33.70
Eval num_timesteps=38500, episode_reward=-158.96 +/- 23.99
Episode length: 159.96 +/- 23.99
Eval num_timesteps=39000, episode_reward=-159.54 +/- 26.01
Episode length: 160.54 +/- 26.01
Eval num_timesteps=39500, episode_reward=-161.00 +/- 28.22
Episode length: 162.00 +/- 28.22
Eval num_timesteps=40000, episode_reward=-160.87 +/- 40.74
Episode length: 161.87 +/- 40.74
Eval num_timesteps=40500, episode_reward=-157.81 +/- 32.96
Episode length: 158.81 +/- 32.96
Eval num_timesteps=41000, episode_reward=-165.81 +/- 33.97
Episode length: 166.81 +/- 33.97
Eval num_timesteps=41500, episode_reward=-163.64 +/- 45.85
Episode length: 164.64 +/- 45.85
Eval num_timesteps=42000, episode_reward=-163.67 +/- 40.91
Episode length: 164.67 +/- 40.91
Eval num_timesteps=42500, episode_reward=-160.84 +/- 30.99
Episode length: 161.84 +/- 30.99
Eval num_timesteps=43000, episode_reward=-160.27 +/- 36.16
Episode length: 161.27 +/- 36.16
Eval num_timesteps=43500, episode_reward=-161.24 +/- 30.94
Episode length: 162.24 +/- 30.94
Eval num_timesteps=44000, episode_reward=-163.98 +/- 48.41
Episode length: 164.97 +/- 48.34
Eval num_timesteps=44500, episode_reward=-169.59 +/- 49.39
Episode length: 170.59 +/- 49.39
Eval num_timesteps=45000, episode_reward=-165.00 +/- 32.27
Episode length: 166.00 +/- 32.27
Eval num_timesteps=45500, episode_reward=-157.75 +/- 31.01
Episode length: 158.75 +/- 31.01
Eval num_timesteps=46000, episode_reward=-155.48 +/- 25.90
Episode length: 156.48 +/- 25.90
New best mean reward!
Eval num_timesteps=46500, episode_reward=-163.28 +/- 35.68
Episode length: 164.28 +/- 35.68
Eval num_timesteps=47000, episode_reward=-167.97 +/- 44.72
Episode length: 168.97 +/- 44.72
Eval num_timesteps=47500, episode_reward=-161.35 +/- 31.26
Episode length: 162.35 +/- 31.26
Eval num_timesteps=48000, episode_reward=-161.41 +/- 40.53
Episode length: 162.41 +/- 40.53
Eval num_timesteps=48500, episode_reward=-159.58 +/- 32.59
Episode length: 160.58 +/- 32.59
Eval num_timesteps=49000, episode_reward=-157.33 +/- 33.80
Episode length: 158.33 +/- 33.80
Eval num_timesteps=49500, episode_reward=-159.99 +/- 28.40
Episode length: 160.99 +/- 28.40
Eval num_timesteps=50000, episode_reward=-156.58 +/- 26.90
Episode length: 157.58 +/- 26.90
FINISHED IN 624.3223258610233 s


starting seed  10469 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-352.28 +/- 175.57
Episode length: 352.70 +/- 175.08
New best mean reward!
Eval num_timesteps=10000, episode_reward=-196.81 +/- 152.00
Episode length: 197.63 +/- 151.64
New best mean reward!
Eval num_timesteps=10500, episode_reward=-139.03 +/- 119.40
Episode length: 139.94 +/- 119.13
New best mean reward!
Eval num_timesteps=11000, episode_reward=-104.29 +/- 71.71
Episode length: 105.27 +/- 71.60
New best mean reward!
Eval num_timesteps=11500, episode_reward=-109.72 +/- 74.19
Episode length: 110.69 +/- 74.03
Eval num_timesteps=12000, episode_reward=-116.62 +/- 89.18
Episode length: 117.59 +/- 89.05
Eval num_timesteps=12500, episode_reward=-100.21 +/- 53.20
Episode length: 101.20 +/- 53.13
New best mean reward!
Eval num_timesteps=13000, episode_reward=-88.84 +/- 25.06
Episode length: 89.84 +/- 25.06
New best mean reward!
FINISHED IN 310.7187323489925 s


starting seed  10470 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-486.71 +/- 65.13
Episode length: 486.75 +/- 64.94
New best mean reward!
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-464.88 +/- 100.25
Episode length: 464.99 +/- 99.93
New best mean reward!
Eval num_timesteps=7000, episode_reward=-460.45 +/- 107.42
Episode length: 460.57 +/- 107.09
New best mean reward!
Eval num_timesteps=7500, episode_reward=-191.14 +/- 120.97
Episode length: 192.01 +/- 120.64
New best mean reward!
Eval num_timesteps=8000, episode_reward=-183.66 +/- 169.81
Episode length: 184.44 +/- 169.40
New best mean reward!
Eval num_timesteps=8500, episode_reward=-81.35 +/- 11.88
Episode length: 82.35 +/- 11.88
New best mean reward!
FINISHED IN 210.98126450600103 s


starting seed  10471 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-488.06 +/- 67.99
Episode length: 488.09 +/- 67.82
New best mean reward!
Eval num_timesteps=5000, episode_reward=-197.83 +/- 174.81
Episode length: 198.59 +/- 174.40
New best mean reward!
Eval num_timesteps=5500, episode_reward=-153.25 +/- 136.63
Episode length: 154.12 +/- 136.30
New best mean reward!
Eval num_timesteps=6000, episode_reward=-127.91 +/- 70.56
Episode length: 128.89 +/- 70.46
New best mean reward!
Eval num_timesteps=6500, episode_reward=-92.13 +/- 49.76
Episode length: 93.12 +/- 49.67
New best mean reward!
FINISHED IN 160.61555029999 s


starting seed  10472 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-118.06 +/- 99.86
Episode length: 119.01 +/- 99.67
New best mean reward!
Eval num_timesteps=7500, episode_reward=-110.62 +/- 91.63
Episode length: 111.57 +/- 91.42
New best mean reward!
Eval num_timesteps=8000, episode_reward=-130.02 +/- 112.54
Episode length: 130.96 +/- 112.34
Eval num_timesteps=8500, episode_reward=-135.17 +/- 127.95
Episode length: 136.07 +/- 127.67
Eval num_timesteps=9000, episode_reward=-216.89 +/- 180.53
Episode length: 217.61 +/- 180.09
Eval num_timesteps=9500, episode_reward=-351.62 +/- 190.12
Episode length: 352.00 +/- 189.64
Eval num_timesteps=10000, episode_reward=-370.74 +/- 184.64
Episode length: 371.08 +/- 184.18
Eval num_timesteps=10500, episode_reward=-355.61 +/- 191.98
Episode length: 355.98 +/- 191.51
Eval num_timesteps=11000, episode_reward=-477.13 +/- 90.55
Episode length: 477.19 +/- 90.31
Eval num_timesteps=11500, episode_reward=-311.62 +/- 195.90
Episode length: 312.11 +/- 195.41
Eval num_timesteps=12000, episode_reward=-162.55 +/- 149.80
Episode length: 163.39 +/- 149.44
Eval num_timesteps=12500, episode_reward=-235.09 +/- 182.05
Episode length: 235.79 +/- 181.61
Eval num_timesteps=13000, episode_reward=-286.61 +/- 194.42
Episode length: 287.17 +/- 193.94
Eval num_timesteps=13500, episode_reward=-236.47 +/- 183.37
Episode length: 237.15 +/- 182.91
Eval num_timesteps=14000, episode_reward=-222.94 +/- 182.48
Episode length: 223.65 +/- 182.04
Eval num_timesteps=14500, episode_reward=-176.92 +/- 159.22
Episode length: 177.73 +/- 158.83
Eval num_timesteps=15000, episode_reward=-112.46 +/- 98.89
Episode length: 113.40 +/- 98.65
Eval num_timesteps=15500, episode_reward=-121.23 +/- 102.66
Episode length: 122.18 +/- 102.48
Eval num_timesteps=16000, episode_reward=-131.69 +/- 119.67
Episode length: 132.60 +/- 119.40
Eval num_timesteps=16500, episode_reward=-107.73 +/- 81.71
Episode length: 108.70 +/- 81.56
New best mean reward!
Eval num_timesteps=17000, episode_reward=-116.23 +/- 99.55
Episode length: 117.18 +/- 99.36
Eval num_timesteps=17500, episode_reward=-94.83 +/- 62.19
Episode length: 95.81 +/- 62.06
New best mean reward!
FINISHED IN 355.3001722219633 s


starting seed  10473 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-452.95 +/- 128.55
Episode length: 453.07 +/- 128.23
New best mean reward!
Eval num_timesteps=7000, episode_reward=-89.59 +/- 45.02
Episode length: 90.58 +/- 44.93
New best mean reward!
FINISHED IN 163.8110680800164 s


starting seed  10474 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-218.38 +/- 150.95
Episode length: 219.17 +/- 150.56
New best mean reward!
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-253.05 +/- 170.71
Episode length: 253.73 +/- 170.25
Eval num_timesteps=7500, episode_reward=-142.17 +/- 127.32
Episode length: 143.06 +/- 127.01
New best mean reward!
Eval num_timesteps=8000, episode_reward=-91.06 +/- 28.87
Episode length: 92.06 +/- 28.87
New best mean reward!
FINISHED IN 171.12646838801447 s


starting seed  10475 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-492.76 +/- 23.39
Episode length: 492.92 +/- 23.13
New best mean reward!
Eval num_timesteps=1500, episode_reward=-268.04 +/- 145.15
Episode length: 268.79 +/- 144.76
New best mean reward!
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-250.34 +/- 92.28
Episode length: 251.30 +/- 92.17
New best mean reward!
Eval num_timesteps=8000, episode_reward=-435.40 +/- 87.90
Episode length: 435.90 +/- 87.54
Eval num_timesteps=8500, episode_reward=-195.55 +/- 68.71
Episode length: 196.53 +/- 68.62
New best mean reward!
Eval num_timesteps=9000, episode_reward=-110.64 +/- 38.81
Episode length: 111.64 +/- 38.81
New best mean reward!
Eval num_timesteps=9500, episode_reward=-97.57 +/- 46.14
Episode length: 98.56 +/- 46.06
New best mean reward!
FINISHED IN 238.7555507030338 s


starting seed  10476 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-440.05 +/- 142.76
Episode length: 440.20 +/- 142.41
New best mean reward!
Eval num_timesteps=8000, episode_reward=-119.23 +/- 91.56
Episode length: 120.18 +/- 91.35
New best mean reward!
Eval num_timesteps=8500, episode_reward=-207.97 +/- 183.35
Episode length: 208.69 +/- 182.90
Eval num_timesteps=9000, episode_reward=-113.83 +/- 44.80
Episode length: 114.82 +/- 44.71
New best mean reward!
Eval num_timesteps=9500, episode_reward=-99.53 +/- 71.09
Episode length: 100.51 +/- 70.98
New best mean reward!
FINISHED IN 246.9993858290254 s


starting seed  10477 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-492.88 +/- 49.88
Episode length: 492.90 +/- 49.74
New best mean reward!
Eval num_timesteps=4500, episode_reward=-347.32 +/- 164.39
Episode length: 347.80 +/- 163.90
New best mean reward!
Eval num_timesteps=5000, episode_reward=-327.30 +/- 170.67
Episode length: 327.82 +/- 170.19
New best mean reward!
Eval num_timesteps=5500, episode_reward=-207.27 +/- 116.19
Episode length: 208.16 +/- 115.91
New best mean reward!
Eval num_timesteps=6000, episode_reward=-167.09 +/- 55.28
Episode length: 168.07 +/- 55.16
New best mean reward!
Eval num_timesteps=6500, episode_reward=-335.28 +/- 166.17
Episode length: 335.78 +/- 165.68
Eval num_timesteps=7000, episode_reward=-339.31 +/- 162.69
Episode length: 339.82 +/- 162.20
Eval num_timesteps=7500, episode_reward=-208.77 +/- 123.80
Episode length: 209.62 +/- 123.45
Eval num_timesteps=8000, episode_reward=-156.55 +/- 87.40
Episode length: 157.50 +/- 87.20
New best mean reward!
Eval num_timesteps=8500, episode_reward=-132.90 +/- 70.79
Episode length: 133.87 +/- 70.64
New best mean reward!
Eval num_timesteps=9000, episode_reward=-155.95 +/- 43.95
Episode length: 156.94 +/- 43.87
Eval num_timesteps=9500, episode_reward=-139.15 +/- 28.79
Episode length: 140.15 +/- 28.79
Eval num_timesteps=10000, episode_reward=-148.97 +/- 35.36
Episode length: 149.97 +/- 35.36
Eval num_timesteps=10500, episode_reward=-155.14 +/- 54.25
Episode length: 156.13 +/- 54.19
Eval num_timesteps=11000, episode_reward=-142.45 +/- 50.24
Episode length: 143.44 +/- 50.17
Eval num_timesteps=11500, episode_reward=-130.50 +/- 34.46
Episode length: 131.50 +/- 34.46
New best mean reward!
Eval num_timesteps=12000, episode_reward=-116.75 +/- 35.55
Episode length: 117.75 +/- 35.55
New best mean reward!
Eval num_timesteps=12500, episode_reward=-118.63 +/- 46.25
Episode length: 119.62 +/- 46.17
Eval num_timesteps=13000, episode_reward=-103.45 +/- 24.63
Episode length: 104.45 +/- 24.63
New best mean reward!
Eval num_timesteps=13500, episode_reward=-158.75 +/- 110.74
Episode length: 159.67 +/- 110.49
Eval num_timesteps=14000, episode_reward=-168.16 +/- 128.24
Episode length: 169.06 +/- 127.98
Eval num_timesteps=14500, episode_reward=-168.04 +/- 124.88
Episode length: 168.93 +/- 124.59
Eval num_timesteps=15000, episode_reward=-134.22 +/- 97.93
Episode length: 135.17 +/- 97.74
Eval num_timesteps=15500, episode_reward=-149.20 +/- 136.46
Episode length: 150.08 +/- 136.15
Eval num_timesteps=16000, episode_reward=-130.26 +/- 100.38
Episode length: 131.20 +/- 100.16
Eval num_timesteps=16500, episode_reward=-168.19 +/- 142.98
Episode length: 169.04 +/- 142.63
Eval num_timesteps=17000, episode_reward=-304.34 +/- 192.24
Episode length: 304.86 +/- 191.76
Eval num_timesteps=17500, episode_reward=-240.00 +/- 176.24
Episode length: 240.70 +/- 175.80
Eval num_timesteps=18000, episode_reward=-197.02 +/- 152.82
Episode length: 197.83 +/- 152.44
Eval num_timesteps=18500, episode_reward=-206.00 +/- 172.25
Episode length: 206.76 +/- 171.84
Eval num_timesteps=19000, episode_reward=-196.20 +/- 174.17
Episode length: 196.97 +/- 173.77
Eval num_timesteps=19500, episode_reward=-142.57 +/- 115.90
Episode length: 143.48 +/- 115.63
Eval num_timesteps=20000, episode_reward=-145.54 +/- 144.41
Episode length: 146.40 +/- 144.07
Eval num_timesteps=20500, episode_reward=-128.06 +/- 121.63
Episode length: 128.97 +/- 121.36
Eval num_timesteps=21000, episode_reward=-100.64 +/- 73.95
Episode length: 101.61 +/- 73.79
New best mean reward!
Eval num_timesteps=21500, episode_reward=-108.18 +/- 95.43
Episode length: 109.13 +/- 95.23
Eval num_timesteps=22000, episode_reward=-123.66 +/- 116.33
Episode length: 124.58 +/- 116.07
Eval num_timesteps=22500, episode_reward=-107.06 +/- 85.19
Episode length: 108.02 +/- 85.01
Eval num_timesteps=23000, episode_reward=-106.69 +/- 87.00
Episode length: 107.65 +/- 86.82
Eval num_timesteps=23500, episode_reward=-101.30 +/- 84.18
Episode length: 102.26 +/- 83.99
Eval num_timesteps=24000, episode_reward=-104.75 +/- 87.08
Episode length: 105.72 +/- 86.94
Eval num_timesteps=24500, episode_reward=-110.56 +/- 95.20
Episode length: 111.51 +/- 94.99
Eval num_timesteps=25000, episode_reward=-83.30 +/- 25.10
Episode length: 84.30 +/- 25.10
New best mean reward!
FINISHED IN 336.8984090019949 s


starting seed  10478 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-351.86 +/- 186.19
Episode length: 352.25 +/- 185.71
New best mean reward!
Eval num_timesteps=12000, episode_reward=-328.27 +/- 188.07
Episode length: 328.73 +/- 187.58
New best mean reward!
Eval num_timesteps=12500, episode_reward=-152.05 +/- 119.09
Episode length: 152.95 +/- 118.79
New best mean reward!
Eval num_timesteps=13000, episode_reward=-168.73 +/- 130.63
Episode length: 169.61 +/- 130.33
Eval num_timesteps=13500, episode_reward=-105.22 +/- 43.52
Episode length: 106.21 +/- 43.43
New best mean reward!
Eval num_timesteps=14000, episode_reward=-106.43 +/- 45.46
Episode length: 107.42 +/- 45.38
Eval num_timesteps=14500, episode_reward=-123.64 +/- 79.59
Episode length: 124.60 +/- 79.41
Eval num_timesteps=15000, episode_reward=-107.03 +/- 44.47
Episode length: 108.02 +/- 44.38
Eval num_timesteps=15500, episode_reward=-104.82 +/- 36.10
Episode length: 105.82 +/- 36.10
New best mean reward!
Eval num_timesteps=16000, episode_reward=-111.43 +/- 28.39
Episode length: 112.43 +/- 28.39
Eval num_timesteps=16500, episode_reward=-102.93 +/- 28.47
Episode length: 103.93 +/- 28.47
New best mean reward!
Eval num_timesteps=17000, episode_reward=-101.22 +/- 18.09
Episode length: 102.22 +/- 18.09
New best mean reward!
Eval num_timesteps=17500, episode_reward=-100.45 +/- 22.44
Episode length: 101.45 +/- 22.44
New best mean reward!
Eval num_timesteps=18000, episode_reward=-105.84 +/- 31.35
Episode length: 106.84 +/- 31.35
Eval num_timesteps=18500, episode_reward=-105.94 +/- 34.89
Episode length: 106.94 +/- 34.89
Eval num_timesteps=19000, episode_reward=-100.26 +/- 25.67
Episode length: 101.26 +/- 25.67
New best mean reward!
Eval num_timesteps=19500, episode_reward=-91.02 +/- 18.33
Episode length: 92.02 +/- 18.33
New best mean reward!
FINISHED IN 372.4999580270378 s


starting seed  10479 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-84.15 +/- 21.90
Episode length: 85.15 +/- 21.90
New best mean reward!
FINISHED IN 183.12443969695596 s


starting seed  10480 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-311.17 +/- 112.24
Episode length: 311.93 +/- 111.84
New best mean reward!
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-463.98 +/- 109.76
Episode length: 464.09 +/- 109.47
Eval num_timesteps=7000, episode_reward=-413.26 +/- 159.84
Episode length: 413.49 +/- 159.42
Eval num_timesteps=7500, episode_reward=-240.81 +/- 188.18
Episode length: 241.47 +/- 187.71
New best mean reward!
Eval num_timesteps=8000, episode_reward=-233.88 +/- 189.31
Episode length: 234.56 +/- 188.87
New best mean reward!
Eval num_timesteps=8500, episode_reward=-167.30 +/- 148.71
Episode length: 168.14 +/- 148.35
New best mean reward!
Eval num_timesteps=9000, episode_reward=-144.52 +/- 127.98
Episode length: 145.41 +/- 127.67
New best mean reward!
Eval num_timesteps=9500, episode_reward=-319.55 +/- 196.16
Episode length: 320.01 +/- 195.66
Eval num_timesteps=10000, episode_reward=-126.15 +/- 104.75
Episode length: 127.08 +/- 104.50
New best mean reward!
Eval num_timesteps=10500, episode_reward=-194.30 +/- 169.83
Episode length: 195.07 +/- 169.41
Eval num_timesteps=11000, episode_reward=-176.45 +/- 145.97
Episode length: 177.29 +/- 145.62
Eval num_timesteps=11500, episode_reward=-100.00 +/- 16.50
Episode length: 101.00 +/- 16.50
New best mean reward!
FINISHED IN 239.23860453703674 s


starting seed  10481 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-488.94 +/- 29.43
Episode length: 489.14 +/- 29.13
New best mean reward!
Eval num_timesteps=9000, episode_reward=-499.97 +/- 0.30
Episode length: 499.98 +/- 0.20
Eval num_timesteps=9500, episode_reward=-181.37 +/- 45.45
Episode length: 182.36 +/- 45.38
New best mean reward!
Eval num_timesteps=10000, episode_reward=-148.98 +/- 38.69
Episode length: 149.98 +/- 38.69
New best mean reward!
Eval num_timesteps=10500, episode_reward=-82.70 +/- 30.56
Episode length: 83.70 +/- 30.56
New best mean reward!
FINISHED IN 256.30995286698453 s


starting seed  10482 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=13000, episode_reward=-488.01 +/- 68.18
Episode length: 488.04 +/- 68.01
New best mean reward!
Eval num_timesteps=13500, episode_reward=-384.36 +/- 183.71
Episode length: 384.65 +/- 183.26
New best mean reward!
Eval num_timesteps=14000, episode_reward=-343.65 +/- 193.25
Episode length: 344.05 +/- 192.76
New best mean reward!
Eval num_timesteps=14500, episode_reward=-277.55 +/- 202.63
Episode length: 278.10 +/- 202.14
New best mean reward!
Eval num_timesteps=15000, episode_reward=-300.10 +/- 198.13
Episode length: 300.62 +/- 197.65
Eval num_timesteps=15500, episode_reward=-223.00 +/- 184.23
Episode length: 223.70 +/- 183.78
New best mean reward!
Eval num_timesteps=16000, episode_reward=-255.31 +/- 194.11
Episode length: 255.94 +/- 193.65
Eval num_timesteps=16500, episode_reward=-275.70 +/- 198.38
Episode length: 276.27 +/- 197.89
Eval num_timesteps=17000, episode_reward=-267.20 +/- 198.73
Episode length: 267.79 +/- 198.25
Eval num_timesteps=17500, episode_reward=-257.52 +/- 200.42
Episode length: 258.12 +/- 199.94
Eval num_timesteps=18000, episode_reward=-184.26 +/- 169.39
Episode length: 185.05 +/- 169.00
New best mean reward!
Eval num_timesteps=18500, episode_reward=-170.00 +/- 150.47
Episode length: 170.84 +/- 150.12
New best mean reward!
Eval num_timesteps=19000, episode_reward=-136.05 +/- 125.38
Episode length: 136.95 +/- 125.09
New best mean reward!
Eval num_timesteps=19500, episode_reward=-144.75 +/- 128.92
Episode length: 145.65 +/- 128.64
Eval num_timesteps=20000, episode_reward=-139.47 +/- 129.79
Episode length: 140.36 +/- 129.48
Eval num_timesteps=20500, episode_reward=-135.06 +/- 121.72
Episode length: 135.97 +/- 121.45
New best mean reward!
Eval num_timesteps=21000, episode_reward=-119.76 +/- 98.96
Episode length: 120.71 +/- 98.77
New best mean reward!
Eval num_timesteps=21500, episode_reward=-137.58 +/- 122.77
Episode length: 138.49 +/- 122.51
Eval num_timesteps=22000, episode_reward=-122.88 +/- 99.99
Episode length: 123.82 +/- 99.76
Eval num_timesteps=22500, episode_reward=-120.86 +/- 98.35
Episode length: 121.81 +/- 98.15
Eval num_timesteps=23000, episode_reward=-110.62 +/- 84.56
Episode length: 111.58 +/- 84.38
New best mean reward!
Eval num_timesteps=23500, episode_reward=-113.45 +/- 86.47
Episode length: 114.41 +/- 86.29
Eval num_timesteps=24000, episode_reward=-103.20 +/- 66.22
Episode length: 104.18 +/- 66.10
New best mean reward!
Eval num_timesteps=24500, episode_reward=-110.27 +/- 84.38
Episode length: 111.23 +/- 84.20
Eval num_timesteps=25000, episode_reward=-97.63 +/- 61.54
Episode length: 98.61 +/- 61.41
New best mean reward!
FINISHED IN 606.6095728719956 s


starting seed  10483 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-271.28 +/- 197.71
Episode length: 271.86 +/- 197.23
New best mean reward!
Eval num_timesteps=11500, episode_reward=-226.34 +/- 187.42
Episode length: 227.03 +/- 186.97
New best mean reward!
Eval num_timesteps=12000, episode_reward=-231.08 +/- 176.20
Episode length: 231.79 +/- 175.76
Eval num_timesteps=12500, episode_reward=-282.15 +/- 187.47
Episode length: 282.74 +/- 186.99
Eval num_timesteps=13000, episode_reward=-241.69 +/- 178.03
Episode length: 242.38 +/- 177.58
Eval num_timesteps=13500, episode_reward=-259.81 +/- 177.52
Episode length: 260.46 +/- 177.05
Eval num_timesteps=14000, episode_reward=-343.35 +/- 188.41
Episode length: 343.76 +/- 187.92
Eval num_timesteps=14500, episode_reward=-326.65 +/- 190.90
Episode length: 327.11 +/- 190.41
Eval num_timesteps=15000, episode_reward=-258.65 +/- 186.50
Episode length: 259.29 +/- 186.03
Eval num_timesteps=15500, episode_reward=-308.88 +/- 195.26
Episode length: 309.38 +/- 194.77
Eval num_timesteps=16000, episode_reward=-191.08 +/- 175.45
Episode length: 191.84 +/- 175.03
New best mean reward!
Eval num_timesteps=16500, episode_reward=-186.44 +/- 165.73
Episode length: 187.23 +/- 165.34
New best mean reward!
Eval num_timesteps=17000, episode_reward=-202.59 +/- 175.07
Episode length: 203.34 +/- 174.64
Eval num_timesteps=17500, episode_reward=-199.85 +/- 175.89
Episode length: 200.60 +/- 175.46
Eval num_timesteps=18000, episode_reward=-167.55 +/- 130.35
Episode length: 168.42 +/- 130.02
New best mean reward!
Eval num_timesteps=18500, episode_reward=-189.38 +/- 134.01
Episode length: 190.24 +/- 133.69
Eval num_timesteps=19000, episode_reward=-176.65 +/- 130.98
Episode length: 177.52 +/- 130.66
Eval num_timesteps=19500, episode_reward=-127.05 +/- 71.58
Episode length: 128.02 +/- 71.43
New best mean reward!
Eval num_timesteps=20000, episode_reward=-142.28 +/- 108.68
Episode length: 143.20 +/- 108.42
Eval num_timesteps=20500, episode_reward=-152.28 +/- 116.00
Episode length: 153.19 +/- 115.73
Eval num_timesteps=21000, episode_reward=-120.83 +/- 90.20
Episode length: 121.78 +/- 89.99
New best mean reward!
Eval num_timesteps=21500, episode_reward=-110.33 +/- 83.04
Episode length: 111.29 +/- 82.85
New best mean reward!
Eval num_timesteps=22000, episode_reward=-114.71 +/- 107.75
Episode length: 115.64 +/- 107.50
Eval num_timesteps=22500, episode_reward=-106.89 +/- 70.02
Episode length: 107.87 +/- 69.91
New best mean reward!
Eval num_timesteps=23000, episode_reward=-99.18 +/- 60.85
Episode length: 100.16 +/- 60.72
New best mean reward!
FINISHED IN 503.2593028510455 s


starting seed  10484 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-455.78 +/- 125.99
Episode length: 455.89 +/- 125.68
New best mean reward!
Eval num_timesteps=8500, episode_reward=-99.91 +/- 71.94
Episode length: 100.89 +/- 71.83
New best mean reward!
FINISHED IN 228.25889304705197 s


starting seed  10485 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-486.76 +/- 40.98
Episode length: 486.88 +/- 40.69
New best mean reward!
Eval num_timesteps=9500, episode_reward=-145.70 +/- 76.62
Episode length: 146.66 +/- 76.43
New best mean reward!
Eval num_timesteps=10000, episode_reward=-353.40 +/- 102.62
Episode length: 354.21 +/- 102.35
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-453.84 +/- 72.35
Episode length: 454.19 +/- 71.94
Eval num_timesteps=11500, episode_reward=-477.17 +/- 48.05
Episode length: 477.40 +/- 47.68
Eval num_timesteps=12000, episode_reward=-497.59 +/- 13.84
Episode length: 497.63 +/- 13.67
Eval num_timesteps=12500, episode_reward=-467.52 +/- 62.53
Episode length: 467.81 +/- 62.16
Eval num_timesteps=13000, episode_reward=-342.48 +/- 84.29
Episode length: 343.31 +/- 83.97
Eval num_timesteps=13500, episode_reward=-209.80 +/- 87.13
Episode length: 210.74 +/- 86.93
Eval num_timesteps=14000, episode_reward=-243.69 +/- 123.62
Episode length: 244.51 +/- 123.24
Eval num_timesteps=14500, episode_reward=-219.20 +/- 109.98
Episode length: 220.08 +/- 109.68
Eval num_timesteps=15000, episode_reward=-268.52 +/- 137.83
Episode length: 269.28 +/- 137.43
Eval num_timesteps=15500, episode_reward=-343.52 +/- 147.56
Episode length: 344.06 +/- 147.07
Eval num_timesteps=16000, episode_reward=-211.98 +/- 128.06
Episode length: 212.83 +/- 127.72
Eval num_timesteps=16500, episode_reward=-201.27 +/- 120.13
Episode length: 202.14 +/- 119.80
Eval num_timesteps=17000, episode_reward=-251.49 +/- 146.52
Episode length: 252.26 +/- 146.13
Eval num_timesteps=17500, episode_reward=-243.18 +/- 147.63
Episode length: 243.94 +/- 147.21
Eval num_timesteps=18000, episode_reward=-240.42 +/- 138.36
Episode length: 241.21 +/- 137.97
Eval num_timesteps=18500, episode_reward=-242.12 +/- 132.02
Episode length: 242.92 +/- 131.63
Eval num_timesteps=19000, episode_reward=-260.82 +/- 136.05
Episode length: 261.59 +/- 135.64
Eval num_timesteps=19500, episode_reward=-181.46 +/- 28.28
Episode length: 182.46 +/- 28.28
Eval num_timesteps=20000, episode_reward=-165.73 +/- 47.30
Episode length: 166.73 +/- 47.30
Eval num_timesteps=20500, episode_reward=-162.77 +/- 51.70
Episode length: 163.76 +/- 51.63
Eval num_timesteps=21000, episode_reward=-154.99 +/- 34.65
Episode length: 155.99 +/- 34.65
Eval num_timesteps=21500, episode_reward=-157.68 +/- 40.16
Episode length: 158.68 +/- 40.16
Eval num_timesteps=22000, episode_reward=-166.63 +/- 67.17
Episode length: 167.60 +/- 67.02
Eval num_timesteps=22500, episode_reward=-169.78 +/- 67.32
Episode length: 170.75 +/- 67.17
Eval num_timesteps=23000, episode_reward=-106.92 +/- 58.39
Episode length: 107.91 +/- 58.32
New best mean reward!
Eval num_timesteps=23500, episode_reward=-88.93 +/- 23.82
Episode length: 89.93 +/- 23.82
New best mean reward!
FINISHED IN 474.75632838800084 s


starting seed  10486 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-106.95 +/- 71.37
Episode length: 107.93 +/- 71.26
New best mean reward!
Eval num_timesteps=10000, episode_reward=-85.40 +/- 22.80
Episode length: 86.40 +/- 22.80
New best mean reward!
FINISHED IN 254.50809225195553 s


starting seed  10487 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-434.84 +/- 114.71
Episode length: 435.10 +/- 114.29
New best mean reward!
Eval num_timesteps=3000, episode_reward=-494.93 +/- 19.93
Episode length: 495.03 +/- 19.70
Eval num_timesteps=3500, episode_reward=-177.65 +/- 42.82
Episode length: 178.64 +/- 42.74
New best mean reward!
Eval num_timesteps=4000, episode_reward=-248.85 +/- 129.03
Episode length: 249.65 +/- 128.65
Eval num_timesteps=4500, episode_reward=-459.91 +/- 74.84
Episode length: 460.19 +/- 74.46
Eval num_timesteps=5000, episode_reward=-497.95 +/- 13.68
Episode length: 497.98 +/- 13.54
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-423.75 +/- 101.39
Episode length: 424.16 +/- 100.95
Eval num_timesteps=7000, episode_reward=-385.73 +/- 132.07
Episode length: 386.19 +/- 131.60
Eval num_timesteps=7500, episode_reward=-448.80 +/- 87.13
Episode length: 449.11 +/- 86.72
Eval num_timesteps=8000, episode_reward=-344.21 +/- 103.05
Episode length: 345.00 +/- 102.73
Eval num_timesteps=8500, episode_reward=-303.62 +/- 80.82
Episode length: 304.54 +/- 80.62
Eval num_timesteps=9000, episode_reward=-277.39 +/- 106.29
Episode length: 278.27 +/- 106.04
Eval num_timesteps=9500, episode_reward=-237.70 +/- 73.30
Episode length: 238.69 +/- 73.26
Eval num_timesteps=10000, episode_reward=-131.05 +/- 28.70
Episode length: 132.05 +/- 28.70
New best mean reward!
Eval num_timesteps=10500, episode_reward=-160.92 +/- 42.81
Episode length: 161.92 +/- 42.81
Eval num_timesteps=11000, episode_reward=-95.10 +/- 31.72
Episode length: 96.10 +/- 31.72
New best mean reward!
FINISHED IN 215.32229582901346 s


starting seed  10488 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-387.52 +/- 161.16
Episode length: 387.85 +/- 160.70
New best mean reward!
Eval num_timesteps=10500, episode_reward=-356.81 +/- 187.77
Episode length: 357.18 +/- 187.29
New best mean reward!
Eval num_timesteps=11000, episode_reward=-206.19 +/- 184.35
Episode length: 206.92 +/- 183.92
New best mean reward!
Eval num_timesteps=11500, episode_reward=-109.18 +/- 92.67
Episode length: 110.13 +/- 92.46
New best mean reward!
Eval num_timesteps=12000, episode_reward=-129.06 +/- 121.01
Episode length: 129.97 +/- 120.74
Eval num_timesteps=12500, episode_reward=-115.14 +/- 107.28
Episode length: 116.07 +/- 107.03
Eval num_timesteps=13000, episode_reward=-129.37 +/- 121.79
Episode length: 130.28 +/- 121.51
Eval num_timesteps=13500, episode_reward=-112.55 +/- 92.22
Episode length: 113.51 +/- 92.05
Eval num_timesteps=14000, episode_reward=-89.14 +/- 28.50
Episode length: 90.14 +/- 28.50
New best mean reward!
FINISHED IN 365.8933531320072 s


starting seed  10489 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-90.87 +/- 19.60
Episode length: 91.87 +/- 19.60
New best mean reward!
FINISHED IN 188.24675854301313 s


starting seed  10490 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-454.31 +/- 108.87
Episode length: 454.46 +/- 108.52
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-479.70 +/- 80.58
Episode length: 479.76 +/- 80.34
Eval num_timesteps=9000, episode_reward=-92.93 +/- 28.41
Episode length: 93.93 +/- 28.41
New best mean reward!
FINISHED IN 286.96283209201647 s


starting seed  10491 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-347.72 +/- 172.70
Episode length: 348.17 +/- 172.21
New best mean reward!
Eval num_timesteps=9000, episode_reward=-105.74 +/- 20.87
Episode length: 106.74 +/- 20.87
New best mean reward!
Eval num_timesteps=9500, episode_reward=-116.48 +/- 22.22
Episode length: 117.48 +/- 22.22
Eval num_timesteps=10000, episode_reward=-118.26 +/- 88.94
Episode length: 119.21 +/- 88.73
Eval num_timesteps=10500, episode_reward=-95.58 +/- 29.67
Episode length: 96.58 +/- 29.67
New best mean reward!
FINISHED IN 299.45136743498733 s


starting seed  10492 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-470.49 +/- 51.06
Episode length: 470.83 +/- 50.68
New best mean reward!
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-398.43 +/- 85.87
Episode length: 399.18 +/- 85.57
New best mean reward!
Eval num_timesteps=8000, episode_reward=-229.60 +/- 105.78
Episode length: 230.52 +/- 105.57
New best mean reward!
Eval num_timesteps=8500, episode_reward=-133.61 +/- 54.93
Episode length: 134.60 +/- 54.86
New best mean reward!
Eval num_timesteps=9000, episode_reward=-120.99 +/- 35.85
Episode length: 121.99 +/- 35.85
New best mean reward!
Eval num_timesteps=9500, episode_reward=-144.79 +/- 61.90
Episode length: 145.78 +/- 61.84
Eval num_timesteps=10000, episode_reward=-237.19 +/- 182.19
Episode length: 237.87 +/- 181.73
Eval num_timesteps=10500, episode_reward=-274.80 +/- 204.01
Episode length: 275.36 +/- 203.53
Eval num_timesteps=11000, episode_reward=-492.45 +/- 52.85
Episode length: 492.47 +/- 52.71
Eval num_timesteps=11500, episode_reward=-264.17 +/- 190.41
Episode length: 264.78 +/- 189.93
Eval num_timesteps=12000, episode_reward=-163.01 +/- 142.64
Episode length: 163.88 +/- 142.34
Eval num_timesteps=12500, episode_reward=-120.08 +/- 91.94
Episode length: 121.03 +/- 91.73
New best mean reward!
Eval num_timesteps=13000, episode_reward=-113.63 +/- 93.75
Episode length: 114.58 +/- 93.55
New best mean reward!
Eval num_timesteps=13500, episode_reward=-97.40 +/- 60.88
Episode length: 98.38 +/- 60.75
New best mean reward!
FINISHED IN 258.8294052060228 s


starting seed  10493 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-480.80 +/- 70.38
Episode length: 480.87 +/- 70.12
New best mean reward!
Eval num_timesteps=7000, episode_reward=-295.14 +/- 139.51
Episode length: 295.89 +/- 139.15
New best mean reward!
Eval num_timesteps=7500, episode_reward=-352.73 +/- 113.71
Episode length: 353.53 +/- 113.45
Eval num_timesteps=8000, episode_reward=-207.06 +/- 64.51
Episode length: 208.04 +/- 64.42
New best mean reward!
Eval num_timesteps=8500, episode_reward=-152.56 +/- 118.03
Episode length: 153.46 +/- 117.73
New best mean reward!
Eval num_timesteps=9000, episode_reward=-231.06 +/- 169.68
Episode length: 231.78 +/- 169.23
Eval num_timesteps=9500, episode_reward=-235.81 +/- 132.76
Episode length: 236.64 +/- 132.42
Eval num_timesteps=10000, episode_reward=-204.44 +/- 91.60
Episode length: 205.37 +/- 91.37
Eval num_timesteps=10500, episode_reward=-192.81 +/- 64.88
Episode length: 193.79 +/- 64.78
Eval num_timesteps=11000, episode_reward=-185.72 +/- 73.77
Episode length: 186.69 +/- 73.64
Eval num_timesteps=11500, episode_reward=-196.10 +/- 84.45
Episode length: 197.04 +/- 84.23
Eval num_timesteps=12000, episode_reward=-136.58 +/- 32.31
Episode length: 137.58 +/- 32.31
New best mean reward!
Eval num_timesteps=12500, episode_reward=-129.69 +/- 58.91
Episode length: 130.67 +/- 58.79
New best mean reward!
Eval num_timesteps=13000, episode_reward=-124.26 +/- 23.61
Episode length: 125.26 +/- 23.61
New best mean reward!
Eval num_timesteps=13500, episode_reward=-117.02 +/- 24.42
Episode length: 118.02 +/- 24.42
New best mean reward!
Eval num_timesteps=14000, episode_reward=-110.34 +/- 25.79
Episode length: 111.34 +/- 25.79
New best mean reward!
Eval num_timesteps=14500, episode_reward=-124.37 +/- 40.78
Episode length: 125.37 +/- 40.78
Eval num_timesteps=15000, episode_reward=-107.02 +/- 19.83
Episode length: 108.02 +/- 19.83
New best mean reward!
Eval num_timesteps=15500, episode_reward=-122.57 +/- 54.07
Episode length: 123.57 +/- 54.07
Eval num_timesteps=16000, episode_reward=-116.99 +/- 41.28
Episode length: 117.99 +/- 41.28
Eval num_timesteps=16500, episode_reward=-101.12 +/- 47.58
Episode length: 102.11 +/- 47.49
New best mean reward!
Eval num_timesteps=17000, episode_reward=-88.27 +/- 20.93
Episode length: 89.27 +/- 20.93
New best mean reward!
FINISHED IN 286.38446816505166 s


starting seed  10494 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-462.40 +/- 107.40
Episode length: 462.51 +/- 107.09
New best mean reward!
Eval num_timesteps=9500, episode_reward=-485.97 +/- 68.89
Episode length: 486.01 +/- 68.69
Eval num_timesteps=10000, episode_reward=-468.72 +/- 94.65
Episode length: 468.82 +/- 94.36
Eval num_timesteps=10500, episode_reward=-458.98 +/- 111.42
Episode length: 459.10 +/- 111.10
New best mean reward!
Eval num_timesteps=11000, episode_reward=-390.65 +/- 179.94
Episode length: 390.92 +/- 179.50
New best mean reward!
Eval num_timesteps=11500, episode_reward=-320.44 +/- 202.95
Episode length: 320.88 +/- 202.45
New best mean reward!
Eval num_timesteps=12000, episode_reward=-323.52 +/- 200.65
Episode length: 323.96 +/- 200.16
Eval num_timesteps=12500, episode_reward=-301.47 +/- 202.92
Episode length: 301.96 +/- 202.42
New best mean reward!
Eval num_timesteps=13000, episode_reward=-200.86 +/- 181.45
Episode length: 201.60 +/- 181.02
New best mean reward!
Eval num_timesteps=13500, episode_reward=-171.41 +/- 158.80
Episode length: 172.23 +/- 158.42
New best mean reward!
Eval num_timesteps=14000, episode_reward=-193.81 +/- 176.08
Episode length: 194.57 +/- 175.66
Eval num_timesteps=14500, episode_reward=-155.21 +/- 143.62
Episode length: 156.07 +/- 143.28
New best mean reward!
Eval num_timesteps=15000, episode_reward=-179.53 +/- 171.25
Episode length: 180.32 +/- 170.86
Eval num_timesteps=15500, episode_reward=-143.70 +/- 139.52
Episode length: 144.57 +/- 139.19
New best mean reward!
Eval num_timesteps=16000, episode_reward=-151.76 +/- 148.34
Episode length: 152.61 +/- 147.99
Eval num_timesteps=16500, episode_reward=-200.88 +/- 181.38
Episode length: 201.62 +/- 180.95
Eval num_timesteps=17000, episode_reward=-165.17 +/- 155.32
Episode length: 166.00 +/- 154.95
Eval num_timesteps=17500, episode_reward=-137.47 +/- 131.88
Episode length: 138.36 +/- 131.58
New best mean reward!
Eval num_timesteps=18000, episode_reward=-126.35 +/- 112.59
Episode length: 127.28 +/- 112.36
New best mean reward!
Eval num_timesteps=18500, episode_reward=-109.07 +/- 86.51
Episode length: 110.03 +/- 86.33
New best mean reward!
Eval num_timesteps=19000, episode_reward=-99.82 +/- 72.92
Episode length: 100.79 +/- 72.75
New best mean reward!
FINISHED IN 416.60041231499054 s


starting seed  10495 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-345.38 +/- 125.57
Episode length: 346.01 +/- 125.11
New best mean reward!
Eval num_timesteps=8500, episode_reward=-261.42 +/- 134.74
Episode length: 262.20 +/- 134.35
New best mean reward!
Eval num_timesteps=9000, episode_reward=-236.95 +/- 128.24
Episode length: 237.77 +/- 127.87
New best mean reward!
Eval num_timesteps=9500, episode_reward=-225.28 +/- 133.14
Episode length: 226.10 +/- 132.76
New best mean reward!
Eval num_timesteps=10000, episode_reward=-186.81 +/- 69.46
Episode length: 187.78 +/- 69.32
New best mean reward!
Eval num_timesteps=10500, episode_reward=-185.85 +/- 95.68
Episode length: 186.77 +/- 95.42
New best mean reward!
Eval num_timesteps=11000, episode_reward=-189.81 +/- 107.12
Episode length: 190.71 +/- 106.83
Eval num_timesteps=11500, episode_reward=-196.03 +/- 130.62
Episode length: 196.88 +/- 130.27
Eval num_timesteps=12000, episode_reward=-231.94 +/- 148.85
Episode length: 232.71 +/- 148.43
Eval num_timesteps=12500, episode_reward=-145.20 +/- 117.23
Episode length: 146.12 +/- 116.99
New best mean reward!
Eval num_timesteps=13000, episode_reward=-116.10 +/- 73.51
Episode length: 117.07 +/- 73.35
New best mean reward!
Eval num_timesteps=13500, episode_reward=-106.13 +/- 56.33
Episode length: 107.12 +/- 56.26
New best mean reward!
Eval num_timesteps=14000, episode_reward=-102.02 +/- 47.19
Episode length: 103.01 +/- 47.11
New best mean reward!
Eval num_timesteps=14500, episode_reward=-103.90 +/- 44.95
Episode length: 104.90 +/- 44.95
Eval num_timesteps=15000, episode_reward=-95.53 +/- 31.06
Episode length: 96.53 +/- 31.06
New best mean reward!
FINISHED IN 346.8334549170104 s


starting seed  10496 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-120.97 +/- 41.16
Episode length: 121.97 +/- 41.16
New best mean reward!
Eval num_timesteps=6500, episode_reward=-200.39 +/- 134.86
Episode length: 201.25 +/- 134.55
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-84.89 +/- 22.88
Episode length: 85.89 +/- 22.88
New best mean reward!
FINISHED IN 181.09674706700025 s


starting seed  10497 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-229.01 +/- 143.84
Episode length: 229.81 +/- 143.46
New best mean reward!
Eval num_timesteps=5000, episode_reward=-155.02 +/- 39.56
Episode length: 156.02 +/- 39.56
New best mean reward!
Eval num_timesteps=5500, episode_reward=-194.99 +/- 66.31
Episode length: 195.98 +/- 66.27
Eval num_timesteps=6000, episode_reward=-117.33 +/- 86.55
Episode length: 118.29 +/- 86.37
New best mean reward!
Eval num_timesteps=6500, episode_reward=-92.17 +/- 25.63
Episode length: 93.17 +/- 25.63
New best mean reward!
FINISHED IN 126.5743996129604 s


starting seed  10498 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-215.97 +/- 84.77
Episode length: 216.90 +/- 84.53
New best mean reward!
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-201.22 +/- 92.96
Episode length: 202.16 +/- 92.76
New best mean reward!
Eval num_timesteps=6000, episode_reward=-141.81 +/- 25.68
Episode length: 142.81 +/- 25.68
New best mean reward!
Eval num_timesteps=6500, episode_reward=-351.09 +/- 156.98
Episode length: 351.60 +/- 156.52
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-460.05 +/- 112.68
Episode length: 460.17 +/- 112.37
Eval num_timesteps=8500, episode_reward=-137.51 +/- 123.02
Episode length: 138.41 +/- 122.73
New best mean reward!
Eval num_timesteps=9000, episode_reward=-83.43 +/- 21.72
Episode length: 84.43 +/- 21.72
New best mean reward!
FINISHED IN 235.98072330700234 s


starting seed  10499 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-469.21 +/- 104.47
Episode length: 469.29 +/- 104.20
New best mean reward!
Eval num_timesteps=8500, episode_reward=-217.91 +/- 189.61
Episode length: 218.60 +/- 189.15
New best mean reward!
Eval num_timesteps=9000, episode_reward=-376.15 +/- 185.18
Episode length: 376.46 +/- 184.72
Eval num_timesteps=9500, episode_reward=-131.98 +/- 125.47
Episode length: 132.88 +/- 125.17
New best mean reward!
Eval num_timesteps=10000, episode_reward=-146.19 +/- 145.25
Episode length: 147.05 +/- 144.91
Eval num_timesteps=10500, episode_reward=-89.41 +/- 31.77
Episode length: 90.41 +/- 31.77
New best mean reward!
FINISHED IN 212.15444594295695 s
AVG TIME: 329.0120871119894
