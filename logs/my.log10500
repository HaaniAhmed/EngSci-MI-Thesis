nohup: ignoring input


starting seed  10500 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-235.85 +/- 183.88
Episode length: 236.55 +/- 183.45
New best mean reward!
Eval num_timesteps=8500, episode_reward=-182.86 +/- 154.11
Episode length: 183.68 +/- 153.74
New best mean reward!
Eval num_timesteps=9000, episode_reward=-187.01 +/- 156.49
Episode length: 187.84 +/- 156.15
Eval num_timesteps=9500, episode_reward=-233.40 +/- 181.33
Episode length: 234.12 +/- 180.92
Eval num_timesteps=10000, episode_reward=-222.48 +/- 176.90
Episode length: 223.20 +/- 176.46
Eval num_timesteps=10500, episode_reward=-226.55 +/- 174.08
Episode length: 227.28 +/- 173.65
Eval num_timesteps=11000, episode_reward=-275.85 +/- 188.19
Episode length: 276.46 +/- 187.72
Eval num_timesteps=11500, episode_reward=-127.29 +/- 93.74
Episode length: 128.24 +/- 93.54
New best mean reward!
Eval num_timesteps=12000, episode_reward=-337.52 +/- 184.00
Episode length: 337.98 +/- 183.52
Eval num_timesteps=12500, episode_reward=-153.95 +/- 131.10
Episode length: 154.84 +/- 130.81
Eval num_timesteps=13000, episode_reward=-182.53 +/- 152.21
Episode length: 183.36 +/- 151.85
Eval num_timesteps=13500, episode_reward=-99.20 +/- 30.03
Episode length: 100.20 +/- 30.03
New best mean reward!
FINISHED IN 693.2566955920192 s


starting seed  10501 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-373.52 +/- 162.47
Episode length: 373.91 +/- 161.99
New best mean reward!
Eval num_timesteps=6000, episode_reward=-448.29 +/- 118.81
Episode length: 448.45 +/- 118.45
Eval num_timesteps=6500, episode_reward=-413.02 +/- 142.66
Episode length: 413.30 +/- 142.22
Eval num_timesteps=7000, episode_reward=-380.71 +/- 152.46
Episode length: 381.10 +/- 151.98
Eval num_timesteps=7500, episode_reward=-326.06 +/- 168.98
Episode length: 326.58 +/- 168.48
New best mean reward!
Eval num_timesteps=8000, episode_reward=-416.56 +/- 139.01
Episode length: 416.83 +/- 138.57
Eval num_timesteps=8500, episode_reward=-349.91 +/- 166.78
Episode length: 350.36 +/- 166.28
Eval num_timesteps=9000, episode_reward=-373.54 +/- 165.78
Episode length: 373.91 +/- 165.29
Eval num_timesteps=9500, episode_reward=-183.22 +/- 154.53
Episode length: 184.04 +/- 154.16
New best mean reward!
Eval num_timesteps=10000, episode_reward=-172.47 +/- 100.01
Episode length: 173.39 +/- 99.75
New best mean reward!
Eval num_timesteps=10500, episode_reward=-214.02 +/- 114.80
Episode length: 214.89 +/- 114.48
Eval num_timesteps=11000, episode_reward=-174.83 +/- 75.40
Episode length: 175.79 +/- 75.23
Eval num_timesteps=11500, episode_reward=-163.50 +/- 31.72
Episode length: 164.50 +/- 31.72
New best mean reward!
Eval num_timesteps=12000, episode_reward=-176.97 +/- 71.37
Episode length: 177.93 +/- 71.19
Eval num_timesteps=12500, episode_reward=-206.43 +/- 111.54
Episode length: 207.32 +/- 111.25
Eval num_timesteps=13000, episode_reward=-174.99 +/- 133.99
Episode length: 175.85 +/- 133.65
Eval num_timesteps=13500, episode_reward=-200.65 +/- 146.94
Episode length: 201.47 +/- 146.57
Eval num_timesteps=14000, episode_reward=-178.38 +/- 112.60
Episode length: 179.29 +/- 112.34
Eval num_timesteps=14500, episode_reward=-174.39 +/- 113.17
Episode length: 175.29 +/- 112.89
Eval num_timesteps=15000, episode_reward=-139.85 +/- 91.85
Episode length: 140.80 +/- 91.66
New best mean reward!
Eval num_timesteps=15500, episode_reward=-133.95 +/- 90.86
Episode length: 134.90 +/- 90.66
New best mean reward!
Eval num_timesteps=16000, episode_reward=-111.87 +/- 45.58
Episode length: 112.86 +/- 45.50
New best mean reward!
Eval num_timesteps=16500, episode_reward=-122.30 +/- 63.03
Episode length: 123.29 +/- 62.97
Eval num_timesteps=17000, episode_reward=-141.72 +/- 112.01
Episode length: 142.64 +/- 111.75
Eval num_timesteps=17500, episode_reward=-101.98 +/- 47.80
Episode length: 102.97 +/- 47.71
New best mean reward!
Eval num_timesteps=18000, episode_reward=-98.62 +/- 36.57
Episode length: 99.62 +/- 36.57
New best mean reward!
FINISHED IN 888.3921769299777 s


starting seed  10502 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-475.60 +/- 83.19
Episode length: 475.68 +/- 82.92
New best mean reward!
Eval num_timesteps=1500, episode_reward=-198.34 +/- 85.28
Episode length: 199.30 +/- 85.14
New best mean reward!
Eval num_timesteps=2000, episode_reward=-206.79 +/- 162.71
Episode length: 207.56 +/- 162.29
Eval num_timesteps=2500, episode_reward=-368.45 +/- 164.24
Episode length: 368.85 +/- 163.76
Eval num_timesteps=3000, episode_reward=-392.61 +/- 154.38
Episode length: 392.94 +/- 153.92
Eval num_timesteps=3500, episode_reward=-455.62 +/- 120.32
Episode length: 455.74 +/- 119.99
Eval num_timesteps=4000, episode_reward=-492.77 +/- 50.65
Episode length: 492.79 +/- 50.51
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-496.10 +/- 38.80
Episode length: 496.11 +/- 38.71
Eval num_timesteps=5500, episode_reward=-496.22 +/- 37.61
Episode length: 496.23 +/- 37.51
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-445.91 +/- 134.18
Episode length: 446.05 +/- 133.84
Eval num_timesteps=7000, episode_reward=-443.74 +/- 138.94
Episode length: 443.89 +/- 138.59
Eval num_timesteps=7500, episode_reward=-413.24 +/- 168.46
Episode length: 413.45 +/- 168.05
Eval num_timesteps=8000, episode_reward=-350.21 +/- 196.24
Episode length: 350.58 +/- 195.76
Eval num_timesteps=8500, episode_reward=-354.10 +/- 194.07
Episode length: 354.47 +/- 193.59
Eval num_timesteps=9000, episode_reward=-281.41 +/- 204.23
Episode length: 281.95 +/- 203.74
Eval num_timesteps=9500, episode_reward=-230.20 +/- 187.03
Episode length: 230.89 +/- 186.59
Eval num_timesteps=10000, episode_reward=-213.70 +/- 185.66
Episode length: 214.41 +/- 185.21
Eval num_timesteps=10500, episode_reward=-155.88 +/- 149.33
Episode length: 156.73 +/- 148.99
New best mean reward!
Eval num_timesteps=11000, episode_reward=-111.78 +/- 93.69
Episode length: 112.74 +/- 93.52
New best mean reward!
Eval num_timesteps=11500, episode_reward=-139.07 +/- 136.24
Episode length: 139.95 +/- 135.93
Eval num_timesteps=12000, episode_reward=-106.61 +/- 86.39
Episode length: 107.57 +/- 86.21
New best mean reward!
Eval num_timesteps=12500, episode_reward=-130.22 +/- 123.55
Episode length: 131.13 +/- 123.28
Eval num_timesteps=13000, episode_reward=-97.87 +/- 74.03
Episode length: 98.84 +/- 73.87
New best mean reward!
FINISHED IN 375.5144808999612 s


starting seed  10503 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-121.82 +/- 32.47
Episode length: 122.82 +/- 32.47
New best mean reward!
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-394.19 +/- 137.36
Episode length: 394.61 +/- 136.92
Eval num_timesteps=7500, episode_reward=-377.41 +/- 133.17
Episode length: 377.93 +/- 132.73
Eval num_timesteps=8000, episode_reward=-411.00 +/- 120.50
Episode length: 411.38 +/- 120.05
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-343.83 +/- 174.63
Episode length: 344.29 +/- 174.15
Eval num_timesteps=9500, episode_reward=-497.75 +/- 16.45
Episode length: 497.77 +/- 16.31
Eval num_timesteps=10000, episode_reward=-290.01 +/- 155.82
Episode length: 290.68 +/- 155.38
Eval num_timesteps=10500, episode_reward=-103.81 +/- 34.44
Episode length: 104.81 +/- 34.44
New best mean reward!
Eval num_timesteps=11000, episode_reward=-115.04 +/- 57.57
Episode length: 116.03 +/- 57.51
Eval num_timesteps=11500, episode_reward=-142.13 +/- 92.32
Episode length: 143.08 +/- 92.12
Eval num_timesteps=12000, episode_reward=-115.70 +/- 52.60
Episode length: 116.70 +/- 52.60
Eval num_timesteps=12500, episode_reward=-116.15 +/- 56.71
Episode length: 117.14 +/- 56.65
Eval num_timesteps=13000, episode_reward=-110.78 +/- 44.18
Episode length: 111.78 +/- 44.18
Eval num_timesteps=13500, episode_reward=-106.17 +/- 51.24
Episode length: 107.16 +/- 51.16
Eval num_timesteps=14000, episode_reward=-98.96 +/- 32.44
Episode length: 99.96 +/- 32.44
New best mean reward!
FINISHED IN 300.2020078779897 s


starting seed  10504 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-441.51 +/- 127.73
Episode length: 441.69 +/- 127.36
New best mean reward!
Eval num_timesteps=7000, episode_reward=-95.02 +/- 52.31
Episode length: 96.01 +/- 52.23
New best mean reward!
FINISHED IN 191.54458599700592 s


starting seed  10505 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-400.88 +/- 153.31
Episode length: 401.18 +/- 152.86
New best mean reward!
Eval num_timesteps=10500, episode_reward=-380.90 +/- 160.35
Episode length: 381.26 +/- 159.87
New best mean reward!
Eval num_timesteps=11000, episode_reward=-279.11 +/- 161.37
Episode length: 279.77 +/- 160.90
New best mean reward!
Eval num_timesteps=11500, episode_reward=-225.15 +/- 141.05
Episode length: 225.95 +/- 140.66
New best mean reward!
Eval num_timesteps=12000, episode_reward=-353.89 +/- 172.00
Episode length: 354.31 +/- 171.50
Eval num_timesteps=12500, episode_reward=-439.11 +/- 130.26
Episode length: 439.29 +/- 129.88
Eval num_timesteps=13000, episode_reward=-445.65 +/- 124.84
Episode length: 445.81 +/- 124.48
Eval num_timesteps=13500, episode_reward=-241.86 +/- 177.25
Episode length: 242.55 +/- 176.80
Eval num_timesteps=14000, episode_reward=-104.54 +/- 23.59
Episode length: 105.54 +/- 23.59
New best mean reward!
Eval num_timesteps=14500, episode_reward=-103.35 +/- 63.11
Episode length: 104.33 +/- 62.98
New best mean reward!
Eval num_timesteps=15000, episode_reward=-108.24 +/- 46.80
Episode length: 109.23 +/- 46.71
Eval num_timesteps=15500, episode_reward=-121.60 +/- 106.62
Episode length: 122.53 +/- 106.37
Eval num_timesteps=16000, episode_reward=-91.81 +/- 19.94
Episode length: 92.81 +/- 19.94
New best mean reward!
FINISHED IN 417.0078361979686 s


starting seed  10506 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-127.92 +/- 125.41
Episode length: 128.82 +/- 125.12
New best mean reward!
Eval num_timesteps=10500, episode_reward=-146.37 +/- 146.29
Episode length: 147.23 +/- 145.95
Eval num_timesteps=11000, episode_reward=-171.85 +/- 156.29
Episode length: 172.69 +/- 155.96
Eval num_timesteps=11500, episode_reward=-161.00 +/- 152.57
Episode length: 161.84 +/- 152.21
Eval num_timesteps=12000, episode_reward=-124.97 +/- 119.21
Episode length: 125.88 +/- 118.93
New best mean reward!
Eval num_timesteps=12500, episode_reward=-100.10 +/- 68.88
Episode length: 101.08 +/- 68.77
New best mean reward!
Eval num_timesteps=13000, episode_reward=-87.42 +/- 17.09
Episode length: 88.42 +/- 17.09
New best mean reward!
FINISHED IN 281.96028347697575 s


starting seed  10507 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-109.36 +/- 24.74
Episode length: 110.36 +/- 24.74
New best mean reward!
Eval num_timesteps=8500, episode_reward=-103.06 +/- 43.02
Episode length: 104.06 +/- 43.02
New best mean reward!
Eval num_timesteps=9000, episode_reward=-106.48 +/- 28.76
Episode length: 107.48 +/- 28.76
Eval num_timesteps=9500, episode_reward=-113.61 +/- 55.48
Episode length: 114.60 +/- 55.41
Eval num_timesteps=10000, episode_reward=-117.53 +/- 53.98
Episode length: 118.52 +/- 53.91
Eval num_timesteps=10500, episode_reward=-127.47 +/- 39.15
Episode length: 128.47 +/- 39.15
Eval num_timesteps=11000, episode_reward=-124.08 +/- 26.77
Episode length: 125.08 +/- 26.77
Eval num_timesteps=11500, episode_reward=-184.58 +/- 52.09
Episode length: 185.57 +/- 52.03
Eval num_timesteps=12000, episode_reward=-96.07 +/- 24.64
Episode length: 97.07 +/- 24.64
New best mean reward!
FINISHED IN 220.94917148700915 s


starting seed  10508 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-247.99 +/- 135.97
Episode length: 248.77 +/- 135.56
New best mean reward!
Eval num_timesteps=8500, episode_reward=-174.42 +/- 41.50
Episode length: 175.42 +/- 41.50
New best mean reward!
Eval num_timesteps=9000, episode_reward=-184.34 +/- 39.70
Episode length: 185.34 +/- 39.70
Eval num_timesteps=9500, episode_reward=-172.04 +/- 26.69
Episode length: 173.04 +/- 26.69
New best mean reward!
Eval num_timesteps=10000, episode_reward=-360.36 +/- 137.36
Episode length: 360.89 +/- 136.88
Eval num_timesteps=10500, episode_reward=-180.85 +/- 33.30
Episode length: 181.85 +/- 33.30
Eval num_timesteps=11000, episode_reward=-200.21 +/- 87.83
Episode length: 201.14 +/- 87.59
Eval num_timesteps=11500, episode_reward=-175.69 +/- 54.60
Episode length: 176.68 +/- 54.54
Eval num_timesteps=12000, episode_reward=-216.08 +/- 120.10
Episode length: 216.94 +/- 119.77
Eval num_timesteps=12500, episode_reward=-255.70 +/- 152.29
Episode length: 256.44 +/- 151.87
Eval num_timesteps=13000, episode_reward=-196.31 +/- 101.21
Episode length: 197.23 +/- 100.97
Eval num_timesteps=13500, episode_reward=-174.89 +/- 55.71
Episode length: 175.88 +/- 55.65
Eval num_timesteps=14000, episode_reward=-134.21 +/- 96.57
Episode length: 135.15 +/- 96.35
New best mean reward!
Eval num_timesteps=14500, episode_reward=-108.77 +/- 45.39
Episode length: 109.76 +/- 45.31
New best mean reward!
Eval num_timesteps=15000, episode_reward=-137.78 +/- 88.21
Episode length: 138.73 +/- 88.01
Eval num_timesteps=15500, episode_reward=-139.53 +/- 48.32
Episode length: 140.52 +/- 48.24
Eval num_timesteps=16000, episode_reward=-107.59 +/- 23.61
Episode length: 108.59 +/- 23.61
New best mean reward!
Eval num_timesteps=16500, episode_reward=-130.47 +/- 28.16
Episode length: 131.47 +/- 28.16
Eval num_timesteps=17000, episode_reward=-212.29 +/- 130.73
Episode length: 213.13 +/- 130.38
Eval num_timesteps=17500, episode_reward=-398.33 +/- 153.78
Episode length: 398.64 +/- 153.33
Eval num_timesteps=18000, episode_reward=-300.32 +/- 186.29
Episode length: 300.86 +/- 185.80
Eval num_timesteps=18500, episode_reward=-412.52 +/- 162.10
Episode length: 412.75 +/- 161.68
Eval num_timesteps=19000, episode_reward=-92.73 +/- 46.59
Episode length: 93.72 +/- 46.50
New best mean reward!
FINISHED IN 313.9595549380174 s


starting seed  10509 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-473.48 +/- 90.37
Episode length: 473.56 +/- 90.10
New best mean reward!
Eval num_timesteps=7500, episode_reward=-306.53 +/- 167.55
Episode length: 307.11 +/- 167.07
New best mean reward!
Eval num_timesteps=8000, episode_reward=-214.81 +/- 124.83
Episode length: 215.66 +/- 124.49
New best mean reward!
Eval num_timesteps=8500, episode_reward=-117.51 +/- 106.12
Episode length: 118.44 +/- 105.86
New best mean reward!
Eval num_timesteps=9000, episode_reward=-100.39 +/- 35.61
Episode length: 101.39 +/- 35.61
New best mean reward!
Eval num_timesteps=9500, episode_reward=-96.87 +/- 16.40
Episode length: 97.87 +/- 16.40
New best mean reward!
FINISHED IN 192.23381454194896 s


starting seed  10510 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=12500, episode_reward=-467.39 +/- 110.62
Episode length: 467.47 +/- 110.34
New best mean reward!
Eval num_timesteps=13000, episode_reward=-294.78 +/- 199.97
Episode length: 295.31 +/- 199.49
New best mean reward!
Eval num_timesteps=13500, episode_reward=-244.23 +/- 200.82
Episode length: 244.85 +/- 200.34
New best mean reward!
Eval num_timesteps=14000, episode_reward=-199.34 +/- 184.07
Episode length: 200.07 +/- 183.63
New best mean reward!
Eval num_timesteps=14500, episode_reward=-203.44 +/- 181.92
Episode length: 204.19 +/- 181.51
Eval num_timesteps=15000, episode_reward=-149.72 +/- 144.84
Episode length: 150.59 +/- 144.53
New best mean reward!
Eval num_timesteps=15500, episode_reward=-91.34 +/- 35.63
Episode length: 92.34 +/- 35.63
New best mean reward!
FINISHED IN 436.93695318698883 s


starting seed  10511 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-395.25 +/- 168.19
Episode length: 395.53 +/- 167.74
New best mean reward!
Eval num_timesteps=9000, episode_reward=-194.96 +/- 138.76
Episode length: 195.80 +/- 138.41
New best mean reward!
Eval num_timesteps=9500, episode_reward=-130.86 +/- 77.57
Episode length: 131.83 +/- 77.42
New best mean reward!
Eval num_timesteps=10000, episode_reward=-102.74 +/- 29.63
Episode length: 103.74 +/- 29.63
New best mean reward!
Eval num_timesteps=10500, episode_reward=-103.81 +/- 27.53
Episode length: 104.81 +/- 27.53
Eval num_timesteps=11000, episode_reward=-119.03 +/- 57.14
Episode length: 120.02 +/- 57.07
Eval num_timesteps=11500, episode_reward=-152.41 +/- 83.95
Episode length: 153.36 +/- 83.74
Eval num_timesteps=12000, episode_reward=-231.02 +/- 161.52
Episode length: 231.76 +/- 161.09
Eval num_timesteps=12500, episode_reward=-211.08 +/- 148.36
Episode length: 211.89 +/- 147.99
Eval num_timesteps=13000, episode_reward=-210.34 +/- 144.92
Episode length: 211.15 +/- 144.54
Eval num_timesteps=13500, episode_reward=-273.20 +/- 174.64
Episode length: 273.83 +/- 174.16
Eval num_timesteps=14000, episode_reward=-191.48 +/- 142.90
Episode length: 192.32 +/- 142.55
Eval num_timesteps=14500, episode_reward=-134.92 +/- 73.33
Episode length: 135.89 +/- 73.18
Eval num_timesteps=15000, episode_reward=-218.60 +/- 138.00
Episode length: 219.42 +/- 137.64
Eval num_timesteps=15500, episode_reward=-189.86 +/- 102.69
Episode length: 190.77 +/- 102.42
Eval num_timesteps=16000, episode_reward=-196.69 +/- 97.32
Episode length: 197.62 +/- 97.10
Eval num_timesteps=16500, episode_reward=-203.21 +/- 119.46
Episode length: 204.09 +/- 119.16
Eval num_timesteps=17000, episode_reward=-154.13 +/- 91.14
Episode length: 155.07 +/- 90.91
Eval num_timesteps=17500, episode_reward=-117.68 +/- 40.05
Episode length: 118.68 +/- 40.05
Eval num_timesteps=18000, episode_reward=-132.85 +/- 54.66
Episode length: 133.84 +/- 54.59
Eval num_timesteps=18500, episode_reward=-123.95 +/- 36.25
Episode length: 124.95 +/- 36.25
Eval num_timesteps=19000, episode_reward=-107.56 +/- 36.81
Episode length: 108.56 +/- 36.81
Eval num_timesteps=19500, episode_reward=-100.23 +/- 29.08
Episode length: 101.23 +/- 29.08
New best mean reward!
Eval num_timesteps=20000, episode_reward=-96.05 +/- 29.32
Episode length: 97.05 +/- 29.32
New best mean reward!
FINISHED IN 361.34662284201477 s


starting seed  10512 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-283.16 +/- 200.82
Episode length: 283.70 +/- 200.33
New best mean reward!
Eval num_timesteps=12000, episode_reward=-299.08 +/- 199.91
Episode length: 299.59 +/- 199.41
Eval num_timesteps=12500, episode_reward=-209.70 +/- 181.57
Episode length: 210.43 +/- 181.14
New best mean reward!
Eval num_timesteps=13000, episode_reward=-276.31 +/- 184.31
Episode length: 276.91 +/- 183.83
Eval num_timesteps=13500, episode_reward=-221.32 +/- 187.14
Episode length: 222.02 +/- 186.69
Eval num_timesteps=14000, episode_reward=-328.42 +/- 183.58
Episode length: 328.89 +/- 183.08
Eval num_timesteps=14500, episode_reward=-220.32 +/- 179.06
Episode length: 221.04 +/- 178.62
Eval num_timesteps=15000, episode_reward=-264.32 +/- 182.26
Episode length: 264.95 +/- 181.78
Eval num_timesteps=15500, episode_reward=-291.36 +/- 184.52
Episode length: 291.94 +/- 184.05
Eval num_timesteps=16000, episode_reward=-204.82 +/- 162.82
Episode length: 205.59 +/- 162.40
New best mean reward!
Eval num_timesteps=16500, episode_reward=-241.43 +/- 177.17
Episode length: 242.12 +/- 176.72
Eval num_timesteps=17000, episode_reward=-202.40 +/- 165.33
Episode length: 203.17 +/- 164.91
New best mean reward!
Eval num_timesteps=17500, episode_reward=-160.79 +/- 133.16
Episode length: 161.66 +/- 132.83
New best mean reward!
Eval num_timesteps=18000, episode_reward=-239.99 +/- 177.07
Episode length: 240.69 +/- 176.63
Eval num_timesteps=18500, episode_reward=-224.54 +/- 173.43
Episode length: 225.26 +/- 172.98
Eval num_timesteps=19000, episode_reward=-164.25 +/- 138.88
Episode length: 165.11 +/- 138.54
Eval num_timesteps=19500, episode_reward=-135.27 +/- 117.30
Episode length: 136.19 +/- 117.05
New best mean reward!
Eval num_timesteps=20000, episode_reward=-151.76 +/- 145.40
Episode length: 152.63 +/- 145.08
Eval num_timesteps=20500, episode_reward=-145.64 +/- 139.58
Episode length: 146.51 +/- 139.25
Eval num_timesteps=21000, episode_reward=-155.33 +/- 148.98
Episode length: 156.18 +/- 148.63
Eval num_timesteps=21500, episode_reward=-142.11 +/- 136.16
Episode length: 143.00 +/- 135.87
Eval num_timesteps=22000, episode_reward=-134.63 +/- 130.00
Episode length: 135.52 +/- 129.69
New best mean reward!
Eval num_timesteps=22500, episode_reward=-125.15 +/- 108.26
Episode length: 126.08 +/- 108.02
New best mean reward!
Eval num_timesteps=23000, episode_reward=-133.88 +/- 118.02
Episode length: 134.79 +/- 117.75
Eval num_timesteps=23500, episode_reward=-163.41 +/- 153.36
Episode length: 164.25 +/- 153.00
Eval num_timesteps=24000, episode_reward=-114.01 +/- 92.48
Episode length: 114.96 +/- 92.27
New best mean reward!
Eval num_timesteps=24500, episode_reward=-100.54 +/- 60.84
Episode length: 101.52 +/- 60.71
New best mean reward!
Eval num_timesteps=25000, episode_reward=-142.53 +/- 129.20
Episode length: 143.42 +/- 128.89
Eval num_timesteps=25500, episode_reward=-135.08 +/- 126.05
Episode length: 135.98 +/- 125.76
Eval num_timesteps=26000, episode_reward=-151.93 +/- 149.21
Episode length: 152.78 +/- 148.86
Eval num_timesteps=26500, episode_reward=-157.26 +/- 147.40
Episode length: 158.12 +/- 147.08
Eval num_timesteps=27000, episode_reward=-147.09 +/- 147.18
Episode length: 147.95 +/- 146.85
Eval num_timesteps=27500, episode_reward=-199.33 +/- 174.30
Episode length: 200.08 +/- 173.87
Eval num_timesteps=28000, episode_reward=-165.26 +/- 163.40
Episode length: 166.08 +/- 163.03
Eval num_timesteps=28500, episode_reward=-150.48 +/- 148.51
Episode length: 151.33 +/- 148.16
Eval num_timesteps=29000, episode_reward=-166.29 +/- 162.87
Episode length: 167.10 +/- 162.48
Eval num_timesteps=29500, episode_reward=-165.59 +/- 157.97
Episode length: 166.42 +/- 157.61
Eval num_timesteps=30000, episode_reward=-174.29 +/- 165.37
Episode length: 175.09 +/- 164.98
Eval num_timesteps=30500, episode_reward=-137.36 +/- 124.79
Episode length: 138.27 +/- 124.53
Eval num_timesteps=31000, episode_reward=-139.40 +/- 126.57
Episode length: 140.31 +/- 126.31
Eval num_timesteps=31500, episode_reward=-124.28 +/- 106.54
Episode length: 125.22 +/- 106.33
Eval num_timesteps=32000, episode_reward=-116.44 +/- 96.19
Episode length: 117.39 +/- 95.99
Eval num_timesteps=32500, episode_reward=-97.29 +/- 60.32
Episode length: 98.27 +/- 60.19
New best mean reward!
FINISHED IN 576.3143570219981 s


starting seed  10513 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-164.87 +/- 48.86
Episode length: 165.86 +/- 48.79
New best mean reward!
Eval num_timesteps=9000, episode_reward=-215.41 +/- 176.29
Episode length: 216.14 +/- 175.85
Eval num_timesteps=9500, episode_reward=-234.32 +/- 134.54
Episode length: 235.14 +/- 134.18
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-161.94 +/- 33.70
Episode length: 162.94 +/- 33.70
New best mean reward!
Eval num_timesteps=11000, episode_reward=-175.01 +/- 55.12
Episode length: 175.99 +/- 55.01
Eval num_timesteps=11500, episode_reward=-136.30 +/- 43.23
Episode length: 137.29 +/- 43.15
New best mean reward!
Eval num_timesteps=12000, episode_reward=-125.74 +/- 25.84
Episode length: 126.74 +/- 25.84
New best mean reward!
Eval num_timesteps=12500, episode_reward=-93.06 +/- 44.89
Episode length: 94.05 +/- 44.80
New best mean reward!
FINISHED IN 230.01812369498657 s


starting seed  10514 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-468.14 +/- 93.04
Episode length: 468.25 +/- 92.74
New best mean reward!
Eval num_timesteps=11500, episode_reward=-118.78 +/- 56.21
Episode length: 119.77 +/- 56.15
New best mean reward!
Eval num_timesteps=12000, episode_reward=-123.83 +/- 68.04
Episode length: 124.82 +/- 67.99
Eval num_timesteps=12500, episode_reward=-142.43 +/- 79.78
Episode length: 143.40 +/- 79.65
Eval num_timesteps=13000, episode_reward=-113.02 +/- 55.51
Episode length: 114.01 +/- 55.44
New best mean reward!
Eval num_timesteps=13500, episode_reward=-91.37 +/- 28.77
Episode length: 92.37 +/- 28.77
New best mean reward!
FINISHED IN 280.7288988730288 s


starting seed  10515 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-218.68 +/- 160.68
Episode length: 219.46 +/- 160.30
New best mean reward!
Eval num_timesteps=11500, episode_reward=-319.11 +/- 176.83
Episode length: 319.63 +/- 176.34
Eval num_timesteps=12000, episode_reward=-354.65 +/- 173.05
Episode length: 355.08 +/- 172.57
Eval num_timesteps=12500, episode_reward=-301.09 +/- 189.66
Episode length: 301.63 +/- 189.18
Eval num_timesteps=13000, episode_reward=-302.12 +/- 189.36
Episode length: 302.66 +/- 188.88
Eval num_timesteps=13500, episode_reward=-284.16 +/- 186.96
Episode length: 284.74 +/- 186.47
Eval num_timesteps=14000, episode_reward=-247.37 +/- 179.28
Episode length: 248.05 +/- 178.83
Eval num_timesteps=14500, episode_reward=-294.55 +/- 188.84
Episode length: 295.11 +/- 188.36
Eval num_timesteps=15000, episode_reward=-278.20 +/- 186.12
Episode length: 278.80 +/- 185.64
Eval num_timesteps=15500, episode_reward=-228.42 +/- 174.08
Episode length: 229.15 +/- 173.66
Eval num_timesteps=16000, episode_reward=-219.92 +/- 168.63
Episode length: 220.67 +/- 168.22
Eval num_timesteps=16500, episode_reward=-255.58 +/- 187.06
Episode length: 256.23 +/- 186.60
Eval num_timesteps=17000, episode_reward=-279.87 +/- 182.78
Episode length: 280.48 +/- 182.31
Eval num_timesteps=17500, episode_reward=-261.82 +/- 182.14
Episode length: 262.47 +/- 181.68
Eval num_timesteps=18000, episode_reward=-232.39 +/- 180.73
Episode length: 233.09 +/- 180.28
Eval num_timesteps=18500, episode_reward=-179.79 +/- 149.11
Episode length: 180.62 +/- 148.74
New best mean reward!
Eval num_timesteps=19000, episode_reward=-193.53 +/- 152.72
Episode length: 194.35 +/- 152.36
Eval num_timesteps=19500, episode_reward=-103.95 +/- 31.51
Episode length: 104.95 +/- 31.51
New best mean reward!
Eval num_timesteps=20000, episode_reward=-102.74 +/- 47.31
Episode length: 103.73 +/- 47.23
New best mean reward!
Eval num_timesteps=20500, episode_reward=-103.35 +/- 52.89
Episode length: 104.34 +/- 52.81
Eval num_timesteps=21000, episode_reward=-101.27 +/- 29.69
Episode length: 102.27 +/- 29.69
New best mean reward!
Eval num_timesteps=21500, episode_reward=-99.55 +/- 32.49
Episode length: 100.55 +/- 32.49
New best mean reward!
FINISHED IN 447.9147193870158 s


starting seed  10516 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=5500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=6500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=7500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=8500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=9500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=10500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=11500, episode_reward=-213.54 +/- 65.08
Episode length: 214.50 +/- 64.91
New best mean reward!
Eval num_timesteps=12000, episode_reward=-266.26 +/- 64.10
Episode length: 267.23 +/- 63.99
Eval num_timesteps=12500, episode_reward=-183.09 +/- 31.40
Episode length: 184.09 +/- 31.40
New best mean reward!
Eval num_timesteps=13000, episode_reward=-231.32 +/- 50.84
Episode length: 232.30 +/- 50.73
Eval num_timesteps=13500, episode_reward=-280.20 +/- 67.19
Episode length: 281.19 +/- 67.16
Eval num_timesteps=14000, episode_reward=-239.96 +/- 61.85
Episode length: 240.94 +/- 61.77
Eval num_timesteps=14500, episode_reward=-337.50 +/- 104.47
Episode length: 338.36 +/- 104.26
Eval num_timesteps=15000, episode_reward=-232.60 +/- 64.57
Episode length: 233.58 +/- 64.49
Eval num_timesteps=15500, episode_reward=-237.62 +/- 54.85
Episode length: 238.61 +/- 54.80
Eval num_timesteps=16000, episode_reward=-179.60 +/- 43.27
Episode length: 180.59 +/- 43.20
New best mean reward!
Eval num_timesteps=16500, episode_reward=-182.58 +/- 46.04
Episode length: 183.57 +/- 45.97
Eval num_timesteps=17000, episode_reward=-177.30 +/- 34.79
Episode length: 178.30 +/- 34.79
New best mean reward!
Eval num_timesteps=17500, episode_reward=-225.04 +/- 61.50
Episode length: 226.02 +/- 61.41
Eval num_timesteps=18000, episode_reward=-226.92 +/- 63.19
Episode length: 227.91 +/- 63.15
Eval num_timesteps=18500, episode_reward=-237.21 +/- 64.12
Episode length: 238.20 +/- 64.08
Eval num_timesteps=19000, episode_reward=-221.63 +/- 45.95
Episode length: 222.63 +/- 45.95
Eval num_timesteps=19500, episode_reward=-218.03 +/- 45.09
Episode length: 219.03 +/- 45.09
Eval num_timesteps=20000, episode_reward=-197.04 +/- 66.86
Episode length: 198.01 +/- 66.73
Eval num_timesteps=20500, episode_reward=-190.53 +/- 49.50
Episode length: 191.52 +/- 49.44
Eval num_timesteps=21000, episode_reward=-182.07 +/- 40.12
Episode length: 183.07 +/- 40.12
Eval num_timesteps=21500, episode_reward=-175.94 +/- 32.20
Episode length: 176.94 +/- 32.20
New best mean reward!
Eval num_timesteps=22000, episode_reward=-172.78 +/- 26.20
Episode length: 173.78 +/- 26.20
New best mean reward!
Eval num_timesteps=22500, episode_reward=-183.58 +/- 55.07
Episode length: 184.56 +/- 54.95
Eval num_timesteps=23000, episode_reward=-189.45 +/- 69.86
Episode length: 190.43 +/- 69.77
Eval num_timesteps=23500, episode_reward=-164.13 +/- 38.24
Episode length: 165.13 +/- 38.24
New best mean reward!
Eval num_timesteps=24000, episode_reward=-183.07 +/- 56.22
Episode length: 184.05 +/- 56.10
Eval num_timesteps=24500, episode_reward=-176.36 +/- 40.68
Episode length: 177.36 +/- 40.68
Eval num_timesteps=25000, episode_reward=-161.70 +/- 58.74
Episode length: 162.68 +/- 58.62
New best mean reward!
Eval num_timesteps=25500, episode_reward=-171.46 +/- 33.87
Episode length: 172.46 +/- 33.87
Eval num_timesteps=26000, episode_reward=-172.23 +/- 49.15
Episode length: 173.22 +/- 49.08
Eval num_timesteps=26500, episode_reward=-169.88 +/- 41.15
Episode length: 170.88 +/- 41.15
Eval num_timesteps=27000, episode_reward=-162.93 +/- 28.89
Episode length: 163.93 +/- 28.89
Eval num_timesteps=27500, episode_reward=-181.24 +/- 50.27
Episode length: 182.23 +/- 50.20
Eval num_timesteps=28000, episode_reward=-164.55 +/- 24.85
Episode length: 165.55 +/- 24.85
Eval num_timesteps=28500, episode_reward=-172.35 +/- 58.87
Episode length: 173.33 +/- 58.76
Eval num_timesteps=29000, episode_reward=-170.87 +/- 70.36
Episode length: 171.85 +/- 70.26
Eval num_timesteps=29500, episode_reward=-176.21 +/- 73.83
Episode length: 177.18 +/- 73.70
Eval num_timesteps=30000, episode_reward=-158.98 +/- 46.15
Episode length: 159.97 +/- 46.08
New best mean reward!
Eval num_timesteps=30500, episode_reward=-161.62 +/- 56.75
Episode length: 162.60 +/- 56.63
Eval num_timesteps=31000, episode_reward=-152.18 +/- 44.21
Episode length: 153.17 +/- 44.13
New best mean reward!
Eval num_timesteps=31500, episode_reward=-136.23 +/- 33.59
Episode length: 137.23 +/- 33.59
New best mean reward!
Eval num_timesteps=32000, episode_reward=-125.46 +/- 55.31
Episode length: 126.45 +/- 55.24
New best mean reward!
Eval num_timesteps=32500, episode_reward=-114.27 +/- 33.45
Episode length: 115.27 +/- 33.45
New best mean reward!
Eval num_timesteps=33000, episode_reward=-110.43 +/- 36.83
Episode length: 111.43 +/- 36.83
New best mean reward!
Eval num_timesteps=33500, episode_reward=-106.41 +/- 48.73
Episode length: 107.40 +/- 48.65
New best mean reward!
Eval num_timesteps=34000, episode_reward=-100.64 +/- 32.34
Episode length: 101.64 +/- 32.34
New best mean reward!
Eval num_timesteps=34500, episode_reward=-96.84 +/- 22.28
Episode length: 97.84 +/- 22.28
New best mean reward!
FINISHED IN 514.5153656899929 s


starting seed  10517 


neuron list:  [16, 16]
<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
New best mean reward!
Eval num_timesteps=1000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=1500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=2500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=3500, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4000, episode_reward=-500.00 +/- 0.00
Episode length: 500.00 +/- 0.00
Eval num_timesteps=4500, episode_reward=-186.95 +/- 84.92
Episode length: 187.91 +/- 84.78
New best mean reward!
Eval num_timesteps=5000, episode_reward=-185.79 +/- 73.92
Episode length: 186.75 +/- 73.75
New best mean reward!
Eval num_timesteps=5500, episode_reward=-185.72 +/- 87.03
Episode length: 186.66 +/- 86.82
New best mean reward!
Eval num_timesteps=6000, episode_reward=-211.00 +/- 105.65
Episode length: 211.90 +/- 105.38
Eval num_timesteps=6500, episode_reward=-203.84 +/- 89.57
Episode length: 204.78 +/- 89.37
Eval num_timesteps=7000, episode_reward=-165.20 +/- 45.22
Episode length: 166.19 +/- 45.14
New best mean reward!
Eval num_timesteps=7500, episode_reward=-161.78 +/- 37.92
Episode length: 162.78 +/- 37.92
New best mean reward!
Eval num_timesteps=8000, episode_reward=-169.83 +/- 37.47
Episode length: 170.83 +/- 37.47
Eval num_timesteps=8500, episode_reward=-170.66 +/- 46.73
Episode length: 171.65 +/- 46.66
Eval num_timesteps=9000, episode_reward=-168.16 +/- 46.02
Episode length: 169.15 +/- 45.95
Eval num_timesteps=9500, episode_reward=-164.12 +/- 41.32
Episode length: 165.12 +/- 41.32
Eval num_timesteps=10000, episode_reward=-154.47 +/- 25.18
Episode length: 155.47 +/- 25.18
New best mean reward!
Eval num_timesteps=10500, episode_reward=-168.77 +/- 45.84
Episode length: 169.77 +/- 45.84
Eval num_timesteps=11000, episode_reward=-162.61 +/- 32.87
Episode length: 163.61 +/- 32.87
Eval num_timesteps=11500, episode_reward=-168.73 +/- 40.99
Episode length: 169.73 +/- 40.99
Eval num_timesteps=12000, episode_reward=-173.20 +/- 42.54
Episode length: 174.20 +/- 42.54
Eval num_timesteps=12500, episode_reward=-170.65 +/- 47.71
Episode length: 171.65 +/- 47.71
Eval num_timesteps=13000, episode_reward=-181.65 +/- 68.63
Episode length: 182.62 +/- 68.49
Eval num_timesteps=13500, episode_reward=-168.23 +/- 28.37
Episode length: 169.23 +/- 28.37
Eval num_timesteps=14000, episode_reward=-167.24 +/- 27.86
Episode length: 168.24 +/- 27.86
Traceback (most recent call last):
  File "/home/haani/snap/snapd-desktop-integration/83/Documents/thesis/sb.py", line 168, in <module>
    main(args)
  File "/home/haani/snap/snapd-desktop-integration/83/Documents/thesis/sb.py", line 159, in main
    drl(args, i)
  File "/home/haani/snap/snapd-desktop-integration/83/Documents/thesis/sb.py", line 139, in drl
    model.learn(total_timesteps=args.steps, eval_freq=1, n_eval_episodes=1, log_interval=1, callback=eval_callback)
  File "/home/haani/snap/snapd-desktop-integration/83/Documents/thesis/stable_baselines3_thesis/a2c/a2c.py", line 217, in learn
    return super().learn(
  File "/home/haani/snap/snapd-desktop-integration/83/Documents/thesis/stable_baselines3_thesis/common/on_policy_algorithm.py", line 251, in learn
    continue_trai