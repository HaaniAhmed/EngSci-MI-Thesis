nohup: ignoring input


starting seed  1800 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Eval num_timesteps=5000, episode_reward=-196.93 +/- 30.26
Episode length: 338.19 +/- 66.29
New best mean reward!
Eval num_timesteps=10000, episode_reward=71.96 +/- 122.19
Episode length: 710.72 +/- 163.48
New best mean reward!
Eval num_timesteps=15000, episode_reward=91.54 +/- 116.80
Episode length: 463.94 +/- 196.99
New best mean reward!
Eval num_timesteps=20000, episode_reward=147.58 +/- 125.61
Episode length: 361.93 +/- 95.38
New best mean reward!
Eval num_timesteps=25000, episode_reward=142.78 +/- 116.27
Episode length: 557.42 +/- 74.09
Eval num_timesteps=30000, episode_reward=115.34 +/- 121.08
Episode length: 406.81 +/- 91.82
Eval num_timesteps=35000, episode_reward=111.48 +/- 111.36
Episode length: 581.97 +/- 142.94
Eval num_timesteps=40000, episode_reward=-145.93 +/- 52.41
Episode length: 641.08 +/- 305.29
Eval num_timesteps=45000, episode_reward=-75.90 +/- 89.27
Episode length: 649.17 +/- 332.66
Eval num_timesteps=50000, episode_reward=47.16 +/- 123.76
Episode length: 635.19 +/- 193.82
Eval num_timesteps=55000, episode_reward=-17.70 +/- 83.39
Episode length: 934.53 +/- 150.21
Eval num_timesteps=60000, episode_reward=-24.75 +/- 136.31
Episode length: 521.78 +/- 275.51
Eval num_timesteps=65000, episode_reward=46.14 +/- 122.00
Episode length: 765.91 +/- 166.08
Eval num_timesteps=70000, episode_reward=40.98 +/- 124.11
Episode length: 516.95 +/- 245.64
Eval num_timesteps=75000, episode_reward=-27.05 +/- 90.05
Episode length: 676.22 +/- 313.14
Eval num_timesteps=80000, episode_reward=32.89 +/- 119.48
Episode length: 467.51 +/- 222.33
Eval num_timesteps=85000, episode_reward=-93.70 +/- 38.84
Episode length: 567.24 +/- 352.32
Eval num_timesteps=90000, episode_reward=-48.48 +/- 97.37
Episode length: 517.03 +/- 332.46
Eval num_timesteps=95000, episode_reward=-69.17 +/- 64.21
Episode length: 523.81 +/- 339.08
Eval num_timesteps=100000, episode_reward=-103.97 +/- 38.30
Episode length: 626.78 +/- 380.66
Eval num_timesteps=105000, episode_reward=-124.11 +/- 38.55
Episode length: 564.36 +/- 359.69
Eval num_timesteps=110000, episode_reward=-124.42 +/- 44.83
Episode length: 502.22 +/- 322.01
Eval num_timesteps=115000, episode_reward=-128.20 +/- 42.51
Episode length: 528.49 +/- 351.34
Eval num_timesteps=120000, episode_reward=-112.39 +/- 35.41
Episode length: 470.32 +/- 328.57
Eval num_timesteps=125000, episode_reward=-94.04 +/- 56.69
Episode length: 439.48 +/- 315.92
Eval num_timesteps=130000, episode_reward=-101.02 +/- 49.08
Episode length: 605.45 +/- 368.28
Eval num_timesteps=135000, episode_reward=-105.10 +/- 50.86
Episode length: 533.94 +/- 363.39
Eval num_timesteps=140000, episode_reward=-116.94 +/- 51.00
Episode length: 569.49 +/- 368.01
Eval num_timesteps=145000, episode_reward=-103.97 +/- 40.04
Episode length: 646.79 +/- 371.73
Eval num_timesteps=150000, episode_reward=-112.68 +/- 52.66
Episode length: 584.01 +/- 371.45
FINISHED IN 1094.5027801629913 s


starting seed  1801 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Eval num_timesteps=5000, episode_reward=27.35 +/- 111.55
Episode length: 830.54 +/- 122.87
New best mean reward!
Eval num_timesteps=10000, episode_reward=-73.23 +/- 21.70
Episode length: 998.39 +/- 16.02
Eval num_timesteps=15000, episode_reward=-94.08 +/- 103.09
Episode length: 838.70 +/- 132.94
Eval num_timesteps=20000, episode_reward=-99.57 +/- 28.20
Episode length: 999.06 +/- 9.35
Eval num_timesteps=25000, episode_reward=-28.99 +/- 19.10
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=30000, episode_reward=-52.27 +/- 37.27
Episode length: 994.70 +/- 34.28
Eval num_timesteps=35000, episode_reward=-105.58 +/- 42.34
Episode length: 934.04 +/- 137.08
Eval num_timesteps=40000, episode_reward=-106.81 +/- 23.49
Episode length: 344.13 +/- 139.47
Eval num_timesteps=45000, episode_reward=-63.41 +/- 23.06
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=50000, episode_reward=-94.83 +/- 16.37
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=55000, episode_reward=-50.31 +/- 22.94
Episode length: 989.40 +/- 90.55
Eval num_timesteps=60000, episode_reward=-139.57 +/- 49.65
Episode length: 962.45 +/- 166.66
Eval num_timesteps=65000, episode_reward=-103.06 +/- 30.63
Episode length: 996.03 +/- 39.50
Eval num_timesteps=70000, episode_reward=-91.74 +/- 34.70
Episode length: 995.25 +/- 47.26
Eval num_timesteps=75000, episode_reward=-69.52 +/- 33.56
Episode length: 986.97 +/- 93.20
Eval num_timesteps=80000, episode_reward=-80.83 +/- 43.65
Episode length: 870.01 +/- 258.12
Eval num_timesteps=85000, episode_reward=-122.97 +/- 21.76
Episode length: 947.25 +/- 193.78
Eval num_timesteps=90000, episode_reward=-82.61 +/- 36.45
Episode length: 821.20 +/- 305.35
Eval num_timesteps=95000, episode_reward=-110.45 +/- 49.67
Episode length: 591.32 +/- 341.07
Eval num_timesteps=100000, episode_reward=-70.50 +/- 61.36
Episode length: 670.35 +/- 355.31
Eval num_timesteps=105000, episode_reward=-81.79 +/- 40.60
Episode length: 656.42 +/- 377.76
Eval num_timesteps=110000, episode_reward=-57.94 +/- 32.01
Episode length: 809.22 +/- 330.73
Eval num_timesteps=115000, episode_reward=-83.77 +/- 28.80
Episode length: 789.86 +/- 322.15
Eval num_timesteps=120000, episode_reward=-91.43 +/- 43.08
Episode length: 601.68 +/- 371.17
Eval num_timesteps=125000, episode_reward=-112.16 +/- 39.12
Episode length: 675.54 +/- 355.26
Eval num_timesteps=130000, episode_reward=-88.79 +/- 35.05
Episode length: 735.16 +/- 356.46
Eval num_timesteps=135000, episode_reward=-111.08 +/- 31.02
Episode length: 623.88 +/- 365.03
Eval num_timesteps=140000, episode_reward=-109.85 +/- 32.26
Episode length: 686.26 +/- 361.07
Eval num_timesteps=145000, episode_reward=-119.87 +/- 34.85
Episode length: 635.49 +/- 355.27
Eval num_timesteps=150000, episode_reward=-117.45 +/- 38.23
Episode length: 659.09 +/- 370.30
FINISHED IN 2283.1531785050174 s


starting seed  1802 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-312.17 +/- 167.84
Episode length: 582.62 +/- 355.65
New best mean reward!
Eval num_timesteps=10000, episode_reward=-390.40 +/- 100.82
Episode length: 904.22 +/- 91.76
Eval num_timesteps=15000, episode_reward=-133.92 +/- 24.32
Episode length: 1000.00 +/- 0.00
New best mean reward!
Eval num_timesteps=20000, episode_reward=-19.81 +/- 22.76
Episode length: 1000.00 +/- 0.00
New best mean reward!
Eval num_timesteps=25000, episode_reward=70.38 +/- 100.85
Episode length: 926.42 +/- 85.30
New best mean reward!
Eval num_timesteps=30000, episode_reward=-58.74 +/- 38.27
Episode length: 995.35 +/- 25.27
Eval num_timesteps=35000, episode_reward=-61.90 +/- 59.39
Episode length: 972.49 +/- 80.34
Eval num_timesteps=40000, episode_reward=-50.14 +/- 48.40
Episode length: 996.62 +/- 23.69
Eval num_timesteps=45000, episode_reward=79.03 +/- 119.89
Episode length: 725.95 +/- 86.12
New best mean reward!
Eval num_timesteps=50000, episode_reward=36.49 +/- 152.84
Episode length: 468.34 +/- 114.72
Eval num_timesteps=55000, episode_reward=-16.12 +/- 128.91
Episode length: 692.83 +/- 143.29
Eval num_timesteps=60000, episode_reward=-65.97 +/- 93.18
Episode length: 759.85 +/- 289.51
Eval num_timesteps=65000, episode_reward=-86.09 +/- 59.44
Episode length: 796.49 +/- 299.13
Eval num_timesteps=70000, episode_reward=-76.57 +/- 82.28
Episode length: 738.86 +/- 296.81
Eval num_timesteps=75000, episode_reward=-98.70 +/- 61.16
Episode length: 669.72 +/- 325.12
Eval num_timesteps=80000, episode_reward=-60.76 +/- 118.15
Episode length: 444.94 +/- 204.05
Eval num_timesteps=85000, episode_reward=-14.05 +/- 117.42
Episode length: 627.55 +/- 271.91
Eval num_timesteps=90000, episode_reward=-104.26 +/- 42.63
Episode length: 488.03 +/- 349.13
Eval num_timesteps=95000, episode_reward=-139.23 +/- 34.77
Episode length: 516.17 +/- 350.12
Eval num_timesteps=100000, episode_reward=-99.54 +/- 37.21
Episode length: 568.20 +/- 369.45
Eval num_timesteps=105000, episode_reward=-81.73 +/- 49.88
Episode length: 576.56 +/- 374.28
Eval num_timesteps=110000, episode_reward=-60.59 +/- 97.94
Episode length: 460.34 +/- 284.36
Eval num_timesteps=115000, episode_reward=-72.16 +/- 80.29
Episode length: 504.61 +/- 340.06
Eval num_timesteps=120000, episode_reward=-70.97 +/- 59.53
Episode length: 584.45 +/- 387.97
Eval num_timesteps=125000, episode_reward=-95.31 +/- 35.73
Episode length: 549.39 +/- 379.26
Eval num_timesteps=130000, episode_reward=-77.20 +/- 44.97
Episode length: 606.05 +/- 386.97
Eval num_timesteps=135000, episode_reward=-104.85 +/- 35.52
Episode length: 556.31 +/- 381.75
Eval num_timesteps=140000, episode_reward=-121.82 +/- 38.06
Episode length: 533.19 +/- 362.68
Eval num_timesteps=145000, episode_reward=-115.50 +/- 26.28
Episode length: 454.69 +/- 344.97
Eval num_timesteps=150000, episode_reward=-113.54 +/- 20.63
Episode length: 504.55 +/- 385.45
FINISHED IN 2230.905840261985 s


starting seed  1803 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-453.55 +/- 31.46
Episode length: 494.99 +/- 58.46
New best mean reward!
Eval num_timesteps=10000, episode_reward=-499.22 +/- 45.17
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=7.04 +/- 51.56
Episode length: 985.48 +/- 53.70
New best mean reward!
Eval num_timesteps=20000, episode_reward=-53.38 +/- 33.53
Episode length: 996.81 +/- 22.08
Eval num_timesteps=25000, episode_reward=-151.21 +/- 70.66
Episode length: 879.62 +/- 132.10
Eval num_timesteps=30000, episode_reward=-31.54 +/- 25.69
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=35000, episode_reward=-64.77 +/- 36.79
Episode length: 990.96 +/- 32.12
Eval num_timesteps=40000, episode_reward=-31.11 +/- 28.64
Episode length: 981.97 +/- 123.89
Eval num_timesteps=45000, episode_reward=-45.93 +/- 22.78
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=50000, episode_reward=-52.68 +/- 64.97
Episode length: 976.73 +/- 56.40
Eval num_timesteps=55000, episode_reward=47.42 +/- 93.54
Episode length: 869.31 +/- 116.72
New best mean reward!
Eval num_timesteps=60000, episode_reward=-20.93 +/- 21.48
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=65000, episode_reward=18.46 +/- 101.20
Episode length: 867.07 +/- 123.89
Eval num_timesteps=70000, episode_reward=122.65 +/- 94.76
Episode length: 684.67 +/- 162.61
New best mean reward!
Eval num_timesteps=75000, episode_reward=-2.87 +/- 127.38
Episode length: 551.26 +/- 199.06
Eval num_timesteps=80000, episode_reward=96.13 +/- 118.48
Episode length: 415.67 +/- 124.69
Eval num_timesteps=85000, episode_reward=-48.66 +/- 96.18
Episode length: 789.14 +/- 264.55
Eval num_timesteps=90000, episode_reward=-53.62 +/- 103.32
Episode length: 523.75 +/- 276.47
Eval num_timesteps=95000, episode_reward=-79.83 +/- 59.23
Episode length: 532.97 +/- 332.68
Eval num_timesteps=100000, episode_reward=-30.89 +/- 103.58
Episode length: 385.89 +/- 214.67
Eval num_timesteps=105000, episode_reward=-48.99 +/- 87.42
Episode length: 426.61 +/- 273.84
Eval num_timesteps=110000, episode_reward=-49.87 +/- 39.00
Episode length: 685.36 +/- 379.09
Eval num_timesteps=115000, episode_reward=-53.48 +/- 26.43
Episode length: 862.53 +/- 294.17
Eval num_timesteps=120000, episode_reward=-53.86 +/- 33.22
Episode length: 820.26 +/- 322.96
Eval num_timesteps=125000, episode_reward=-87.27 +/- 41.43
Episode length: 643.68 +/- 373.61
Eval num_timesteps=130000, episode_reward=-114.25 +/- 38.11
Episode length: 644.76 +/- 375.43
Eval num_timesteps=135000, episode_reward=-89.86 +/- 25.32
Episode length: 589.38 +/- 386.13
Eval num_timesteps=140000, episode_reward=-73.73 +/- 30.47
Episode length: 661.31 +/- 393.05
Eval num_timesteps=145000, episode_reward=-98.72 +/- 48.19
Episode length: 565.03 +/- 364.70
Eval num_timesteps=150000, episode_reward=-94.64 +/- 36.59
Episode length: 648.05 +/- 369.20
FINISHED IN 2548.187818080012 s


starting seed  1804 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-566.65 +/- 151.60
Episode length: 66.33 +/- 12.54
New best mean reward!
Eval num_timesteps=10000, episode_reward=-128.51 +/- 42.13
Episode length: 69.23 +/- 12.31
New best mean reward!
Eval num_timesteps=15000, episode_reward=-67.90 +/- 105.46
Episode length: 774.98 +/- 192.26
New best mean reward!
Eval num_timesteps=20000, episode_reward=126.53 +/- 107.76
Episode length: 335.82 +/- 236.84
New best mean reward!
Eval num_timesteps=25000, episode_reward=-38.08 +/- 115.11
Episode length: 441.67 +/- 214.80
Eval num_timesteps=30000, episode_reward=-61.22 +/- 24.77
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=35000, episode_reward=169.94 +/- 87.36
Episode length: 514.39 +/- 132.16
New best mean reward!
Eval num_timesteps=40000, episode_reward=61.42 +/- 128.24
Episode length: 326.07 +/- 154.86
Eval num_timesteps=45000, episode_reward=53.47 +/- 108.48
Episode length: 709.47 +/- 204.47
Eval num_timesteps=50000, episode_reward=22.23 +/- 93.69
Episode length: 919.38 +/- 144.60
Eval num_timesteps=55000, episode_reward=-49.61 +/- 76.79
Episode length: 887.91 +/- 188.46
Eval num_timesteps=60000, episode_reward=-135.37 +/- 51.29
Episode length: 667.04 +/- 275.47
Eval num_timesteps=65000, episode_reward=-169.18 +/- 47.09
Episode length: 883.28 +/- 184.09
Eval num_timesteps=70000, episode_reward=-43.48 +/- 69.03
Episode length: 898.87 +/- 159.50
Eval num_timesteps=75000, episode_reward=-104.37 +/- 96.95
Episode length: 713.22 +/- 266.76
Eval num_timesteps=80000, episode_reward=-30.18 +/- 26.26
Episode length: 990.50 +/- 66.68
Eval num_timesteps=85000, episode_reward=-67.28 +/- 50.39
Episode length: 941.64 +/- 148.45
Eval num_timesteps=90000, episode_reward=-108.93 +/- 55.09
Episode length: 790.72 +/- 291.45
Eval num_timesteps=95000, episode_reward=-110.68 +/- 27.13
Episode length: 899.06 +/- 262.73
Eval num_timesteps=100000, episode_reward=-63.81 +/- 33.10
Episode length: 963.97 +/- 149.72
Eval num_timesteps=105000, episode_reward=-108.84 +/- 24.77
Episode length: 964.66 +/- 154.13
Eval num_timesteps=110000, episode_reward=-58.35 +/- 26.10
Episode length: 967.68 +/- 123.45
Eval num_timesteps=115000, episode_reward=-110.86 +/- 40.17
Episode length: 908.05 +/- 198.03
Eval num_timesteps=120000, episode_reward=-39.20 +/- 98.02
Episode length: 772.96 +/- 242.12
Eval num_timesteps=125000, episode_reward=-28.94 +/- 115.44
Episode length: 487.81 +/- 183.05
Eval num_timesteps=130000, episode_reward=-70.90 +/- 62.71
Episode length: 782.38 +/- 267.10
Eval num_timesteps=135000, episode_reward=-74.52 +/- 63.94
Episode length: 828.05 +/- 253.81
Eval num_timesteps=140000, episode_reward=-70.15 +/- 67.60
Episode length: 764.59 +/- 258.35
Eval num_timesteps=145000, episode_reward=-67.03 +/- 73.69
Episode length: 781.21 +/- 270.80
Eval num_timesteps=150000, episode_reward=-53.41 +/- 83.38
Episode length: 736.70 +/- 278.53
FINISHED IN 2535.6048770990164 s


starting seed  1805 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-121.06 +/- 94.12
Episode length: 75.29 +/- 12.89
New best mean reward!
Eval num_timesteps=10000, episode_reward=-190.69 +/- 48.76
Episode length: 692.12 +/- 214.64
Eval num_timesteps=15000, episode_reward=-126.20 +/- 60.13
Episode length: 929.54 +/- 170.67
Eval num_timesteps=20000, episode_reward=-16.99 +/- 133.54
Episode length: 630.18 +/- 136.17
New best mean reward!
Eval num_timesteps=25000, episode_reward=-58.24 +/- 23.99
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=30000, episode_reward=-37.05 +/- 32.23
Episode length: 998.66 +/- 8.43
Eval num_timesteps=35000, episode_reward=-10.36 +/- 52.17
Episode length: 988.07 +/- 39.62
New best mean reward!
Eval num_timesteps=40000, episode_reward=-36.08 +/- 65.09
Episode length: 948.80 +/- 171.53
Eval num_timesteps=45000, episode_reward=177.31 +/- 93.90
Episode length: 326.47 +/- 176.77
New best mean reward!
Eval num_timesteps=50000, episode_reward=-19.98 +/- 85.33
Episode length: 144.80 +/- 65.99
Eval num_timesteps=55000, episode_reward=20.59 +/- 86.80
Episode length: 152.90 +/- 66.88
Eval num_timesteps=60000, episode_reward=-37.22 +/- 104.16
Episode length: 306.83 +/- 171.15
Eval num_timesteps=65000, episode_reward=92.46 +/- 121.67
Episode length: 262.67 +/- 164.41
Eval num_timesteps=70000, episode_reward=-39.44 +/- 103.30
Episode length: 289.35 +/- 180.54
Eval num_timesteps=75000, episode_reward=-12.73 +/- 91.12
Episode length: 186.69 +/- 159.58
Eval num_timesteps=80000, episode_reward=-3.83 +/- 112.86
Episode length: 660.55 +/- 322.99
Eval num_timesteps=85000, episode_reward=-2.35 +/- 136.01
Episode length: 736.87 +/- 302.96
Eval num_timesteps=90000, episode_reward=5.72 +/- 66.69
Episode length: 969.07 +/- 80.83
Eval num_timesteps=95000, episode_reward=83.23 +/- 114.99
Episode length: 614.04 +/- 134.12
Eval num_timesteps=100000, episode_reward=15.03 +/- 81.05
Episode length: 925.24 +/- 148.91
Eval num_timesteps=105000, episode_reward=201.81 +/- 90.68
Episode length: 363.31 +/- 138.53
New best mean reward!
FINISHED IN 1315.7407809240103 s


starting seed  1806 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-463.21 +/- 150.12
Episode length: 171.52 +/- 97.27
New best mean reward!
Eval num_timesteps=10000, episode_reward=-217.78 +/- 40.54
Episode length: 763.09 +/- 132.26
New best mean reward!
Eval num_timesteps=15000, episode_reward=77.30 +/- 87.86
Episode length: 923.34 +/- 77.95
New best mean reward!
Eval num_timesteps=20000, episode_reward=-172.02 +/- 27.71
Episode length: 465.95 +/- 156.23
Eval num_timesteps=25000, episode_reward=-18.87 +/- 92.99
Episode length: 945.66 +/- 87.28
Eval num_timesteps=30000, episode_reward=-116.17 +/- 96.45
Episode length: 783.57 +/- 180.92
Eval num_timesteps=35000, episode_reward=-75.10 +/- 57.08
Episode length: 946.66 +/- 112.35
Eval num_timesteps=40000, episode_reward=9.21 +/- 130.94
Episode length: 496.20 +/- 154.62
Eval num_timesteps=45000, episode_reward=-17.49 +/- 117.10
Episode length: 406.18 +/- 161.78
Eval num_timesteps=50000, episode_reward=-35.83 +/- 111.60
Episode length: 294.05 +/- 112.52
Eval num_timesteps=55000, episode_reward=-110.43 +/- 89.22
Episode length: 368.99 +/- 229.59
Eval num_timesteps=60000, episode_reward=-120.53 +/- 41.35
Episode length: 577.01 +/- 313.90
Eval num_timesteps=65000, episode_reward=-100.04 +/- 41.25
Episode length: 889.86 +/- 255.53
Eval num_timesteps=70000, episode_reward=-108.54 +/- 41.49
Episode length: 909.74 +/- 248.96
Eval num_timesteps=75000, episode_reward=-114.34 +/- 103.65
Episode length: 602.59 +/- 285.49
Eval num_timesteps=80000, episode_reward=-121.82 +/- 47.15
Episode length: 486.36 +/- 315.52
Eval num_timesteps=85000, episode_reward=-123.25 +/- 44.97
Episode length: 486.32 +/- 286.71
Eval num_timesteps=90000, episode_reward=-137.51 +/- 43.84
Episode length: 660.51 +/- 370.48
Eval num_timesteps=95000, episode_reward=-143.10 +/- 42.23
Episode length: 507.34 +/- 362.04
Eval num_timesteps=100000, episode_reward=-135.51 +/- 36.88
Episode length: 501.15 +/- 349.40
Eval num_timesteps=105000, episode_reward=-140.11 +/- 47.72
Episode length: 541.61 +/- 352.27
Eval num_timesteps=110000, episode_reward=-132.99 +/- 43.25
Episode length: 468.03 +/- 308.54
Eval num_timesteps=115000, episode_reward=-143.72 +/- 43.61
Episode length: 525.40 +/- 338.65
Eval num_timesteps=120000, episode_reward=-146.76 +/- 36.41
Episode length: 420.36 +/- 315.95
Eval num_timesteps=125000, episode_reward=-114.05 +/- 52.02
Episode length: 483.43 +/- 327.24
Eval num_timesteps=130000, episode_reward=-82.10 +/- 79.99
Episode length: 465.51 +/- 327.98
Eval num_timesteps=135000, episode_reward=-99.16 +/- 52.62
Episode length: 478.40 +/- 329.93
Eval num_timesteps=140000, episode_reward=-118.04 +/- 52.58
Episode length: 523.87 +/- 334.06
Eval num_timesteps=145000, episode_reward=-118.64 +/- 41.53
Episode length: 410.99 +/- 314.37
Eval num_timesteps=150000, episode_reward=-121.17 +/- 33.90
Episode length: 408.49 +/- 300.97
FINISHED IN 1808.0373057109828 s


starting seed  1807 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-819.95 +/- 640.95
Episode length: 128.42 +/- 56.08
New best mean reward!
Eval num_timesteps=10000, episode_reward=-77.35 +/- 69.67
Episode length: 95.03 +/- 47.87
New best mean reward!
Eval num_timesteps=15000, episode_reward=-695.58 +/- 60.69
Episode length: 548.55 +/- 112.68
Eval num_timesteps=20000, episode_reward=-15.18 +/- 56.15
Episode length: 926.42 +/- 232.59
New best mean reward!
Eval num_timesteps=25000, episode_reward=-192.65 +/- 42.79
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=30000, episode_reward=11.50 +/- 61.69
Episode length: 989.48 +/- 34.97
New best mean reward!
Eval num_timesteps=35000, episode_reward=-26.99 +/- 101.83
Episode length: 821.01 +/- 208.08
Eval num_timesteps=40000, episode_reward=210.15 +/- 60.64
Episode length: 480.61 +/- 118.54
New best mean reward!
FINISHED IN 544.4383392549935 s


starting seed  1808 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-417.45 +/- 82.77
Episode length: 687.33 +/- 107.00
New best mean reward!
Eval num_timesteps=10000, episode_reward=-117.33 +/- 26.61
Episode length: 980.00 +/- 84.30
New best mean reward!
Eval num_timesteps=15000, episode_reward=-98.85 +/- 51.99
Episode length: 897.96 +/- 162.41
New best mean reward!
Eval num_timesteps=20000, episode_reward=-21.46 +/- 120.65
Episode length: 462.38 +/- 197.92
New best mean reward!
Eval num_timesteps=25000, episode_reward=-200.43 +/- 55.54
Episode length: 515.19 +/- 247.26
Eval num_timesteps=30000, episode_reward=-173.92 +/- 47.18
Episode length: 584.84 +/- 282.80
Eval num_timesteps=35000, episode_reward=-121.33 +/- 35.69
Episode length: 783.24 +/- 291.91
Eval num_timesteps=40000, episode_reward=-139.55 +/- 47.07
Episode length: 581.27 +/- 308.51
Eval num_timesteps=45000, episode_reward=-99.54 +/- 80.28
Episode length: 561.85 +/- 315.52
Eval num_timesteps=50000, episode_reward=-165.64 +/- 51.47
Episode length: 442.90 +/- 286.33
Eval num_timesteps=55000, episode_reward=-124.01 +/- 42.30
Episode length: 498.38 +/- 331.86
Eval num_timesteps=60000, episode_reward=-129.77 +/- 34.42
Episode length: 435.92 +/- 313.08
Eval num_timesteps=65000, episode_reward=-152.25 +/- 36.30
Episode length: 403.45 +/- 290.46
Eval num_timesteps=70000, episode_reward=-167.90 +/- 45.93
Episode length: 411.67 +/- 329.60
Eval num_timesteps=75000, episode_reward=-101.54 +/- 38.52
Episode length: 607.92 +/- 390.52
Eval num_timesteps=80000, episode_reward=-178.21 +/- 42.23
Episode length: 443.94 +/- 288.87
Eval num_timesteps=85000, episode_reward=-128.97 +/- 37.49
Episode length: 442.59 +/- 321.60
Eval num_timesteps=90000, episode_reward=-119.39 +/- 33.26
Episode length: 471.21 +/- 338.12
Eval num_timesteps=95000, episode_reward=-142.51 +/- 34.06
Episode length: 405.75 +/- 306.90
Eval num_timesteps=100000, episode_reward=-104.18 +/- 36.22
Episode length: 648.06 +/- 376.18
Eval num_timesteps=105000, episode_reward=-113.41 +/- 34.62
Episode length: 552.72 +/- 364.41
Eval num_timesteps=110000, episode_reward=-131.37 +/- 50.11
Episode length: 518.50 +/- 361.82
Eval num_timesteps=115000, episode_reward=-134.85 +/- 40.85
Episode length: 471.38 +/- 299.78
Eval num_timesteps=120000, episode_reward=-125.20 +/- 31.32
Episode length: 475.13 +/- 351.45
Eval num_timesteps=125000, episode_reward=-149.63 +/- 54.61
Episode length: 474.02 +/- 306.13
Eval num_timesteps=130000, episode_reward=-117.94 +/- 39.57
Episode length: 565.17 +/- 366.51
Eval num_timesteps=135000, episode_reward=-115.96 +/- 34.39
Episode length: 516.76 +/- 338.05
Eval num_timesteps=140000, episode_reward=-108.75 +/- 37.53
Episode length: 488.73 +/- 355.00
Eval num_timesteps=145000, episode_reward=-114.78 +/- 33.01
Episode length: 451.86 +/- 324.07
Eval num_timesteps=150000, episode_reward=-112.75 +/- 37.60
Episode length: 524.14 +/- 361.91
FINISHED IN 1841.609198561986 s


starting seed  1809 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-834.76 +/- 301.04
Episode length: 130.74 +/- 33.85
New best mean reward!
Eval num_timesteps=10000, episode_reward=-403.17 +/- 78.45
Episode length: 833.10 +/- 139.50
New best mean reward!
Eval num_timesteps=15000, episode_reward=-191.37 +/- 32.92
Episode length: 1000.00 +/- 0.00
New best mean reward!
Eval num_timesteps=20000, episode_reward=-102.45 +/- 29.90
Episode length: 1000.00 +/- 0.00
New best mean reward!
Eval num_timesteps=25000, episode_reward=-23.74 +/- 41.72
Episode length: 986.14 +/- 55.59
New best mean reward!
Eval num_timesteps=30000, episode_reward=-157.86 +/- 67.08
Episode length: 784.06 +/- 193.56
Eval num_timesteps=35000, episode_reward=-86.46 +/- 22.02
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=40000, episode_reward=5.22 +/- 103.57
Episode length: 815.98 +/- 144.65
New best mean reward!
Eval num_timesteps=45000, episode_reward=-29.44 +/- 27.32
Episode length: 996.50 +/- 21.49
Eval num_timesteps=50000, episode_reward=-38.50 +/- 23.40
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=55000, episode_reward=15.28 +/- 41.20
Episode length: 996.39 +/- 14.95
New best mean reward!
Eval num_timesteps=60000, episode_reward=-137.24 +/- 57.17
Episode length: 611.78 +/- 288.81
Eval num_timesteps=65000, episode_reward=-84.88 +/- 45.50
Episode length: 832.46 +/- 278.79
Eval num_timesteps=70000, episode_reward=-1.53 +/- 106.05
Episode length: 697.10 +/- 287.17
Eval num_timesteps=75000, episode_reward=-18.35 +/- 110.77
Episode length: 517.64 +/- 299.67
Eval num_timesteps=80000, episode_reward=-71.49 +/- 97.24
Episode length: 390.34 +/- 223.46
Eval num_timesteps=85000, episode_reward=-83.25 +/- 46.97
Episode length: 708.26 +/- 343.87
Eval num_timesteps=90000, episode_reward=-93.05 +/- 51.56
Episode length: 611.16 +/- 358.96
Eval num_timesteps=95000, episode_reward=-85.27 +/- 49.63
Episode length: 588.12 +/- 377.70
Eval num_timesteps=100000, episode_reward=-88.73 +/- 41.49
Episode length: 689.08 +/- 366.20
Eval num_timesteps=105000, episode_reward=-105.95 +/- 74.88
Episode length: 426.13 +/- 277.73
Eval num_timesteps=110000, episode_reward=-94.50 +/- 33.13
Episode length: 657.58 +/- 383.42
Eval num_timesteps=115000, episode_reward=-103.31 +/- 33.34
Episode length: 593.72 +/- 369.23
Eval num_timesteps=120000, episode_reward=-100.84 +/- 41.51
Episode length: 579.84 +/- 351.90
Eval num_timesteps=125000, episode_reward=-105.24 +/- 48.87
Episode length: 486.71 +/- 349.06
Eval num_timesteps=130000, episode_reward=-107.56 +/- 45.44
Episode length: 498.98 +/- 346.18
Eval num_timesteps=135000, episode_reward=-114.20 +/- 26.92
Episode length: 411.40 +/- 304.23
Eval num_timesteps=140000, episode_reward=-107.94 +/- 31.63
Episode length: 401.42 +/- 324.70
Eval num_timesteps=145000, episode_reward=-105.03 +/- 40.78
Episode length: 496.65 +/- 348.60
Eval num_timesteps=150000, episode_reward=-106.64 +/- 36.61
Episode length: 478.29 +/- 349.43
FINISHED IN 2386.9133765320003 s


starting seed  1810 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=66.09 +/- 107.21
Episode length: 411.66 +/- 237.87
New best mean reward!
Eval num_timesteps=10000, episode_reward=167.02 +/- 28.43
Episode length: 811.37 +/- 57.99
New best mean reward!
Eval num_timesteps=15000, episode_reward=23.89 +/- 112.92
Episode length: 920.81 +/- 94.12
Eval num_timesteps=20000, episode_reward=8.39 +/- 36.26
Episode length: 155.40 +/- 33.62
Eval num_timesteps=25000, episode_reward=23.29 +/- 107.25
Episode length: 521.39 +/- 251.06
Eval num_timesteps=30000, episode_reward=-79.84 +/- 30.37
Episode length: 818.69 +/- 300.54
Eval num_timesteps=35000, episode_reward=-195.40 +/- 71.96
Episode length: 615.66 +/- 284.29
Eval num_timesteps=40000, episode_reward=9.59 +/- 75.78
Episode length: 949.01 +/- 115.31
Eval num_timesteps=45000, episode_reward=-88.57 +/- 27.21
Episode length: 978.54 +/- 126.14
Eval num_timesteps=50000, episode_reward=-96.74 +/- 53.90
Episode length: 671.01 +/- 353.54
Eval num_timesteps=55000, episode_reward=-91.95 +/- 28.92
Episode length: 806.12 +/- 324.98
Eval num_timesteps=60000, episode_reward=-117.76 +/- 47.92
Episode length: 667.74 +/- 333.70
Eval num_timesteps=65000, episode_reward=-74.15 +/- 38.61
Episode length: 728.60 +/- 374.40
Eval num_timesteps=70000, episode_reward=-106.71 +/- 40.51
Episode length: 492.41 +/- 342.26
Eval num_timesteps=75000, episode_reward=-67.20 +/- 57.68
Episode length: 644.61 +/- 373.05
Eval num_timesteps=80000, episode_reward=-107.93 +/- 45.62
Episode length: 418.65 +/- 325.01
Eval num_timesteps=85000, episode_reward=-101.31 +/- 35.63
Episode length: 526.93 +/- 364.07
Eval num_timesteps=90000, episode_reward=-153.35 +/- 45.57
Episode length: 438.94 +/- 322.10
Eval num_timesteps=95000, episode_reward=-132.08 +/- 32.28
Episode length: 509.99 +/- 365.41
Eval num_timesteps=100000, episode_reward=-99.36 +/- 37.75
Episode length: 531.91 +/- 359.43
Eval num_timesteps=105000, episode_reward=-95.63 +/- 58.60
Episode length: 384.00 +/- 271.60
Eval num_timesteps=110000, episode_reward=-50.73 +/- 97.85
Episode length: 338.80 +/- 205.99
Eval num_timesteps=115000, episode_reward=-5.46 +/- 127.22
Episode length: 298.85 +/- 173.61
Eval num_timesteps=120000, episode_reward=5.67 +/- 124.50
Episode length: 242.54 +/- 102.63
Eval num_timesteps=125000, episode_reward=-6.44 +/- 115.29
Episode length: 231.85 +/- 135.42
Eval num_timesteps=130000, episode_reward=-12.63 +/- 114.56
Episode length: 304.67 +/- 218.95
Eval num_timesteps=135000, episode_reward=-15.95 +/- 118.35
Episode length: 288.41 +/- 156.33
Eval num_timesteps=140000, episode_reward=-9.59 +/- 124.22
Episode length: 316.66 +/- 191.34
Eval num_timesteps=145000, episode_reward=-26.02 +/- 114.86
Episode length: 305.93 +/- 163.60
Eval num_timesteps=150000, episode_reward=-12.39 +/- 115.41
Episode length: 313.53 +/- 185.85
FINISHED IN 1750.1922968080034 s


starting seed  1811 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-76.66 +/- 20.21
Episode length: 990.93 +/- 90.25
New best mean reward!
Eval num_timesteps=10000, episode_reward=-168.72 +/- 28.33
Episode length: 678.17 +/- 129.67
Eval num_timesteps=15000, episode_reward=-43.77 +/- 26.64
Episode length: 1000.00 +/- 0.00
New best mean reward!
Eval num_timesteps=20000, episode_reward=-2.82 +/- 140.64
Episode length: 763.99 +/- 184.77
New best mean reward!
Eval num_timesteps=25000, episode_reward=-101.17 +/- 107.65
Episode length: 906.27 +/- 122.96
Eval num_timesteps=30000, episode_reward=-67.55 +/- 41.09
Episode length: 993.37 +/- 65.97
Eval num_timesteps=35000, episode_reward=99.13 +/- 89.14
Episode length: 721.80 +/- 205.19
New best mean reward!
Eval num_timesteps=40000, episode_reward=124.17 +/- 122.41
Episode length: 355.82 +/- 168.38
New best mean reward!
Eval num_timesteps=45000, episode_reward=-14.43 +/- 115.53
Episode length: 295.26 +/- 79.20
Eval num_timesteps=50000, episode_reward=-49.87 +/- 28.27
Episode length: 998.28 +/- 17.11
Eval num_timesteps=55000, episode_reward=-199.59 +/- 30.91
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=60000, episode_reward=-118.54 +/- 26.58
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=65000, episode_reward=-114.37 +/- 25.77
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=70000, episode_reward=-44.75 +/- 25.22
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=75000, episode_reward=-61.76 +/- 27.69
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=80000, episode_reward=-25.29 +/- 33.80
Episode length: 997.67 +/- 21.45
Eval num_timesteps=85000, episode_reward=-59.44 +/- 23.29
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=90000, episode_reward=-86.26 +/- 23.83
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=95000, episode_reward=-52.36 +/- 25.14
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=100000, episode_reward=-32.65 +/- 22.48
Episode length: 997.34 +/- 26.47
Eval num_timesteps=105000, episode_reward=-5.90 +/- 34.66
Episode length: 994.75 +/- 32.56
Eval num_timesteps=110000, episode_reward=142.03 +/- 53.87
Episode length: 803.10 +/- 108.86
New best mean reward!
Eval num_timesteps=115000, episode_reward=175.43 +/- 55.51
Episode length: 617.45 +/- 118.30
New best mean reward!
Eval num_timesteps=120000, episode_reward=127.81 +/- 68.84
Episode length: 786.03 +/- 107.26
Eval num_timesteps=125000, episode_reward=130.52 +/- 54.93
Episode length: 826.48 +/- 107.06
Eval num_timesteps=130000, episode_reward=147.16 +/- 60.08
Episode length: 727.07 +/- 129.36
Eval num_timesteps=135000, episode_reward=76.50 +/- 76.23
Episode length: 912.04 +/- 122.28
Eval num_timesteps=140000, episode_reward=130.69 +/- 62.89
Episode length: 807.12 +/- 116.61
Eval num_timesteps=145000, episode_reward=148.45 +/- 60.05
Episode length: 754.31 +/- 139.34
Eval num_timesteps=150000, episode_reward=141.18 +/- 68.61
Episode length: 737.36 +/- 138.54
FINISHED IN 2983.542153590999 s


starting seed  1812 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-307.78 +/- 32.31
Episode length: 991.31 +/- 86.46
New best mean reward!
Eval num_timesteps=10000, episode_reward=-119.91 +/- 39.32
Episode length: 993.90 +/- 25.21
New best mean reward!
Eval num_timesteps=15000, episode_reward=-116.71 +/- 24.02
Episode length: 1000.00 +/- 0.00
New best mean reward!
Eval num_timesteps=20000, episode_reward=36.85 +/- 83.34
Episode length: 927.19 +/- 88.63
New best mean reward!
Eval num_timesteps=25000, episode_reward=-84.27 +/- 66.08
Episode length: 972.97 +/- 59.61
Eval num_timesteps=30000, episode_reward=-58.09 +/- 114.97
Episode length: 594.17 +/- 147.94
Eval num_timesteps=35000, episode_reward=-104.42 +/- 42.82
Episode length: 851.84 +/- 283.24
Eval num_timesteps=40000, episode_reward=-59.83 +/- 62.60
Episode length: 959.18 +/- 107.26
Eval num_timesteps=45000, episode_reward=-61.33 +/- 33.78
Episode length: 921.77 +/- 238.30
Eval num_timesteps=50000, episode_reward=-67.06 +/- 90.79
Episode length: 596.66 +/- 298.49
Eval num_timesteps=55000, episode_reward=-108.90 +/- 84.47
Episode length: 463.29 +/- 294.57
Eval num_timesteps=60000, episode_reward=-81.37 +/- 94.37
Episode length: 406.87 +/- 251.13
Eval num_timesteps=65000, episode_reward=-164.03 +/- 42.57
Episode length: 470.98 +/- 295.02
Eval num_timesteps=70000, episode_reward=-124.38 +/- 39.81
Episode length: 438.29 +/- 311.62
Eval num_timesteps=75000, episode_reward=-128.49 +/- 48.76
Episode length: 398.83 +/- 287.33
Eval num_timesteps=80000, episode_reward=-135.58 +/- 38.08
Episode length: 431.09 +/- 291.55
Eval num_timesteps=85000, episode_reward=-68.18 +/- 84.75
Episode length: 455.20 +/- 307.12
Eval num_timesteps=90000, episode_reward=-112.50 +/- 58.44
Episode length: 458.21 +/- 322.91
Eval num_timesteps=95000, episode_reward=-41.76 +/- 107.57
Episode length: 375.14 +/- 243.97
Eval num_timesteps=100000, episode_reward=-83.19 +/- 80.93
Episode length: 326.56 +/- 247.49
Eval num_timesteps=105000, episode_reward=-107.64 +/- 56.66
Episode length: 404.44 +/- 301.60
Eval num_timesteps=110000, episode_reward=-98.49 +/- 73.83
Episode length: 372.86 +/- 281.85
Eval num_timesteps=115000, episode_reward=-84.07 +/- 85.58
Episode length: 407.68 +/- 254.86
Eval num_timesteps=120000, episode_reward=-101.71 +/- 93.38
Episode length: 394.21 +/- 286.37
Eval num_timesteps=125000, episode_reward=-51.44 +/- 101.94
Episode length: 312.64 +/- 231.99
Eval num_timesteps=130000, episode_reward=-72.97 +/- 97.45
Episode length: 473.63 +/- 345.98
Eval num_timesteps=135000, episode_reward=-104.71 +/- 66.57
Episode length: 379.94 +/- 308.39
Eval num_timesteps=140000, episode_reward=-103.26 +/- 97.59
Episode length: 395.10 +/- 301.78
Eval num_timesteps=145000, episode_reward=-110.53 +/- 87.16
Episode length: 417.80 +/- 326.20
Eval num_timesteps=150000, episode_reward=-115.58 +/- 87.28
Episode length: 382.69 +/- 275.84
FINISHED IN 1872.2642284329922 s


starting seed  1813 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-208.38 +/- 97.94
Episode length: 387.78 +/- 203.42
New best mean reward!
Eval num_timesteps=10000, episode_reward=93.74 +/- 111.46
Episode length: 615.71 +/- 236.47
New best mean reward!
Eval num_timesteps=15000, episode_reward=-41.19 +/- 77.23
Episode length: 934.92 +/- 103.95
Eval num_timesteps=20000, episode_reward=-161.40 +/- 90.42
Episode length: 756.33 +/- 209.20
Eval num_timesteps=25000, episode_reward=46.12 +/- 128.53
Episode length: 424.03 +/- 142.09
Eval num_timesteps=30000, episode_reward=-2.44 +/- 129.26
Episode length: 346.58 +/- 139.26
Eval num_timesteps=35000, episode_reward=-77.98 +/- 88.66
Episode length: 482.62 +/- 248.10
Eval num_timesteps=40000, episode_reward=-112.75 +/- 62.30
Episode length: 488.91 +/- 240.27
Eval num_timesteps=45000, episode_reward=-63.94 +/- 71.92
Episode length: 671.59 +/- 316.55
Eval num_timesteps=50000, episode_reward=-78.15 +/- 27.99
Episode length: 982.19 +/- 91.72
Eval num_timesteps=55000, episode_reward=3.40 +/- 81.02
Episode length: 906.26 +/- 170.67
Eval num_timesteps=60000, episode_reward=27.38 +/- 116.03
Episode length: 767.79 +/- 183.90
Eval num_timesteps=65000, episode_reward=28.07 +/- 122.26
Episode length: 645.84 +/- 241.46
Eval num_timesteps=70000, episode_reward=-94.56 +/- 62.04
Episode length: 639.25 +/- 345.85
Eval num_timesteps=75000, episode_reward=-94.11 +/- 60.07
Episode length: 531.85 +/- 309.31
Eval num_timesteps=80000, episode_reward=-93.65 +/- 72.05
Episode length: 481.83 +/- 294.94
Eval num_timesteps=85000, episode_reward=-126.09 +/- 36.63
Episode length: 415.94 +/- 311.06
Eval num_timesteps=90000, episode_reward=-101.92 +/- 65.61
Episode length: 523.16 +/- 331.92
Eval num_timesteps=95000, episode_reward=-148.44 +/- 40.52
Episode length: 458.73 +/- 321.28
Eval num_timesteps=100000, episode_reward=-114.46 +/- 38.33
Episode length: 426.62 +/- 308.21
Eval num_timesteps=105000, episode_reward=-115.73 +/- 41.23
Episode length: 478.82 +/- 318.99
Eval num_timesteps=110000, episode_reward=-96.55 +/- 55.25
Episode length: 511.11 +/- 330.68
Eval num_timesteps=115000, episode_reward=-95.31 +/- 53.19
Episode length: 516.14 +/- 323.88
Eval num_timesteps=120000, episode_reward=-81.35 +/- 74.74
Episode length: 520.23 +/- 335.20
Eval num_timesteps=125000, episode_reward=-80.29 +/- 62.06
Episode length: 547.14 +/- 332.94
Eval num_timesteps=130000, episode_reward=-51.45 +/- 81.47
Episode length: 558.83 +/- 330.92
Eval num_timesteps=135000, episode_reward=-77.15 +/- 81.74
Episode length: 504.03 +/- 307.14
Eval num_timesteps=140000, episode_reward=-108.25 +/- 41.46
Episode length: 431.47 +/- 310.39
Eval num_timesteps=145000, episode_reward=-106.11 +/- 55.68
Episode length: 419.39 +/- 303.65
Eval num_timesteps=150000, episode_reward=-106.55 +/- 43.50
Episode length: 408.73 +/- 279.64
FINISHED IN 1980.0261238279927 s


starting seed  1814 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-1051.05 +/- 199.40
Episode length: 283.73 +/- 55.37
New best mean reward!
Eval num_timesteps=10000, episode_reward=-220.20 +/- 49.30
Episode length: 1000.00 +/- 0.00
New best mean reward!
Eval num_timesteps=15000, episode_reward=-168.85 +/- 49.01
Episode length: 816.97 +/- 142.15
New best mean reward!
Eval num_timesteps=20000, episode_reward=-71.37 +/- 25.13
Episode length: 997.90 +/- 15.00
New best mean reward!
Eval num_timesteps=25000, episode_reward=10.15 +/- 78.61
Episode length: 984.59 +/- 38.35
New best mean reward!
Eval num_timesteps=30000, episode_reward=110.70 +/- 99.37
Episode length: 672.25 +/- 128.21
New best mean reward!
Eval num_timesteps=35000, episode_reward=61.89 +/- 100.00
Episode length: 857.78 +/- 144.20
Eval num_timesteps=40000, episode_reward=-44.38 +/- 68.47
Episode length: 966.52 +/- 95.93
Eval num_timesteps=45000, episode_reward=-36.67 +/- 43.77
Episode length: 990.37 +/- 45.48
Eval num_timesteps=50000, episode_reward=156.58 +/- 101.91
Episode length: 548.96 +/- 83.81
New best mean reward!
Eval num_timesteps=55000, episode_reward=126.94 +/- 97.82
Episode length: 666.62 +/- 111.36
Eval num_timesteps=60000, episode_reward=-28.82 +/- 92.61
Episode length: 786.49 +/- 261.74
Eval num_timesteps=65000, episode_reward=-77.87 +/- 65.31
Episode length: 772.86 +/- 267.51
Eval num_timesteps=70000, episode_reward=-5.41 +/- 113.43
Episode length: 620.84 +/- 270.36
Eval num_timesteps=75000, episode_reward=-43.51 +/- 93.74
Episode length: 392.44 +/- 229.55
Eval num_timesteps=80000, episode_reward=-144.30 +/- 44.44
Episode length: 367.83 +/- 252.03
Eval num_timesteps=85000, episode_reward=-120.31 +/- 38.05
Episode length: 433.89 +/- 297.00
Eval num_timesteps=90000, episode_reward=-115.49 +/- 45.82
Episode length: 587.66 +/- 347.02
Eval num_timesteps=95000, episode_reward=-133.45 +/- 46.55
Episode length: 543.34 +/- 340.92
Eval num_timesteps=100000, episode_reward=-114.30 +/- 33.82
Episode length: 480.81 +/- 328.75
Eval num_timesteps=105000, episode_reward=-157.25 +/- 53.35
Episode length: 439.16 +/- 287.37
Eval num_timesteps=110000, episode_reward=-119.34 +/- 33.16
Episode length: 563.06 +/- 355.23
Eval num_timesteps=115000, episode_reward=-121.73 +/- 38.83
Episode length: 464.32 +/- 323.06
Eval num_timesteps=120000, episode_reward=-127.66 +/- 55.30
Episode length: 534.86 +/- 340.03
Eval num_timesteps=125000, episode_reward=-156.14 +/- 66.61
Episode length: 530.78 +/- 328.37
Eval num_timesteps=130000, episode_reward=-129.38 +/- 39.84
Episode length: 404.29 +/- 292.78
Eval num_timesteps=135000, episode_reward=-142.90 +/- 34.61
Episode length: 463.36 +/- 324.60
Eval num_timesteps=140000, episode_reward=-144.94 +/- 36.86
Episode length: 383.35 +/- 267.57
Eval num_timesteps=145000, episode_reward=-144.85 +/- 44.84
Episode length: 469.84 +/- 337.20
Eval num_timesteps=150000, episode_reward=-146.48 +/- 38.95
Episode length: 429.92 +/- 290.41
FINISHED IN 2041.4056304130063 s


starting seed  1815 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-130.01 +/- 46.77
Episode length: 68.51 +/- 13.44
New best mean reward!
Eval num_timesteps=10000, episode_reward=-191.69 +/- 21.99
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-55.03 +/- 23.99
Episode length: 998.98 +/- 10.15
New best mean reward!
Eval num_timesteps=20000, episode_reward=40.04 +/- 113.48
Episode length: 729.37 +/- 137.92
New best mean reward!
Eval num_timesteps=25000, episode_reward=-30.66 +/- 29.86
Episode length: 999.42 +/- 5.77
Eval num_timesteps=30000, episode_reward=-16.00 +/- 46.77
Episode length: 996.32 +/- 24.68
Eval num_timesteps=35000, episode_reward=3.25 +/- 39.18
Episode length: 998.97 +/- 6.79
Eval num_timesteps=40000, episode_reward=-14.87 +/- 41.61
Episode length: 994.62 +/- 34.50
Eval num_timesteps=45000, episode_reward=-78.99 +/- 33.02
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=50000, episode_reward=-38.53 +/- 33.52
Episode length: 999.01 +/- 9.85
Eval num_timesteps=55000, episode_reward=-109.15 +/- 64.12
Episode length: 692.92 +/- 313.72
Eval num_timesteps=60000, episode_reward=-101.70 +/- 55.35
Episode length: 666.42 +/- 334.49
Eval num_timesteps=65000, episode_reward=-75.47 +/- 25.51
Episode length: 975.08 +/- 141.84
Eval num_timesteps=70000, episode_reward=-72.96 +/- 33.23
Episode length: 872.49 +/- 279.19
Eval num_timesteps=75000, episode_reward=-89.79 +/- 49.42
Episode length: 607.29 +/- 372.66
Eval num_timesteps=80000, episode_reward=-156.84 +/- 52.55
Episode length: 545.75 +/- 349.41
Eval num_timesteps=85000, episode_reward=-157.39 +/- 43.91
Episode length: 448.08 +/- 337.70
Eval num_timesteps=90000, episode_reward=-112.89 +/- 35.37
Episode length: 712.35 +/- 347.54
Eval num_timesteps=95000, episode_reward=-105.24 +/- 39.29
Episode length: 587.58 +/- 362.72
Eval num_timesteps=100000, episode_reward=-135.10 +/- 44.61
Episode length: 504.42 +/- 339.35
Eval num_timesteps=105000, episode_reward=-97.73 +/- 51.56
Episode length: 666.59 +/- 367.90
Eval num_timesteps=110000, episode_reward=-95.18 +/- 54.88
Episode length: 616.45 +/- 370.21
Eval num_timesteps=115000, episode_reward=-118.38 +/- 33.90
Episode length: 505.88 +/- 343.05
Eval num_timesteps=120000, episode_reward=-126.20 +/- 32.94
Episode length: 566.70 +/- 358.71
Eval num_timesteps=125000, episode_reward=-124.78 +/- 36.67
Episode length: 512.16 +/- 353.99
Eval num_timesteps=130000, episode_reward=-123.94 +/- 33.40
Episode length: 513.26 +/- 361.06
Eval num_timesteps=135000, episode_reward=-127.44 +/- 40.14
Episode length: 599.60 +/- 360.86
Eval num_timesteps=140000, episode_reward=-125.54 +/- 34.74
Episode length: 532.29 +/- 366.99
Eval num_timesteps=145000, episode_reward=-124.71 +/- 37.57
Episode length: 501.87 +/- 349.93
Eval num_timesteps=150000, episode_reward=-120.91 +/- 30.90
Episode length: 537.95 +/- 354.49
FINISHED IN 2343.3466420510085 s


starting seed  1816 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-128.37 +/- 50.34
Episode length: 938.22 +/- 121.57
New best mean reward!
Eval num_timesteps=10000, episode_reward=-139.80 +/- 26.20
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-152.98 +/- 38.82
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=20000, episode_reward=-103.74 +/- 23.34
Episode length: 1000.00 +/- 0.00
New best mean reward!
Eval num_timesteps=25000, episode_reward=43.34 +/- 115.09
Episode length: 758.16 +/- 153.92
New best mean reward!
Eval num_timesteps=30000, episode_reward=32.89 +/- 83.56
Episode length: 885.70 +/- 148.33
Eval num_timesteps=35000, episode_reward=-5.98 +/- 127.04
Episode length: 668.25 +/- 149.20
Eval num_timesteps=40000, episode_reward=105.59 +/- 99.47
Episode length: 663.80 +/- 120.40
New best mean reward!
Eval num_timesteps=45000, episode_reward=-62.21 +/- 107.78
Episode length: 696.82 +/- 296.16
Eval num_timesteps=50000, episode_reward=-74.17 +/- 45.21
Episode length: 665.43 +/- 363.81
Eval num_timesteps=55000, episode_reward=-19.98 +/- 91.65
Episode length: 744.63 +/- 334.59
Eval num_timesteps=60000, episode_reward=-111.21 +/- 47.66
Episode length: 603.38 +/- 351.74
Eval num_timesteps=65000, episode_reward=-86.07 +/- 39.35
Episode length: 752.63 +/- 342.37
Eval num_timesteps=70000, episode_reward=-105.56 +/- 47.53
Episode length: 648.29 +/- 350.39
Eval num_timesteps=75000, episode_reward=-95.97 +/- 35.10
Episode length: 626.03 +/- 365.82
Eval num_timesteps=80000, episode_reward=-99.28 +/- 70.40
Episode length: 387.07 +/- 258.44
Eval num_timesteps=85000, episode_reward=-116.53 +/- 52.47
Episode length: 451.85 +/- 304.10
Eval num_timesteps=90000, episode_reward=-60.57 +/- 95.16
Episode length: 512.22 +/- 286.14
Eval num_timesteps=95000, episode_reward=-95.58 +/- 70.12
Episode length: 455.41 +/- 295.58
Eval num_timesteps=100000, episode_reward=-111.36 +/- 52.73
Episode length: 427.62 +/- 317.85
Eval num_timesteps=105000, episode_reward=-91.01 +/- 59.06
Episode length: 359.38 +/- 282.79
Eval num_timesteps=110000, episode_reward=-110.22 +/- 53.85
Episode length: 421.59 +/- 297.63
Eval num_timesteps=115000, episode_reward=-107.95 +/- 55.20
Episode length: 437.02 +/- 289.60
Eval num_timesteps=120000, episode_reward=-106.49 +/- 54.89
Episode length: 394.32 +/- 286.83
Eval num_timesteps=125000, episode_reward=-77.61 +/- 92.46
Episode length: 428.12 +/- 276.89
Eval num_timesteps=130000, episode_reward=-87.36 +/- 84.84
Episode length: 436.03 +/- 306.13
Eval num_timesteps=135000, episode_reward=-86.85 +/- 69.98
Episode length: 396.52 +/- 267.93
Eval num_timesteps=140000, episode_reward=-98.34 +/- 58.33
Episode length: 406.85 +/- 291.51
Eval num_timesteps=145000, episode_reward=-85.23 +/- 85.96
Episode length: 467.13 +/- 311.92
Eval num_timesteps=150000, episode_reward=-97.65 +/- 65.68
Episode length: 435.10 +/- 319.02
FINISHED IN 2267.402596269996 s


starting seed  1817 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-3929.61 +/- 1005.91
Episode length: 468.26 +/- 96.95
New best mean reward!
Eval num_timesteps=10000, episode_reward=-22.40 +/- 113.47
Episode length: 328.22 +/- 265.66
New best mean reward!
Eval num_timesteps=15000, episode_reward=-11.31 +/- 94.98
Episode length: 152.42 +/- 68.67
New best mean reward!
Eval num_timesteps=20000, episode_reward=-73.41 +/- 73.75
Episode length: 214.76 +/- 62.05
Eval num_timesteps=25000, episode_reward=-183.94 +/- 26.46
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=30000, episode_reward=-57.09 +/- 19.24
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=35000, episode_reward=-75.39 +/- 49.73
Episode length: 993.34 +/- 34.66
Eval num_timesteps=40000, episode_reward=-29.92 +/- 116.70
Episode length: 923.71 +/- 112.91
Eval num_timesteps=45000, episode_reward=-25.73 +/- 122.01
Episode length: 464.39 +/- 186.74
Eval num_timesteps=50000, episode_reward=-81.09 +/- 76.14
Episode length: 558.90 +/- 292.03
Eval num_timesteps=55000, episode_reward=-64.26 +/- 43.81
Episode length: 823.77 +/- 301.55
Eval num_timesteps=60000, episode_reward=-101.33 +/- 52.42
Episode length: 780.93 +/- 276.56
Eval num_timesteps=65000, episode_reward=-50.25 +/- 39.23
Episode length: 888.02 +/- 245.88
Eval num_timesteps=70000, episode_reward=-103.20 +/- 54.58
Episode length: 750.99 +/- 333.78
Eval num_timesteps=75000, episode_reward=-81.33 +/- 50.33
Episode length: 788.75 +/- 299.62
Eval num_timesteps=80000, episode_reward=-96.04 +/- 50.39
Episode length: 434.19 +/- 302.29
Eval num_timesteps=85000, episode_reward=-113.37 +/- 43.92
Episode length: 523.54 +/- 364.48
Eval num_timesteps=90000, episode_reward=-128.39 +/- 39.11
Episode length: 374.26 +/- 278.73
Eval num_timesteps=95000, episode_reward=-72.10 +/- 69.87
Episode length: 544.41 +/- 346.78
Eval num_timesteps=100000, episode_reward=-92.37 +/- 50.34
Episode length: 559.33 +/- 372.20
Eval num_timesteps=105000, episode_reward=-107.38 +/- 55.98
Episode length: 542.21 +/- 328.18
Eval num_timesteps=110000, episode_reward=-88.95 +/- 49.63
Episode length: 477.21 +/- 354.54
Eval num_timesteps=115000, episode_reward=-121.22 +/- 44.52
Episode length: 464.32 +/- 330.12
Eval num_timesteps=120000, episode_reward=-114.05 +/- 52.99
Episode length: 488.40 +/- 334.08
Eval num_timesteps=125000, episode_reward=-94.54 +/- 74.58
Episode length: 635.02 +/- 348.10
Eval num_timesteps=130000, episode_reward=-110.02 +/- 34.06
Episode length: 623.12 +/- 386.06
Eval num_timesteps=135000, episode_reward=-127.07 +/- 61.25
Episode length: 623.74 +/- 372.70
Eval num_timesteps=140000, episode_reward=-121.76 +/- 41.29
Episode length: 624.86 +/- 381.07
Eval num_timesteps=145000, episode_reward=-128.96 +/- 40.46
Episode length: 641.06 +/- 373.89
Eval num_timesteps=150000, episode_reward=-120.70 +/- 38.93
Episode length: 614.18 +/- 367.17
FINISHED IN 2121.5031419929874 s


starting seed  1818 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-318.19 +/- 79.86
Episode length: 118.22 +/- 33.66
New best mean reward!
Eval num_timesteps=10000, episode_reward=-177.76 +/- 25.84
Episode length: 583.89 +/- 157.45
New best mean reward!
Eval num_timesteps=15000, episode_reward=-0.87 +/- 61.44
Episode length: 986.30 +/- 43.31
New best mean reward!
Eval num_timesteps=20000, episode_reward=150.00 +/- 73.56
Episode length: 717.51 +/- 104.85
New best mean reward!
Eval num_timesteps=25000, episode_reward=-102.11 +/- 24.11
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=30000, episode_reward=158.77 +/- 70.65
Episode length: 734.11 +/- 90.91
New best mean reward!
Eval num_timesteps=35000, episode_reward=120.10 +/- 110.21
Episode length: 687.28 +/- 104.32
Eval num_timesteps=40000, episode_reward=33.91 +/- 113.77
Episode length: 203.14 +/- 78.45
Eval num_timesteps=45000, episode_reward=105.15 +/- 118.87
Episode length: 595.35 +/- 154.85
Eval num_timesteps=50000, episode_reward=-100.27 +/- 68.55
Episode length: 766.83 +/- 282.64
Eval num_timesteps=55000, episode_reward=6.37 +/- 122.57
Episode length: 524.74 +/- 298.82
Eval num_timesteps=60000, episode_reward=-3.79 +/- 130.69
Episode length: 632.16 +/- 271.93
Eval num_timesteps=65000, episode_reward=-4.29 +/- 117.56
Episode length: 467.33 +/- 227.13
Eval num_timesteps=70000, episode_reward=-72.05 +/- 38.55
Episode length: 789.75 +/- 341.51
Eval num_timesteps=75000, episode_reward=-71.19 +/- 41.89
Episode length: 692.74 +/- 371.63
Eval num_timesteps=80000, episode_reward=-132.55 +/- 45.43
Episode length: 573.59 +/- 353.31
Eval num_timesteps=85000, episode_reward=-103.84 +/- 54.91
Episode length: 571.37 +/- 366.37
Eval num_timesteps=90000, episode_reward=-1.84 +/- 97.08
Episode length: 720.80 +/- 342.49
Eval num_timesteps=95000, episode_reward=-79.69 +/- 23.12
Episode length: 736.00 +/- 370.55
Eval num_timesteps=100000, episode_reward=-37.66 +/- 63.29
Episode length: 782.40 +/- 344.04
Eval num_timesteps=105000, episode_reward=-93.63 +/- 42.67
Episode length: 681.92 +/- 361.07
Eval num_timesteps=110000, episode_reward=-77.76 +/- 61.91
Episode length: 510.58 +/- 366.07
Eval num_timesteps=115000, episode_reward=-79.08 +/- 69.80
Episode length: 528.65 +/- 351.26
Eval num_timesteps=120000, episode_reward=-77.49 +/- 71.71
Episode length: 463.19 +/- 326.78
Eval num_timesteps=125000, episode_reward=-55.17 +/- 95.93
Episode length: 431.64 +/- 258.04
Eval num_timesteps=130000, episode_reward=-89.30 +/- 68.47
Episode length: 534.79 +/- 325.34
Eval num_timesteps=135000, episode_reward=-67.12 +/- 79.09
Episode length: 567.17 +/- 333.82
Eval num_timesteps=140000, episode_reward=-60.97 +/- 81.11
Episode length: 572.59 +/- 333.83
Eval num_timesteps=145000, episode_reward=-65.43 +/- 73.46
Episode length: 593.96 +/- 345.29
Eval num_timesteps=150000, episode_reward=-48.93 +/- 81.89
Episode length: 609.78 +/- 356.28
FINISHED IN 2109.864388871996 s


starting seed  1819 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-170.06 +/- 104.83
Episode length: 103.26 +/- 51.52
New best mean reward!
Eval num_timesteps=10000, episode_reward=-151.37 +/- 73.15
Episode length: 101.74 +/- 37.60
New best mean reward!
Eval num_timesteps=15000, episode_reward=-658.56 +/- 152.48
Episode length: 122.47 +/- 28.56
Eval num_timesteps=20000, episode_reward=-228.38 +/- 40.43
Episode length: 658.76 +/- 122.39
Eval num_timesteps=25000, episode_reward=-152.53 +/- 31.42
Episode length: 994.60 +/- 53.73
Eval num_timesteps=30000, episode_reward=-153.21 +/- 35.49
Episode length: 582.09 +/- 213.35
Eval num_timesteps=35000, episode_reward=-142.18 +/- 32.32
Episode length: 944.90 +/- 130.29
New best mean reward!
Eval num_timesteps=40000, episode_reward=-63.92 +/- 21.49
Episode length: 1000.00 +/- 0.00
New best mean reward!
Eval num_timesteps=45000, episode_reward=-156.93 +/- 56.96
Episode length: 846.58 +/- 175.69
Eval num_timesteps=50000, episode_reward=-133.71 +/- 52.22
Episode length: 458.53 +/- 145.90
Eval num_timesteps=55000, episode_reward=-119.31 +/- 42.47
Episode length: 943.70 +/- 112.13
Eval num_timesteps=60000, episode_reward=-113.36 +/- 27.59
Episode length: 972.81 +/- 111.70
Eval num_timesteps=65000, episode_reward=-156.04 +/- 62.25
Episode length: 758.54 +/- 269.45
Eval num_timesteps=70000, episode_reward=-136.45 +/- 53.10
Episode length: 644.25 +/- 300.43
Eval num_timesteps=75000, episode_reward=-126.40 +/- 39.85
Episode length: 863.37 +/- 216.07
Eval num_timesteps=80000, episode_reward=-142.58 +/- 45.75
Episode length: 894.00 +/- 187.63
Eval num_timesteps=85000, episode_reward=-147.93 +/- 43.29
Episode length: 814.05 +/- 275.91
Eval num_timesteps=90000, episode_reward=-139.83 +/- 53.75
Episode length: 806.19 +/- 256.76
Eval num_timesteps=95000, episode_reward=-78.78 +/- 34.65
Episode length: 931.76 +/- 181.98
Eval num_timesteps=100000, episode_reward=-97.42 +/- 56.80
Episode length: 806.39 +/- 282.10
Eval num_timesteps=105000, episode_reward=-49.94 +/- 19.36
Episode length: 961.62 +/- 167.83
New best mean reward!
Eval num_timesteps=110000, episode_reward=-102.02 +/- 35.21
Episode length: 865.32 +/- 249.75
Eval num_timesteps=115000, episode_reward=-109.04 +/- 34.76
Episode length: 853.30 +/- 267.76
Eval num_timesteps=120000, episode_reward=-125.57 +/- 37.89
Episode length: 745.38 +/- 300.78
Eval num_timesteps=125000, episode_reward=-111.59 +/- 43.33
Episode length: 662.88 +/- 324.88
Eval num_timesteps=130000, episode_reward=-122.83 +/- 46.51
Episode length: 660.19 +/- 337.33
Eval num_timesteps=135000, episode_reward=-115.90 +/- 32.24
Episode length: 529.56 +/- 331.69
Eval num_timesteps=140000, episode_reward=-121.03 +/- 43.70
Episode length: 531.80 +/- 330.52
Eval num_timesteps=145000, episode_reward=-115.91 +/- 36.32
Episode length: 479.50 +/- 296.43
Eval num_timesteps=150000, episode_reward=-121.06 +/- 41.97
Episode length: 496.38 +/- 308.87
FINISHED IN 2297.995379903994 s


starting seed  1820 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-620.08 +/- 83.35
Episode length: 83.56 +/- 17.08
New best mean reward!
Eval num_timesteps=10000, episode_reward=-581.88 +/- 143.86
Episode length: 68.25 +/- 10.96
New best mean reward!
Eval num_timesteps=15000, episode_reward=-366.19 +/- 77.04
Episode length: 325.75 +/- 94.07
New best mean reward!
Eval num_timesteps=20000, episode_reward=-183.34 +/- 58.83
Episode length: 1000.00 +/- 0.00
New best mean reward!
Eval num_timesteps=25000, episode_reward=70.75 +/- 111.40
Episode length: 632.45 +/- 234.39
New best mean reward!
Eval num_timesteps=30000, episode_reward=51.61 +/- 122.07
Episode length: 679.93 +/- 213.97
Eval num_timesteps=35000, episode_reward=-61.78 +/- 27.28
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=40000, episode_reward=137.86 +/- 86.58
Episode length: 694.31 +/- 156.30
New best mean reward!
Eval num_timesteps=45000, episode_reward=134.22 +/- 115.97
Episode length: 501.35 +/- 183.73
Eval num_timesteps=50000, episode_reward=126.56 +/- 73.79
Episode length: 798.10 +/- 138.40
Eval num_timesteps=55000, episode_reward=153.07 +/- 102.24
Episode length: 488.30 +/- 150.19
New best mean reward!
Eval num_timesteps=60000, episode_reward=145.33 +/- 116.18
Episode length: 341.84 +/- 176.32
Eval num_timesteps=65000, episode_reward=112.21 +/- 124.48
Episode length: 334.60 +/- 179.95
Eval num_timesteps=70000, episode_reward=12.57 +/- 122.76
Episode length: 473.98 +/- 239.79
Eval num_timesteps=75000, episode_reward=86.00 +/- 135.55
Episode length: 318.68 +/- 169.21
Eval num_timesteps=80000, episode_reward=59.61 +/- 134.58
Episode length: 247.12 +/- 150.29
Eval num_timesteps=85000, episode_reward=81.83 +/- 117.65
Episode length: 531.94 +/- 244.27
Eval num_timesteps=90000, episode_reward=159.98 +/- 93.41
Episode length: 542.09 +/- 237.06
New best mean reward!
Eval num_timesteps=95000, episode_reward=30.75 +/- 120.77
Episode length: 620.97 +/- 296.51
Eval num_timesteps=100000, episode_reward=43.44 +/- 117.25
Episode length: 548.12 +/- 302.65
Eval num_timesteps=105000, episode_reward=32.21 +/- 124.80
Episode length: 563.05 +/- 327.30
Eval num_timesteps=110000, episode_reward=-43.12 +/- 111.81
Episode length: 668.71 +/- 323.33
Eval num_timesteps=115000, episode_reward=33.65 +/- 122.75
Episode length: 671.53 +/- 286.78
Eval num_timesteps=120000, episode_reward=55.26 +/- 130.62
Episode length: 541.92 +/- 286.21
Eval num_timesteps=125000, episode_reward=56.38 +/- 128.09
Episode length: 524.23 +/- 268.69
Eval num_timesteps=130000, episode_reward=2.92 +/- 131.62
Episode length: 392.49 +/- 270.08
Eval num_timesteps=135000, episode_reward=30.03 +/- 128.69
Episode length: 337.14 +/- 212.93
Eval num_timesteps=140000, episode_reward=37.31 +/- 131.11
Episode length: 393.57 +/- 252.49
Eval num_timesteps=145000, episode_reward=54.03 +/- 136.45
Episode length: 297.40 +/- 164.97
Eval num_timesteps=150000, episode_reward=39.09 +/- 132.34
Episode length: 308.23 +/- 197.00
FINISHED IN 1476.8502315430087 s


starting seed  1821 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-648.39 +/- 62.15
Episode length: 79.68 +/- 12.10
New best mean reward!
Eval num_timesteps=10000, episode_reward=-198.56 +/- 28.78
Episode length: 436.39 +/- 132.93
New best mean reward!
Eval num_timesteps=15000, episode_reward=-119.02 +/- 64.93
Episode length: 851.64 +/- 179.19
New best mean reward!
Eval num_timesteps=20000, episode_reward=-157.73 +/- 39.95
Episode length: 615.37 +/- 167.55
Traceback (most recent call last):
  File "/home/haani/snap/snapd-deskt