nohup: ignoring input


starting seed  2600 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Eval num_timesteps=5000, episode_reward=-814.24 +/- 501.16
Episode length: 118.57 +/- 50.07
New best mean reward!
Eval num_timesteps=10000, episode_reward=-142.98 +/- 70.45
Episode length: 675.88 +/- 206.86
New best mean reward!
Eval num_timesteps=15000, episode_reward=-223.53 +/- 53.20
Episode length: 568.23 +/- 195.00
Eval num_timesteps=20000, episode_reward=56.30 +/- 142.75
Episode length: 696.15 +/- 169.16
New best mean reward!
Eval num_timesteps=25000, episode_reward=-275.17 +/- 69.31
Episode length: 819.07 +/- 214.94
Eval num_timesteps=30000, episode_reward=-27.31 +/- 24.15
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=35000, episode_reward=-21.83 +/- 95.08
Episode length: 922.01 +/- 110.78
Eval num_timesteps=40000, episode_reward=-25.15 +/- 82.24
Episode length: 984.81 +/- 31.66
Eval num_timesteps=45000, episode_reward=2.43 +/- 125.57
Episode length: 811.10 +/- 117.60
Eval num_timesteps=50000, episode_reward=-98.87 +/- 30.36
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=55000, episode_reward=-52.71 +/- 67.35
Episode length: 212.83 +/- 74.41
Eval num_timesteps=60000, episode_reward=-70.28 +/- 50.55
Episode length: 930.22 +/- 170.83
Eval num_timesteps=65000, episode_reward=-49.68 +/- 33.34
Episode length: 999.14 +/- 6.29
Eval num_timesteps=70000, episode_reward=16.67 +/- 91.93
Episode length: 977.30 +/- 45.35
Eval num_timesteps=75000, episode_reward=-55.68 +/- 35.91
Episode length: 998.29 +/- 17.01
Eval num_timesteps=80000, episode_reward=-104.55 +/- 68.92
Episode length: 562.86 +/- 244.40
Eval num_timesteps=85000, episode_reward=-118.05 +/- 68.46
Episode length: 622.02 +/- 326.78
Eval num_timesteps=90000, episode_reward=-116.62 +/- 51.47
Episode length: 564.89 +/- 288.03
Eval num_timesteps=95000, episode_reward=-130.01 +/- 43.30
Episode length: 481.57 +/- 271.82
Eval num_timesteps=100000, episode_reward=-44.87 +/- 104.99
Episode length: 443.80 +/- 247.58
Eval num_timesteps=105000, episode_reward=30.05 +/- 114.69
Episode length: 793.89 +/- 234.40
Eval num_timesteps=110000, episode_reward=-69.15 +/- 69.97
Episode length: 766.51 +/- 315.07
Eval num_timesteps=115000, episode_reward=-75.35 +/- 39.15
Episode length: 694.52 +/- 358.27
Eval num_timesteps=120000, episode_reward=-92.55 +/- 45.03
Episode length: 741.64 +/- 344.04
Eval num_timesteps=125000, episode_reward=-54.51 +/- 103.22
Episode length: 500.33 +/- 257.20
Eval num_timesteps=130000, episode_reward=-61.08 +/- 75.25
Episode length: 602.43 +/- 365.47
Eval num_timesteps=135000, episode_reward=-64.53 +/- 25.48
Episode length: 775.10 +/- 361.36
Eval num_timesteps=140000, episode_reward=-63.58 +/- 32.08
Episode length: 787.81 +/- 350.49
Eval num_timesteps=145000, episode_reward=-106.57 +/- 38.13
Episode length: 574.74 +/- 376.22
Eval num_timesteps=150000, episode_reward=-96.52 +/- 56.91
Episode length: 549.67 +/- 358.46
Eval num_timesteps=155000, episode_reward=-98.34 +/- 67.29
Episode length: 478.95 +/- 346.88
Eval num_timesteps=160000, episode_reward=-118.99 +/- 58.00
Episode length: 450.22 +/- 328.12
Eval num_timesteps=165000, episode_reward=-106.51 +/- 47.94
Episode length: 442.34 +/- 353.26
Eval num_timesteps=170000, episode_reward=-137.78 +/- 45.79
Episode length: 429.08 +/- 310.57
Eval num_timesteps=175000, episode_reward=-128.31 +/- 35.85
Episode length: 393.16 +/- 301.22
Eval num_timesteps=180000, episode_reward=-132.82 +/- 34.37
Episode length: 405.22 +/- 325.35
Eval num_timesteps=185000, episode_reward=-129.43 +/- 29.88
Episode length: 388.21 +/- 322.28
Eval num_timesteps=190000, episode_reward=-125.91 +/- 38.59
Episode length: 410.88 +/- 326.35
Eval num_timesteps=195000, episode_reward=-117.01 +/- 49.65
Episode length: 423.59 +/- 324.81
Eval num_timesteps=200000, episode_reward=-124.63 +/- 36.59
Episode length: 487.20 +/- 347.28
FINISHED IN 2913.721532361 s


starting seed  2601 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Eval num_timesteps=5000, episode_reward=-861.32 +/- 566.93
Episode length: 164.86 +/- 71.71
New best mean reward!
Eval num_timesteps=10000, episode_reward=26.05 +/- 105.31
Episode length: 741.29 +/- 136.93
New best mean reward!
Eval num_timesteps=15000, episode_reward=-54.94 +/- 84.66
Episode length: 938.95 +/- 82.67
Eval num_timesteps=20000, episode_reward=-141.17 +/- 33.06
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=25000, episode_reward=-98.27 +/- 30.56
Episode length: 996.89 +/- 21.22
Eval num_timesteps=30000, episode_reward=-69.84 +/- 101.12
Episode length: 505.77 +/- 133.58
Eval num_timesteps=35000, episode_reward=-169.42 +/- 39.22
Episode length: 983.61 +/- 114.76
Eval num_timesteps=40000, episode_reward=-101.12 +/- 20.39
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=45000, episode_reward=-60.54 +/- 31.12
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=50000, episode_reward=-64.78 +/- 26.64
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=55000, episode_reward=-92.39 +/- 44.73
Episode length: 945.95 +/- 163.87
Eval num_timesteps=60000, episode_reward=-65.35 +/- 32.77
Episode length: 958.86 +/- 151.12
Eval num_timesteps=65000, episode_reward=-112.95 +/- 71.37
Episode length: 643.55 +/- 281.22
Eval num_timesteps=70000, episode_reward=-125.46 +/- 42.19
Episode length: 729.74 +/- 333.22
Eval num_timesteps=75000, episode_reward=-112.31 +/- 52.92
Episode length: 583.14 +/- 348.81
Eval num_timesteps=80000, episode_reward=-87.96 +/- 47.12
Episode length: 754.21 +/- 339.24
Eval num_timesteps=85000, episode_reward=-146.94 +/- 43.90
Episode length: 658.29 +/- 351.94
Eval num_timesteps=90000, episode_reward=-122.30 +/- 57.80
Episode length: 572.22 +/- 320.42
Eval num_timesteps=95000, episode_reward=-110.08 +/- 36.17
Episode length: 705.73 +/- 355.38
Eval num_timesteps=100000, episode_reward=-90.17 +/- 43.78
Episode length: 714.97 +/- 350.45
Eval num_timesteps=105000, episode_reward=-95.76 +/- 51.57
Episode length: 653.50 +/- 365.60
Eval num_timesteps=110000, episode_reward=-129.89 +/- 43.21
Episode length: 583.08 +/- 349.37
Eval num_timesteps=115000, episode_reward=-128.26 +/- 35.26
Episode length: 431.39 +/- 308.61
Eval num_timesteps=120000, episode_reward=-113.87 +/- 50.81
Episode length: 552.88 +/- 353.43
Eval num_timesteps=125000, episode_reward=-107.37 +/- 51.71
Episode length: 501.20 +/- 340.95
Eval num_timesteps=130000, episode_reward=-112.44 +/- 35.77
Episode length: 506.06 +/- 360.65
Eval num_timesteps=135000, episode_reward=-104.24 +/- 36.28
Episode length: 654.21 +/- 373.93
Eval num_timesteps=140000, episode_reward=-131.94 +/- 20.93
Episode length: 497.72 +/- 366.43
Eval num_timesteps=145000, episode_reward=-133.24 +/- 22.59
Episode length: 475.27 +/- 353.26
Eval num_timesteps=150000, episode_reward=-158.38 +/- 48.62
Episode length: 559.79 +/- 336.83
Eval num_timesteps=155000, episode_reward=-149.70 +/- 43.24
Episode length: 510.63 +/- 359.17
Eval num_timesteps=160000, episode_reward=-147.31 +/- 32.67
Episode length: 408.02 +/- 298.43
Eval num_timesteps=165000, episode_reward=-158.30 +/- 36.32
Episode length: 468.78 +/- 339.77
Eval num_timesteps=170000, episode_reward=-142.09 +/- 35.66
Episode length: 462.70 +/- 335.83
Eval num_timesteps=175000, episode_reward=-149.89 +/- 37.80
Episode length: 392.65 +/- 308.26
Eval num_timesteps=180000, episode_reward=-116.87 +/- 33.76
Episode length: 484.49 +/- 371.02
Eval num_timesteps=185000, episode_reward=-131.30 +/- 41.50
Episode length: 502.49 +/- 337.21
Eval num_timesteps=190000, episode_reward=-126.73 +/- 34.27
Episode length: 479.70 +/- 343.71
Eval num_timesteps=195000, episode_reward=-130.46 +/- 35.80
Episode length: 436.20 +/- 328.07
Eval num_timesteps=200000, episode_reward=-137.40 +/- 36.53
Episode length: 476.27 +/- 330.66
FINISHED IN 2960.2462956490053 s


starting seed  2602 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-236.25 +/- 143.55
Episode length: 697.53 +/- 393.70
New best mean reward!
Eval num_timesteps=10000, episode_reward=-265.52 +/- 45.78
Episode length: 484.29 +/- 135.45
Eval num_timesteps=15000, episode_reward=-263.48 +/- 32.94
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=20000, episode_reward=-77.86 +/- 26.15
Episode length: 1000.00 +/- 0.00
New best mean reward!
Eval num_timesteps=25000, episode_reward=-5.74 +/- 39.01
Episode length: 993.67 +/- 25.83
New best mean reward!
Eval num_timesteps=30000, episode_reward=-32.64 +/- 23.64
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=35000, episode_reward=103.98 +/- 91.51
Episode length: 759.26 +/- 133.41
New best mean reward!
Eval num_timesteps=40000, episode_reward=-2.23 +/- 124.03
Episode length: 737.90 +/- 185.86
Eval num_timesteps=45000, episode_reward=-43.69 +/- 90.30
Episode length: 730.08 +/- 258.76
Eval num_timesteps=50000, episode_reward=12.67 +/- 120.64
Episode length: 343.10 +/- 200.27
Eval num_timesteps=55000, episode_reward=-70.67 +/- 102.79
Episode length: 436.97 +/- 249.14
Eval num_timesteps=60000, episode_reward=-73.88 +/- 81.41
Episode length: 659.45 +/- 333.45
Eval num_timesteps=65000, episode_reward=-90.77 +/- 54.79
Episode length: 487.35 +/- 291.14
Eval num_timesteps=70000, episode_reward=-156.42 +/- 57.70
Episode length: 632.22 +/- 328.94
Eval num_timesteps=75000, episode_reward=-55.72 +/- 97.19
Episode length: 481.26 +/- 281.91
Eval num_timesteps=80000, episode_reward=-144.96 +/- 44.04
Episode length: 504.32 +/- 300.53
Eval num_timesteps=85000, episode_reward=-95.37 +/- 47.05
Episode length: 630.31 +/- 353.57
Eval num_timesteps=90000, episode_reward=-163.13 +/- 45.17
Episode length: 415.64 +/- 281.60
Eval num_timesteps=95000, episode_reward=-102.35 +/- 35.46
Episode length: 547.43 +/- 366.21
Eval num_timesteps=100000, episode_reward=-140.69 +/- 32.61
Episode length: 581.76 +/- 371.63
Eval num_timesteps=105000, episode_reward=-123.73 +/- 37.86
Episode length: 484.36 +/- 341.90
Eval num_timesteps=110000, episode_reward=-93.77 +/- 84.31
Episode length: 530.99 +/- 326.78
Eval num_timesteps=115000, episode_reward=-91.60 +/- 83.53
Episode length: 491.01 +/- 337.42
Eval num_timesteps=120000, episode_reward=-129.35 +/- 49.09
Episode length: 450.61 +/- 305.35
Eval num_timesteps=125000, episode_reward=-122.76 +/- 40.58
Episode length: 474.25 +/- 323.88
Eval num_timesteps=130000, episode_reward=-139.34 +/- 38.97
Episode length: 457.72 +/- 314.79
Eval num_timesteps=135000, episode_reward=-119.80 +/- 38.97
Episode length: 409.45 +/- 316.93
Eval num_timesteps=140000, episode_reward=-125.99 +/- 34.27
Episode length: 353.41 +/- 250.24
Eval num_timesteps=145000, episode_reward=-127.03 +/- 34.76
Episode length: 358.65 +/- 260.57
Eval num_timesteps=150000, episode_reward=-120.46 +/- 34.07
Episode length: 388.13 +/- 272.10
Eval num_timesteps=155000, episode_reward=-108.23 +/- 32.12
Episode length: 384.45 +/- 306.58
Eval num_timesteps=160000, episode_reward=-102.59 +/- 49.60
Episode length: 404.72 +/- 314.43
Eval num_timesteps=165000, episode_reward=-113.20 +/- 46.63
Episode length: 422.17 +/- 303.28
Eval num_timesteps=170000, episode_reward=-116.10 +/- 35.01
Episode length: 424.27 +/- 331.74
Eval num_timesteps=175000, episode_reward=-119.15 +/- 32.01
Episode length: 483.45 +/- 342.69
Eval num_timesteps=180000, episode_reward=-122.30 +/- 28.94
Episode length: 484.03 +/- 362.37
Eval num_timesteps=185000, episode_reward=-133.00 +/- 32.11
Episode length: 451.50 +/- 330.74
Eval num_timesteps=190000, episode_reward=-140.23 +/- 40.09
Episode length: 423.93 +/- 329.74
Eval num_timesteps=195000, episode_reward=-133.77 +/- 36.88
Episode length: 427.47 +/- 320.81
Eval num_timesteps=200000, episode_reward=-139.21 +/- 40.35
Episode length: 487.36 +/- 340.95
FINISHED IN 2718.160069520003 s


starting seed  2603 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-853.06 +/- 823.96
Episode length: 119.99 +/- 66.28
New best mean reward!
Eval num_timesteps=10000, episode_reward=-500.29 +/- 95.41
Episode length: 68.60 +/- 9.66
New best mean reward!
Eval num_timesteps=15000, episode_reward=-224.02 +/- 145.73
Episode length: 248.81 +/- 88.53
New best mean reward!
Eval num_timesteps=20000, episode_reward=-38.04 +/- 124.97
Episode length: 790.71 +/- 145.86
New best mean reward!
Eval num_timesteps=25000, episode_reward=154.82 +/- 126.25
Episode length: 295.40 +/- 89.43
New best mean reward!
Eval num_timesteps=30000, episode_reward=39.70 +/- 112.63
Episode length: 900.79 +/- 138.59
Eval num_timesteps=35000, episode_reward=215.72 +/- 85.73
Episode length: 328.14 +/- 70.30
New best mean reward!
FINISHED IN 287.18752851401223 s


starting seed  2604 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-1470.67 +/- 947.51
Episode length: 347.07 +/- 157.44
New best mean reward!
Eval num_timesteps=10000, episode_reward=-384.50 +/- 25.06
Episode length: 492.19 +/- 84.81
New best mean reward!
Eval num_timesteps=15000, episode_reward=-92.83 +/- 26.42
Episode length: 1000.00 +/- 0.00
New best mean reward!
Eval num_timesteps=20000, episode_reward=96.01 +/- 95.82
Episode length: 772.10 +/- 196.53
New best mean reward!
Eval num_timesteps=25000, episode_reward=-132.65 +/- 53.64
Episode length: 682.73 +/- 181.11
Eval num_timesteps=30000, episode_reward=-71.89 +/- 84.64
Episode length: 273.79 +/- 81.64
Eval num_timesteps=35000, episode_reward=-140.27 +/- 65.35
Episode length: 742.54 +/- 261.15
Eval num_timesteps=40000, episode_reward=-43.37 +/- 129.98
Episode length: 770.90 +/- 194.30
Eval num_timesteps=45000, episode_reward=-45.92 +/- 97.89
Episode length: 723.86 +/- 289.29
Eval num_timesteps=50000, episode_reward=-74.76 +/- 75.17
Episode length: 635.17 +/- 297.69
Eval num_timesteps=55000, episode_reward=-118.15 +/- 36.17
Episode length: 535.07 +/- 360.37
Eval num_timesteps=60000, episode_reward=-126.18 +/- 45.02
Episode length: 499.30 +/- 323.52
Eval num_timesteps=65000, episode_reward=-99.36 +/- 52.40
Episode length: 676.65 +/- 339.04
Eval num_timesteps=70000, episode_reward=-75.04 +/- 22.11
Episode length: 905.42 +/- 248.74
Eval num_timesteps=75000, episode_reward=-65.68 +/- 46.36
Episode length: 839.70 +/- 267.07
Eval num_timesteps=80000, episode_reward=-39.64 +/- 123.96
Episode length: 464.44 +/- 242.50
Eval num_timesteps=85000, episode_reward=-96.69 +/- 58.51
Episode length: 398.37 +/- 235.60
Eval num_timesteps=90000, episode_reward=-109.52 +/- 45.72
Episode length: 505.83 +/- 325.95
Eval num_timesteps=95000, episode_reward=-22.99 +/- 109.97
Episode length: 522.98 +/- 237.69
Eval num_timesteps=100000, episode_reward=6.60 +/- 102.32
Episode length: 760.81 +/- 269.35
Eval num_timesteps=105000, episode_reward=-117.76 +/- 63.80
Episode length: 521.59 +/- 287.42
Eval num_timesteps=110000, episode_reward=-87.73 +/- 51.27
Episode length: 669.45 +/- 350.54
Eval num_timesteps=115000, episode_reward=-116.26 +/- 56.30
Episode length: 602.38 +/- 332.73
Eval num_timesteps=120000, episode_reward=-55.40 +/- 67.56
Episode length: 746.93 +/- 319.63
Eval num_timesteps=125000, episode_reward=-101.68 +/- 49.29
Episode length: 688.63 +/- 343.08
Eval num_timesteps=130000, episode_reward=-60.98 +/- 93.32
Episode length: 676.57 +/- 313.66
Eval num_timesteps=135000, episode_reward=15.46 +/- 119.12
Episode length: 593.78 +/- 276.22
Eval num_timesteps=140000, episode_reward=0.66 +/- 129.52
Episode length: 529.86 +/- 289.06
Eval num_timesteps=145000, episode_reward=1.98 +/- 115.30
Episode length: 632.03 +/- 289.62
Eval num_timesteps=150000, episode_reward=-54.12 +/- 83.90
Episode length: 705.22 +/- 341.23
Eval num_timesteps=155000, episode_reward=-56.49 +/- 40.42
Episode length: 760.57 +/- 347.98
Eval num_timesteps=160000, episode_reward=-18.67 +/- 104.53
Episode length: 670.39 +/- 303.46
Eval num_timesteps=165000, episode_reward=-73.57 +/- 43.10
Episode length: 777.92 +/- 323.34
Eval num_timesteps=170000, episode_reward=-57.31 +/- 58.89
Episode length: 771.95 +/- 326.99
Eval num_timesteps=175000, episode_reward=-55.52 +/- 64.66
Episode length: 715.31 +/- 335.83
Eval num_timesteps=180000, episode_reward=2.09 +/- 109.63
Episode length: 697.94 +/- 271.00
Eval num_timesteps=185000, episode_reward=-51.99 +/- 85.73
Episode length: 709.58 +/- 328.82
Eval num_timesteps=190000, episode_reward=-27.26 +/- 110.23
Episode length: 644.86 +/- 320.95
Eval num_timesteps=195000, episode_reward=5.97 +/- 116.05
Episode length: 654.26 +/- 296.66
Eval num_timesteps=200000, episode_reward=-13.94 +/- 104.91
Episode length: 595.34 +/- 331.24
FINISHED IN 2921.110018552019 s


starting seed  2605 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-594.67 +/- 174.22
Episode length: 68.89 +/- 13.69
New best mean reward!
Eval num_timesteps=10000, episode_reward=-596.83 +/- 165.77
Episode length: 68.82 +/- 13.18
Eval num_timesteps=15000, episode_reward=-277.65 +/- 59.46
Episode length: 112.34 +/- 12.66
New best mean reward!
Eval num_timesteps=20000, episode_reward=-172.92 +/- 22.99
Episode length: 287.39 +/- 83.42
New best mean reward!
Eval num_timesteps=25000, episode_reward=-139.52 +/- 43.30
Episode length: 771.42 +/- 232.79
New best mean reward!
Eval num_timesteps=30000, episode_reward=-198.44 +/- 30.62
Episode length: 581.68 +/- 125.49
Eval num_timesteps=35000, episode_reward=-254.13 +/- 50.69
Episode length: 840.34 +/- 121.82
Eval num_timesteps=40000, episode_reward=-259.57 +/- 39.46
Episode length: 994.08 +/- 58.90
Eval num_timesteps=45000, episode_reward=-185.50 +/- 37.20
Episode length: 649.62 +/- 150.38
Eval num_timesteps=50000, episode_reward=-110.93 +/- 37.05
Episode length: 1000.00 +/- 0.00
New best mean reward!
Eval num_timesteps=55000, episode_reward=-81.56 +/- 70.21
Episode length: 952.95 +/- 166.55
New best mean reward!
Eval num_timesteps=60000, episode_reward=-79.35 +/- 61.87
Episode length: 949.02 +/- 174.64
New best mean reward!
Eval num_timesteps=65000, episode_reward=-90.41 +/- 90.72
Episode length: 871.81 +/- 255.99
Eval num_timesteps=70000, episode_reward=-18.32 +/- 89.25
Episode length: 824.91 +/- 295.61
New best mean reward!
Eval num_timesteps=75000, episode_reward=36.68 +/- 108.45
Episode length: 836.50 +/- 291.56
New best mean reward!
Eval num_timesteps=80000, episode_reward=70.00 +/- 105.09
Episode length: 166.92 +/- 57.63
New best mean reward!
Eval num_timesteps=85000, episode_reward=174.17 +/- 120.41
Episode length: 273.53 +/- 129.42
New best mean reward!
Eval num_timesteps=90000, episode_reward=129.67 +/- 65.91
Episode length: 814.80 +/- 219.06
Eval num_timesteps=95000, episode_reward=11.64 +/- 91.87
Episode length: 932.37 +/- 135.85
Eval num_timesteps=100000, episode_reward=142.06 +/- 115.69
Episode length: 228.26 +/- 138.51
Eval num_timesteps=105000, episode_reward=68.49 +/- 101.07
Episode length: 143.51 +/- 46.65
Eval num_timesteps=110000, episode_reward=6.42 +/- 20.42
Episode length: 102.21 +/- 10.22
Eval num_timesteps=115000, episode_reward=26.06 +/- 61.95
Episode length: 171.39 +/- 62.20
Eval num_timesteps=120000, episode_reward=-17.34 +/- 36.34
Episode length: 146.26 +/- 24.06
Eval num_timesteps=125000, episode_reward=-92.92 +/- 62.88
Episode length: 101.55 +/- 19.71
Eval num_timesteps=130000, episode_reward=-155.69 +/- 26.01
Episode length: 535.26 +/- 110.39
Eval num_timesteps=135000, episode_reward=-148.05 +/- 20.67
Episode length: 999.19 +/- 8.06
Eval num_timesteps=140000, episode_reward=-140.09 +/- 50.64
Episode length: 960.08 +/- 86.16
Eval num_timesteps=145000, episode_reward=-81.16 +/- 25.24
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=150000, episode_reward=-67.22 +/- 20.75
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=155000, episode_reward=-100.82 +/- 21.64
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=160000, episode_reward=-78.96 +/- 20.21
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=165000, episode_reward=-52.22 +/- 20.48
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=170000, episode_reward=-49.57 +/- 27.23
Episode length: 998.98 +/- 10.15
Eval num_timesteps=175000, episode_reward=-94.14 +/- 46.82
Episode length: 938.65 +/- 127.02
Eval num_timesteps=180000, episode_reward=-82.30 +/- 43.75
Episode length: 941.30 +/- 131.19
Eval num_timesteps=185000, episode_reward=-91.44 +/- 50.85
Episode length: 933.08 +/- 132.69
Eval num_timesteps=190000, episode_reward=-83.46 +/- 55.01
Episode length: 894.41 +/- 181.04
Eval num_timesteps=195000, episode_reward=-75.55 +/- 55.08
Episode length: 916.58 +/- 169.44
Eval num_timesteps=200000, episode_reward=-96.88 +/- 51.81
Episode length: 861.92 +/- 198.27
FINISHED IN 2869.397518612008 s


starting seed  2606 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-594.20 +/- 95.67
Episode length: 172.83 +/- 46.76
New best mean reward!
Eval num_timesteps=10000, episode_reward=-269.18 +/- 71.11
Episode length: 803.29 +/- 152.09
New best mean reward!
Eval num_timesteps=15000, episode_reward=-111.65 +/- 46.84
Episode length: 981.04 +/- 58.54
New best mean reward!
Eval num_timesteps=20000, episode_reward=-144.68 +/- 46.33
Episode length: 632.11 +/- 209.02
Eval num_timesteps=25000, episode_reward=-205.50 +/- 82.26
Episode length: 651.67 +/- 267.05
Eval num_timesteps=30000, episode_reward=-159.19 +/- 40.82
Episode length: 566.39 +/- 236.89
Eval num_timesteps=35000, episode_reward=-142.06 +/- 72.72
Episode length: 383.94 +/- 247.46
Eval num_timesteps=40000, episode_reward=-130.64 +/- 46.12
Episode length: 437.39 +/- 298.57
Eval num_timesteps=45000, episode_reward=-150.62 +/- 36.55
Episode length: 332.42 +/- 225.09
Eval num_timesteps=50000, episode_reward=-152.45 +/- 43.05
Episode length: 434.51 +/- 281.82
Eval num_timesteps=55000, episode_reward=-178.93 +/- 51.99
Episode length: 468.12 +/- 300.87
Eval num_timesteps=60000, episode_reward=-177.10 +/- 50.05
Episode length: 462.42 +/- 273.39
Eval num_timesteps=65000, episode_reward=-147.64 +/- 43.41
Episode length: 585.19 +/- 353.76
Eval num_timesteps=70000, episode_reward=-137.63 +/- 42.60
Episode length: 751.32 +/- 319.40
Eval num_timesteps=75000, episode_reward=-146.39 +/- 71.44
Episode length: 667.51 +/- 320.77
Eval num_timesteps=80000, episode_reward=-144.49 +/- 39.72
Episode length: 593.21 +/- 317.97
Eval num_timesteps=85000, episode_reward=-147.27 +/- 43.65
Episode length: 522.64 +/- 307.54
Eval num_timesteps=90000, episode_reward=-151.82 +/- 41.60
Episode length: 529.82 +/- 352.17
Eval num_timesteps=95000, episode_reward=-190.95 +/- 55.55
Episode length: 484.71 +/- 332.33
Eval num_timesteps=100000, episode_reward=-162.52 +/- 43.42
Episode length: 435.74 +/- 298.45
Eval num_timesteps=105000, episode_reward=-132.10 +/- 37.43
Episode length: 529.70 +/- 341.99
Eval num_timesteps=110000, episode_reward=-136.66 +/- 34.13
Episode length: 578.80 +/- 354.80
Eval num_timesteps=115000, episode_reward=-159.41 +/- 41.10
Episode length: 526.81 +/- 341.35
Eval num_timesteps=120000, episode_reward=-150.63 +/- 39.49
Episode length: 506.32 +/- 323.10
Eval num_timesteps=125000, episode_reward=-125.83 +/- 43.76
Episode length: 541.38 +/- 335.30
Eval num_timesteps=130000, episode_reward=-128.04 +/- 33.34
Episode length: 443.59 +/- 302.56
Eval num_timesteps=135000, episode_reward=-141.97 +/- 39.23
Episode length: 459.50 +/- 310.86
Eval num_timesteps=140000, episode_reward=-151.39 +/- 35.79
Episode length: 330.86 +/- 218.27
Eval num_timesteps=145000, episode_reward=-127.86 +/- 41.85
Episode length: 362.00 +/- 233.13
Eval num_timesteps=150000, episode_reward=-103.61 +/- 59.46
Episode length: 351.49 +/- 218.72
New best mean reward!
Eval num_timesteps=155000, episode_reward=-62.38 +/- 88.83
Episode length: 338.73 +/- 225.62
New best mean reward!
Eval num_timesteps=160000, episode_reward=-69.58 +/- 69.85
Episode length: 409.24 +/- 284.90
Eval num_timesteps=165000, episode_reward=-61.94 +/- 85.05
Episode length: 393.99 +/- 270.89
New best mean reward!
Eval num_timesteps=170000, episode_reward=-105.34 +/- 48.67
Episode length: 457.14 +/- 325.58
Eval num_timesteps=175000, episode_reward=-114.31 +/- 37.14
Episode length: 457.21 +/- 321.99
Eval num_timesteps=180000, episode_reward=-114.86 +/- 36.24
Episode length: 430.24 +/- 301.71
Eval num_timesteps=185000, episode_reward=-111.22 +/- 47.46
Episode length: 444.92 +/- 309.83
Eval num_timesteps=190000, episode_reward=-105.39 +/- 30.48
Episode length: 393.25 +/- 295.70
Eval num_timesteps=195000, episode_reward=-109.81 +/- 35.14
Episode length: 415.39 +/- 318.35
Eval num_timesteps=200000, episode_reward=-106.70 +/- 33.52
Episode length: 412.02 +/- 311.17
FINISHED IN 2273.0069401859946 s


starting seed  2607 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-139.11 +/- 85.41
Episode length: 86.53 +/- 16.31
New best mean reward!
Eval num_timesteps=10000, episode_reward=-68.62 +/- 111.43
Episode length: 927.56 +/- 203.48
New best mean reward!
Eval num_timesteps=15000, episode_reward=21.05 +/- 125.68
Episode length: 568.02 +/- 113.32
New best mean reward!
Eval num_timesteps=20000, episode_reward=88.01 +/- 130.98
Episode length: 328.39 +/- 135.49
New best mean reward!
Eval num_timesteps=25000, episode_reward=-183.20 +/- 60.61
Episode length: 501.96 +/- 242.52
Eval num_timesteps=30000, episode_reward=-88.91 +/- 59.38
Episode length: 619.10 +/- 308.16
Eval num_timesteps=35000, episode_reward=-104.96 +/- 51.74
Episode length: 714.98 +/- 288.55
Eval num_timesteps=40000, episode_reward=-228.82 +/- 50.53
Episode length: 954.31 +/- 128.48
Eval num_timesteps=45000, episode_reward=-118.75 +/- 50.96
Episode length: 753.79 +/- 264.27
Eval num_timesteps=50000, episode_reward=-24.23 +/- 101.28
Episode length: 458.67 +/- 257.17
Eval num_timesteps=55000, episode_reward=-61.18 +/- 95.76
Episode length: 477.91 +/- 244.63
Eval num_timesteps=60000, episode_reward=-136.91 +/- 42.01
Episode length: 609.48 +/- 331.85
Eval num_timesteps=65000, episode_reward=-111.99 +/- 60.30
Episode length: 614.56 +/- 323.33
Eval num_timesteps=70000, episode_reward=-118.31 +/- 46.59
Episode length: 426.47 +/- 285.54
Eval num_timesteps=75000, episode_reward=-141.49 +/- 42.52
Episode length: 434.99 +/- 277.71
Eval num_timesteps=80000, episode_reward=-103.65 +/- 34.75
Episode length: 622.12 +/- 357.93
Eval num_timesteps=85000, episode_reward=-109.82 +/- 39.44
Episode length: 539.29 +/- 334.67
Eval num_timesteps=90000, episode_reward=-104.68 +/- 25.51
Episode length: 625.47 +/- 361.30
Eval num_timesteps=95000, episode_reward=-128.58 +/- 46.99
Episode length: 725.10 +/- 353.99
Eval num_timesteps=100000, episode_reward=-191.83 +/- 66.95
Episode length: 532.37 +/- 325.95
Eval num_timesteps=105000, episode_reward=-140.99 +/- 36.03
Episode length: 596.23 +/- 377.53
Eval num_timesteps=110000, episode_reward=-164.86 +/- 48.82
Episode length: 518.07 +/- 338.62
Eval num_timesteps=115000, episode_reward=-159.10 +/- 54.48
Episode length: 435.47 +/- 312.83
Eval num_timesteps=120000, episode_reward=-115.66 +/- 62.82
Episode length: 428.10 +/- 303.88
Eval num_timesteps=125000, episode_reward=-113.02 +/- 42.06
Episode length: 413.29 +/- 318.59
Eval num_timesteps=130000, episode_reward=-106.75 +/- 33.60
Episode length: 449.71 +/- 343.72
Eval num_timesteps=135000, episode_reward=-95.15 +/- 43.28
Episode length: 474.33 +/- 337.62
Eval num_timesteps=140000, episode_reward=-98.58 +/- 48.50
Episode length: 552.50 +/- 360.46
Eval num_timesteps=145000, episode_reward=-102.19 +/- 41.09
Episode length: 564.19 +/- 380.52
Eval num_timesteps=150000, episode_reward=-84.00 +/- 55.90
Episode length: 435.32 +/- 307.12
Eval num_timesteps=155000, episode_reward=-72.96 +/- 73.62
Episode length: 452.25 +/- 325.90
Eval num_timesteps=160000, episode_reward=-48.10 +/- 89.34
Episode length: 465.83 +/- 328.43
Eval num_timesteps=165000, episode_reward=-73.44 +/- 62.31
Episode length: 432.34 +/- 329.32
Eval num_timesteps=170000, episode_reward=-63.97 +/- 93.80
Episode length: 372.96 +/- 238.47
Eval num_timesteps=175000, episode_reward=-8.29 +/- 121.94
Episode length: 383.26 +/- 250.98
Eval num_timesteps=180000, episode_reward=-35.70 +/- 102.05
Episode length: 376.47 +/- 258.90
Eval num_timesteps=185000, episode_reward=-60.51 +/- 83.62
Episode length: 415.31 +/- 300.46
Eval num_timesteps=190000, episode_reward=-54.65 +/- 92.87
Episode length: 379.02 +/- 260.31
Eval num_timesteps=195000, episode_reward=-62.29 +/- 88.65
Episode length: 332.94 +/- 208.49
Eval num_timesteps=200000, episode_reward=-47.48 +/- 101.46
Episode length: 423.04 +/- 281.36
FINISHED IN 2227.1173156419827 s


starting seed  2608 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-540.02 +/- 157.32
Episode length: 65.10 +/- 13.95
New best mean reward!
Eval num_timesteps=10000, episode_reward=-332.51 +/- 75.07
Episode length: 446.05 +/- 99.81
New best mean reward!
Eval num_timesteps=15000, episode_reward=-178.26 +/- 53.38
Episode length: 741.62 +/- 214.22
New best mean reward!
Eval num_timesteps=20000, episode_reward=-151.54 +/- 22.18
Episode length: 367.25 +/- 111.20
New best mean reward!
Eval num_timesteps=25000, episode_reward=-212.67 +/- 49.87
Episode length: 725.10 +/- 241.24
Eval num_timesteps=30000, episode_reward=-134.97 +/- 29.75
Episode length: 1000.00 +/- 0.00
New best mean reward!
Eval num_timesteps=35000, episode_reward=-131.38 +/- 63.34
Episode length: 861.75 +/- 177.19
New best mean reward!
Eval num_timesteps=40000, episode_reward=-139.44 +/- 52.57
Episode length: 564.67 +/- 195.62
Eval num_timesteps=45000, episode_reward=-75.85 +/- 49.54
Episode length: 970.92 +/- 84.41
New best mean reward!
Eval num_timesteps=50000, episode_reward=-92.78 +/- 42.99
Episode length: 957.42 +/- 115.52
Eval num_timesteps=55000, episode_reward=-82.92 +/- 45.87
Episode length: 971.53 +/- 80.58
Eval num_timesteps=60000, episode_reward=44.81 +/- 135.20
Episode length: 531.75 +/- 195.81
New best mean reward!
Eval num_timesteps=65000, episode_reward=94.10 +/- 125.63
Episode length: 455.13 +/- 86.23
New best mean reward!
Eval num_timesteps=70000, episode_reward=-36.06 +/- 118.05
Episode length: 449.19 +/- 185.98
Eval num_timesteps=75000, episode_reward=-23.07 +/- 109.96
Episode length: 355.62 +/- 182.84
Eval num_timesteps=80000, episode_reward=-26.34 +/- 113.51
Episode length: 365.72 +/- 160.85
Eval num_timesteps=85000, episode_reward=-57.06 +/- 104.98
Episode length: 362.76 +/- 177.85
Eval num_timesteps=90000, episode_reward=-67.74 +/- 76.55
Episode length: 603.21 +/- 320.97
Eval num_timesteps=95000, episode_reward=-107.78 +/- 48.91
Episode length: 616.39 +/- 321.38
Eval num_timesteps=100000, episode_reward=-112.52 +/- 45.45
Episode length: 574.54 +/- 321.66
Eval num_timesteps=105000, episode_reward=-92.76 +/- 52.34
Episode length: 635.76 +/- 327.93
Eval num_timesteps=110000, episode_reward=-71.34 +/- 66.54
Episode length: 579.06 +/- 320.61
Eval num_timesteps=115000, episode_reward=-93.89 +/- 36.73
Episode length: 545.47 +/- 350.84
Eval num_timesteps=120000, episode_reward=-118.76 +/- 30.21
Episode length: 393.01 +/- 272.14
Eval num_timesteps=125000, episode_reward=-112.36 +/- 32.28
Episode length: 436.58 +/- 305.69
Eval num_timesteps=130000, episode_reward=-121.30 +/- 42.17
Episode length: 530.29 +/- 334.88
Eval num_timesteps=135000, episode_reward=-88.84 +/- 44.55
Episode length: 503.92 +/- 358.92
Eval num_timesteps=140000, episode_reward=-100.68 +/- 57.88
Episode length: 372.05 +/- 264.50
Eval num_timesteps=145000, episode_reward=-107.83 +/- 38.05
Episode length: 546.68 +/- 348.58
Eval num_timesteps=150000, episode_reward=-131.62 +/- 45.67
Episode length: 569.34 +/- 359.30
Eval num_timesteps=155000, episode_reward=-157.61 +/- 39.09
Episode length: 512.45 +/- 346.66
Eval num_timesteps=160000, episode_reward=-135.59 +/- 40.18
Episode length: 562.47 +/- 356.17
Eval num_timesteps=165000, episode_reward=-123.42 +/- 23.74
Episode length: 616.25 +/- 377.91
Eval num_timesteps=170000, episode_reward=-135.32 +/- 40.65
Episode length: 582.78 +/- 366.29
Eval num_timesteps=175000, episode_reward=-126.89 +/- 32.20
Episode length: 535.33 +/- 359.74
Eval num_timesteps=180000, episode_reward=-115.21 +/- 30.04
Episode length: 523.15 +/- 368.68
Eval num_timesteps=185000, episode_reward=-129.52 +/- 32.89
Episode length: 375.14 +/- 276.24
Eval num_timesteps=190000, episode_reward=-132.27 +/- 35.95
Episode length: 438.58 +/- 311.72
Eval num_timesteps=195000, episode_reward=-126.26 +/- 31.55
Episode length: 423.89 +/- 309.65
Eval num_timesteps=200000, episode_reward=-130.46 +/- 36.18
Episode length: 439.85 +/- 335.47
FINISHED IN 2528.9231451310043 s


starting seed  2609 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-16.14 +/- 95.88
Episode length: 896.57 +/- 176.39
New best mean reward!
Eval num_timesteps=10000, episode_reward=-99.52 +/- 21.87
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-314.94 +/- 96.86
Episode length: 935.83 +/- 110.08
Eval num_timesteps=20000, episode_reward=-71.39 +/- 52.34
Episode length: 984.93 +/- 70.80
Eval num_timesteps=25000, episode_reward=-101.62 +/- 65.79
Episode length: 386.24 +/- 150.05
Eval num_timesteps=30000, episode_reward=-126.87 +/- 68.19
Episode length: 619.80 +/- 170.83
Eval num_timesteps=35000, episode_reward=-47.52 +/- 34.40
Episode length: 984.38 +/- 93.78
Eval num_timesteps=40000, episode_reward=-112.17 +/- 45.70
Episode length: 902.49 +/- 170.55
Eval num_timesteps=45000, episode_reward=-3.52 +/- 106.35
Episode length: 509.75 +/- 259.41
New best mean reward!
Eval num_timesteps=50000, episode_reward=-73.99 +/- 84.02
Episode length: 570.92 +/- 288.08
Eval num_timesteps=55000, episode_reward=-123.99 +/- 57.30
Episode length: 600.68 +/- 286.44
Eval num_timesteps=60000, episode_reward=71.34 +/- 128.35
Episode length: 439.18 +/- 146.71
New best mean reward!
Eval num_timesteps=65000, episode_reward=-54.73 +/- 109.24
Episode length: 565.41 +/- 298.13
Eval num_timesteps=70000, episode_reward=-22.66 +/- 99.49
Episode length: 757.85 +/- 314.39
Eval num_timesteps=75000, episode_reward=-28.42 +/- 115.13
Episode length: 697.66 +/- 299.93
Eval num_timesteps=80000, episode_reward=-40.03 +/- 110.17
Episode length: 329.85 +/- 165.34
Eval num_timesteps=85000, episode_reward=-53.52 +/- 101.51
Episode length: 397.52 +/- 229.27
Eval num_timesteps=90000, episode_reward=-1.38 +/- 119.10
Episode length: 398.42 +/- 164.33
Eval num_timesteps=95000, episode_reward=-92.31 +/- 37.03
Episode length: 775.93 +/- 348.46
Eval num_timesteps=100000, episode_reward=-110.71 +/- 30.42
Episode length: 705.81 +/- 357.10
Eval num_timesteps=105000, episode_reward=-98.50 +/- 35.90
Episode length: 729.40 +/- 354.20
Eval num_timesteps=110000, episode_reward=-136.24 +/- 48.17
Episode length: 514.25 +/- 315.79
Eval num_timesteps=115000, episode_reward=-109.97 +/- 32.67
Episode length: 682.90 +/- 376.60
Eval num_timesteps=120000, episode_reward=-102.69 +/- 36.02
Episode length: 632.33 +/- 368.33
Eval num_timesteps=125000, episode_reward=-84.68 +/- 77.37
Episode length: 309.59 +/- 196.73
Eval num_timesteps=130000, episode_reward=-59.51 +/- 92.32
Episode length: 307.45 +/- 178.48
Eval num_timesteps=135000, episode_reward=-77.29 +/- 77.44
Episode length: 368.58 +/- 267.49
Eval num_timesteps=140000, episode_reward=-87.07 +/- 74.19
Episode length: 387.78 +/- 255.64
Eval num_timesteps=145000, episode_reward=-59.28 +/- 94.17
Episode length: 370.92 +/- 234.53
Eval num_timesteps=150000, episode_reward=-23.84 +/- 104.92
Episode length: 399.71 +/- 254.85
Eval num_timesteps=155000, episode_reward=-26.09 +/- 109.10
Episode length: 399.04 +/- 245.91
Eval num_timesteps=160000, episode_reward=-33.11 +/- 97.01
Episode length: 506.28 +/- 305.87
Eval num_timesteps=165000, episode_reward=-45.59 +/- 99.33
Episode length: 576.57 +/- 318.37
Eval num_timesteps=170000, episode_reward=-82.88 +/- 93.63
Episode length: 466.70 +/- 298.41
Eval num_timesteps=175000, episode_reward=-67.23 +/- 77.99
Episode length: 429.51 +/- 293.76
Eval num_timesteps=180000, episode_reward=-55.03 +/- 88.08
Episode length: 484.69 +/- 314.20
Eval num_timesteps=185000, episode_reward=-49.66 +/- 102.70
Episode length: 541.22 +/- 320.18
Eval num_timesteps=190000, episode_reward=-44.58 +/- 104.96
Episode length: 472.48 +/- 274.69
Eval num_timesteps=195000, episode_reward=-69.85 +/- 84.42
Episode length: 417.37 +/- 271.97
Eval num_timesteps=200000, episode_reward=-61.47 +/- 86.24
Episode length: 446.21 +/- 292.07
FINISHED IN 2733.2662636199966 s


starting seed  2610 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-173.78 +/- 22.39
Episode length: 492.84 +/- 117.96
New best mean reward!
Eval num_timesteps=10000, episode_reward=-183.94 +/- 27.69
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=15000, episode_reward=-133.10 +/- 23.20
Episode length: 1000.00 +/- 0.00
New best mean reward!
Eval num_timesteps=20000, episode_reward=-52.22 +/- 23.16
Episode length: 1000.00 +/- 0.00
New best mean reward!
Eval num_timesteps=25000, episode_reward=-54.04 +/- 21.27
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=30000, episode_reward=-85.92 +/- 24.78
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=35000, episode_reward=-189.33 +/- 42.40
Episode length: 634.44 +/- 208.72
Eval num_timesteps=40000, episode_reward=-77.85 +/- 25.31
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=45000, episode_reward=-17.24 +/- 23.50
Episode length: 1000.00 +/- 0.00
New best mean reward!
Eval num_timesteps=50000, episode_reward=60.04 +/- 140.49
Episode length: 339.38 +/- 155.32
New best mean reward!
Eval num_timesteps=55000, episode_reward=120.78 +/- 121.42
Episode length: 524.61 +/- 85.59
New best mean reward!
Eval num_timesteps=60000, episode_reward=70.76 +/- 122.99
Episode length: 567.79 +/- 205.50
Eval num_timesteps=65000, episode_reward=-6.85 +/- 100.05
Episode length: 796.79 +/- 251.70
Eval num_timesteps=70000, episode_reward=-100.27 +/- 64.81
Episode length: 547.84 +/- 298.55
Eval num_timesteps=75000, episode_reward=25.13 +/- 123.24
Episode length: 460.56 +/- 199.65
Eval num_timesteps=80000, episode_reward=16.91 +/- 109.89
Episode length: 638.30 +/- 266.41
Eval num_timesteps=85000, episode_reward=-80.84 +/- 78.20
Episode length: 440.70 +/- 252.36
Eval num_timesteps=90000, episode_reward=-31.31 +/- 97.21
Episode length: 544.31 +/- 320.81
Eval num_timesteps=95000, episode_reward=-46.18 +/- 55.79
Episode length: 938.02 +/- 184.47
Eval num_timesteps=100000, episode_reward=-61.62 +/- 32.51
Episode length: 907.81 +/- 250.23
Eval num_timesteps=105000, episode_reward=-97.00 +/- 46.52
Episode length: 700.20 +/- 339.55
Eval num_timesteps=110000, episode_reward=-83.19 +/- 41.63
Episode length: 782.55 +/- 343.17
Eval num_timesteps=115000, episode_reward=-34.36 +/- 111.84
Episode length: 420.49 +/- 209.24
Eval num_timesteps=120000, episode_reward=-58.57 +/- 87.44
Episode length: 601.10 +/- 320.13
Eval num_timesteps=125000, episode_reward=-79.18 +/- 53.67
Episode length: 597.15 +/- 359.58
Eval num_timesteps=130000, episode_reward=-48.76 +/- 81.99
Episode length: 569.39 +/- 306.75
Eval num_timesteps=135000, episode_reward=-61.42 +/- 92.21
Episode length: 723.50 +/- 330.47
Eval num_timesteps=140000, episode_reward=-48.80 +/- 77.39
Episode length: 629.56 +/- 320.97
Eval num_timesteps=145000, episode_reward=-96.28 +/- 48.06
Episode length: 677.00 +/- 352.31
Eval num_timesteps=150000, episode_reward=-45.78 +/- 79.63
Episode length: 631.97 +/- 352.61
Eval num_timesteps=155000, episode_reward=-40.32 +/- 85.75
Episode length: 582.12 +/- 342.00
Eval num_timesteps=160000, episode_reward=-34.21 +/- 90.49
Episode length: 504.38 +/- 324.69
Eval num_timesteps=165000, episode_reward=-20.16 +/- 102.05
Episode length: 539.16 +/- 323.47
Eval num_timesteps=170000, episode_reward=-11.01 +/- 89.80
Episode length: 605.85 +/- 332.15
Eval num_timesteps=175000, episode_reward=-46.98 +/- 94.29
Episode length: 535.32 +/- 312.05
Eval num_timesteps=180000, episode_reward=-58.79 +/- 97.59
Episode length: 494.13 +/- 305.03
Eval num_timesteps=185000, episode_reward=-61.80 +/- 97.40
Episode length: 486.33 +/- 296.74
Eval num_timesteps=190000, episode_reward=-37.47 +/- 94.62
Episode length: 549.27 +/- 327.57
Eval num_timesteps=195000, episode_reward=-45.61 +/- 97.92
Episode length: 516.07 +/- 327.47
Eval num_timesteps=200000, episode_reward=-44.39 +/- 92.40
Episode length: 468.50 +/- 310.35
FINISHED IN 2951.7028398169787 s


starting seed  2611 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-650.20 +/- 126.50
Episode length: 98.98 +/- 23.61
New best mean reward!
Eval num_timesteps=10000, episode_reward=-61.23 +/- 94.82
Episode length: 761.41 +/- 346.04
New best mean reward!
Eval num_timesteps=15000, episode_reward=-114.55 +/- 25.28
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=20000, episode_reward=-87.61 +/- 79.18
Episode length: 882.73 +/- 175.12
Eval num_timesteps=25000, episode_reward=-99.28 +/- 51.02
Episode length: 938.44 +/- 124.51
Eval num_timesteps=30000, episode_reward=-87.93 +/- 33.20
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=35000, episode_reward=77.64 +/- 87.88
Episode length: 812.26 +/- 167.11
New best mean reward!
Eval num_timesteps=40000, episode_reward=-16.20 +/- 82.60
Episode length: 934.82 +/- 109.62
Eval num_timesteps=45000, episode_reward=-20.31 +/- 25.34
Episode length: 999.70 +/- 2.98
Eval num_timesteps=50000, episode_reward=39.86 +/- 122.91
Episode length: 721.55 +/- 128.21
Eval num_timesteps=55000, episode_reward=65.52 +/- 146.14
Episode length: 438.88 +/- 86.80
Eval num_timesteps=60000, episode_reward=21.06 +/- 114.21
Episode length: 811.58 +/- 163.03
Eval num_timesteps=65000, episode_reward=-32.15 +/- 116.36
Episode length: 626.97 +/- 236.08
Eval num_timesteps=70000, episode_reward=-78.58 +/- 38.99
Episode length: 906.71 +/- 221.43
Eval num_timesteps=75000, episode_reward=-144.16 +/- 61.31
Episode length: 697.14 +/- 281.95
Eval num_timesteps=80000, episode_reward=-82.30 +/- 49.31
Episode length: 816.00 +/- 313.64
Eval num_timesteps=85000, episode_reward=64.51 +/- 128.41
Episode length: 454.77 +/- 137.48
Eval num_timesteps=90000, episode_reward=-52.41 +/- 103.43
Episode length: 619.18 +/- 301.12
Eval num_timesteps=95000, episode_reward=-9.25 +/- 125.05
Episode length: 399.17 +/- 165.39
Eval num_timesteps=100000, episode_reward=-79.07 +/- 95.04
Episode length: 442.50 +/- 247.29
Eval num_timesteps=105000, episode_reward=32.58 +/- 127.04
Episode length: 428.85 +/- 208.32
Eval num_timesteps=110000, episode_reward=-11.32 +/- 99.70
Episode length: 840.54 +/- 246.49
Eval num_timesteps=115000, episode_reward=28.53 +/- 115.65
Episode length: 640.05 +/- 261.44
Eval num_timesteps=120000, episode_reward=-45.46 +/- 98.48
Episode length: 699.16 +/- 323.33
Eval num_timesteps=125000, episode_reward=-37.20 +/- 106.59
Episode length: 490.08 +/- 278.44
Eval num_timesteps=130000, episode_reward=-14.88 +/- 118.07
Episode length: 421.44 +/- 253.20
Eval num_timesteps=135000, episode_reward=-50.63 +/- 122.81
Episode length: 544.28 +/- 272.06
Eval num_timesteps=140000, episode_reward=-57.87 +/- 101.53
Episode length: 628.36 +/- 312.02
Eval num_timesteps=145000, episode_reward=-52.49 +/- 129.01
Episode length: 652.34 +/- 310.81
Eval num_timesteps=150000, episode_reward=-70.42 +/- 83.92
Episode length: 640.57 +/- 319.81
Eval num_timesteps=155000, episode_reward=-57.21 +/- 93.55
Episode length: 627.34 +/- 333.84
Eval num_timesteps=160000, episode_reward=-65.43 +/- 72.64
Episode length: 697.57 +/- 348.19
Eval num_timesteps=165000, episode_reward=-58.24 +/- 98.60
Episode length: 609.55 +/- 341.18
Eval num_timesteps=170000, episode_reward=-57.04 +/- 85.88
Episode length: 583.23 +/- 317.75
Eval num_timesteps=175000, episode_reward=-78.59 +/- 68.47
Episode length: 576.09 +/- 337.84
Eval num_timesteps=180000, episode_reward=-55.86 +/- 102.01
Episode length: 596.67 +/- 316.27
Eval num_timesteps=185000, episode_reward=-58.93 +/- 95.21
Episode length: 540.16 +/- 317.38
Eval num_timesteps=190000, episode_reward=-62.14 +/- 87.67
Episode length: 502.37 +/- 340.82
Eval num_timesteps=195000, episode_reward=-50.44 +/- 99.16
Episode length: 477.19 +/- 265.43
Eval num_timesteps=200000, episode_reward=-50.59 +/- 110.35
Episode length: 499.20 +/- 292.55
FINISHED IN 2868.583511884004 s


starting seed  2612 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-558.72 +/- 151.00
Episode length: 67.64 +/- 13.30
New best mean reward!
Eval num_timesteps=10000, episode_reward=-183.61 +/- 38.63
Episode length: 123.03 +/- 34.84
New best mean reward!
Eval num_timesteps=15000, episode_reward=-440.35 +/- 287.29
Episode length: 399.44 +/- 130.82
Eval num_timesteps=20000, episode_reward=-30.75 +/- 108.22
Episode length: 861.78 +/- 287.51
New best mean reward!
Eval num_timesteps=25000, episode_reward=-51.16 +/- 79.87
Episode length: 906.91 +/- 239.05
Eval num_timesteps=30000, episode_reward=-76.64 +/- 23.84
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=35000, episode_reward=-120.89 +/- 59.16
Episode length: 629.97 +/- 150.95
Eval num_timesteps=40000, episode_reward=-42.78 +/- 25.37
Episode length: 999.55 +/- 4.48
Eval num_timesteps=45000, episode_reward=144.57 +/- 108.87
Episode length: 541.85 +/- 155.69
New best mean reward!
Eval num_timesteps=50000, episode_reward=-109.11 +/- 41.12
Episode length: 948.08 +/- 123.94
Eval num_timesteps=55000, episode_reward=-59.73 +/- 20.90
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=60000, episode_reward=-149.48 +/- 32.55
Episode length: 638.26 +/- 172.17
Eval num_timesteps=65000, episode_reward=-115.94 +/- 49.04
Episode length: 519.41 +/- 200.69
Eval num_timesteps=70000, episode_reward=-62.68 +/- 35.79
Episode length: 990.73 +/- 41.82
Eval num_timesteps=75000, episode_reward=-14.17 +/- 23.88
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=80000, episode_reward=-12.50 +/- 27.40
Episode length: 998.25 +/- 14.55
Eval num_timesteps=85000, episode_reward=16.24 +/- 113.43
Episode length: 774.54 +/- 134.33
Eval num_timesteps=90000, episode_reward=74.29 +/- 129.55
Episode length: 735.62 +/- 169.33
Eval num_timesteps=95000, episode_reward=-49.72 +/- 31.49
Episode length: 990.56 +/- 57.90
Eval num_timesteps=100000, episode_reward=80.41 +/- 95.66
Episode length: 796.88 +/- 193.33
Eval num_timesteps=105000, episode_reward=139.06 +/- 105.24
Episode length: 534.67 +/- 140.56
Eval num_timesteps=110000, episode_reward=126.93 +/- 113.45
Episode length: 515.16 +/- 155.35
Eval num_timesteps=115000, episode_reward=85.93 +/- 130.27
Episode length: 589.90 +/- 207.02
Eval num_timesteps=120000, episode_reward=77.13 +/- 131.46
Episode length: 379.34 +/- 162.28
Eval num_timesteps=125000, episode_reward=29.18 +/- 140.05
Episode length: 499.79 +/- 241.29
Eval num_timesteps=130000, episode_reward=39.00 +/- 117.06
Episode length: 645.87 +/- 246.33
Eval num_timesteps=135000, episode_reward=-16.30 +/- 100.02
Episode length: 713.92 +/- 250.05
Eval num_timesteps=140000, episode_reward=30.30 +/- 123.09
Episode length: 373.86 +/- 223.20
Eval num_timesteps=145000, episode_reward=32.52 +/- 124.90
Episode length: 263.69 +/- 134.37
Eval num_timesteps=150000, episode_reward=13.33 +/- 113.88
Episode length: 239.79 +/- 131.34
Eval num_timesteps=155000, episode_reward=19.03 +/- 124.93
Episode length: 237.10 +/- 117.75
Eval num_timesteps=160000, episode_reward=-22.77 +/- 116.90
Episode length: 213.70 +/- 75.84
Eval num_timesteps=165000, episode_reward=-70.93 +/- 93.70
Episode length: 250.74 +/- 134.12
Eval num_timesteps=170000, episode_reward=-68.03 +/- 91.05
Episode length: 207.66 +/- 112.66
Eval num_timesteps=175000, episode_reward=-79.26 +/- 100.65
Episode length: 205.08 +/- 119.66
Eval num_timesteps=180000, episode_reward=-77.86 +/- 88.86
Episode length: 160.54 +/- 61.44
Eval num_timesteps=185000, episode_reward=-75.48 +/- 86.31
Episode length: 165.57 +/- 65.04
Eval num_timesteps=190000, episode_reward=-66.56 +/- 83.51
Episode length: 178.94 +/- 148.86
Eval num_timesteps=195000, episode_reward=-65.50 +/- 83.53
Episode length: 161.41 +/- 116.76
Eval num_timesteps=200000, episode_reward=-89.83 +/- 71.26
Episode length: 141.10 +/- 73.26
FINISHED IN 2488.487211994012 s


starting seed  2613 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-2088.33 +/- 77.48
Episode length: 658.94 +/- 28.29
New best mean reward!
Eval num_timesteps=10000, episode_reward=-169.27 +/- 48.91
Episode length: 954.16 +/- 111.67
New best mean reward!
Eval num_timesteps=15000, episode_reward=-49.59 +/- 35.12
Episode length: 989.06 +/- 42.91
New best mean reward!
Eval num_timesteps=20000, episode_reward=120.02 +/- 118.64
Episode length: 499.92 +/- 104.72
New best mean reward!
Eval num_timesteps=25000, episode_reward=137.61 +/- 110.80
Episode length: 605.37 +/- 153.30
New best mean reward!
Eval num_timesteps=30000, episode_reward=-38.53 +/- 150.97
Episode length: 670.28 +/- 199.93
Eval num_timesteps=35000, episode_reward=22.34 +/- 131.08
Episode length: 679.02 +/- 153.73
Eval num_timesteps=40000, episode_reward=-96.65 +/- 57.64
Episode length: 948.79 +/- 120.88
Eval num_timesteps=45000, episode_reward=57.57 +/- 109.37
Episode length: 827.59 +/- 108.11
Eval num_timesteps=50000, episode_reward=-28.57 +/- 93.76
Episode length: 908.88 +/- 141.79
Eval num_timesteps=55000, episode_reward=-66.33 +/- 48.16
Episode length: 897.29 +/- 227.50
Eval num_timesteps=60000, episode_reward=-75.72 +/- 105.94
Episode length: 620.09 +/- 316.21
Eval num_timesteps=65000, episode_reward=-41.38 +/- 103.24
Episode length: 796.92 +/- 232.50
Eval num_timesteps=70000, episode_reward=-99.81 +/- 50.34
Episode length: 631.39 +/- 335.08
Eval num_timesteps=75000, episode_reward=-75.97 +/- 24.27
Episode length: 782.50 +/- 350.41
Eval num_timesteps=80000, episode_reward=-43.09 +/- 60.67
Episode length: 851.85 +/- 287.04
Eval num_timesteps=85000, episode_reward=-81.20 +/- 53.16
Episode length: 519.19 +/- 346.46
Eval num_timesteps=90000, episode_reward=-105.27 +/- 62.13
Episode length: 444.06 +/- 276.89
Eval num_timesteps=95000, episode_reward=-81.17 +/- 80.08
Episode length: 468.98 +/- 314.82
Eval num_timesteps=100000, episode_reward=-33.31 +/- 86.08
Episode length: 680.60 +/- 350.31
Eval num_timesteps=105000, episode_reward=-42.92 +/- 96.46
Episode length: 644.39 +/- 346.26
Eval num_timesteps=110000, episode_reward=-127.74 +/- 65.18
Episode length: 455.54 +/- 309.57
Eval num_timesteps=115000, episode_reward=-111.93 +/- 57.30
Episode length: 445.47 +/- 316.04
Eval num_timesteps=120000, episode_reward=-113.43 +/- 36.19
Episode length: 470.34 +/- 340.10
Eval num_timesteps=125000, episode_reward=-114.37 +/- 47.62
Episode length: 548.28 +/- 345.10
Eval num_timesteps=130000, episode_reward=-111.17 +/- 48.57
Episode length: 733.66 +/- 348.00
Eval num_timesteps=135000, episode_reward=-71.54 +/- 47.20
Episode length: 684.31 +/- 369.21
Eval num_timesteps=140000, episode_reward=-105.74 +/- 34.43
Episode length: 515.29 +/- 341.54
Eval num_timesteps=145000, episode_reward=-113.96 +/- 46.99
Episode length: 325.51 +/- 237.74
Eval num_timesteps=150000, episode_reward=-104.58 +/- 50.68
Episode length: 403.59 +/- 291.81
Eval num_timesteps=155000, episode_reward=-116.74 +/- 47.11
Episode length: 418.60 +/- 297.51
Eval num_timesteps=160000, episode_reward=-109.50 +/- 32.69
Episode length: 406.31 +/- 310.06
Eval num_timesteps=165000, episode_reward=-103.30 +/- 29.35
Episode length: 429.32 +/- 329.51
Eval num_timesteps=170000, episode_reward=-100.52 +/- 30.04
Episode length: 494.40 +/- 355.77
Eval num_timesteps=175000, episode_reward=-92.70 +/- 32.74
Episode length: 481.07 +/- 356.98
Eval num_timesteps=180000, episode_reward=-97.96 +/- 33.37
Episode length: 399.47 +/- 315.28
Eval num_timesteps=185000, episode_reward=-102.12 +/- 37.90
Episode length: 464.98 +/- 337.79
Eval num_timesteps=190000, episode_reward=-98.58 +/- 42.44
Episode length: 506.20 +/- 363.78
Eval num_timesteps=195000, episode_reward=-92.39 +/- 42.92
Episode length: 576.25 +/- 370.53
Eval num_timesteps=200000, episode_reward=-98.20 +/- 32.45
Episode length: 524.53 +/- 357.21
FINISHED IN 2877.107415256003 s


starting seed  2614 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-895.40 +/- 52.41
Episode length: 593.78 +/- 67.89
New best mean reward!
Eval num_timesteps=10000, episode_reward=-92.39 +/- 65.62
Episode length: 247.64 +/- 57.17
New best mean reward!
Eval num_timesteps=15000, episode_reward=-88.79 +/- 67.46
Episode length: 255.80 +/- 81.35
New best mean reward!
Eval num_timesteps=20000, episode_reward=-37.91 +/- 38.00
Episode length: 997.33 +/- 26.57
New best mean reward!
Eval num_timesteps=25000, episode_reward=2.27 +/- 44.08
Episode length: 988.52 +/- 46.04
New best mean reward!
Eval num_timesteps=30000, episode_reward=-43.45 +/- 26.75
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=35000, episode_reward=-39.78 +/- 52.58
Episode length: 983.33 +/- 48.95
Eval num_timesteps=40000, episode_reward=-20.95 +/- 36.59
Episode length: 996.34 +/- 24.27
Eval num_timesteps=45000, episode_reward=78.56 +/- 106.90
Episode length: 769.55 +/- 183.30
New best mean reward!
Eval num_timesteps=50000, episode_reward=-64.91 +/- 26.71
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=55000, episode_reward=-15.80 +/- 22.61
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=60000, episode_reward=10.48 +/- 106.23
Episode length: 882.70 +/- 158.29
Eval num_timesteps=65000, episode_reward=-21.04 +/- 35.30
Episode length: 948.19 +/- 177.72
Eval num_timesteps=70000, episode_reward=13.01 +/- 126.53
Episode length: 477.98 +/- 237.10
Eval num_timesteps=75000, episode_reward=-95.38 +/- 51.89
Episode length: 496.88 +/- 311.40
Eval num_timesteps=80000, episode_reward=-115.73 +/- 47.09
Episode length: 591.66 +/- 369.29
Eval num_timesteps=85000, episode_reward=-131.61 +/- 47.15
Episode length: 708.87 +/- 346.78
Eval num_timesteps=90000, episode_reward=-136.70 +/- 64.42
Episode length: 576.25 +/- 363.00
Eval num_timesteps=95000, episode_reward=-150.03 +/- 50.89
Episode length: 627.83 +/- 374.49
Eval num_timesteps=100000, episode_reward=-137.82 +/- 53.51
Episode length: 618.13 +/- 355.68
Eval num_timesteps=105000, episode_reward=-83.88 +/- 46.34
Episode length: 657.24 +/- 367.95
Eval num_timesteps=110000, episode_reward=-91.28 +/- 40.28
Episode length: 559.52 +/- 364.05
Eval num_timesteps=115000, episode_reward=-80.68 +/- 74.38
Episode length: 506.46 +/- 337.70
Eval num_timesteps=120000, episode_reward=-94.29 +/- 43.56
Episode length: 505.83 +/- 341.47
Eval num_timesteps=125000, episode_reward=-85.31 +/- 49.19
Episode length: 514.79 +/- 357.05
Eval num_timesteps=130000, episode_reward=-95.07 +/- 52.84
Episode length: 457.08 +/- 328.64
Eval num_timesteps=135000, episode_reward=-48.30 +/- 108.40
Episode length: 355.69 +/- 199.19
Eval num_timesteps=140000, episode_reward=-104.73 +/- 52.46
Episode length: 433.19 +/- 300.93
Eval num_timesteps=145000, episode_reward=-85.77 +/- 57.14
Episode length: 591.07 +/- 375.64
Eval num_timesteps=150000, episode_reward=-81.49 +/- 31.09
Episode length: 603.69 +/- 394.24
Eval num_timesteps=155000, episode_reward=-99.62 +/- 36.28
Episode length: 491.18 +/- 354.25
Eval num_timesteps=160000, episode_reward=-95.09 +/- 38.77
Episode length: 479.17 +/- 346.50
Eval num_timesteps=165000, episode_reward=-98.08 +/- 35.14
Episode length: 430.23 +/- 326.87
Eval num_timesteps=170000, episode_reward=-106.82 +/- 32.20
Episode length: 478.65 +/- 341.92
Eval num_timesteps=175000, episode_reward=-113.91 +/- 38.79
Episode length: 417.76 +/- 306.37
Eval num_timesteps=180000, episode_reward=-106.58 +/- 34.64
Episode length: 451.42 +/- 333.37
Eval num_timesteps=185000, episode_reward=-106.63 +/- 45.07
Episode length: 468.03 +/- 342.14
Eval num_timesteps=190000, episode_reward=-99.06 +/- 48.44
Episode length: 470.39 +/- 326.25
Eval num_timesteps=195000, episode_reward=-107.50 +/- 47.72
Episode length: 471.31 +/- 336.37
Eval num_timesteps=200000, episode_reward=-98.32 +/- 56.32
Episode length: 442.10 +/- 330.18
FINISHED IN 2878.2446331210085 s


starting seed  2615 


<class 'torch.nn.modules.activation.ReLU'>
running on  cuda
Eval num_timesteps=5000, episode_reward=-856.67 +/- 638.72
Episode length: 125.44 +/- 59.33
New best mean reward!
Eval num_timesteps=10000, episode_reward=-571.62 +/- 73.85
Episode length: 104.78 +/- 14.58
New best mean reward!
Eval num_timesteps=15000, episode_reward=-875.76 +/- 136.32
Episode length: 899.34 +/- 97.10
Eval num_timesteps=20000, episode_reward=-95.48 +/- 25.75
Episode length: 1000.00 +/- 0.00
New best mean reward!
Eval num_timesteps=25000, episode_reward=-183.56 +/- 38.71
Episode length: 1000.00 +/- 0.00
Eval num_timesteps=30000, episode_reward=-65.81 +/- 65.74
Episode length: 963.05 +/- 80.11
New best mean reward!
Eval num_timesteps=35000, episode_reward=-19.09 +/- 31.70
Episode length: 994.69 +/- 32.91
New best mean reward!
Eval num_timesteps=40000, episode_reward=-55.98 +/- 28.42
Episode length: 998.99 +/- 10.05
Eval num_timesteps=45000, episode_reward=-48.35 +/- 64.07
Episode length: 976.74 +/- 48.39
Eval num_timesteps=50000, episode_reward=-82.27 +/- 82.24
Episode length: 914.95 +/- 119.92
Eval num_timesteps=55000, episode_reward=-17.83 +/- 74.57
Episode length: 970.90 +/- 58.83
New best mean reward!
Eval num_timesteps=60000, episode_reward=-41.99 +/- 40.54
Episode length: 989.23 +/- 53.16
Eval num_timesteps=65000, episode_reward=-96.08 +/- 61.17
Episode length: 805.52 +/- 261.24
Eval num_timesteps=70000, episode_reward=-33.65 +/- 82.03
Episode length: 753.21 +/- 295.01
Eval num_timesteps=75000, episode_reward=-132.39 +/- 57.69
Episode length: 590.26 +/- 284.92
Traceback (most recent call last):
  File "/home/haani/snap/snapd-desktop-integration/83/Documents/thesis/sb.py", line 167, in <module>
    main(args)
  File "/home/haani/snap/snapd-desktop-integration/83/Documents/thesis/sb.py", line 158, in main
    drl(args, i)
  File "/home/haani/snap/snapd-desktop-integration/83/Documents/thesis/sb.py", line 138, in drl
    model.learn(total_timesteps=args.steps, eval_freq=1, n_eval_episodes=1, log_interval=1, callback=eval_callback)
  File "/home/haani/snap/snapd-desktop-integration/83/Documents/thesis/stable_baselines3_thesis/a2c/a2c.py", line 217, in learn
    return super().learn(
  File "/home/haani/snap/snapd-desktop-integration/83/Documents/thesis/stable_baselines3_thesis/common/on_policy_algorithm.py", line 251, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/haani/snap/snapd-desktop-integration/83/Documents/thesis/stable_baselines3_thesis/common/on_policy_algorithm.py", line 185, in collect_rollouts
    if callback.on_step() is False:
  File "/home/haani/snap/snapd-desktop-integration/83/Documents/thesis/stable_baselines3_thesis/common/callbacks.py", line 88, in on_step
    return self._on_step()
  File "/home/haani/snap/snapd-desktop-integration/83/Documents/thesis/stable_baselines3_thesis/common/callbacks.py", line 435, in _on_step
    episode_rewards, episode_lengths = evaluate_policy(
  File "/home/haani/snap/snapd-desktop-integration/83/Documents/thesis/stable_baselines3_thesis/common/evaluation.py", line 86, in evaluate_policy
    actions, states = model.predict(observations, state=states, episode_start=episode_starts, deterministic=deterministic)
  File "/home/haani/snap/snapd-desktop-integration/83/Documents/thesis/stable_baselines3_thesis/common/base_class.py", line 589, in predict
    return self.policy.predict(observation, state, episode_start, deterministic)
  File "/home/haani/snap/snapd-desktop-integration/83/Documents/thesis/stable_baselines3_thesis/common/policies.py", line 341, in predict
    actions = self._predict(observation, deterministic=deterministic)
  File "/home/haani/snap/snapd-desktop-integration/83/Documents/thesis/stable_baselines3_thesis/common/policies.py", line 647, in _predict
    return self.get_distribution(observation).get_actions(deterministic=deterministic)
  File "/home/haani/snap/snapd-desktop-integration/83/Documents/thesis/stable_baselines3_thesis/common/policies.py", line 684, in get_distribution
    latent_pi = self.mlp_extractor.forward_actor(features)
  File "/home/haani/snap/snapd-desktop-integration/83/Documents/thesis/stable_baselines3_thesis/common/torch_layers.py", line 259, in forward_actor
    return self.policy_net(self.shared_net(features))
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    retur